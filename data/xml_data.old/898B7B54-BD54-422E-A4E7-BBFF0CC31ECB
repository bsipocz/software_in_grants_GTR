<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/33FB9370-F755-487B-89C4-8C2DCA4FA3B0"><gtr:id>33FB9370-F755-487B-89C4-8C2DCA4FA3B0</gtr:id><gtr:name>Mitsubishi Corporation (UK) Plc</gtr:name><gtr:address><gtr:line1>Bow Bells House 11 Bread Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC4M 9BE</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5F471C3D-9BEC-4461-B3F0-E7693B350BFD"><gtr:id>5F471C3D-9BEC-4461-B3F0-E7693B350BFD</gtr:id><gtr:firstName>Terry</gtr:firstName><gtr:surname>Windeatt</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE061664%2F1"><gtr:id>898B7B54-BD54-422E-A4E7-BBFF0CC31ECB</gtr:id><gtr:title>Ensemble Classifier Design applied to face expression classification</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E061664/1</gtr:grantReference><gtr:abstractText>Pattern classification involves assignment of an object to one of several pre-specified categories or classes, and is a key component in many data interpretation activities. The proposed approach focuses on classifiers that learn from examples, and it is assumed that each example pattern is represented by a set of numbers, which are known as the pattern features. In the case of face expression classification (for example distinguish between a smiling and frowning face), these features could consist of numbers representing different aspects of facial features. In order to design a system it is customary to divide the example patterns into two sets, a training set to design the classifier and a test set which is subsequently used to predict the performance when previously unseen examples are applied. A problem arises when there are many features and relatively few examples, and the classifier can learn the training set too well, known as over-fitting so that performance on the test set decreases. The field of ensemble classifiers has been developed to address the problem of achieving the best pattern classification performance using a combination of relatively simple classifiers. It has been found that the combination has the advantage that it is less likely to over-fit. However, there is still the difficulty of tuning the individual classifiers, a process that is normally performed using classifier parameters (for example complexity of a neural network classifier). The common approach is to further divide the training set to produce a validation set that can be used to adjust appropriate parameters. However, when the number of examples is in short supply theses techniques are either inappropriate or very time-consuming.In recent work, the Principal Investigator has developed an ensemble class separability measure that is computed on the training set and that can detect over-fitting. Therefore there is no need for a validation set, thereby making more data available for training. The project proposal is to test the method on real data and confirm the results that have been obtained previously on benchmark data.The technique was proposed for two-class problems, and the proposal is to develop the method for multi-class problems using Error-Correcting-Output-Coding (ECOC). ECOC is an ensemble technique that works by decomposing a multiclass problem into two-class sub-problems. In this proposal the aim is to understand why the technique works well, and to propose a design methodology with the aim of applying it to problems in face expression classification. A further objective is to apply the method to predicting the optimal number of features in feature selection using only the training set. There has been for many years a great deal of effort in discovering the most relevant features, since the result has been shown to be more accurate and efficient classifers. Some early research indicates that using class separability measure, this is a feasible approach. The problem is particulary challenging when there are hudreds or thousands of features,as there are in certain biometric, bio-informatics and data mining applications.It is known that even a small improvement in performance of a pattern classification system can affect commercial viability, and the successful outcome of the project should impact other biometric, bio-informatics and data mining applications. The proposed research is relevant to the EPSRC mission since it is aimed at advancing knowledge and technology with practical application relevance. It is anticipated that the likely result of this research will be the enhancement of UK competitiveness through exploitation of the technology.With the help of project partner Mitsubishi Electric, the developed techniques will be applied to stress analysis for physical security systems and driver fatigue for automotive applications.</gtr:abstractText><gtr:fund><gtr:end>2011-04-20</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-04-21</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>281287</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>160000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>OPTDIVA - Optimisation of accuracy/diversity trade-off using ECOC Ensembles</gtr:description><gtr:end>2013-04-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:id>B94CC4D0-EA0F-4830-93FB-E26EEC82F425</gtr:id><gtr:outcomeId>5ec730b45ec730c8</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>how to recognise facial action units and therefore emotion recognition and other applications</gtr:description><gtr:exploitationPathways>any application that requires understanding of facial expression</gtr:exploitationPathways><gtr:id>113DC9C1-E6C8-4287-9419-01F7622C1D91</gtr:id><gtr:outcomeId>54479e7d4cc073.02967819</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Retail,Security and Diplomacy,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>757BC4E4-2125-4D72-BBBF-D5D5BC9FC99B</gtr:id><gtr:title>Facial action unit recognition using multi-class classification</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd3f7bdd77d0a480311e5acfbb11500b"><gtr:id>fd3f7bdd77d0a480311e5acfbb11500b</gtr:id><gtr:otherNames>Smith R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5475e77e28ac46.05776896</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AE4712CC-B49A-4C8D-8A1A-703F62180BF2</gtr:id><gtr:title>Embedded feature ranking for ensemble MLP classifiers.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on neural networks</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e7113996df34302c6e3420416432fa99"><gtr:id>e7113996df34302c6e3420416432fa99</gtr:id><gtr:otherNames>Windeatt T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1045-9227</gtr:issn><gtr:outcomeId>doi_53d05e05ed92c468</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F5657FA9-EE73-49CD-B4EF-9AE05171FB35</gtr:id><gtr:title>Minimising added classification error using Walsh coefficients.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on neural networks</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e7113996df34302c6e3420416432fa99"><gtr:id>e7113996df34302c6e3420416432fa99</gtr:id><gtr:otherNames>Windeatt T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1045-9227</gtr:issn><gtr:outcomeId>doi_53d05e05edb10f7e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C796690E-88C6-4881-88B8-656A4BBBAF87</gtr:id><gtr:title>Ensemble pruning using spectral coefficients.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on neural networks and learning systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e7113996df34302c6e3420416432fa99"><gtr:id>e7113996df34302c6e3420416432fa99</gtr:id><gtr:otherNames>Windeatt T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>2162-237X</gtr:issn><gtr:outcomeId>doi_53d05e05ee03e8f4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A2412E81-6969-4D0C-8F45-832161C91E26</gtr:id><gtr:title>Pruning of Error Correcting Output Codes by optimization of accuracy-diversity trade off</gtr:title><gtr:parentPublicationTitle>Machine Learning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/526021ae57caa45102aeef2783ca8346"><gtr:id>526021ae57caa45102aeef2783ca8346</gtr:id><gtr:otherNames>?z?g?r-Aky?z S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d0805aaab7b5.04069494</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E061664/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>