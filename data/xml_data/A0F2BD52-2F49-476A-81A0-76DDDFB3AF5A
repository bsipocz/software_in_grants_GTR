<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2"><gtr:id>E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2</gtr:id><gtr:firstName>Barry-John</gtr:firstName><gtr:surname>Theobald</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD049075%2F1"><gtr:id>A0F2BD52-2F49-476A-81A0-76DDDFB3AF5A</gtr:id><gtr:title>Creating Expressive Three-Dimensional Talking Faces</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D049075/1</gtr:grantReference><gtr:abstractText>The broad aim of this work is to develop a life-like expressive talking head. This is difficult to achieve as we are extremely sensitive to subtle changes in the features of the face. Flaws in animated sequences are easy to detect and severely degrade the perceived quality of the output. This is especially true for systems that strive for videorealism (indistinguishable from real video).All previous approaches that achieve close to videorealism areimage-based and two-dimensional; the pose of the character is always face-on, emotion is usually ignored, and the vocabulary is often limited. This work will overcome, for the first time, all of these limitations.To generate realistic animated sequences, a user need only supply the text (or voice) of the sentence they wish to animate. Contrast this with animation studios, such as Pixar, that require months (or years) of manual tuning of animation parameters to create realistic animated sequences. Of course, these sequences are limited to the script of the movie - to generate further sequences would require further manual specification of the parameters. This system will generate expressive visual speech for any arbitrary utterance from the limited training data available, without the need for user intervention. Also, a user can specify a desired expression, e.g. a happy expression for good news, and the output will automatically be adapted to that expression.</gtr:abstractText><gtr:fund><gtr:end>2008-10-04</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-06-05</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>114824</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B501AC39-2DB2-4ED2-B1E0-8E5C8847CF90</gtr:id><gtr:title>Something in the way we move: Motion dynamics, not perceived sex, influence head movements in conversation.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/98bdb140cdbf50accd979df6eedd6e1f"><gtr:id>98bdb140cdbf50accd979df6eedd6e1f</gtr:id><gtr:otherNames>Boker SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>doi_55f95a95a3414a5b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A28D9538-E925-41C7-A27C-5424EA2A1293</gtr:id><gtr:title>Mapping and manipulating facial expression.</gtr:title><gtr:parentPublicationTitle>Language and speech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5e02fb343906aa30f450f5f908a829e0"><gtr:id>5e02fb343906aa30f450f5f908a829e0</gtr:id><gtr:otherNames>Theobald BJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0023-8309</gtr:issn><gtr:outcomeId>doi_53d079079709b8e2</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D049075/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>