<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/223FB599-8054-417B-87C2-B5A02FFB43B7"><gtr:id>223FB599-8054-417B-87C2-B5A02FFB43B7</gtr:id><gtr:name>Motion Robotics Limited</gtr:name><gtr:address><gtr:line1>89 HIGH STREET,WEST END</gtr:line1><gtr:city>SOUTHAMPTON</gtr:city><gtr:postCode>SO30 3DS</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:typeInd>P</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/223FB599-8054-417B-87C2-B5A02FFB43B7" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="gtr:organisationParticipantRole"><gtr:id>223FB599-8054-417B-87C2-B5A02FFB43B7</gtr:id><gtr:name>Motion Robotics Limited</gtr:name><gtr:address><gtr:line1>89 HIGH STREET,WEST END</gtr:line1><gtr:city>SOUTHAMPTON</gtr:city><gtr:postCode>SO30 3DS</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PARTICIPANT</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_PARTICIPANT</gtr:name></gtr:role></gtr:roles><gtr:offerGrant>286964.0</gtr:offerGrant><gtr:projectCost>409948.0</gtr:projectCost></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/89343D6F-4B46-48FD-9A2E-E10D81947619" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="gtr:organisationParticipantRole"><gtr:id>89343D6F-4B46-48FD-9A2E-E10D81947619</gtr:id><gtr:name>Loughborough College Loughborough</gtr:name><gtr:address><gtr:line1>,</gtr:line1><gtr:region>East Midlands</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PARTICIPANT</gtr:name></gtr:role></gtr:roles><gtr:offerGrant>380228.0</gtr:offerGrant><gtr:projectCost>380228.0</gtr:projectCost></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/D4AA4B7F-5502-41D8-BB74-3ECF912B3076"><gtr:id>D4AA4B7F-5502-41D8-BB74-3ECF912B3076</gtr:id><gtr:firstName>Dennis</gtr:firstName><gtr:surname>Majoe</gtr:surname><gtr:roles><gtr:role><gtr:name>PROJECT_MANAGER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=102871"><gtr:id>35692D7C-B34B-4AC5-888F-9020BDD23D8A</gtr:id><gtr:title>Newton Fund - YOBAN- a companion robot to assist walking, sitting down and standing up for older people</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Collaborative R&amp;D</gtr:grantCategory><gtr:grantReference>102871</gtr:grantReference><gtr:abstractText>Older people in general have difficulty walking, getting in and out of chairs, toileting or standing. Traditional walking and sit-to-stand aids (rolators or couch canes) are too big or heavy to use indoors, in a toilet or cramped corridor. Risk of falling is high. Older people also suffer from a lack of companionship and stimulation. Inactivity, falls and depression leads towards poor physical and mental health and low independence. YOBAN is the first robot of its kind to combine several important features that stimulate the user's mind, provide companionship and physically assist locomotion. YOBAN will help support the user to walk both indoors and outdoors since the variable wheelbase moves in cramped spaces yet provides strong support to help getting in and out of chairs. Integrating Cloud Services into the robot functionality, YOBAN can interact with the user obeying his voice so they can play action games together, can ask it to play old era music, stories or recite
poems (mental stimulation and companionship). The user can place friendly or emergency voice calls and at all times YOBAN will monitor negative trends in the user's activity alerting caregivers if needed (24/7 safety).</gtr:abstractText><gtr:fund><gtr:end>2019-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/E18E2F0F-AC7D-4E02-9559-669F7C8FEC74"><gtr:id>E18E2F0F-AC7D-4E02-9559-669F7C8FEC74</gtr:id><gtr:name>Innovate UK</gtr:name></gtr:funder><gtr:start>2017-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>667192</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">102871</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>