<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:department>Engineering</gtr:department><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C514A3A9-E140-4888-AFB2-5814D127FD79"><gtr:id>C514A3A9-E140-4888-AFB2-5814D127FD79</gtr:id><gtr:name>MRC Centre Cambridge</gtr:name><gtr:address><gtr:line1>Hills Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 0QH</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/465A95CE-B9CC-4267-AA3E-CB35040AFDDD"><gtr:id>465A95CE-B9CC-4267-AA3E-CB35040AFDDD</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:otherNames>Eric</gtr:otherNames><gtr:surname>Turner</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL000776%2F1"><gtr:id>C0A363B8-4B54-4A92-AAB4-1974BD58C1B1</gtr:id><gtr:title>Unifying audio signal processing and machine learning: a fundamental framework for machine hearing</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/L000776/1</gtr:grantReference><gtr:abstractText>Modern technology is leading to a flood of audio data. For example, over seventy two hours of unstructured and unlabelled sound-tracks are uploaded to internet sites every minute. Automatic systems are urgently needed for recognising audio content so that these sound-tracks can be tagged for categorisation and search. Moreover, an increasing proportion of recordings are made on hand-held devices in challenging environments that contain multiple sound sources and noise. Such uncurated and noisy data necessitate automatic systems for cleaning the audio content and separating sources from mixtures. On a related note, devices for the hearing impaired currently perform poorly in noise. In fact, this is a major reason why six million people in the UK who would benefit from a hearing aid, do not use them (a market worth &amp;pound;18 billion p.a.). Patients fitted with cochlear implants suffer from similar limitations, and as the population ages more people are affected. 

It is clear that audio recognition and enhancement methods are required to stop us drowning in audio-data, for processing in hearing devices, and to
support new technological innovations. Current approaches to these problems use a combination of audio signal processing (which places the audio data into a convenient format and reduces the data-rate) and machine learning (which removes noise, separates sources, or classifies the content). It is widely believed that these two fields must become increasingly integrated in the future. However, this union is currently a troubled one, suffering from four problems. 

Inefficiency: The methods are too inefficient when we have vast amounts of data (as is the case for audio-tracks on the web) or for real-time applications (such as is necessary in hearing aids)
Impoverished models: The machine learning modules tend to be statistically limited.
Unadapted: The signal processing modules are unadapted despite evidence from other fields, like computer vision, which suggests thatautomatic tuning leads to significant performance gains 
Distorted mixtures: The signal processing modules introduce non-linear distortions which are not captured by the machine learning modules.

In this project we address these four limitations by introducing a new theoretical framework which unifies signal processing and machine learning. The key step is to view the signal processing module as solving an inference problem. Since the machine-learning modules are often framed in this way, the two modules can be integrated into a single coherent approach allowing technologies from the two fields to be completely integrated. In the project we will then use the new approach to develop efficient, rich, adaptive, and distortion free approaches to audio denoising, source separation and recognition. We will evaluate the the noise reduction and source separations algorithms on the hearing impaired, and the audio recognition algorithms on audio-sound track data.

We believe this new framework will form a foundation of the emerging field of machine hearing. In the future, machine hearing will be deployed in a vast range of applications from music processing tasks to augmented reality systems (in conjunction with technologies from computer vision). We believe that this project will kick start this proliferation.</gtr:abstractText><gtr:potentialImpactText>The signal processing and machine learning methods developed in this project are keystone technologies upon which upstream research depends. In particular, the project will have a significant impact for the health-care and digital industries. The following specific groups will benefit from the research:

The hearing impaired: hearing aid users

Six million people in the UK who would benefit from a hearing aid do not use them (a market worth 18 billion p.a.). This group of people is expanding rapidly as the population ages (the number of people aged 65 or older is expected to double by 2050). One of the main reasons why hearing aids are not as widely used as they should be is that they perform poorly in noisy environments. The efficient and adaptive noise removal systems developed for hearing aids in this project will address this key issue. This proposal will therefore contribute to the EPSRC Healthcare Technologies theme. Prof. Moore will be involved in translating the research into hearing aids, including providing access to hearing impaired patients for testing.

The hearing impaired: cochlear implantees

Cochlear implants allow the profoundly deaf, who get little or no benefit from a normal hearing aid, to gain awareness of environmental sounds and, in most cases, understand speech without lip-reading. 8000 people in the UK currently use cochlear implants and there are 1000 new implantees each year. Again, perhaps the major limitation of the current devices is their poor performance in noisy environments. The efficient and adaptive noise removal systems developed in this project aim to address this key issue thereby contributing to the EPSRC Healthcare Technologies theme. Dr. Carlyon will be involved in translating the research into cochlear implants, including providing access to implanted patients for testing.

Audio search and information retrieval: digital-industries and society

Many companies preside over large, uncurated collections of audio data and they would benefit from methods for searching and categorizing the data. For example, over seventy two hours of unstructured and unlabelled sound-tracks are uploaded to internet sites every minute and this number continues to grow. Automatic systems are urgently needed for recognising audio content so that these sound-tracks can be tagged (possibly at precise times throughout the clips) for categorisation and search. Often the audio-tracks are recorded with video and so the audio tags can also be used to search the video. The audio recognition technology developed in this project therefore has wide commercial application to information retrieval. Similarly, the BBC's public space project is attempting to organise and unlock the archives of the BBC, making them accessible to the public. The audio-recognition technologies will make significant contributions to this project, thereby improving public services and enhancing life.

Audio denoising and source separation: digital-industries and society 

Poor quality audio data is becoming common place. For example, 3hrs of recordings made on hand-held devices are uploaded to YouTube every minute. These recording are often made in challenging environments that contain multiple sound sources and noise. Such noisy data necessitate automatic systems for cleaning the audio content and separating sources from mixtures. This project will provide tools for this purpose. Moreover, upstream technologies such as Automatic Speech Recognition (ASR) and Audio Diarisation (AD) systems perform poorly in noisy environments. As such the noise removal methods developed in the project can be coupled with these approaches to improve performance. Since modern approaches to ASR and AD are probabilistic, this raises the possibility of integrated approaches that jointly estimate the noise at the same time as interpreting the content. The advisory group, and in particular Prof. Gales, will advise on possible technology transfer</gtr:potentialImpactText><gtr:fund><gtr:end>2015-11-19</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-11-20</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>97101</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Cambridge</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Department of Psychology</gtr:department><gtr:description>Collaboration with Prof. Brian Moore</gtr:description><gtr:id>38DDE00B-C425-4300-8C69-68A36B03B77E</gtr:id><gtr:impact>Awarded EPSRC research grant (grant number EP/G050821/1)
Multi-disciplinary: machine learning, signal processing, time-series modelling, hearing, hearing aids, deafness research</gtr:impact><gtr:outcomeId>56df439e947998.04878139-1</gtr:outcomeId><gtr:partnerContribution>Together we devised the new research programme (based around intelligent hearing devices and intelligent hearing tests). Prof. Brian Moore brought expertise in hearing and hearing devices and industrial collaborators (Phonak). I provided expertise in audio time series modelling and machine learning.</gtr:partnerContribution><gtr:piContribution>Based on the research in grant EP/L000776/1, Prof. Moore approached us to apply for an EPSRC research grant which we were subsequently awarded (grant number EP/G050821/1).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Appeared on BBC Radio 5 Live</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>4289BEC4-9253-44EE-8E61-5EAD70D7D001</gtr:id><gtr:impact>Appeared on BBC 5Live programme the Naked Scientist talking about hearing and my research. The show was conducted in front of a live audience of school children.</gtr:impact><gtr:outcomeId>56df459dc6e626.57313196</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.thenakedscientists.com/HTML/interviews/interview/1001043/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>40000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Google European Doctoral Programme</gtr:description><gtr:end>2017-09-02</gtr:end><gtr:fundingOrg>Google</gtr:fundingOrg><gtr:id>E88F07B8-B40E-4AD9-BFAB-C1D674AD0ECC</gtr:id><gtr:outcomeId>56df444a64ef97.25346896</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-09-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The grant lead to technology being developed that formed the basis for a successful EPSRC Research Grant application.

1. efficient methods for removing noise from audio and recognising content for tagging video sound tracks (in collaboration with Prof. Brian Moore)

2. improving hearing devices for the hearing impaired (together with Dr. Robert Carlyon, Prof. Brian Moore and Dr. David Baguley)

 In addition it also led to a strong collaboration with Google which has resulted in follow up funding (worth roughly &amp;pound;120k).</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>9BF994ED-527A-43BC-97A2-ED9CF4692826</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545f57879ceea1.92915532</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This research grant has only been running for 3 months. However, in that short time we have made the following contributions:
1. we have shown a theoretical connection between probabilistic machine learning approaches and classical signal processing processing methods which bring these fields closer together and makes it simpler and more efficient to combine methods from both. This has led to better methods for removing noise from audio and filling in missing data. This contribution has been accepted for publication in IEEE Transactions in Signal Processing.

2. we have scaled up a fundamental machine learning tool -- called a Gaussian process -- so that it can handle large scale datasets containing millions of datapoints. Previously these methods were limited to handling ten thousand datapoints. This contribution has been accepted for publication in the Neural Information Processing Systems Conference.</gtr:description><gtr:exploitationPathways>We are pursuing the following opportunities:
-- applying the technology to improving audio uploaded to the internet and automatically recognising content (together with Google) 
-- applying methods to improving hearing devices (including cochlear implants and hearing aids)</gtr:exploitationPathways><gtr:id>C9305249-0F47-459C-A1C2-6B9F7FCD1539</gtr:id><gtr:outcomeId>545f56e5c29101.26315612</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://cbl.eng.cam.ac.uk/Public/Turner/ResearchAreas</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>87188FF0-3A7B-4088-90B3-751C700D1E82</gtr:id><gtr:title>Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3eb8315ffaa4c0315a973e3a8f5f1fb0"><gtr:id>3eb8315ffaa4c0315a973e3a8f5f1fb0</gtr:id><gtr:otherNames>Gal Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df3f003c8223.83618961</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9F61696A-AD0E-4C01-BB71-8455DFECF457</gtr:id><gtr:title>Unsupervised State-Space Modeling Using Reproducing Kernels</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e4ea27686c0e5b5c199b301ad6af3596"><gtr:id>e4ea27686c0e5b5c199b301ad6af3596</gtr:id><gtr:otherNames>Tobar F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f8efdf04d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6340BBDC-AFE7-4499-B82C-8018A16B2592</gtr:id><gtr:title>A role for amplitude modulation phase relationships in speech rhythm perception.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41cb315199dddfb75ed5f18accb58eb4"><gtr:id>41cb315199dddfb75ed5f18accb58eb4</gtr:id><gtr:otherNames>Leong V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>54539e7e5fcc01.23718719</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6173ABA0-6F97-4E02-A118-0AF7B7AAA76D</gtr:id><gtr:title>Efficient Occlusive Components Analysis</gtr:title><gtr:parentPublicationTitle>Journal of Machine Learning Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4d0c319ab20b7ff386dee435cb73f5b3"><gtr:id>4d0c319ab20b7ff386dee435cb73f5b3</gtr:id><gtr:otherNames>M. Henniges</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54539c7bdd6323.21251692</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6F89509A-99AE-4E88-BB83-F743E6B2A1A0</gtr:id><gtr:title>Design of Positive-Definite Quaternion Kernels</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e4ea27686c0e5b5c199b301ad6af3596"><gtr:id>e4ea27686c0e5b5c199b301ad6af3596</gtr:id><gtr:otherNames>Tobar F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f8edd242b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CDE681F1-EEDC-4F03-AEEE-24C4D67C8CBD</gtr:id><gtr:title>Tree-structured Gaussian Process Approximations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e220d158630481ad2da643c3d44b28f4"><gtr:id>e220d158630481ad2da643c3d44b28f4</gtr:id><gtr:otherNames>Bui T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545f58df55f059.83650501</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>35521271-0C5F-49C2-ADEE-CE0AAEBCB521</gtr:id><gtr:title>On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ddb896365ffe1cf74b74086e8193cf62"><gtr:id>ddb896365ffe1cf74b74086e8193cf62</gtr:id><gtr:otherNames>Matthews A G de G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56df407b81c8e8.15899819</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8B9747AD-AF4E-4F10-98C0-DC0FAF095329</gtr:id><gtr:title>Neural Adaptive Sequential Monte Carlo</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/67d7cd2e85ac22147b965ce0d0147b7f"><gtr:id>67d7cd2e85ac22147b965ce0d0147b7f</gtr:id><gtr:otherNames>Gu S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df3e241825f7.86069298</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8AACE61D-CFDB-4402-8521-7A49C8B16B58</gtr:id><gtr:title>Learning Stationary Time Series using Gaussian Processes with Nonparametric Kernels</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e4ea27686c0e5b5c199b301ad6af3596"><gtr:id>e4ea27686c0e5b5c199b301ad6af3596</gtr:id><gtr:otherNames>Tobar F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df37958a7a58.09248303</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ACFF4DE1-62FD-44AF-A3BB-64A2DB2D5DAF</gtr:id><gtr:title>Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/67d7cd2e85ac22147b965ce0d0147b7f"><gtr:id>67d7cd2e85ac22147b965ce0d0147b7f</gtr:id><gtr:otherNames>Gu S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c8149d82ea88.08880496</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95CDB6AC-1F5C-44D1-94AD-860503210874</gtr:id><gtr:title>The Multivariate Generalised von Mises Distribution: Inference and Applications</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8bc975367333bca90b1f969d8be308c"><gtr:id>a8bc975367333bca90b1f969d8be308c</gtr:id><gtr:otherNames>Navarro A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c8138a81f512.64135221</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46CFBD64-BB23-4A73-A4D0-EF6CEE5BC36D</gtr:id><gtr:title>Time-Frequency Analysis as Probabilistic Inference</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/592bdc57433c2bba1cf9ee5ade23576b"><gtr:id>592bdc57433c2bba1cf9ee5ade23576b</gtr:id><gtr:otherNames>Turner R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545f5984513d36.81762718</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F429D9B4-D928-4FA5-8C11-7CAF04D9487E</gtr:id><gtr:title>Stochastic Expectation Propagation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd75dbe105014309cbdded985198a393"><gtr:id>bd75dbe105014309cbdded985198a393</gtr:id><gtr:otherNames>Li Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df36ce059782.13334742</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A90F59CB-8BA9-4F22-8B4B-CBDD371792BE</gtr:id><gtr:title>A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</gtr:title><gtr:parentPublicationTitle>JOURNAL OF MACHINE LEARNING RESEARCH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/54d5e03c176093864d066aadb50ac6b3"><gtr:id>54d5e03c176093864d066aadb50ac6b3</gtr:id><gtr:otherNames>Bui Thang D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1532-4435</gtr:issn><gtr:outcomeId>5aa6e2fc4c5c66.76059023</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>27BA3D39-52D3-458F-9AC8-A8D2FF628041</gtr:id><gtr:title>Perception of stochastic envelopes by normal-hearing and cochlear-implant listeners.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0c873d7815fe3b8929e3cff270f0ed89"><gtr:id>0c873d7815fe3b8929e3cff270f0ed89</gtr:id><gtr:otherNames>Gomersall PA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>56df40da696172.16164376</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>792DEEB2-5C5B-4EF4-B13E-C7A2E0F45D43</gtr:id><gtr:title>Deep Gaussian processes for regression using approximate expectation propagation</gtr:title><gtr:parentPublicationTitle>33rd International Conference on Machine Learning, ICML 2016</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5e8e12b2d4b767a089179433376d47d2"><gtr:id>5e8e12b2d4b767a089179433376d47d2</gtr:id><gtr:otherNames>Bui T.D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c815f3786168.35152735</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L000776/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>