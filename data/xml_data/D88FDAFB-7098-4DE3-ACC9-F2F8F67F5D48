<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Computing Science</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C04EA393-5137-4DC1-850C-4C2BD8251E91"><gtr:id>C04EA393-5137-4DC1-850C-4C2BD8251E91</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Brewster</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ005312%2F1"><gtr:id>D88FDAFB-7098-4DE3-ACC9-F2F8F67F5D48</gtr:id><gtr:title>Exploration of Ultrasound-based Haptic Interaction on a Multi-touch Surface</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J005312/1</gtr:grantReference><gtr:abstractText>Multi-touch tables, such as Microsoft Surface, are now widely available. Users can walk-up and use these systems in hotel lobbies and other public settings with very little instruction and with no need to wear or hold intrusive sensors with their hands or body. The ability to 'walk-up and use' with unadorned hands and fingers removes the barrier between human and technology, encouraging spontaneous use. 

One of the primary disadvantages of current interactive surfaces is that users can touch objects, but they are unable to feel them. There are a plethora of applications where it is beneficial for the user to have their touches augmented with 'feel-based' haptic feedback. For example, medical applications, virtual training, and modelling applications require precise control from the user-haptic feedback can aid the user in effectively performing these tasks. These applications demonstrate the benefit of augmenting haptic feedback with visual feedback in an interactive application. Often, the visual space has been disconnected from the force-feedback, requiring a prolonged training period for the user to become accustomed to moving a digital object and watching the interactions a small distance away on a monitor. 

In this proposal we will investigate the use of ultrasound transducers to provide 'feelable' feedback through air. The skin on a human hand can feel the ultrasonic pressure wave produced by a carefully calibrated series of transducers, in much the same manner that is apparent from loud sub-woofers on a stereo system. Ultrasound waves are outside the human's range of hearing and so provide silent, through-air haptic feedback. We will use this technology to provide multi-point haptic feedback on the surface of a multi-touch horizontal surface.

The team consists of Dr. Sriram Subramanian, Dr. Mark Marshall and Dr. Jason Alexander from the computer science departments and Prof. Bruce Drinkwater from the Mechanical Engineering department of the University of Bristol and Prof. Stephen Brewster from the Computer Science department of the University of Glasgow. The team is internationally recognised for its research in Human-Computer Interaction (HCI), novel integration of hardware for HCI, and Ultrasonic sensors. Microsoft research (Cambridge) and XMOS will serve as project partners and offer valuable resources and support for the project.</gtr:abstractText><gtr:fund><gtr:end>2015-01-22</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-07-23</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>202736</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:department>Horizon 2020</gtr:department><gtr:description>Levitate</gtr:description><gtr:end>2021-12-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:id>B0885CA7-A857-429A-98B9-3946FB1AF978</gtr:id><gtr:outcomeId>58c934b5ce6557.28004117</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>we have demonstrated the work to many companies. the bristol half of the project have spun off a company.

we are currently working on using this in the automotive sector</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>D2F64A47-6EF7-482B-97E5-F89C427FCB70</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5464c87754a1a1.65129028</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>we have developed new knowledge about how to create and design with ultrasound haptics. we have done pyschophysical studies to understand perception and application oriented work to create interaction technqiues</gtr:description><gtr:exploitationPathways>we have created the foundations for ultrasound based ui's of the future.</gtr:exploitationPathways><gtr:id>05A0508E-5944-4BCC-8BB8-E7111456AFDD</gtr:id><gtr:outcomeId>5464cee3a3db59.90636528</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>42C11144-FC68-49B8-86AE-EBC9D4F55542</gtr:id><gtr:title>Touching the invisible: Localizing ultrasonic haptic cues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75646b3f2136476cd62eb8b53eab97cf"><gtr:id>75646b3f2136476cd62eb8b53eab97cf</gtr:id><gtr:otherNames>Dong-Bach Vo</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df612b64c046.70042925</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J005312/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>