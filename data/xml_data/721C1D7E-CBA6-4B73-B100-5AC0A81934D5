<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/DDCA16B5-1844-4825-AF8F-D18E387891A7"><gtr:id>DDCA16B5-1844-4825-AF8F-D18E387891A7</gtr:id><gtr:name>Delcam International plc</gtr:name><gtr:address><gtr:line1>Talbot Way</gtr:line1><gtr:line2>Small Heath Business Park</gtr:line2><gtr:line4>Birmingham</gtr:line4><gtr:postCode>B10 0HJ</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/740692F5-F1DB-41C9-B2C3-7DCF7366CFA1"><gtr:id>740692F5-F1DB-41C9-B2C3-7DCF7366CFA1</gtr:id><gtr:firstName>Jeremy</gtr:firstName><gtr:otherNames>Leonard</gtr:otherNames><gtr:surname>Wyatt</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/377D269F-3BB1-4B4D-9F8F-73F6AEEBF786"><gtr:id>377D269F-3BB1-4B4D-9F8F-73F6AEEBF786</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Schofield</gtr:surname><gtr:orcidId>0000-0002-0589-4678</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/EC999D1C-7B7E-45E8-AAD2-812AE18CF90D"><gtr:id>EC999D1C-7B7E-45E8-AAD2-812AE18CF90D</gtr:id><gtr:firstName>Michael</gtr:firstName><gtr:surname>Spann</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF026269%2F1"><gtr:id>721C1D7E-CBA6-4B73-B100-5AC0A81934D5</gtr:id><gtr:title>Estimating the intrinsic characteristics of real images to aid analysis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F026269/1</gtr:grantReference><gtr:abstractText>Humans find seeing things effortless and this hides the fact that making sense of the visual world is a very difficult problem. Vision is difficult because each image we see could have been made by an infinite number of object and lighting combinations. Think of the simplest image property - grey level. The grey level of each pixel in an image is determined by the amount of light falling onto each object and the amount of light that is reflected back from each object. Dark objects have lower grey levels than light ones but even light objects have low grey levels when in shadow. We cannot tell whether we are looking at a dark object in bright light or a light object in shadow just by measuring grey levels. Even worse, when grey levels are different in different parts of an image we cannot tell if this difference is due to there being two objects or a change in the amount of light. Despite this problem humans are very good at working out the reasons for grey level changes; we CAN tell objects from shadows.One reason why we are so good at working out what's going on in images maybe that we use other properties such as colour and pattern to tell us what the grey levels mean. This idea has led to the concept of 'intrinsic images'. An intrinsic image is an image that describes one property of the scene. So instead of having a single image that mixes up shadows and object reflectances we might produce two intrinsic images one each for shadows and reflectance. Scientists have already succeeded in producing intrinsic images like these by using colour changes to work out what the grey levels mean. But, there is more than one type of shadow and more than one type of reflection. We want to improve on the existing methods by producing four intrinsic images instead of two. Our first intrinsic image will contain the type of gentle shading that is found on undulating surfaces. Our second intrinsic image will contain the hard shadows that are produced when an object blocks the light. Our third intrinsic image will describe the reflectance of matte objects and our forth image the reflections from shiny objects. To separate out these four images we will need to use additional information beyond colour. We think that surface patterns (e.g. wood grain) will provide the necessary information.Extracting four intrinsic images will be very helpful to those engineers who try to make computers understand what's going on in an image. To take just one example, humans seem to be very good at is estimating the shape of undulations on a surface from the way that it is shaded. We are so good at this that we do it automatically and the people who write computer software can trick us into thinking that their 'buttons' stand out from the screen just by adding a some highlights and shading to the edges. There are many computer programs that try to interpret shape-from-shading. While many of these programs work well they tend to assume that all changes in grey level are due to shading which is in tern due to surface undulations. We know that this assumption is not true in real pictures and these programs tend to do badly when looking at such images. But if we can produce shading only images from real images then these programs may work better.To test our ideas and decide on the best way achieve our desired results we will collect a large number of photographs of objects whose shape we either already know or can work out. We will calibrate these pictures very carefully and then use them to workout what information is conveyed by colour and pattern that can help us to workout the meaning of each grey level change. We will also test humans to see which cues they might be using. We will make our images available on the Internet so that others can try out their ideas too. We intend to work with a software company who will take the best of our ideas and implement them in a computer program that can automatically design embossed jewellery from photographs.</gtr:abstractText><gtr:fund><gtr:end>2011-10-06</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-04-07</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>381159</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Meet the Scientist</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>BE99BCA6-F751-4BD7-A877-482B384AC0C4</gtr:id><gtr:impact>Demonstrations sparked questions.

None</gtr:impact><gtr:outcomeId>5460e67d196fb7.77077520</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We developed and tested a novel algorithm for separating shadows and illumination changes from material changes in natural images. We developed a new model for how humans might perform the same operation. We studies the way that humans use lighting cues in shape-from-shading judgement. we developed a stereoscopic database of objects, faces and natural environments under specified lighting conditions.</gtr:description><gtr:exploitationPathways>intrinsic image separation method could be used for shadow removal by the creative arts and software industries.</gtr:exploitationPathways><gtr:id>05AF4AE6-B879-41EE-8E20-95018C9C193D</gtr:id><gtr:outcomeId>5460e320e80385.50612715</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Database of stereoscopic image pairs for objects, faces and outdoor scenes for a range of specified lighting conditions.</gtr:description><gtr:id>D146E4F0-3069-470C-A356-62765D4E372C</gtr:id><gtr:impact>The database has been accessed by other research groups</gtr:impact><gtr:outcomeId>5460e71f302f28.54820263</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Birmingham Object Lighting Database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.bold.bham.ac.uk</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>FA2BEB15-DB31-4349-A2B2-9D7410D61375</gtr:id><gtr:title>Using texture to recover shading and reflectance information from real images</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0632a8440b655019944e0a4ef472fffa"><gtr:id>0632a8440b655019944e0a4ef472fffa</gtr:id><gtr:otherNames>Sun Peng</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>58be80338cd664.45235273</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D9D34B2A-C0A3-452E-9F6E-48B3D7ADDFD5</gtr:id><gtr:title>Two operational modes in the perception of shape from shading revealed by the effects of edge information in slant settings.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/98d58840d8b22d8064549ac13e30b0d1"><gtr:id>98d58840d8b22d8064549ac13e30b0d1</gtr:id><gtr:otherNames>Sun P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>56b38eaaa91fa3.75255472</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>65D550C8-92BF-4936-A4C3-5474A146944A</gtr:id><gtr:title>Perceptual learning for second-order cues in a shape-from-shading task</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/51722b622132c283bf71afde7df7454d"><gtr:id>51722b622132c283bf71afde7df7454d</gtr:id><gtr:otherNames>D?vencioglu DN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>54660e96cdac35.23998089</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7DD4335F-0F57-47A9-AA92-0F66793EBB4F</gtr:id><gtr:title>Shape Perception in Human and Computer Vision</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fda6db837f365704ed30c87847785f29"><gtr:id>fda6db837f365704ed30c87847785f29</gtr:id><gtr:otherNames>Schofield A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4471-5194-4</gtr:isbn><gtr:outcomeId>5460a1b9cfc574.51727770</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38940261-3541-40EA-9632-4052BD2ED107</gtr:id><gtr:title>Using texture to recover shading and reflectance information from real images</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/98d58840d8b22d8064549ac13e30b0d1"><gtr:id>98d58840d8b22d8064549ac13e30b0d1</gtr:id><gtr:otherNames>Sun P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>5460d9397b1966.22359220</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5741F87F-F174-4A10-97C4-61853B9690CB</gtr:id><gtr:title>The efficacy of local luminance amplitude in disambiguating the origin of luminance signals depends on carrier frequency: further evidence for the active role of second-order vision in layer decomposition.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/98d58840d8b22d8064549ac13e30b0d1"><gtr:id>98d58840d8b22d8064549ac13e30b0d1</gtr:id><gtr:otherNames>Sun P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>5460da54692809.95937336</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3BA64942-FF94-4FD9-AC1B-32ADDC188D2E</gtr:id><gtr:title>Texture variations suppress suprathreshold brightness and colour variations.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afa602c44ca35d197d8e34fb2e904aef"><gtr:id>afa602c44ca35d197d8e34fb2e904aef</gtr:id><gtr:otherNames>Schofield AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>56b3899035f7b1.07946369</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B7A60DCE-690A-474B-8334-E3F0B38D1AE1</gtr:id><gtr:title>Perceptual learning of second order cues for layer decomposition.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/51722b622132c283bf71afde7df7454d"><gtr:id>51722b622132c283bf71afde7df7454d</gtr:id><gtr:otherNames>D?vencioglu DN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00f00f0e37e6e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A08BDE28-89DC-4938-91BB-296144C5E2C2</gtr:id><gtr:title>The effects of lighting direction and elevation on judgements of shape-from-shading.</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/376c90ef3388b809b9d2f8ccbb98262c"><gtr:id>376c90ef3388b809b9d2f8ccbb98262c</gtr:id><gtr:otherNames>Mazzilli G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5460e40c786ba8.46135519</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB9EE83F-0FB1-4F27-B5C0-592BF57153B3</gtr:id><gtr:title>Layer segmentation using hue, texture and luminance amplitude in a steerable filter framework</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6c6ce4499b488f055b587a99a020b5d4"><gtr:id>6c6ce4499b488f055b587a99a020b5d4</gtr:id><gtr:otherNames>Jiang X.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>58be8032a90870.28053248</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3A98D6A8-10C6-47C2-902A-38937BAC0764</gtr:id><gtr:title>What is second-order vision for? Discriminating illumination versus material changes.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afa602c44ca35d197d8e34fb2e904aef"><gtr:id>afa602c44ca35d197d8e34fb2e904aef</gtr:id><gtr:otherNames>Schofield AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770773de976d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6EDDC99B-7172-47B6-9FE2-063A6577E2FA</gtr:id><gtr:title>A cue-free method to probe human lighting biases.</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/376c90ef3388b809b9d2f8ccbb98262c"><gtr:id>376c90ef3388b809b9d2f8ccbb98262c</gtr:id><gtr:otherNames>Mazzilli G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>54609ffed301c1.74879663</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3E1D0B2D-6C4E-4E73-84B1-F8BECEE3BF45</gtr:id><gtr:title>The role of second-order vision in discriminating shading versus material changes</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afa602c44ca35d197d8e34fb2e904aef"><gtr:id>afa602c44ca35d197d8e34fb2e904aef</gtr:id><gtr:otherNames>Schofield AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>5460d6690572c5.12075611</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FECC80C7-F5EF-4641-8FF3-4489D34BDAFC</gtr:id><gtr:title>The Birmingham Object Lighting Database: a stereoscopic database of objects, faces, surfaces and outdoor scenes captured under characterised lighting</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6c919e1d430c25be988a688574b6efa1"><gtr:id>6c919e1d430c25be988a688574b6efa1</gtr:id><gtr:otherNames> Schofield AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_338086799663dcdb30</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E3FC777-4A08-417F-ACED-AA60CCD9F100</gtr:id><gtr:title>Perceptual integration for qualitatively different 3-D cues in the human brain.</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7817670fce50c9857ab819aa8360b4b5"><gtr:id>7817670fce50c9857ab819aa8360b4b5</gtr:id><gtr:otherNames>D?vencioglu D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>doi_55f95b95bf58bbfe</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3514B3DE-235A-49A6-827B-30BC6813301B</gtr:id><gtr:title>Sun and sky: Does human vision assume a mixture of point and diffuse illumination when interpreting shape-from-shading?</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afa602c44ca35d197d8e34fb2e904aef"><gtr:id>afa602c44ca35d197d8e34fb2e904aef</gtr:id><gtr:otherNames>Schofield AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00f00f08a57d0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>941C7EAE-2FAD-4D52-930D-0869F822B2AB</gtr:id><gtr:title>Binocular functional architecture for detection of contrast-modulated gratings.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d14311e8f34322798b7a9de3e84289fb"><gtr:id>d14311e8f34322798b7a9de3e84289fb</gtr:id><gtr:otherNames>Georgeson MA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>585d6fdf7cd1c8.62570581</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9EBDEC0E-4A66-47A3-94B9-047E586D150F</gtr:id><gtr:title>Layer segmentation using hue, texture and luminance amplitude in a steerable filter framework</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/412d760353ea93b46e9eb632ad8b5d0a"><gtr:id>412d760353ea93b46e9eb632ad8b5d0a</gtr:id><gtr:otherNames> Jiang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_316832852263dcd9aa</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>69246102-77BF-4F35-82B9-DC9C86F6ED6B</gtr:id><gtr:title>The Birmingham Object Lighting Database: a stereoscopic database of objects, faces, surfaces and outdoor scenes captured under characterised lighting conditions</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/231203fad965b0c7ee1651136b41698c"><gtr:id>231203fad965b0c7ee1651136b41698c</gtr:id><gtr:otherNames>Schofield A. J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>58be80328506d7.64660322</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F026269/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>