<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Experimental Psychology</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F11E21C9-E05F-4173-939C-61F868FE1FBA"><gtr:id>F11E21C9-E05F-4173-939C-61F868FE1FBA</gtr:id><gtr:name>The Wellcome Trust Ltd</gtr:name><gtr:address><gtr:line1>215 Euston Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>NW1 2BE</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/38006814-9130-49B7-A5D5-274211E5856A"><gtr:id>38006814-9130-49B7-A5D5-274211E5856A</gtr:id><gtr:name>Liverpool City Council</gtr:name><gtr:address><gtr:line1>Municipal Buildings</gtr:line1><gtr:line2>Dale Street</gtr:line2><gtr:postCode>L69 2DH</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/EC753F8B-C223-4BB8-8BA8-2FC457912BE2"><gtr:id>EC753F8B-C223-4BB8-8BA8-2FC457912BE2</gtr:id><gtr:firstName>Alison</gtr:firstName><gtr:surname>Holmes</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0846378D-427A-4051-95F1-9A87CA64F63E"><gtr:id>0846378D-427A-4051-95F1-9A87CA64F63E</gtr:id><gtr:firstName>Tom</gtr:firstName><gtr:surname>Troscianko</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3F5FF400-3C1E-434C-A2E6-73F6536F2ECA"><gtr:id>3F5FF400-3C1E-434C-A2E6-73F6536F2ECA</gtr:id><gtr:firstName>Iain</gtr:firstName><gtr:otherNames>Donald</gtr:otherNames><gtr:surname>Gilchrist</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE010067%2F1"><gtr:id>7702F304-0B65-453B-A18D-B7BA8F079BAB</gtr:id><gtr:title>Cognitive Systems Foresight: Human Attention and Machine Learning</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E010067/1</gtr:grantReference><gtr:abstractText>Human observers move their eyes in order to direct their attention to important aspects of a visual scene. There are models called salience maps; they predict where the eyes will move to when looking at a scene. At present, these models do not deal with video input, nor do they predict how an observer's task will affect where they look. In other words, there are no models for real-life viewing situations, where an observer has a specific task.We are proposing a new approach to this problem. We have access to video information from cameras used in urban surveillance, and to the operators whose job it is to spot abnormal behaviour in such video inputs. We shall obtain (previously unseen) video recordings of events in UK urban streets, and display them in a simulated control room to operators familiar with the town in question. We shall monitor where they look on the bank of video screens, and also when they decide that an event is abnormal and/or requires some form of intervention, e.g. calling the police. We shall use the record of eye fixations to teach a computer system to distinguish between normal and abnormal events. In this way, we shall be able to learn what is important for humans to do such surveillance by observing their eye fixation behaviour, for a realistic (and difficult) task and set of real-life video sequences. The project is important for four reasons. First, this will be the first attempt to develop a model of human attention/eye movements which will be firmly based on realistic video input and a real task. Second, this will be the first time that a computer system is able to learn from human behaviour in this way. Third, we will learn much about the ability of trained observers to cope with a demanding task as the number of TV monitors increases. Finally, we will develop an automated system which will be able to analyse the input from any urban CCTV camera in order to alert operators to look at that video stream - at present, most CCTV video streams are not observed by anyone since there are too many cameras for the number of human observers. Therefore, an automated alerting system is greatly neeeded and this project constitutes the best attempt to date to produce one.</gtr:abstractText><gtr:fund><gtr:end>2010-09-27</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-12-28</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>382906</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>CC45CC52-6CEB-4624-85B0-EB01184F3483</gtr:id><gtr:title>Eye-response lags during a continuous monitoring task.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7ba05a5f152fd01a291efd822cf39df1"><gtr:id>7ba05a5f152fd01a291efd822cf39df1</gtr:id><gtr:otherNames>Howard CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>doi_53d0880882930641</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E010067/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>