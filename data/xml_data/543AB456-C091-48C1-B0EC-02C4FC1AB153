<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:department>Computing Department</gtr:department><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7D1FF4C7-DD25-4073-920E-B040B150B711"><gtr:id>7D1FF4C7-DD25-4073-920E-B040B150B711</gtr:id><gtr:name>Google Inc</gtr:name><gtr:address><gtr:line1>1600 Amphitheatre Parkway</gtr:line1><gtr:line2>Building 42</gtr:line2><gtr:postCode>94043</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/7C06497B-9C2E-47DC-9DE6-70FBEFF72614"><gtr:id>7C06497B-9C2E-47DC-9DE6-70FBEFF72614</gtr:id><gtr:firstName>Thor</gtr:firstName><gtr:surname>Magnusson</gtr:surname><gtr:orcidId>0000-0002-3731-0630</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D919DC20-F15B-4F10-A061-F8683136AA59"><gtr:id>D919DC20-F15B-4F10-A061-F8683136AA59</gtr:id><gtr:firstName>Michael</gtr:firstName><gtr:otherNames>Stuart</gtr:otherNames><gtr:surname>Grierson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D8EDD504-6976-41A4-9049-F733296A9D60"><gtr:id>D8EDD504-6976-41A4-9049-F733296A9D60</gtr:id><gtr:firstName>Matthew</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Yee-King</gtr:surname><gtr:orcidId>0000-0001-6606-2448</gtr:orcidId><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3DC4E4AF-A67F-477E-BFBA-9B58BC007B13"><gtr:id>3DC4E4AF-A67F-477E-BFBA-9B58BC007B13</gtr:id><gtr:firstName>Chris</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>Kiefer</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/996C98DB-DD6A-42C0-B559-A68D3A7D00ED"><gtr:id>996C98DB-DD6A-42C0-B559-A68D3A7D00ED</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:surname>Collins</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/C58BC45F-F8E2-4B71-BF2E-00DEF357D9C2"><gtr:id>C58BC45F-F8E2-4B71-BF2E-00DEF357D9C2</gtr:id><gtr:firstName>Rebecca</gtr:firstName><gtr:surname>Fiebrink</gtr:surname><gtr:orcidId>0000-0002-7609-2234</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=AH%2FR002657%2F1"><gtr:id>543AB456-C091-48C1-B0EC-02C4FC1AB153</gtr:id><gtr:title>MIMIC: Musically Intelligent Machines Interacting Creatively</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/R002657/1</gtr:grantReference><gtr:abstractText>This project is a direct response to significant changes taking place in the domain of computing and the arts. Recent developments in Artificial Intelligence and Machine Learning are leading to a revolution in how music and art is being created by researchers (Broad and Grierson, 2016). However, this technology has not yet been integrated into software aimed at creatives. Due to the complexities of machine learning, and the lack of usable tools, such approaches are only usable by experts. In order to address this, we will create new, user-friendly technologies that enable the lay user - composers as well as amateur musicians - to understand and apply these new computational techniques in their own creative work.

The potential for machine learning to support creative activity is increasing at a significant rate, both in terms of creative understanding and potential applications. Emerging work in the field of music and sound generation extends from musical robots to generative apps, and from advanced machine listening to devices that can compose in any given style. By leveraging the internet as a live software ecosystem, the proposed project examines how such technology can best reach artists, and live up to its potential to fundamentally change creative practice in the field. Rather than focussing on the computer as an original creator, we will create platforms where the newest techniques can be used by artists as part of their day-to-day creative practices.
 
Current research in artificial intelligence, and in particular machine learning, have led to an incredible leap forward in the performance of AI systems in areas such as speech and image recognition (Cortana, Siri etc.). Google and others have demonstrated how these approaches can be used for creative purposes, including the generation of speech and music (DeepMinds's WaveNet and Google's Magenta), images (Deep Dream) and game intelligence (DeepMind's AlphaGo). The investigators in this project have been using Deep Learning, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and other approaches to develop intelligent systems that can be used by artists to create sound and music. We are already among the first in the world to create reusable software that can 'listen' to large amounts of sound recordings, and use these as examples to create entirely new recordings at the level of audio. Our systems produce outcomes that out-perform many other previously funded research outputs in these areas.

In this three-year project, we will develop and disseminate creative systems that can be used by musicians and artists in the creation of entirely new music and sound. We will show how such approaches can affect the future of other forms of media, such as film and the visual arts. We will do so by developing a creative platform, using the most accessible public forum available: the World Wide Web. We will achieve this through development of a high level live coding language for novice users, with simplified metaphors for the understanding of complex techniques including deep learning. We will also release the machine learning libraries we create for more advanced users who want to use machine learning technology as part of their creative tools. 

The project will involve end-users throughout, incorporating graduate students, professional artists, and participants in online learning environments. We will disseminate our work early, gaining the essential feedback required to deliver a solid final product and outcome. The efficacy of such techniques has been demonstrated with systems such as Sonic Pi and Ixi Lang, within a research domain already supported by the AHRC through the Live Coding Network (AH/L007266/1), and by EC in the H2020 project, RAPID-MIX. Finally, this research will strongly contribute to dialogues surrounding the future of music and the arts, consolidating the UK's leadership in these fields.</gtr:abstractText><gtr:potentialImpactText>We will directly engage stakeholders in the process of music making with creative tools, exploring the role that AI will play in the future of the creative industries. We will bring complex AI and machine learning technologies to the general user of creative software; we will democratise technologies that are still emerging in academia and corporate R&amp;amp;D labs.

These groups will benefit from new software, course materials, events, artistic outputs and industry collaborations:

a) creative practitioners and their audiences; specifically musicians, composers and their audiences;
b) the hacker/maker community;
c) industry professionals; including through existing industry partnerships with record labels (XL, Universal), music technology companies (Akai, Roli, Ableton, Reactable, Cycling74, Abbey Road Red) and our project partner, Google Magenta;
e) learners; including those from secondary and higher education, home learners, and academics and professionals
f) the general public.

A key aim of our project is to create a simplified live coding language for coding in the browser where novices can learn about AI and machine learning through a clear and simple, yet powerful, live coding programming language written on top of JavaScript , which is well supported and popular. This simplified live coding language will be designed specifically for musicians and artists and will allow them to pursue new routes for creating music. 

We will facilitate a range of high-quality use cases with creative professionals. This will bridge gaps between research and industry, accelerating the impact of artificial intelligence by deploying it in real-world and professional music-making and listening contexts.

Our events series will bring musicians, composers and audiences together, providing an important platform for the continued dissemination our work and the work of those practitioners whom we support through the creation of new tools. 

In addition to concerts, we will run symposia to expand and further develop critical thought in these fields, inviting participation from a range of stakeholders. We will also disseminate and support artistic output through the creation of our platform, making it simple not just to create work, but also to share it amongst friends and colleagues, from both outside and inside our connected communities.

Our background technologies will be open source and available to academics and SMEs alike, allowing them to use contemporary AI in ways that are currently very challenging for novices. 

We will generate significant, engaging and unique course materials, associated with our existing MOOC provision, and targeted at a range of different learners, from secondary education, through HE, to home learners, academics and professionals. This will help people to acquire skills in machine learning at any stage. 

Our track record indicates we are capable of meeting the significant interest from the general public around these issues. Recent public engagement activities from team members have included:

- applying Deep Learning to the creation of artworks we are currently exhibiting at the Whitney Museum of American Art, with an accompanying paper at SIGGRAPH
- significant press around the use of AI and Machine Learning for music 
- generative AI music software created used by Sigur Ros for the release of their most recent single (Route One), and a generative remix of the track broadcast for 24 hours on Icelandic National TV and watched by millions of people online
- contribution to the first computer generated West End musical 
- high profile experiments on live national radio, as well as experience developing large scale, online collaboration platforms
- machine learning software for composers and musicians, downloaded over 5,000 times and the world's first MOOC on machine learning for creative practice; 
- design of various popular live coding systems (ixiQuarks, ixi lang, the Threnoscope).</gtr:potentialImpactText><gtr:fund><gtr:end>2020-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2018-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>806693</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">AH/R002657/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>