<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CDD64036-C7C4-4924-8321-6076FB363042"><gtr:id>CDD64036-C7C4-4924-8321-6076FB363042</gtr:id><gtr:name>University of Sunderland</gtr:name><gtr:department>Computing Engineering and Technology</gtr:department><gtr:address><gtr:line1>Floor 2</gtr:line1><gtr:line2>Edinburgh Building</gtr:line2><gtr:line3>Chester Road</gtr:line3><gtr:line4>Sunderland</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>SR1   3SD</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CDD64036-C7C4-4924-8321-6076FB363042"><gtr:id>CDD64036-C7C4-4924-8321-6076FB363042</gtr:id><gtr:name>University of Sunderland</gtr:name><gtr:address><gtr:line1>Floor 2</gtr:line1><gtr:line2>Edinburgh Building</gtr:line2><gtr:line3>Chester Road</gtr:line3><gtr:line4>Sunderland</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>SR1   3SD</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C1622480-4129-4213-B987-46C785711C08"><gtr:id>C1622480-4129-4213-B987-46C785711C08</gtr:id><gtr:firstName>Harry</gtr:firstName><gtr:otherNames>Richard</gtr:otherNames><gtr:surname>Erwin</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/AB6A5C7E-8101-40C0-8D64-05DC0AB5DB7A"><gtr:id>AB6A5C7E-8101-40C0-8D64-05DC0AB5DB7A</gtr:id><gtr:firstName>Stefan</gtr:firstName><gtr:surname>Wermter</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD055466%2F1"><gtr:id>D98D9DB7-26D3-42E5-9E35-EEFC8872F43E</gtr:id><gtr:title>Midbrain Computational and Robotic Auditory Model for focused hearing (MiCRAM)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D055466/1</gtr:grantReference><gtr:abstractText>Our aim is to replicate in a computer model the processing that occurs in the auditory midbrain, the inferior colliculus, to analyse sounds. Our hypothesis is that a biologically inspired model of precortical processing, based on the way the brain processes sounds, will be more efficient than existing computational models. One way in which our hearing is superior to computer based sound recognition systems is in its ability to separate and identify sounds in noisy environments; for example when we hold a conversation at a noisy party. There would be great benefits to the quality of life if we could achieve this level of performance with computer interfaces or robotic systems. In particular we intend to demonstrate the utility of our model by adapting it to control a robot that is able to respond to sound stimuli in a noisy environment. In turn, we expect our model to generate predictions about brain function. We aim to do this through a collaboration between a team experts in computing, robotics, auditory processing and brain science.We hear sounds in the world around us when they activate receptors in our ears. These receptors encode sound to electrical impulses that activate a chain of processing centres in the brain and eventually the cortex. The cochlea, the part of the ear where sound is sensed, is shaped like a snail shell. Running down its length is a row of inner hair cells. Each cell is most sensitive to a particular frequency, and these hair cells in turn activate auditory nerve fibres. Because each nerve fibre only responds to a narrow range of sound frequencies and projects in an orderly way to the brain, frequency is represented in a spatially ordered or topographic manner called a tonotopic representation.The information carried by the nerve fibres enters the brain and divides into a number of processing channels that emphasise different aspects of the sound stream. These individual tonotopic representations converge and are processed in the inferior colliculus (IC), which in turn sends outputs to the thalamus and then to the auditory cortex. Evidence suggests that because these streams converge at the IC there is sufficient information available at this level to identify what the sound is and where it comes from. The IC also receives feedback from the cortex. This may provide an expected pattern of sound signals that the inferior colliculus compares with the incoming sound pattern to identify differences. It also may be that this acts in some way to spotlight auditory attention to emphasize some sounds and de-emphasise others.To build our model, the neuroscientists in our team will construct a database of current knowledge about the wiring and connections of the auditory brainstem, and add to this where necessary with new experiments about auditory processing in the brainstem of animals. The modellers will use this information to guide the creation of a computer model that can control the actions of a robot. We will use robots because animals do not just passively listen but actively seek out sounds to build an auditory picture of their world. This active behaviour allows them to put together experimental scenarios that have different results depending on how sound is processed. Working with robots makes it more likely that the results will have practical uses in helping to build better hearing aids, speech understanding systems, sound tracking systems, and sound-controlled robots. We expect that our model will help us to make predictions about how the midbrain functions and we will test these predictions in animal and robot experiments. The experts in modelling sound processing are very interested in testing whether some theories on how the ear and brain handle sound are consistent with what is seen in biology and thus our modelling will help to guide the direction of future research and reduce animal use. The database we develop will be made available to other researchers in the field.</gtr:abstractText><gtr:fund><gtr:end>2009-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>146108</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>57C2FBCE-C6CA-46CF-B59B-1A70184960A6</gtr:id><gtr:title>A biologically inspired spiking neural network model of the auditory midbrain for sound source localisation</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/242b3836ccf09f5018352f691c3cf3bd"><gtr:id>242b3836ccf09f5018352f691c3cf3bd</gtr:id><gtr:otherNames>Liu J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d00200222bfe1a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BD5F3B3-D9D4-4143-AED6-407EEF99109F</gtr:id><gtr:title>Mobile robot broadband sound localisation using a biologically inspired spiking neural network</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01786dc41f4036d8cbd3f8971abdec94"><gtr:id>01786dc41f4036d8cbd3f8971abdec94</gtr:id><gtr:otherNames>Jindong Liu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-1-4244-2057-5</gtr:isbn><gtr:outcomeId>doi_53d0590599c4651b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>385DC1A1-A90A-4F71-8554-BBE4136FAD43</gtr:id><gtr:title>Artificial Neural Networks - ICANN 2008</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/242b3836ccf09f5018352f691c3cf3bd"><gtr:id>242b3836ccf09f5018352f691c3cf3bd</gtr:id><gtr:otherNames>Liu J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-3-540-87558-1</gtr:isbn><gtr:outcomeId>doi_53cfcdfcd164bd7c</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D055466/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EFFEC6B1-6BC8-4C9D-9D77-02CEF5E4E301</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Biomedical neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>