<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/36712AC7-25CB-47F2-8E69-8F154ED3C9CD"><gtr:id>36712AC7-25CB-47F2-8E69-8F154ED3C9CD</gtr:id><gtr:name>Centre for Research and Technology Hellas (CERTH)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/36712AC7-25CB-47F2-8E69-8F154ED3C9CD"><gtr:id>36712AC7-25CB-47F2-8E69-8F154ED3C9CD</gtr:id><gtr:name>Centre for Research and Technology Hellas (CERTH)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/655C6837-F593-4799-8D1A-C91B859A7EF7"><gtr:id>655C6837-F593-4799-8D1A-C91B859A7EF7</gtr:id><gtr:name>Samsung Electronics Research Institute</gtr:name><gtr:address><gtr:line1>Communications House</gtr:line1><gtr:line2>South Street</gtr:line2><gtr:line4>Staines</gtr:line4><gtr:postCode>TW18 4QE</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/EF09EF02-2C6D-4167-9904-8E0D479DE8EA"><gtr:id>EF09EF02-2C6D-4167-9904-8E0D479DE8EA</gtr:id><gtr:firstName>Tae-Kyun</gtr:firstName><gtr:surname>Kim</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ012106%2F1"><gtr:id>B5941C02-D2AE-442D-A9B8-A17B7EC9C424</gtr:id><gtr:title>3D Intrinsic Shape Recognition Under Deformation and View Changes</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J012106/1</gtr:grantReference><gtr:abstractText>In the proposal, we tackle the novel visual recognition problem of 3D (three-dimensional) deformable object shape identities or categories. Images of 3D objects undergo large appearance changes due to different object poses (articulation or deformation) as well as camera view-points. We attempt to recognise objects from single images by their 3D shape identities (or intrinsic shapes) regardless of their present poses and camera view-points. Humans can perceive 3D shapes of objects from single images, provided that they have previously seen 3D shapes of similar other objects. The knowledge formerly learnt on 3D shapes is called 3D shape prior. A key idea for fulfilling the proposed task is to learn and exploit the shape priors for object recognition.

The proposed research is well-lined with and goes beyond important topics of computer vision. Whereas much work for view-point invariant object recognition is limited to rigid object classes with bountiful textures, we consider deformable object shapes. In a series of work in the field of single view reconstruction, promising results have been shown for human body shape reconstruction under pose variations. There has also been a notable latest success in 3D human pose recognition. On the top of these results, we go beyond to capture 3D intrinsic shape variations for object recognition. The intended outcomes would benefit the relevant academic fields and their existing markets, and would also lead to potential new applications such as automatic monitoring of public obesity and animal tracking.</gtr:abstractText><gtr:potentialImpactText>As the relevant academic topics have been long-explored and matured to have yielded commercial application markets, our research potentially benefits those existing markets: e.g. content based image retrieval, photo album management and surveillance systems, by recognising intrinsic object shapes. Human shape modeling in the proposed work itself is a useful output for man-machine interface such as avatar, when it explicitly yields shape reconstruction. It also helps human pose recognition, which was recently deployed for a commercial console game with a huge success, in an invariant manner to human shape changes.

The proposed research potentially spins out new applications, especially interesting for public health cares. Public heath, particularly obesity has doubled since 1980 and has been a top social issue in UK and the world. Systematic ways of monitoring and caring individual and public degree of overweight are highly demanding. An interesting new sub-topic of computer vision study, food recognition [49,57], is relevant to this issue. Owing to the widespread use of smart phones equipped with a camera, users can take photos of daily food to consume and the software automatically recognises food types and provides users with useful dietary information according to a pre-defined food calorie table. Various mobile health projects (called m- or e-health) have also been carried out for health services and information. The proposed system can automatically categorise human shapes in real-time and without users' attention or notice (no particular pose and camera view-point needed). The estimated shapes can lead to more direct measurements for obesity i.e. waist circumferences and weights. We intend to carry on our research in the context of vision-based health-care, by human shape recognition, food recognition and also their combination in future.

Our solution aims at recognising human individual shapes or shape categories. This further strengths existing biometric solutions (face, gait recognition) in an appropriate environment. A X-ray type scanner that produces &amp;quot;naked&amp;quot; images of passengers has been adopted for speeding up security checks at Manchester Airport. With the aid of the sensor, clothing effects are removed in the deployment of the proposed silhouette-based algorithm. The proposed algorithm can be combined with existing face or gait recognisers to improve the accuracy. The real-time solution does not need to save images for further use and quickly destroys images to lessen privacy issues.

Our proposed research is for generic object categories, not limited to human bodies. Animal tracking and monitoring especially for horse or big fish has been long studied. They need an non-invasive manner of monitoring without human observers and present satellite navigation tracking system is costly and requires specific sensors. The proposed vision-based solution would be useful to help and improve existing solutions by consideration of large shape and pose variations, and to detect animal sub-species by their shape categories in a reasonably constrained environment.

Our research and successful outcomes would positively influence security and health related policy-makers and government agencies. With adverse impacts of government funding cuts on UK national health services, technological development in mobile health-cares can help achieve the productivity and cost-efficiency savings necessary in both public and private healthcare sectors.

From the intended project activities, the P.I. would learn skills for project management and professional networking, and the R.A. the skills to carry out main research activities including efficient coding, management of different program versions and code sharing. Both P.I. and R.A. would be involved in and learn skills of interacting with a wide spectrum of audiences, and giving a clear description of the project goals and achievements for maximum dissemination of the results.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-08-19</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-06-20</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>97751</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Centre for Research and Technology Hellas (CERTH)</gtr:collaboratingOrganisation><gtr:country>Greece, Hellenic Republic</gtr:country><gtr:description>Joint papers with Center for Research and Technology Hellas (CERTH), Thessaloniki, Greece</gtr:description><gtr:id>AA32FCC9-7948-4C77-96E3-A36D71593DEC</gtr:id><gtr:impact>A. Doumanoglou, T-K. Kim, X. Zhao, S. Malassiotis, Active Random Forests: An application to Autonomous Unfolding of Clothes, Proc. of European Conference on Computer Vision (ECCV), Zurich, Switzerland, 2014. 

A. Doumanoglou, A. Kargakos, T-K. Kim, S. Malassiotis, Autonomous Active Recognition and Unfolding of Clothes using Random Decision Forests and Probabilistic Planning, Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA), Hong Kong, China, 2014 (KUKA best service robotics paper award).</gtr:impact><gtr:outcomeId>5469129355abe2.01604534-1</gtr:outcomeId><gtr:partnerContribution>detailed formulations of ideas, implementations, data work, experiments, writing-up</gtr:partnerContribution><gtr:piContribution>novel ideas to perform active visual perception, comparative experiments</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Cambridge</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>A joint paper with Prof. Cipolla's group in Univ. of Cambridge</gtr:description><gtr:id>D42B2FB9-8038-4504-A311-2392E2473CAB</gtr:id><gtr:impact>T.H. Yu, T-K. Kim, and R. Cipolla, Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-modality Regression Forest, Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Portland, Oregon, USA, 2013.</gtr:impact><gtr:outcomeId>546911adec16c3.28249714-1</gtr:outcomeId><gtr:partnerContribution>detailed formulations, implementations, experiments, writing-up</gtr:partnerContribution><gtr:piContribution>a new benchmark on 3D human body pose and action detection, a new framework to conduct pose and action estimation simultaneoulsy</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Associate Editor of IPSJ Transactions on Computer Vision and Applications (CVA), 2016-2018</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>1AF0DB7F-E3C5-4026-872D-6695651C4593</gtr:id><gtr:impact>I joined the editorial board of an international journal in the field of computer vision.</gtr:impact><gtr:outcomeId>58ae515b313865.52390322</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at Deep learning summit, London, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E1C50DE9-2409-453C-A4F1-4410BCE60C5A</gtr:id><gtr:impact>About +300 engineers/entrepreneurs and postgraduate students attended for the summit, I gave an invited talk with a live demo on the topic of deep learning and random forest, for the applications on hand pose estimation and face recognition. The summit organisers reported very good feedback from audience on the event/topics.</gtr:impact><gtr:outcomeId>58ae474ab416e5.38090891</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Lecture at BMVA Computer Vision Summer School, Swansea, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>77F47177-C047-49EF-B2C9-A5FE5B5918DB</gtr:id><gtr:impact>attracted the participant phd students and practitioners into the topic

increase in requests about further involvement in related research/teaching activities</gtr:impact><gtr:outcomeId>5469157cb75351.74621818</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Associate Editor of (Elsevier) Image and Vision Computing Journal</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>92892D6A-C963-4AB1-B576-811A120479C5</gtr:id><gtr:impact>I joined the editorial board of a premium international journal in the field of computer vision.</gtr:impact><gtr:outcomeId>58ae506a84a872.70267318</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at Omron Corporation, Kyoto, Japan, 2013</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C8B056E5-B774-46E8-8C03-F7BAFF95DD4A</gtr:id><gtr:impact>The talk is about our own contributions for challenging novel problems: real-time action, 3D posture recognition and object phenotype recognition, which are a part of the project. Randomised Decision Forests and tree-structured methods are proposed for real-time vision solutions.

increase in request in further involvement in the related research activities</gtr:impact><gtr:outcomeId>r-7234956992.0776010cb89990</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Demo, Imperial College Science Festival May 2016 (500+ visitors)</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>C3779B45-B70F-4562-AD14-64CA066C0023</gtr:id><gtr:impact>We did a hands-on demo for (deformable and articulated object) pose estimation at Imperial College Science Festival, May 2016. Our demo attracted/received 500+ visitors.</gtr:impact><gtr:outcomeId>58ae547052cda5.46041134</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Guest Editor of (Elsevier) Pattern Recognition Letters Special Issue on Personalised and Context-sensitive Interfaces in the Wild, 2016.</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>2BFF742D-51FD-465E-A1E4-64F6D5D8DBA9</gtr:id><gtr:impact>I have served as a guest editor of a special issue in Pattern Recognition Letter journal.</gtr:impact><gtr:outcomeId>58ae50f941d8a7.91759909</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk in Seminar Series of the Robotics Research Group at University of Oxford, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F61CB72A-ECE5-4D0D-8C2E-83BD66E8B5B9</gtr:id><gtr:impact>The talk is about Randomised Decision Forest, an emerging technique in relevant fields, being highly successful for various real-time vision solutions. In this talk, Randomised Decision Forests and tree-structured methods are reviewed with comparative and insightful discussions, leading to our own contributions for challenging novel problems: real-time action and 3D posture recognition and object phenotype recognition, which we recently tackled at Imperial College London.

increase in request in further involvement in the related research activities</gtr:impact><gtr:outcomeId>r-9905372308.4790380c0e45f8</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BMVA Executive Committee member</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>AB774202-2246-4C5E-829A-99BF338FFA79</gtr:id><gtr:impact>I joined the BMVA Executive Committee for the period of Jan 2016-Dec 2018. The BMVA organises/supports various academic events and activities in the field of computer vision, and the BMVA executive committee meet to discuss regularly throughout each year.</gtr:impact><gtr:outcomeId>58ae4fa3f09b56.15229295</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited lecture/lab, BMVA computer vision summer school, Swansea, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>82AC3AFF-D279-4CC9-B036-B54A14D614C7</gtr:id><gtr:impact>About +60 PG students attended this school, and I gave 1.5 hour lecture and 1.5 hour hands-on session, on random decision forest with deep learning. The participants expressed lots of interest on the topics and told they would use the learnt for their PG studies. The school reported very good feedback received from the students.</gtr:impact><gtr:outcomeId>58ae4ab25baf09.60710914</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>General co-chair of IEEE 2nd workshop on observing and understanding hands in action (HANDS, in conjunction with CVPR), Las Vegas, USA, Jun 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8A2638A1-88D9-4056-A19D-E94AD8CBEAFF</gtr:id><gtr:impact>I co-organised the international workshop on the topic of hand pose estimation in conjunction with CVPR 2016. About +55 PG students/academics/professionals attended this workshop, and presented 4 oral papers and 10 posters. We also had 5 invited talks from renowned experts on the topic.</gtr:impact><gtr:outcomeId>58ae52ea294f28.61959429</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Live demo at Imperial College Science Festival</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>629EC3E2-F73B-4BD4-8897-726F21B6D7D8</gtr:id><gtr:impact>6D robot vision demo, in the Robotics Forum stand, Imperial College Science Festival May 2015. About 20,000 visitors were received.</gtr:impact><gtr:outcomeId>56afcae302f790.66671881</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>General co-chair of British Machine Vision Conference (BMVC), London, Sep 2017</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>524392B6-19A3-44AE-91D7-FD5933320748</gtr:id><gtr:impact>I co-organise this premium international conference on computer vision in London. About +350 participants and +600 high quality paper submissions are expected. The most renowned academic figures in the field are confirmed as keynotes and tutorial speakers for the event. The event this year is expected to be a unique monument in various aspects.</gtr:impact><gtr:outcomeId>58ae4e3ef3a738.43222907</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote at Korea-Japan joint workshop on Frontiers of Computer Vision, Takayama, Japan</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>65F9AC17-25F5-47C2-8528-61B4FC2E4714</gtr:id><gtr:impact>About +150 PG students and professors attended this conference, where I gave a keynote. This is a unique event for computer vision researchers especially to promote collaborations and knowledge-sharing between Korea and Japan. My talk on the topic of deep learning and tree structure algorithm sparked lots of questions and interests, the conference organisers reported increased attendance, and excellent feedback on my talk.</gtr:impact><gtr:outcomeId>58ae4c965def98.88033490</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at Korean Conf. on Computer Vision</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>779C4104-6B51-4ED8-88A7-AB3BC12E9D75</gtr:id><gtr:impact>About +200 PG students and engineers from industry attended this event, where I gave an invited talk on the topic of deep learning and tree-structure algorithms, for hand (articulated object) pose and face (deformable object) recognition, and got lots of questions and discussions during the event.</gtr:impact><gtr:outcomeId>58ae495bafc352.30605069</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>82000</gtr:amountPounds><gtr:country>Japan</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Industrial grant (human pose/activity detection)</gtr:description><gtr:end>2016-09-02</gtr:end><gtr:fundingOrg>OMRON</gtr:fundingOrg><gtr:id>76FCE8E2-4CD2-4869-88DA-A62D79603FDF</gtr:id><gtr:outcomeId>56afc96c8c3e48.68259999</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>5958623</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC program grant (Face Matching for Automatic Identity Retrieval, Recognition, Verification and Management)</gtr:description><gtr:end>2020-12-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>88784785-0B65-4620-BD5B-6BFEB24901A3</gtr:id><gtr:outcomeId>56afc877dd4f69.09304426</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The developed system and program have been used for (1) CloPeMa (http://clopema.eu/) which deals with Clothes Perception and Manipulation. (2) Samsung GRO (Global Research Outreach) program on Robust Face Recognition Algorithm in Surveillance. In these applications, the key findings are applied to address the face and clothes (as highly deformable object) recognition problems.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>42D5B38E-9FB7-4D33-A67B-261A6640D429</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>546625fd67eae1.88865954</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We studied the intrinsic shape recognition problem and applied to the recognition of various kinds of highly deformed objects, such as human body, face, and clothes. Our key findings are: (1) Shape information plays an important role in deformable object recognition problem. (2) The shape information can be combined with other object attributes information to help the understanding of objects. We developed a method for unified method for face recognition and its attribute analysis. (3) The shape information based recognition is really helpful for clothes recognition. We validated this in the automatically clothes unfolding problems.</gtr:description><gtr:exploitationPathways>(1) The developed algorithm and program can be directly applied to human body, face and clothes recognition problems. 
(2) The corresponding datasets can also be used for future research. These databases are very helpful for other researchers in the field.</gtr:exploitationPathways><gtr:id>A26B84C6-60C5-4F14-B37C-CE98E4C4D9CA</gtr:id><gtr:outcomeId>5466212860ea42.00297737</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A new benchmark to evaluate the joint performance on sub-problems of face</gtr:description><gtr:id>32635B5B-BFB4-4254-B114-44B7E517F776</gtr:id><gtr:impact>Will be helpful for other researchers in the field</gtr:impact><gtr:outcomeId>54691984241a83.19791433</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Unified Face Analysis Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8CDC8377-0298-41EA-8EFC-7115F0FD26E6</gtr:id><gtr:title>Unified Face Analysis by Iterative Multi-Output Random Forests</gtr:title><gtr:parentPublicationTitle>CVPR</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bfe57cecf79ebeabbfab2cc4b9418e3b"><gtr:id>bfe57cecf79ebeabbfab2cc4b9418e3b</gtr:id><gtr:otherNames>Zhao X. (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_3434985898140a9718</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>76156D80-BF15-47FB-B010-8D91ED232560</gtr:id><gtr:title>Conditional Convolutional Neural Network for Modality-aware Face Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7dbda761d8aae1fd65c579d813c675d3"><gtr:id>7dbda761d8aae1fd65c579d813c675d3</gtr:id><gtr:otherNames>C. Xiong</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58ae5688aa0533.01659945</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5B8A852-48AF-4CB3-B958-A05F5DBF78B6</gtr:id><gtr:title>Convolutional Fusion Network for Face Verification in the Wild</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3181b5eb6380981af49ac1282a1054d0"><gtr:id>3181b5eb6380981af49ac1282a1054d0</gtr:id><gtr:otherNames>Xiong C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adad0a7c9611.80068906</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>897E9DC8-B29B-4097-AE30-3C8F9DE73D02</gtr:id><gtr:title>Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ba10b31e166e637caf1a0e8f7b13c671"><gtr:id>ba10b31e166e637caf1a0e8f7b13c671</gtr:id><gtr:otherNames>Q. Ye</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ae5987b1c768.39443679</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E7A471D-45E1-4CCC-9D75-32B0229A95CD</gtr:id><gtr:title>Folding Clothes Autonomously: A Complete Pipeline</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Robotics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e55ce26e5e7636670d33249382cda50c"><gtr:id>e55ce26e5e7636670d33249382cda50c</gtr:id><gtr:otherNames>Doumanoglou A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d425aa83657.71611490</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A674487-57B9-45BE-A574-A05E058BD581</gtr:id><gtr:title>Autonomous Active Recognition and Unfolding of Clothes using Random Decision Forests and Probabilistic Planning</gtr:title><gtr:parentPublicationTitle>ICRA</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8910ce1bbc8200c532406d8491bd0181"><gtr:id>8910ce1bbc8200c532406d8491bd0181</gtr:id><gtr:otherNames>Doumanoglou  A. (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_317132348713d7b924</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EE563076-A5F4-4BD0-9E43-C489E835DD9F</gtr:id><gtr:title>Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a473c6ca58fb60c03536f49c6dbcaf0d"><gtr:id>a473c6ca58fb60c03536f49c6dbcaf0d</gtr:id><gtr:otherNames>Yu T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d057057434bd33</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB67935B-E379-494F-A13E-954A3566E129</gtr:id><gtr:title>3D Finger CAPE: Clicking Action and Position Estimation under Self-Occlusions in Egocentric Viewpoint.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/16d7103b3f0ae4c1d0296a5d71efbec1"><gtr:id>16d7103b3f0ae4c1d0296a5d71efbec1</gtr:id><gtr:otherNames>Youngkyoon Jang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>58ae57544b1d96.14487574</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>17CDC97C-C86A-4E58-BAFD-A2B379F11D89</gtr:id><gtr:title>Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/424fef9127e4638c39a49896f120001f"><gtr:id>424fef9127e4638c39a49896f120001f</gtr:id><gtr:otherNames>D. Tang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58ae563b34ae38.60623193</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B4C4340F-48A0-412F-85DA-F2F4FB066844</gtr:id><gtr:title>Latent Regression Forest: Structural Estimation of 3D Articulated Hand Posture</gtr:title><gtr:parentPublicationTitle>IEEE Trans on PAMI (TPAMI)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/424fef9127e4638c39a49896f120001f"><gtr:id>424fef9127e4638c39a49896f120001f</gtr:id><gtr:otherNames>D. Tang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ae58b7230409.96393511</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5F18D57E-5A55-4CA7-B6A2-009024522061</gtr:id><gtr:title>SD Gesture: Static and Dynamic Gesture Estimation for Manipulating a Function-Equipped AR Object</gtr:title><gtr:parentPublicationTitle>IEEE Trans. on Human-Machine Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/404902793b17fb7f59b4aa9d10a2ae03"><gtr:id>404902793b17fb7f59b4aa9d10a2ae03</gtr:id><gtr:otherNames>Y. Jang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ae5a8e2e6653.65882200</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>739D1121-69EF-4BDC-A965-9BCC0AEE11B9</gtr:id><gtr:title>Errata for &amp;acirc;??Distortion-Free Image Mosaicing for Tunnel Inspection Based on Robust Cylindrical Surface Estimation through Structure from Motion&amp;acirc;?? by Krisada Chaiyasarn, Tae-Kyun Kim, Fabio Viola, Roberto Cipolla, and Kenichi Soga</gtr:title><gtr:parentPublicationTitle>Journal of Computing in Civil Engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/37142413c5642bf8cecf47e9e61296cc"><gtr:id>37142413c5642bf8cecf47e9e61296cc</gtr:id><gtr:otherNames>Chaiyasarn K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d699081c7b1.70325749</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J012106/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>