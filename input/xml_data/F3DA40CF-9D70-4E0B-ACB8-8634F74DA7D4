<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name><gtr:address><gtr:line1>Economic and Social Research Council</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:line3>Polaris Way</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:line5>Wiltshire</gtr:line5><gtr:postCode>SN2 1UJ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name><gtr:address><gtr:line1>BBSRC</gtr:line1><gtr:line2>Polaris House</gtr:line2><gtr:line3>North Star Avenue</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:postCode>SN2 1UH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B66F5D37-07D1-43CE-8654-3BBED6CEA03D"><gtr:id>B66F5D37-07D1-43CE-8654-3BBED6CEA03D</gtr:id><gtr:firstName>Martin</gtr:firstName><gtr:surname>Fischer</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/35C3A81C-38AE-426D-A032-A11604F040FE"><gtr:id>35C3A81C-38AE-426D-A032-A11604F040FE</gtr:id><gtr:firstName>Trevor</gtr:firstName><gtr:surname>Harley</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF028598%2F1"><gtr:id>F3DA40CF-9D70-4E0B-ACB8-8634F74DA7D4</gtr:id><gtr:title>VALUE: Vision, Action, and Language Unified by Embodiment</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F028598/1</gtr:grantReference><gtr:abstractText>The primary aim of this project is to develop a simulation of the processes involved in solving the following problem: how to select, based on the agent's knowledge and representations of the world, one object from several, grasp the object and use it in an appropriate manner. This mundane activity in fact requires the simultaneous solution of several deep problems at various levels. The agent's visual system must represent potential target objects, the target must be selected based on task instructions or the agent's knowledge of the functions of the represented objects, and the hand (in this case) must be moved to the target and shaped so as to grip it in a manner appropriate for its use. We propose to develop a robotic simulation model inspired by recent theories of embodied cognition, in which the vision, action and semantic systems are linked together, in a dynamic and mutually interactive manner, within a connectionist architecture. Human experimental work will constrain the temporal and dynamic properties of the system in an effort to develop a psychologically plausible model of embodied selection for action. As much of the cognitive mechanisms leading to the integration between action and vision for actions such as object assembly tasks are not fully known, new empirical studies in this project will also improve our insight of these embodied cognitive dynamics. New experiments and the use of the embodied cognitive model will also be used to further our understanding of language and cognition integration e.g. by providing further predictions and insights on the dynamics of language and action knowledge in object representation.This is an interdisciplinary project which involves expertise and methodologies from cognitive psychology, motor control, and computational/robotics modelling. The interdisciplinary nature of the project and the design and experimentation of cognitive agents make the project highly relevant to the Cognitive Systems Foresight programmeBehavioural studies, as proposed here, will be based on the eye-tracking methodology. This permits the identification of the time-course of visuo-attentional processes in action and language processing and will provide converging evidence from stimulus-response compatibility studies on object selection. Eye tracking data will also be used to constrain the behavioural and attentional strategies used by simulated cognitive robots during tasks involving object naming and selection. In eye-tracking experiments we will show arrays of novel objects and study three levels of action representation. At the encoding level, we manipulate the location and onset time of a visual detection probe in this array to reveal how observers attend and prepare their actions (Fischer et al., in press). At the representational/linguistic level, we present auditory object names and register the observer's eye movements towards the named objects (visual world paradigm, e.g. Altmann &amp;amp; Kamide, 2004). Linguistic manipulations, such as using phonological competitors ( candle-candy ), reveal the time course of the interplay between covert and overt attention and the relative strength of top-down (linguistic) vs bottom-up visual control over action prediction. Finally, at the execution level, we instruct participants to pick up the named object and record their overt manual responses (e.g., Chambers et al., 2002, 2004). Orthogonal to these three levels of embodiment, we gradually associate each novel object with a particular name and manual response, and we design object arrays with congruent and incongruent response requirements. This learning approach enables us to track embodied concept acquisition and its implications for action control, separately at the encoding, linguistic/representational, and execution level.</gtr:abstractText><gtr:fund><gtr:end>2012-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>297092</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>First and foremost, the results of this project have been fed into the design of software that is consistent with the user-friendly interaction between humans and humanoid robots. 
Secondly, our findings have been disseminated both on international conferences and in high-impact peer-reviewed journals, in order to benefit the wider research community. 
Finally, the work has also been used to enrich student training and inspire young researchers about &amp;quot;cognitive robotics&amp;quot;, to set up new international collaboration projects and to apply for follow-up funding.</gtr:description><gtr:firstYearOfImpact>2009</gtr:firstYearOfImpact><gtr:id>BF6DCAAB-7DCE-4D9E-AE3A-81F9BA296B5E</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>544943a1b39885.32196390</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>First and foremost, the results of this project have been fed into the design of software that is consistent with the user-friendly interaction between humans and humanoid robots. 
Secondly, our findings have been disseminated both on international conferences and in high-impact peer-reviewed journals, in order to benefit the wider research community. 
Finally, the work has also been used to enrich student training and inspire young researchers about &amp;quot;cognitive robotics&amp;quot;, to set up new international collaboration projects and to apply for follow-up funding.</gtr:description><gtr:exploitationPathways>The field of cognitive robotics should find the findings very useful.</gtr:exploitationPathways><gtr:id>3B77859C-3291-4BA8-B386-6BD9CA991DE1</gtr:id><gtr:outcomeId>544a31bb34d404.31496678</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.tech.plym.ac.uk/soc/research/ABC/value/Publications.htm</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>46D6E482-A646-4EBF-B02A-C88DCB1B57EA</gtr:id><gtr:title>Object affordance influences instruction span.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f886dbadbfc52d5c4f04ac77bfa625f"><gtr:id>8f886dbadbfc52d5c4f04ac77bfa625f</gtr:id><gtr:otherNames>Apel JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>544941e935d430.12829523</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A96BE534-EE22-4696-91BD-8D9E8307258E</gtr:id><gtr:title>Visual and linguistic cues to graspable objects.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70570332e65608cf01598bbc299ba3c0"><gtr:id>70570332e65608cf01598bbc299ba3c0</gtr:id><gtr:otherNames>Myachykov A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>5449413346bf60.37227826</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43FF9816-4629-4D12-9961-6502CE91F877</gtr:id><gtr:title>Grounding language in action and perception: from cognitive agents to humanoid robots.</gtr:title><gtr:parentPublicationTitle>Physics of life reviews</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/285d9c01a1539d568a5a4773cffe0c42"><gtr:id>285d9c01a1539d568a5a4773cffe0c42</gtr:id><gtr:otherNames>Cangelosi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1571-0645</gtr:issn><gtr:outcomeId>doi_53d0050058547ca0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BDA4599-EC97-4F39-ABA7-5BA7228E6CE6</gtr:id><gtr:title>The oculomotor resonance effect in spatial-numerical mapping.</gtr:title><gtr:parentPublicationTitle>Acta psychologica</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70570332e65608cf01598bbc299ba3c0"><gtr:id>70570332e65608cf01598bbc299ba3c0</gtr:id><gtr:otherNames>Myachykov A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0001-6918</gtr:issn><gtr:outcomeId>56c1ccf699aa49.64531913</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2DDB1C85-DA7A-4489-9AEF-52A07C540751</gtr:id><gtr:title>Electrophysiological examination of embodiment in vision and action.</gtr:title><gtr:parentPublicationTitle>Psychological science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a2e5ee5cda0339ee5bc74b90343c2e32"><gtr:id>a2e5ee5cda0339ee5bc74b90343c2e32</gtr:id><gtr:otherNames>Goslin J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0956-7976</gtr:issn><gtr:outcomeId>54494246ebb382.97557965</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2B4DB00-EEBB-498B-8F70-4DC92240AD19</gtr:id><gtr:title>The mechanics of embodiment: a dialog on embodiment and computational modeling.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d93971f4f21570d991792849625dc7ea"><gtr:id>d93971f4f21570d991792849625dc7ea</gtr:id><gtr:otherNames>Pezzulo G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>5449413373d136.58876610</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4B2DB61B-840B-4232-849B-0D428278277F</gtr:id><gtr:title>Attention deployment during memorizing and executing complex instructions.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f886dbadbfc52d5c4f04ac77bfa625f"><gtr:id>8f886dbadbfc52d5c4f04ac77bfa625f</gtr:id><gtr:otherNames>Apel JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>544942866ad290.54617681</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A556642-D1E8-4267-8637-10AB2F9EF1AC</gtr:id><gtr:title>Computational Grounded Cognition: a new alliance between grounded cognition and computational modeling.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d93971f4f21570d991792849625dc7ea"><gtr:id>d93971f4f21570d991792849625dc7ea</gtr:id><gtr:otherNames>Pezzulo G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>5449418e640814.26861603</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F028598/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>