<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/3B3A27E2-6C51-4BBF-9FAA-32301E602A1B"><gtr:id>3B3A27E2-6C51-4BBF-9FAA-32301E602A1B</gtr:id><gtr:firstName>Caroline</gtr:firstName><gtr:otherNames>Elizabeth</gtr:otherNames><gtr:surname>Jay</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/5870D14E-9E3C-4161-91E2-11DDA17A5989"><gtr:id>5870D14E-9E3C-4161-91E2-11DDA17A5989</gtr:id><gtr:firstName>Robert</gtr:firstName><gtr:surname>Haines</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM017133%2F1"><gtr:id>5C1A236A-8C06-4DD4-A52F-2810963D90D5</gtr:id><gtr:title>IDInteraction: Capturing Indicative Usage Models in Software for Implicit Device Interaction</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M017133/1</gtr:grantReference><gtr:abstractText>The IDInteraction project asks: can we exploit models of human behaviour to move away from direct, unambiguous user commands, towards seamless user-device interaction? It will investigate and develop the techniques to capture 'Indicative Usage Models (IUMs), behavioural patterns or cues that precede a particular activity, and translate these into software-based 'Indicative Usage Patterns' (IUPs), to drive interaction with an app. The project will focus on future television broadcasting, examining the extent to which it is possible to capture IUMs from device sensor and event data, and deploy these as IUPs to pull content (additional information or related activities) to a 'second screen companion app', which a viewer watches on a mobile device alongside a TV programme.

The core of the research - investigating the extent to which we can anticipate user commands and requests - will play an important role in shaping the way we use future technology. The Internet of Things is fast becoming a reality, but the appropriate paradigms for interacting with it are far from understood. A rapidly growing number of connected devices means a potentially vast increase in the types of technology which people must use successfully. Whilst a person may be prepared to learn how to use a new home automation system or car, expecting him or her to do this for every machine with which there is a cursory encounter is unrealistic. We have already started to understand that interactive experiences which take fundamental human perception and thought processes into account are more successful than those which require significant learning on the part of the user. Implicit Device Interaction, where devices automatically know what the user wants, before a command is issued, is the next step.

The scenario investigated - second screen viewing - is also particularly timely. Broadcasters are keen to exploit the creative potential of a context where people are watching television with a mobile device, not least because it is set to become the principle form of TV viewing in the next few years. At present, however, the scenario is not well understood: whilst some research has focused on social aspects of this situation, very little has examined the perceptual or behavioural aspects of second screen interaction. Current companion apps, designed to complement the main programme, push information to the mobile device at given points in time, and this change on the secondary screen, which occurs in peripheral vision, is potentially distracting. A situation where information flow intuitively stops and starts according to the location of the viewer's attention is a highly desirable goal.

IDInteraction is an ambitious research project, investigating an aspect of human behaviour that is crucial to the future development of implicit user interfaces, and has been planned to tackle the problem from end-to-end. By studying a slice of the research problem, from modelling behaviour to testing a new user interface, it will provide an overview of the theoretical and technical challenges that the development of seamless user-device interaction will entail, and flag key areas for further investigation. From the perspective of effective software development, the project entails a considerable degree of risk: IUMs may be difficult to capture and deploy, and seamless information provision may be challenging to implement in this context. From the perspective of building theory in this area, such results would still have significant value, however. Improving our understanding of the limits of implicit interaction is crucial to moving this important, emergent, field forward.</gtr:abstractText><gtr:potentialImpactText>IDInteraction will create societal and economic impacts from two perspectives.

(1) Determining the potential of using device sensor and event data to anticipate and pre-empt user commands and requests, leading to a significant shift, and potentially a major improvement, in the way in which we interact with technology.

(2) Investigating the increasingly common but under-researched home entertainment activity of watching television with a 'second screen' mobile device, and offering a rich understanding of the ways in which the viewing experience can be improved, in particular through the seamless provision of information at the moment the user requires it.

The impact of (1) is potentially very wide; the impact of (2) will be felt primarily within the domain of television broadcasting.

Key impacts, as described in the RCUK Typology, are therefore as follows:

'Contributing towards wealth creation and economic development'
IDInteraction will impact on how we use devices in the future. The models developed will be based, in the first instance, on interactivity during second screen viewing, but the techniques used for both creating Indicative Usage Models, and mapping them to software-based Indicative Usage Patterns, will have potential relevance for any interactive device. In particular, IDInteraction will contribute to our understanding of how to develop successful user interfaces for the rapidly growing 'Internet of Things', through examining anticipatory interaction with multiple networked devices. The step-change in interaction promised by this research will enhance 'economic prosperity' in the UK by improving the user experience of interactive devices, and enabling people to access information or services more quickly (even half a second saved during an interaction will have a significant effect when considered over long periods of time and across multiple users).

'Shaping and enhancing the effectiveness of a Public Service Broadcaster.'
The models and software outputs that will result from the project have already been identified by the BBC to be of direct relevance to their mission of delivering distinctive and transformational services both to the UK and internationally. The systematic investigation of second screen viewing entailed by IDInteraction and the resulting ability to provide content precisely when the viewer requires it, will both improve the user experience of watching television, and open the door to gathering a more detailed understanding of how people watch television with a mobile device.

'Enhancing the research capacity, knowledge and skills of a Public Service Broadcaster.'
The Department for Culture, Media and Sport have recently identified the production of 'world-beating, innovative content and services that originate here in the UK, but that are in demand across the globe,' as a strategic priority. UK media organisations, who have always been world leaders in broadcast technology, are finding it increasingly challenging to compete on the global stage (Future of Innovation in Television Technology Task Force Report, 2014). This research is therefore particularly timely from the perspective of supporting the UK television industry.

'Enhancing cultural enrichment and quality of life.'
The improvement to the viewing experience offered by the user-focused novel interaction paradigms investigated in IDInteration will also contribute to the Digital Economy 'Communities and Culture' theme. Watching television whilst using a mobile device is considered to be the dominant viewing environment of the future, and supporting it as effectively as possible will enhance education, enjoyment, relaxation and well-being.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-08-15</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>107252</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have demonstrated that it is possible to use object tracking to 'code' interaction behaviour (in this case, determine the location of someone's visual attention). This is significant because behavioural coding is usually done manually, by watching videos in slow motion, and is therefore extremely time-consuming, subjective and hard to quantify.</gtr:description><gtr:exploitationPathways>We have developed open source object tracking and behavioural recognition software that can be used by other researchers, and the BBC.</gtr:exploitationPathways><gtr:id>E5BC9917-4E31-43D7-A23C-803CB5D58B72</gtr:id><gtr:outcomeId>56d9bfef680461.25723941</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://idinteraction.github.io</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>Takes a directory of videos and outputs facial tracking data for each frame of each video.</gtr:description><gtr:id>F7F0715A-454A-405B-B3E0-120289335CE7</gtr:id><gtr:impact>Reproducible execution environment for generating facial tracking data</gtr:impact><gtr:outcomeId>58c12e8b0a43c6.97287197</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>OpenFace Docker Image</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>https://hub.docker.com/r/idinteraction/openface/</gtr:url></gtr:researchMaterialOutput><gtr:researchMaterialOutput><gtr:description>Takes a video and bounding box position as input, performs tracking on the object within the bounding box, and outputs a video with the bounding box overlaid, and a CSV file containing the positional and rotational information for the object.</gtr:description><gtr:id>B33BDABB-0815-4FE2-8E2E-1479D3FA537F</gtr:id><gtr:impact>Execution environment used for the research described in http://dx.doi.org/10.1145/2851581.2892483.</gtr:impact><gtr:outcomeId>56dd890378e0d4.04174326</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>IDInteraction Object Tracking Docker Image</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>https://hub.docker.com/r/idinteraction/object-tracking/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This is a modification to the CppMT object tracking tool, to output the object tracking information (position and rotation of object) in CSV format, and to allow more accurate definition of the bounding box.</gtr:description><gtr:id>FAD66917-1EED-44D5-8C04-A847D38F1EE8</gtr:id><gtr:impact>This has enabled object tracking information to be used for behavioural modelling, which has thus far been applied to determining the location of attention of people watching television with a mobile device (see http://dx.doi.org/10.1145/2851581.2892483).

Modifications have also been merged into the original CppMT tool by the original author, and are available as open source software for use and reuse by other researchers.</gtr:impact><gtr:outcomeId>56ddcf244857d7.61942623</gtr:outcomeId><gtr:title>CppMT (IDInteraction version)</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/IDInteraction/CppMT</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>CppMT-Replay takes a video and associated object tracking data output from CppMT (https://github.com/gnebehay/CppMT) and combines the two, overlaying the bounding box outline and centre point in the output video.</gtr:description><gtr:id>8B133EE9-569D-4EC6-8063-BB662C85F65E</gtr:id><gtr:impact>This software enables users to review the outputs of the object tracking of the CppMT tool by eye, to determine its accuracy. It has been used to validate the results of behavioural coding using CppMT (IDInteraction Version) by eye (see http://dx.doi.org/10.1145/2851581.2892483), as a precursor to quantitative analysis.</gtr:impact><gtr:outcomeId>56ddcf94734c27.43763942</gtr:outcomeId><gtr:title>CppMT-replay</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/IDInteraction/CppMT-replay</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>abc-display-tool allows users to classify behaviours in a random subset of video frames. External tracking data, from e.g., CppMT (https://github.com/gnebehay/CppMT) is used to construct a Decision Tree Classifier, that is used to predict behaviours in the remaining frames. The accuracy of the classifier can be evaluated using external ground-truth data, or via a cross-validation approach.</gtr:description><gtr:id>DF2240B6-C1DC-44B5-BF64-3260D6EA0E6C</gtr:id><gtr:impact>The tool has the potential to dramatically reduce the time needed to classify behaviors in video streams.</gtr:impact><gtr:outcomeId>58c03490a68465.48385624</gtr:outcomeId><gtr:title>abc-display-tool</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/IDInteraction/abc-display-tool</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>A59679E4-0A19-4C2B-8601-59E8FAEEA2A3</gtr:id><gtr:title>ABC: Using Object Tracking to Automate Behavioural Coding</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d256fe6eea99d23412f709f60637039c"><gtr:id>d256fe6eea99d23412f709f60637039c</gtr:id><gtr:otherNames>Apoalaza A</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56d9bd689ac184.07733476</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M017133/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>