<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/B0DA50CA-D11E-4251-9678-4AA2F93DB545"><gtr:id>B0DA50CA-D11E-4251-9678-4AA2F93DB545</gtr:id><gtr:name>Polytechnic University of Catalonia</gtr:name><gtr:address><gtr:line1>Polytechnic University of Catalonia</gtr:line1><gtr:line2>C/ Jordi Girona, 31</gtr:line2><gtr:line4>Barcelona</gtr:line4><gtr:line5>E-08034</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Spain</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/22748C58-39CF-4110-8F6C-B0D1D5DF85F3"><gtr:id>22748C58-39CF-4110-8F6C-B0D1D5DF85F3</gtr:id><gtr:name>?cole normale sup?rieure de Lyon (ENS Lyon)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B0DA50CA-D11E-4251-9678-4AA2F93DB545"><gtr:id>B0DA50CA-D11E-4251-9678-4AA2F93DB545</gtr:id><gtr:name>Polytechnic University of Catalonia</gtr:name><gtr:address><gtr:line1>Polytechnic University of Catalonia</gtr:line1><gtr:line2>C/ Jordi Girona, 31</gtr:line2><gtr:line4>Barcelona</gtr:line4><gtr:line5>E-08034</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Spain</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/22748C58-39CF-4110-8F6C-B0D1D5DF85F3"><gtr:id>22748C58-39CF-4110-8F6C-B0D1D5DF85F3</gtr:id><gtr:name>?cole normale sup?rieure de Lyon (ENS Lyon)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FA6E1CBA-992B-4060-8F8F-513D55A737C6"><gtr:id>FA6E1CBA-992B-4060-8F8F-513D55A737C6</gtr:id><gtr:firstName>Krystian</gtr:firstName><gtr:surname>Mikolajczyk</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK01904X%2F2"><gtr:id>C60C72EA-53D1-4FB7-909B-A78BBB51D6BB</gtr:id><gtr:title>Visual Sense. Tagging visual data with semantic descriptions</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K01904X/2</gtr:grantReference><gtr:abstractText>Recent years have witnessed an unprecedented growth in the number of image and video collections, partially due to the increased popularity of photo and video sharing websites. One such website alone (Flickr) stores billions of images. And this is not the only way in which visual content is present on the Web: in fact most web pages contain some form of visual content. However, while most traditional tools for search and retrieval can successfully handle textual content, they are not prepared to handle heterogeneous documents. This new type of content demands the development of new efficient tools for search and retrieval.


The large number of readily accessible multi-media data-collections pose both an opportunity and a challenge. The opportunity lies in the potential to mine this data to automatically discover mappings between visual and textual content. The challenge is to develop tools to classify, filter, browse and search such heterogeneous data. In brief, the data is available, but the tools to make sense of it are missing.

The Visual Sense project aims to automatically mine the semantic content of visual data to enable &amp;quot;machine reading&amp;quot; of images. In recent years, we have witnessed significant advances in the automatic recognition of visual concepts. These advances allowed for the creation of systems that can automatically generate keyword-based image annotations. However, these annotations, e.g. &amp;quot;man&amp;quot; and &amp;quot;pot&amp;quot;, fall far short of the sort of more meaningful descriptive captions necessary for indexing and retrieval of images, for example,&amp;quot;Man cooking in kitchen&amp;quot;. The goal of this project is to move a step forward and predict semantic image representations that can be used to generate more informative sentence-based image annotations, thus facilitating search and browsing of large multi-modal collections. It will address the following key open research challenges:

1) Develop methods that can derive a semantic representation of visual content. Such representations must go beyond the detection of objects and scenes and also include a wide range of object relations.
2) Extend state-of-the-art natural language techniques to the tasks of mining large collections of multi-modal documents and generating image captions using both semantic representations of visual content and object/scene type models derived from semantic representations of the textual component of multi-modal documents.
3) Develop learning algorithms that can exploit available multi-modal data to discover mappings between visual and textual content. These algorithms should be able to leverage 'weakly' annotated data and be robust to large amounts of noise.

Thus, the main focus of the Visual Sense project is the development of machine learning methods for knowledge and information extraction from large collections of visual and textual content and for the fusion of this information across modalities. The tools and techniques developed in this project will have a variety of applications. To demonstrate them, we will address three case studies: 1) evaluation of generated descriptive image captions in established international image annotation benchmarks, 2) re-ranking for improved image search and 3) automatic illustration of articles with images.

To address these broad challenges, the project will build on expertise from multiple disciplines, including computer vision, machine learning and natural language processing (NLP). It brings together four research groups from University of Surrey (Surrey, UK), Institut de Robotica i Informatica Industrial (IRI, Spain), Ecole Centrale de Lyon (ECL, France), and University of Sheffield (Sheffield, UK) having each well established and complementary expertise in their respective areas of research.</gtr:abstractText><gtr:potentialImpactText>Innovations in rich annotation of visual data directly impact the European ICT, Digital Media and Creative industries as well as having broader societal impact for any individual seeking to make sense of the huge amount of valuable, yet unstructured, image and video published online. Specifically, the project will:

Reinforce the position of the European ICT and Digital Media research, widening marketing opportunities especially for technology-providing SMEs who consume, produce or have need to search or aggregate visual data. 
Stimulate greater creativity through technologies and tools to search professional and user-generated digital media content. 
Provide digital media/service search engines with innovative offers for interactive and personalised digital media. 
Enhance opportunities for education through illustrative images and videos complementing text descriptions.
This project technologies focus on visual data search, and metadata is the key to improving image and video search capability. The development of metadata and associated tools is in its infancy, and here lies an opportunity within both the ICT and Digital Media sectors. Although metadata frequently accompanies image and video sources, it is often patchy and limited to technical data on media capture rather than on content itself. When editorial metadata is present, this is often interest-specific and in a non-standardised form. As the NEM note within their 2009 Strategic Research Agenda (NEM-SRA), &amp;quot;without metadata [visual data] content is almost valueless&amp;quot;. Our core innovations include the automated annotation of visual data to enrich source-supplied metadata with additional metadata derived automatically from visual and text content, so enabling search and aggregation. NEM-SRA notes this topic of &amp;quot;automatic video indexing&amp;quot; as one of five key promising topics for further research; another being the fusion of visual, aural and meta-data in multimedia search. These topics are explicitly addressed by WP2-WP5. Not only will enhanced techniques for semantic metadata annotation add value to Digital Media, but visual recognition also benefit other areas of ICT e.g. for surveillance, robotics, sports and medical analysis, automated manufacturing, and assisted living.

ViSen is timely in that the opportunities to exploit its outcomes are just beginning to appear. Professional customers are starting to use textual metadata tools for searching archives and managing stored content; they are already acutely aware of the need for similar and more advanced tools for live content. Consumers are becoming accustomed to searching social network sites for visual content. Other professional users
have expressed interest in the ability to search live feeds for specified content or contexts. Market conditions are favourable and will be actively monitored during the final year of ViSen through a living website reflecting routes to market and exploitation opportunities.
Project partners are in close collaboration with the BBC and SMEs, e.g. Omniperception UK, which offers multimedia search tools. We will also investigate commercial exploitation possibilities arising from ViSen with the assistance of the University's Commercialisation of IP Team and via its agreement with Fusion IP plc (http://www.fusionip.co.uk), whose mission is to seek commercial avenues to exploit IP generated in research projects by the University of Sheffield.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>58562</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Polytechnic University of Catalonia</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:description>IRI</gtr:description><gtr:id>B2FC8D0D-9846-4BAC-B537-61A7E8A400E6</gtr:id><gtr:impact>Publications, datasets, evaluation benchmarks.</gtr:impact><gtr:outcomeId>56e0a065b86e49.07115784-1</gtr:outcomeId><gtr:partnerContribution>Collection of a large news dataset for development and evaluation of computer vision and natural language processing methods.</gtr:partnerContribution><gtr:piContribution>Development of new methods for text and image analysis in news articles.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>BBC R &amp;amp; D</gtr:description><gtr:id>90ED0827-F3F7-4C59-9A2B-450DF4CF8E08</gtr:id><gtr:impact>The collaboration defined several benchmark dataset for evaluating image classification and retrieval systems. Software packages were produced for feature extraction, image classification, and tracking. 
Several publications were output during this project which are listed in the publications section.</gtr:impact><gtr:outcomeId>545cee5aacda53.53915965-1</gtr:outcomeId><gtr:partnerContribution>BBC has provided image and video data for analysis, defined user requirements, participated in the data annotation and evaluation of the developed system.</gtr:partnerContribution><gtr:piContribution>Development of image retrieval and classification for exploring video archives of the BBC and user generated content for the BBC website. The system was presented at the BBC festival of research in 2008.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>?cole normale sup?rieure de Lyon (ENS Lyon)</gtr:collaboratingOrganisation><gtr:country>France, French Republic</gtr:country><gtr:description>ECL</gtr:description><gtr:id>83B370BD-0829-4156-87DB-B69C6B47D9FB</gtr:id><gtr:impact>Software took=ls for image annotations and ImageCLEF benchmark dataset.</gtr:impact><gtr:outcomeId>56e0a5526569b5.18367488-1</gtr:outcomeId><gtr:partnerContribution>Development of image feature extractors and object detectors for image annotation and retrieval.</gtr:partnerContribution><gtr:piContribution>Development of machine learning methods for image annotations</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Sheffield</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Sheffield</gtr:description><gtr:id>E1B8FAC2-5E5B-4468-B2DD-C8016775F1A8</gtr:id><gtr:impact>Software and datasets such as Deep Canonical Correlation, ImageCLEF benchmark.</gtr:impact><gtr:outcomeId>56e0a492360de0.75450953-1</gtr:outcomeId><gtr:partnerContribution>Development of natural language processing methods for generating natural image captions.</gtr:partnerContribution><gtr:piContribution>Development of image annotation methods for vision and language</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings , software and datasets were used mainly in the scientific community but are open source and available for use by the creative industries.</gtr:description><gtr:id>79A4BA4C-24C7-4363-9DD9-E239F7744D40</gtr:id><gtr:impactTypes/><gtr:outcomeId>58c7f9a0a97e78.59313666</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Retail</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The visual sense project aims at mining automatically the semantic content of visual data to enable &amp;quot;machine reading&amp;quot; of images. In recent years, we have witnessed significant advances in the automatic recognition of visual concepts (VCR). These advances allowed for the creation of systems that can automatically generate keyword-based image annotations. The goal of this project is to move a step forward and predict semantic image representations that can be used to generate more informative sentence-based image annotations. Thus, facilitating search and browsing of large multi-modal collections. More specifically, the project targets three case studies, namely image annotation, re-ranking for image search, and automatic image illustration of articles. It addresses the following key open research challenges:

1. It developesmethods that can predict a semantic representation of visual content. This representation goes beyond the detection of objects and scenes and also recognizes a wide range of object relations.

2. It extends state-of-the-art natural language techniques to the tasks of mining large collections of multi-modal documents and generating image captions using both semantic representations of visual content and object/scene type models derived from semantic representations of the multi-modal documents.

3. It develops learning algorithms that can exploit available multi-modal data to discover mappings between visual and textual content. These algorithms should be able to leverage 'weakly' annotated data and be robust to large amounts of noise.</gtr:description><gtr:exploitationPathways>Image Annotation Benchmarks can be used for evaluating approaches for annotation and retrieval of visual documents. This work provides quantitative results in comparison to currently available methods.

Re-ranking for Image Search is a system that can improve image retrieval quality on complex queries such as ' a man cooking pasta'. The search engine generates a rich semantic representation of each database image and uses it to re-rank the output of a baseline image search engine.

Illustrating Articles involves creating a system that can take an article or a blog entry and find images that are likely to be a good illustration for that article. This can generate useful textual and visual content representations and to map between them. 

These techniques broaden marketing opportunities especially for technologyproviding SMEs who consume, produce or have need to search or aggregate visual data.
It can stimulate greater creativity through technologies and tools to search professional and usergenerated digital media content.
Provide digital media/service search engines with innovative offers for interactive and personalised digital media.
Enhance opportunities for education through illustrative images and videos complementing text descriptions.</gtr:exploitationPathways><gtr:id>1827C3E5-27F7-4F3B-8756-86176935A7B4</gtr:id><gtr:outcomeId>58c7f84621da11.96975213</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Environment,Culture, Heritage, Museums and Collections,Retail,Transport,Other</gtr:sector></gtr:sectors><gtr:url>http://www.iis.ee.ic.ac.uk/~vbalnt/shallow_descr/index.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>ImageCLEF 2015, 2016 benchmark data for image annotation tasks.</gtr:description><gtr:id>56D7651D-9F0E-4ED2-AE68-D237EE65DF28</gtr:id><gtr:impact>Used by many internationally recognised research labs for evaluation of their computer vision approaches.</gtr:impact><gtr:outcomeId>56e0a3a868ecd1.98974953</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>ImageCLEF</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.imageclef.org/2015/annotation</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>BreakingNews, a novel dataset with approximately 100K news articles including images, text
and captions, and enriched with heterogeneous meta-data (such as GPS coordinates and user comments). We show this dataset
to be appropriate to explore intersection of computer vision and
natural language processing have achieved unprecedented breakthroughs in tasks like automatic captioning or image retrieval.</gtr:description><gtr:id>165C6ED5-00DF-4F22-8E9F-980D2485541F</gtr:id><gtr:impact>This is the largest existing database for analysing visual and natural language content in form of news articles rather than short captions. The database has already been used and referenced by other research labs.</gtr:impact><gtr:outcomeId>58c7f4b4369052.62189231</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>BreakingNews: Article Annotation by Image and Text Processing</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.iri.upc.edu/people/aramisa/BreakingNews/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Low level image descriptors for large scale matching and retrieval</gtr:description><gtr:id>96BA81DD-E3BB-4A45-8A52-0C564FE63184</gtr:id><gtr:impact>Has been used by other researchers in machining and retrieval applications</gtr:impact><gtr:outcomeId>56e0a2dc510b67.21029657</gtr:outcomeId><gtr:title>BOLD</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/vbalnt/bold</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>It has recently been demonstrated that local feature descriptors based on convolutional neural networks (CNN) can significantly improve the matching performance. Previous work on learning such descriptors has focused on exploiting pairs of positive and negative patches to learn discriminative CNN representations. In this code, w utilize triplets of training samples, together with in-triplet mining of hard negatives. This implementation achieves state of the art results, without the computational overhead typically associated with mining of negatives and with lower complexity of the network architecture. We compare our approach to recently introduced convolutional local feature descriptors, and demonstrate the advantages of the proposed methods in terms of performance and speed. We also examine different loss functions associated with triplets.</gtr:description><gtr:id>D438B4DB-7345-4D3A-A14B-26CA533D81CB</gtr:id><gtr:impact>The method gives state of the art results in standard matching benchmarks. It has been followed by 32 developers in Github and forked by 26 which extended this approach.</gtr:impact><gtr:outcomeId>58c7f649523ca2.76467328</gtr:outcomeId><gtr:title>TFeat</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/vbalnt/tfeat</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>16529C7E-9DC4-4853-80DC-292E0094A170</gtr:id><gtr:title>Binary Online Learned Descriptors</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/931443eb7388e9495a76a89eb97c303e"><gtr:id>931443eb7388e9495a76a89eb97c303e</gtr:id><gtr:otherNames>V. Baltnas</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c957a1afe283.62667398</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA51A4C4-E1D2-41DD-BB3F-AEEFCFFDDD56</gtr:id><gtr:title>Binary Online Learned Descriptors.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62700cf1ac424352a942168a3f7254b8"><gtr:id>62700cf1ac424352a942168a3f7254b8</gtr:id><gtr:otherNames>Balntas V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5a578b09608fa6.90710326</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DEEBAFCB-EC88-477C-A4F6-3C891E53FADC</gtr:id><gtr:title>Learning local feature descriptors with triplets and shallow convolutional neural networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/157418722878442191fa74a84a1b99f3"><gtr:id>157418722878442191fa74a84a1b99f3</gtr:id><gtr:otherNames>V. Balntas</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c9581c1318c0.16127576</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>58902002-53D7-4750-B3B4-4458719403BC</gtr:id><gtr:title>BOLD - Binary online learned descriptor for efficient image matching</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62700cf1ac424352a942168a3f7254b8"><gtr:id>62700cf1ac424352a942168a3f7254b8</gtr:id><gtr:otherNames>Balntas V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e09d77c7b3f5.09875260</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>460F39DA-7620-41D3-8CEE-BF1A50A3BFBA</gtr:id><gtr:title>Deep correlation for matching images and text</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8bb95557325dc92e149eaa32cb7a250"><gtr:id>a8bb95557325dc92e149eaa32cb7a250</gtr:id><gtr:otherNames>Yan F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e09d77a79f17.60161367</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A66469B-3E61-49DD-AA56-0F6469A0D24A</gtr:id><gtr:title>Full ranking as local descriptor for visual recognition: A comparison of distance metrics on sn</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd0cd8cbc0f4907c2ee85c98c1972869"><gtr:id>fd0cd8cbc0f4907c2ee85c98c1972869</gtr:id><gtr:otherNames>Chan C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e09d77e4cd47.17034840</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K01904X/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>65020977-180A-4F73-849A-E875DA942AFC</gtr:id><gtr:grantRef>EP/K01904X/1</gtr:grantRef><gtr:amount>331317.28</gtr:amount><gtr:start>2013-02-08</gtr:start><gtr:end>2015-08-31</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>C60C72EA-53D1-4FB7-909B-A78BBB51D6BB</gtr:id><gtr:grantRef>EP/K01904X/2</gtr:grantRef><gtr:amount>58562.08</gtr:amount><gtr:start>2015-12-01</gtr:start><gtr:end>2016-09-30</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>