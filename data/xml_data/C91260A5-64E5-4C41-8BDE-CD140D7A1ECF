<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C0805014-A6FD-41C1-81BE-B94AE4139F25"><gtr:id>C0805014-A6FD-41C1-81BE-B94AE4139F25</gtr:id><gtr:name>Max Planck Society</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/20CDC832-4950-4C53-B6F4-CF756C778325"><gtr:id>20CDC832-4950-4C53-B6F4-CF756C778325</gtr:id><gtr:name>Drake Music</gtr:name><gtr:address><gtr:line1>60 - 61 Old Nichol Street</gtr:line1><gtr:postCode>E2 7HP</gtr:postCode><gtr:region>London</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/6C97AF45-BB9F-4A5F-BC36-F5994FE9BAE9"><gtr:id>6C97AF45-BB9F-4A5F-BC36-F5994FE9BAE9</gtr:id><gtr:name>Catholic University of Portugal</gtr:name><gtr:address><gtr:line1>Palma de Cima</gtr:line1><gtr:postCode>P-1649-023</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9218106A-E8CE-46A5-AABC-B4C8ED148690"><gtr:id>9218106A-E8CE-46A5-AABC-B4C8ED148690</gtr:id><gtr:name>University of Amsterdam</gtr:name><gtr:address><gtr:line1>Liaison Office</gtr:line1><gtr:line2>PO Box 19268</gtr:line2><gtr:line4>Amsterdam</gtr:line4><gtr:line5>Amsterdam</gtr:line5><gtr:postCode>1000 GG</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Netherlands</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D7ABADDF-9FA8-420E-B619-3CCB05095D12"><gtr:id>D7ABADDF-9FA8-420E-B619-3CCB05095D12</gtr:id><gtr:name>London Chamber Orchestra</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/BB12A8E2-9FD6-47D7-ACD3-CDD2D2C5A1BB"><gtr:id>BB12A8E2-9FD6-47D7-ACD3-CDD2D2C5A1BB</gtr:id><gtr:name>Aalborg University</gtr:name><gtr:address><gtr:line1>Fredrik Bajers Vej 5</gtr:line1><gtr:line2>PO Box 159</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/6BF54710-98D0-4EBA-888C-B842615FDD01"><gtr:id>6BF54710-98D0-4EBA-888C-B842615FDD01</gtr:id><gtr:name>Hunan Agricultural University, Changsha.</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D394BCC7-2CE9-41C0-A124-1A05C8E4FD6E"><gtr:id>D394BCC7-2CE9-41C0-A124-1A05C8E4FD6E</gtr:id><gtr:name>City, University of London</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/7C777568-AEA8-464E-8AF8-B1DFE6358842"><gtr:id>7C777568-AEA8-464E-8AF8-B1DFE6358842</gtr:id><gtr:name>Bar-Ilan University</gtr:name><gtr:address><gtr:line1>Bar-Ilan University</gtr:line1><gtr:line4>Ramat-Gan</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Israel</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/FD32C73C-B676-49A9-B2B1-F95871AC62B6"><gtr:id>FD32C73C-B676-49A9-B2B1-F95871AC62B6</gtr:id><gtr:name>University of Piraeus</gtr:name><gtr:address><gtr:line1>80 Karaoli and Dimitriou St</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C43F49D5-9E05-4A01-AE8A-7C68B8AC37BF"><gtr:id>C43F49D5-9E05-4A01-AE8A-7C68B8AC37BF</gtr:id><gtr:name>Guildhall School of Music and Drama</gtr:name><gtr:address><gtr:line1>Silk Street</gtr:line1><gtr:line2>Barbican</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>EC2Y 8DT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/86A3E46F-01F0-4CBF-8692-6345AA2CC0D3"><gtr:id>86A3E46F-01F0-4CBF-8692-6345AA2CC0D3</gtr:id><gtr:name>British Library The</gtr:name><gtr:address><gtr:line1>96 Euston Road</gtr:line1><gtr:postCode>NW1 2DB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1923C0A2-81E7-4040-BDDB-FBD9CF7DACBF"><gtr:id>1923C0A2-81E7-4040-BDDB-FBD9CF7DACBF</gtr:id><gtr:name>London Sinfonietta</gtr:name><gtr:address><gtr:line1>Kings Place</gtr:line1><gtr:line2>90 York Way</gtr:line2><gtr:postCode>N1 9AG</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/EF859C01-3BEE-4198-94C6-9B7E7D64428C"><gtr:id>EF859C01-3BEE-4198-94C6-9B7E7D64428C</gtr:id><gtr:name>Ecole des Mines de Saint-etienne (ENSMSE)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/F574E08B-5D1F-4585-A88E-A385E68E4F80"><gtr:id>F574E08B-5D1F-4585-A88E-A385E68E4F80</gtr:id><gtr:name>Science Museum Group</gtr:name><gtr:address><gtr:line1>Science Museum</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2DD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C0805014-A6FD-41C1-81BE-B94AE4139F25"><gtr:id>C0805014-A6FD-41C1-81BE-B94AE4139F25</gtr:id><gtr:name>Max Planck Society</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/20CDC832-4950-4C53-B6F4-CF756C778325"><gtr:id>20CDC832-4950-4C53-B6F4-CF756C778325</gtr:id><gtr:name>Drake Music</gtr:name><gtr:address><gtr:line1>60 - 61 Old Nichol Street</gtr:line1><gtr:postCode>E2 7HP</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6C97AF45-BB9F-4A5F-BC36-F5994FE9BAE9"><gtr:id>6C97AF45-BB9F-4A5F-BC36-F5994FE9BAE9</gtr:id><gtr:name>Catholic University of Portugal</gtr:name><gtr:address><gtr:line1>Palma de Cima</gtr:line1><gtr:postCode>P-1649-023</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9218106A-E8CE-46A5-AABC-B4C8ED148690"><gtr:id>9218106A-E8CE-46A5-AABC-B4C8ED148690</gtr:id><gtr:name>University of Amsterdam</gtr:name><gtr:address><gtr:line1>Liaison Office</gtr:line1><gtr:line2>PO Box 19268</gtr:line2><gtr:line4>Amsterdam</gtr:line4><gtr:line5>Amsterdam</gtr:line5><gtr:postCode>1000 GG</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Netherlands</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D7ABADDF-9FA8-420E-B619-3CCB05095D12"><gtr:id>D7ABADDF-9FA8-420E-B619-3CCB05095D12</gtr:id><gtr:name>London Chamber Orchestra</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BB12A8E2-9FD6-47D7-ACD3-CDD2D2C5A1BB"><gtr:id>BB12A8E2-9FD6-47D7-ACD3-CDD2D2C5A1BB</gtr:id><gtr:name>Aalborg University</gtr:name><gtr:address><gtr:line1>Fredrik Bajers Vej 5</gtr:line1><gtr:line2>PO Box 159</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6BF54710-98D0-4EBA-888C-B842615FDD01"><gtr:id>6BF54710-98D0-4EBA-888C-B842615FDD01</gtr:id><gtr:name>Hunan Agricultural University, Changsha.</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D394BCC7-2CE9-41C0-A124-1A05C8E4FD6E"><gtr:id>D394BCC7-2CE9-41C0-A124-1A05C8E4FD6E</gtr:id><gtr:name>City, University of London</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7C777568-AEA8-464E-8AF8-B1DFE6358842"><gtr:id>7C777568-AEA8-464E-8AF8-B1DFE6358842</gtr:id><gtr:name>Bar-Ilan University</gtr:name><gtr:address><gtr:line1>Bar-Ilan University</gtr:line1><gtr:line4>Ramat-Gan</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Israel</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FD32C73C-B676-49A9-B2B1-F95871AC62B6"><gtr:id>FD32C73C-B676-49A9-B2B1-F95871AC62B6</gtr:id><gtr:name>University of Piraeus</gtr:name><gtr:address><gtr:line1>80 Karaoli and Dimitriou St</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C43F49D5-9E05-4A01-AE8A-7C68B8AC37BF"><gtr:id>C43F49D5-9E05-4A01-AE8A-7C68B8AC37BF</gtr:id><gtr:name>Guildhall School of Music and Drama</gtr:name><gtr:address><gtr:line1>Silk Street</gtr:line1><gtr:line2>Barbican</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>EC2Y 8DT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/86A3E46F-01F0-4CBF-8692-6345AA2CC0D3"><gtr:id>86A3E46F-01F0-4CBF-8692-6345AA2CC0D3</gtr:id><gtr:name>British Library The</gtr:name><gtr:address><gtr:line1>96 Euston Road</gtr:line1><gtr:postCode>NW1 2DB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1923C0A2-81E7-4040-BDDB-FBD9CF7DACBF"><gtr:id>1923C0A2-81E7-4040-BDDB-FBD9CF7DACBF</gtr:id><gtr:name>London Sinfonietta</gtr:name><gtr:address><gtr:line1>Kings Place</gtr:line1><gtr:line2>90 York Way</gtr:line2><gtr:postCode>N1 9AG</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF859C01-3BEE-4198-94C6-9B7E7D64428C"><gtr:id>EF859C01-3BEE-4198-94C6-9B7E7D64428C</gtr:id><gtr:name>Ecole des Mines de Saint-etienne (ENSMSE)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F574E08B-5D1F-4585-A88E-A385E68E4F80"><gtr:id>F574E08B-5D1F-4585-A88E-A385E68E4F80</gtr:id><gtr:name>Science Museum Group</gtr:name><gtr:address><gtr:line1>Science Museum</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2DD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A3C215EE-5645-4543-ACD7-878BFB368AC7"><gtr:id>A3C215EE-5645-4543-ACD7-878BFB368AC7</gtr:id><gtr:firstName>Panos</gtr:firstName><gtr:surname>Kudumakis</gtr:surname><gtr:orcidId>0000-0003-0518-4198</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/1C1BF8EF-D607-4DA9-B396-CBBAAA1E8446"><gtr:id>1C1BF8EF-D607-4DA9-B396-CBBAAA1E8446</gtr:id><gtr:firstName>Matthew</gtr:firstName><gtr:surname>Purver</gtr:surname><gtr:orcidId>0000-0003-2297-1273</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/C29F5E21-9A4A-4904-B47D-A257EB274AD8"><gtr:id>C29F5E21-9A4A-4904-B47D-A257EB274AD8</gtr:id><gtr:firstName>Samer</gtr:firstName><gtr:surname>Abdallah</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B25E8DB2-C5FE-4179-AE1D-7EE91BDCFA92"><gtr:id>B25E8DB2-C5FE-4179-AE1D-7EE91BDCFA92</gtr:id><gtr:firstName>Daniel</gtr:firstName><gtr:otherNames>Frank</gtr:otherNames><gtr:surname>Stowell</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F54C6961-678D-4229-9960-4F1F6D31BF56"><gtr:id>F54C6961-678D-4229-9960-4F1F6D31BF56</gtr:id><gtr:firstName>Marcus</gtr:firstName><gtr:otherNames>Thomas</gtr:otherNames><gtr:surname>Pearce</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F08D5405-BBCD-42A2-BBD4-0021935C68C4"><gtr:id>F08D5405-BBCD-42A2-BBD4-0021935C68C4</gtr:id><gtr:firstName>Tony</gtr:firstName><gtr:surname>Stockman</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2026A4F7-C872-4123-9247-CA1646190CF3"><gtr:id>2026A4F7-C872-4123-9247-CA1646190CF3</gtr:id><gtr:firstName>Elaine</gtr:firstName><gtr:surname>Chew</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0C941CCA-2B6E-4ADC-B915-3543BAEF5164"><gtr:id>0C941CCA-2B6E-4ADC-B915-3543BAEF5164</gtr:id><gtr:firstName>Chris</gtr:firstName><gtr:surname>Cannam</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/62D3CD4B-2318-4A02-B23E-646358C34B6D"><gtr:id>62D3CD4B-2318-4A02-B23E-646358C34B6D</gtr:id><gtr:firstName>Geraint</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Wiggins</gtr:surname><gtr:orcidId>0000-0002-1587-112X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7AC2BED0-677B-4CF0-8D99-5E42D5DF1D28"><gtr:id>7AC2BED0-677B-4CF0-8D99-5E42D5DF1D28</gtr:id><gtr:firstName>Patrick</gtr:firstName><gtr:otherNames>George</gtr:otherNames><gtr:surname>Healey</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/27621176-A325-4AF5-BF39-E1D235FA912B"><gtr:id>27621176-A325-4AF5-BF39-E1D235FA912B</gtr:id><gtr:firstName>Maria</gtr:firstName><gtr:otherNames>Grazia</gtr:otherNames><gtr:surname>Jafari</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/058B7945-94E9-450F-A970-E88BA39F33D9"><gtr:id>058B7945-94E9-450F-A970-E88BA39F33D9</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Johnathan</gtr:otherNames><gtr:surname>Bryan-Kinns</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK009559%2F1"><gtr:id>C91260A5-64E5-4C41-8BDE-CD140D7A1ECF</gtr:id><gtr:title>Platform Grant: Digital Music</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K009559/1</gtr:grantReference><gtr:abstractText>The Centre for Digital Music (C4DM) at Queen Mary University of London is a world-leading multidisciplinary research group in the field of Music &amp;amp; Audio Technology. Established in 2001, and now with over 60 members, including academics, research staff/fellows, research students, and regular visitors, we are arguably the largest group in this area in the world. Much of our research is within the important RCUK Digital Economy / Communities and Culture theme, and our core competence in Digital Signal Processing has already been identified by EPSRC as &amp;quot;important to the UK&amp;quot; and an area for growth. The UK has the largest creative sector in Europe, and one of the world's largest music industries (CBI: Creating Growth, 2010).
Over the last 5 years, the music industry has undergone massive changes, with new digital and streaming services such as Spotify opening up new opportunities for startup businesses and SMEs, as well as large-scale music research. In parallel with this, as part of a strategic investment in creative technology and digital media at Queen Mary (&amp;quot;qMedia&amp;quot;), we have expanded our group to strengthen our expertise in music perception &amp;amp; cognition, music &amp;amp; language, and performance, to complement our existing strengths in signal processing, music informatics, and interaction.
This aim of this Platform Grant is to provide underpinning funding to allow us to continue to take a strategic view of our research. Our short- and medium-term strategy is threefold: &amp;quot;Music is out there&amp;quot; (Exo) plus &amp;quot;Music is in here&amp;quot; (Endo), built on underlying core competencies such as in digital signal processing (Core). We will do this through: undertaking adventurous feasibility studies, allowing rapid progress towards our strategic objectives; retaining strategic skills in Key RAs, maintaining important strengths and competencies in the group; strengthening our international links and building new interdisciplinary links through internship and outreach activities; maximizing the impact of our research through a new knowledge exchange strategy, including business partnerships and reproducible software and data; and undertaking public engagement activities to promote our research to a wide audience. We will also continue to help our RAs successfully develop their careers towards academia or industry, through activities such as mentoring, guest lectures, event organization, exchange visits and training courses, and we will also introduce new mentoring schemes for Investigators to maximize their potential.
Over the 5 years of the proposed Platform Grant, this concerted programme of research will achieve a step-change in modelling of human music processing and in its application to challenging industrial-scale digital music opportunities.</gtr:abstractText><gtr:potentialImpactText>Potential beneficiaries of the research supported by this Platform Grant outside of the academic research community include anyone who could benefit from latest research in audio and music. Examples from various sectors are given below. 

Commercial private sector: 
* Commercial companies designing audio equipment, through easier access to new audio research
* Companies wishing to provide new ways for their customers to discover or access music based on music recommendation research
* Musicians and sound artists, through their ability to use new research methods and processes for new creative outputs
* Computer games companies, for improved game music and audio, or for new types of audio or music-based computer games 
* Hearing aid and cochlear implant manufacturers, through access to new research applicable to hearing improvement 
* Audio archiving companies, through access to the latest algorithms and methods for music information retrieval 
* Television and radio companies, through ability to use the latest research in the design of new technology-based programmes or in new audio production processes.

Policy-makers and others in government and government agencies: 
* Police and security services, through access to methods for analysing and separating audio signals.

Public sector, third sector and others: 
* Healthcare workers who work with music and audio, such as music therapists, through new music and audio visualization tools 
* Museums, through tools to measure and visualise acoustic properties of objects 
* Libraries, through improved open access to our research results for the benefit of their users
* Standards organizations, through access to new research methods on which to base forthcoming standards 
* Science promotion organizations, through availability of high-quality usable research tools attractive to people who may be interested in science.

Wider public: 
* People interested in exploring music or other audio recordings at home, school, college or university, using the latest research, either for educational or general interest purposes 
* Teachers in schools, colleges or universities who want to use our research-based software for teaching audio or music
* Audiences of creative output involving audio and music, through availability of new creative outputs or technology facilitated by our audio and digital music research.

Researchers employed on the project, other members of the group, or visitors to the group: 
* Improved skills in research in the digital music sector, which may be transferred into e.g. the commercial private sector when they leave the group.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-01-20</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-01-21</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1161334</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>The event took place on Monday 23 November. The full performance lasted 90 mins. Featuring music by Stephen Travis Pope, Oded Ben-Tal, Dan Stowell, and Bob L. Sturm.</gtr:description><gtr:id>1A2CE345-72D6-4459-9ABF-0FF8CF3E8B84</gtr:id><gtr:impact>C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrates the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.</gtr:impact><gtr:outcomeId>56ddb61e8646e4.60826713</gtr:outcomeId><gtr:title>Computer Music! by Stephen Travis Pope and other deep neural networks (curated by Bob Sturm)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://sites.google.com/site/c4dmconcerts1516/home/fixedmedia/computermusic</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Mood Conductor is an audience-performer interactive system developed to let audience members contribute to the creation of improvised performances by conveying emotional cues to performers. The system relies on a web app and a visualisation system. The Mood Conductor app can be used from mobile devices or laptops. Mood Conductor was selected as the single artistic event of the fifth biannual Humaine Association Conference on Affective Computing and Intelligent Interaction (ACII 2013) which was held in Geneva, Switzerland on September 2-5, 2013. The performance was held in the International Conference Centre of Geneva (ICCG), near the United Nations headquarters. The performance was given by a professional vocal quartet (VoXP) from Strasbourg (France) with whom we collaborated.</gtr:description><gtr:id>42D119C8-6E25-4EAC-8548-C52994B8FDF3</gtr:id><gtr:impact>The performance was very successful and a large number of audience members attended it (about 250 people). More than 100 audience members interacted with our Mood Conductor technology during the performance, creating a novel type of concert experience. We received a lot of very positive feedback on the system from the Affective Computing community gathered by the ACII conference.</gtr:impact><gtr:outcomeId>54635782a491e3.50238313</gtr:outcomeId><gtr:title>Mood Conductor Performance at ACII2013</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://www.flickr.com/photos/matdiffusion/sets/72157634162667686/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>The event took place on Monday 1 February 2016. The full performance lasted 120 mins. The Chinese New Year Concert is organised jointly by the Chinese Students' Association (CSSA) and Confucius Institute at Queen Mary. It not only celebrates Chinese New Year, but celebrates the fusion of arts and science at Queen Mary, combining music and performance with digital instruments. The 2016 event featured award-winning Chinese instrumentalists, C4DM members and the Magnetic Resonator Piano and the Mood Rater music technology.</gtr:description><gtr:id>C66082D4-F4BE-4C1B-BBA3-6223226BA102</gtr:id><gtr:impact>C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrates the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.</gtr:impact><gtr:outcomeId>56ddb70d0af297.44316543</gtr:outcomeId><gtr:title>Chinese New Year Concert (curated by Luwei Yang)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>http://www.qmul-cssa.org.uk/concert2016/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Five new works by young composers are workshopped in an open rehearsal with a London Chamber Orchestra ensemble, conductor Fergus Macleod, composer Cheryl Frances-Hoad, and researchers Elaine Chew, Andrew McPherson, and Andrew Robertson from QMUL's Centre for Digital Music.</gtr:description><gtr:id>359B734D-E0E9-4D46-AC07-CA7CF8D9E0D0</gtr:id><gtr:impact>Music and technological education and outreach to the university population.</gtr:impact><gtr:outcomeId>54630c96d52459.64245619</gtr:outcomeId><gtr:title>Inspired by Digital: Workshop Day</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>http://www.qmul.ac.uk/publicengagement/activities/items/89531.html</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Open Symphony (OS) is a novel technology-mediated immersive music performance system which reimagines the music experience for a digital age. Open Symphony promotes person-person engagement and allows the social benefits of music-making to foster a meaningful mutual experience. The idea was conceived to acknowledge and capitalise on a listener's ever-growing digital presence within live music settings; designing a system to resolve the disconnect between reality and digital-reality and transform digital technology's possibilities to encourage active creative experiences.

Western performing arts practices have traditionally restricted creative interactions from audiences. Open Symphony was designed to explore audience-performer interaction in live music performance assisted by digital technology. The Open Symphony system transforms the traditional unidirectional musical chain by adding creative communication in the reverse direction, from the audience to performers.

During the project, the system was experienced by about 250 audience and performer participants in controlled (lab) and &amp;quot;real world&amp;quot; settings. Feedback on usability and user experience was overall positive and live interactions demonstrated significant levels of audience creative engagement. We identified further design challenges around audience sense of control, learnability and compositional structure. Both audience and performer participants who trialed the system felt challenged by the novelty-audience members took creative decisions and decoded their effects, and performers followed scores generated while they were playing. Such challenges lead to positive affective responses for some audience participants, who felt engaged in the performance and closely connected to performers, and to frustration for others, who misunderstood the system or wished to have more control. Open Symphony showed the potential to create performances that are &amp;quot;open,&amp;quot; &amp;quot;engaging,&amp;quot; &amp;quot;empowering,&amp;quot; and &amp;quot;unusual&amp;quot;. Our results highlighted the potential of our system for music pedagogy and audience engagement agendas.</gtr:description><gtr:id>59709A68-7581-46BE-9BB0-D92DC5099D48</gtr:id><gtr:impact>The Platform Grant (PG) funding enabled to (i) evaluate and improve our participatory performance digital technology by collaborating with performers and audiences outside academia through focus groups and concerts, and to (ii) disseminate our research as part of public engagement events (about 250 participants) as well as academic publications.

Our audience can be segmented into expert musicians and non experts from a wide range of domains (school, university, creative industry, design, engineering, administration) and age (11-50+). Thanks to the Platform Grant, our work was published and presented in the tier 1 ACM Conference on Human Factors in Computing Systems (CHI), 7-12 May 2016, California (USA). We collaborated with musicians from University of California Berkeley and organised a series of interactive performances and an evaluation study at CHI 2016. This helped strengthen an ongoing publication which was accepted in a high impact factor journal in our field after we included the new results we obtained thanks to the grant (IEEE Multimedia Magazine, Impact Factor February 2017: 1.36).

The PG award also allowed two PhD students to attend talks on state-of-the-art in Human Computer Interaction from top universities and companies on a variety of topics, such as ubiquitous computing, visualization, usability and user experience design. This proved a useful source of inspiration for the students' PhD research directions.

We have also raised the awareness of QMUL and our EPSRC FAST-IMPACt Program Grant project at the international ACM CHI conference by showcasing short demo videos from our group at the booth.

The award supported the participation of QMUL PhD students Yongmeng Wu and Leshao Zhang in the tier 1 ACM Conference on Human Factors in Computing Systems (CHI), 7-12 May 2016, California (USA).

We hired professional musicians (eight in total) for our concerts using complementary funding sources (AHRC Audience Labs and QMUL Centre for Public Engagement grants, PI: Barthet).

In 2015-2016, this project was awarded the Open Symphony, AHRC Sound and Music Organisation Audience Labs Award (only sixth UK projects selected).</gtr:impact><gtr:outcomeId>58aececae85ed9.05365148</gtr:outcomeId><gtr:title>Open Symphony Audience Labs project (M Barthet, C4DM)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Friday 24 June 2016, 7.30pm
Schott Music Shop (48 Great Marlborough St) - performance-dialogue: 75 mins
(an educational event created in collaboration with Kingston University)
Elaine Chew presented the world premiere of Oded Ben-Tal's Sonata for Magnetic Resonator Piano and Electronics together with performances of contemporary piano pieces by Carmine Cella, Cheryl Frances-Hoad, and Dorien Herreman's computer program MorpheuS. The concert featured dual performances of pieces, with and without the magnetic resonator piano (MRP), interspersed with performer/composer presentations on the making of the music. MRP inventor Andrew McPherson also gave an introduction to the instrument. Audience members had a chance to engage in further dialogue with the creators in an open Q&amp;amp;A.</gtr:description><gtr:id>727029EB-4FAA-4B38-AB38-3CCA3D3CF7DA</gtr:id><gtr:impact>C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrated the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.</gtr:impact><gtr:outcomeId>58b573ffefea87.84489721</gtr:outcomeId><gtr:title>With/Without (curated by Elaine Chew) - C4DM Concert Series 2015-2016</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://sites.google.com/site/c4dmconcerts1516/home/classical/with_without</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Mycelia is an experimental project by the artist Imogen Heap based on a fair trade music eco-system. 

As quoted in the Guardian: &amp;quot;Imogene Heap's current project is to bring to life an entirely new landscape for distributing and monetising music and all its related data and content. Spurred on by the technology originally designed by libertarians to create the crypto-currency bitcoin, she's releasing her next song, Tiny Human, as an event and an experiment. What she hopes to emerge is the core of a revolutionary system she refers to as Mycelia. It could completely transform the music industry.&amp;quot;</gtr:description><gtr:id>E49CAA51-FE4C-4812-B6BE-04FC11DC78AC</gtr:id><gtr:impact>Imogen Heap (UK) is releasing raw music material (i.e., multi-track audio, lyrics, album cover and credits) suitable for IM AF encoding. The IM AF format is part of a software application developed by C4DM (see appropriate section of this report for further details of the software).</gtr:impact><gtr:outcomeId>58ac56176385b4.37369590</gtr:outcomeId><gtr:title>Mycelia by Imogene Heap</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:url>http://myceliaformusic.org/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>RTSFX (Real-Time Sound Effects) is an online library of real-time synthesised (rather than sampled) sound effects for a variety of different objects and environmental sounds;</gtr:description><gtr:id>B82CDF4A-BC46-4D94-B758-309076536196</gtr:id><gtr:impact>Report from Will Wiklinson, C4DM PhD student:

For our RTSFX project, the most encouraging thing was the number of people that interacted with our sound effects demo and how easily they grasped the concept, even if they had no experience in the area. This was the aim of the project - to engage non-technical people in a conversation about sound synthesis and why it is worth pursuing - to help them understand its potential and encourage more interest / investment in synthesis research.

Around the same time / just after Sonar our sound effects website was presented to industry professionals (sound designers working on film/tv), and observing so many non-technical people playing with the site at Sonar helped me understand better the way people will interact with it and lead to some tweaks being made to make the experience more reliable and enjoyable.</gtr:impact><gtr:outcomeId>58ac4d8a21f363.98575206</gtr:outcomeId><gtr:title>C4DM RTSFX project at SONAR+D 2016 festival</gtr:title><gtr:type>Artistic/Creative Exhibition</gtr:type><gtr:url>https://sonar.es/en/2016/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Wednesday 4 November 2015, 7pm
This event is part of the C4DM Concert Series 2015-2016. 

The full performance lasted 120 mins. Laurel Pardue performed on an augmented violin with live visuals. Dianne Verdonk performed on &amp;quot;La Diantenne&amp;quot;, a new metal-sheet instrument capturing rich gestural interactions. Tim Exile performed on his &amp;quot;Flow Machine&amp;quot;, a hardware/software setup for live sampling and transformation of sound. Bogdan Vera and friends performs on the D-Box. The performance was coupled with a study of audience response conducted by C4DM/MAT PhD student Astrid Bin. The audience was introduced to the instruments before the performance and record their responses during the performance using a web-based system developed by Astrid.</gtr:description><gtr:id>18737F20-CCDB-4B2C-B253-62A52007E009</gtr:id><gtr:impact>C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrates the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.</gtr:impact><gtr:outcomeId>56ddb5bf3aa7e7.66353391</gtr:outcomeId><gtr:title>New Interfaces Concert (curated by Andrew McPherson and Astrid Bin)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://sites.google.com/site/c4dmconcerts1516/home/newinterfaces/newinterfacesconcert</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Maria participated in the monthly Spotify music hackathon held at Spotify New York. She collaborated with two other researchers and created folkviz (https://github.com/nazareno/folkmusicmap) which stands for folk music visualization. This project explores three ways to visualize results from undergoing folk music research and facilitate the exploration of music similarity in big data collections.</gtr:description><gtr:id>92957A0E-5CB8-4A74-8EE3-4EF425A0E5D1</gtr:id><gtr:impact>Not known.</gtr:impact><gtr:outcomeId>58ab0c8a8cd924.53697010</gtr:outcomeId><gtr:title>Participation in the monthly Spotify Music hackaton. Spotify New York</gtr:title><gtr:type>Artistic/Creative Exhibition</gtr:type><gtr:url>https://github.com/nazareno/folkmusicmap</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>The event took place on Monday 21 December 2015. The full performance lasted 60 mins. As Stravinsky famously said, &amp;quot;Lesser artists borrow, great artists steal.&amp;quot; The art of stealing was illustrated by Bob Sturm's Concatenative Variations of a Passage by Mahler and by Paul Lansky's Etudes and Parodies (winner of the 2005 International Horn Society competition) for horn trio as performed by Joel Ashford, Hilary Sturt, and Elaine Chew. Part of Lansky's piece was compared to the Beatles' While My Guitar Gently Weeps, as performed by Florian Guillaume, Giulio Moro, Keunwoo Choi, and Elio Quenton, culminating in a concurrent performance of the two. Musical similarities were made explicit using C4DM research in music informatics.</gtr:description><gtr:id>7E88BD4C-F7FB-4C54-BBD7-5EAD7E80D00E</gtr:id><gtr:impact>C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrates the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.</gtr:impact><gtr:outcomeId>56ddb687e82d50.65837990</gtr:outcomeId><gtr:title>Musical Banditry (curated by Elaine Chew)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://sites.google.com/site/c4dmconcerts1516/home/classical/musicalbanditry</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Mood Conductor is an audience-performer interactive system developed to let audience members contribute to the creation of improvised performances by conveying emotional cues to performers. The system relies on a web app and a visualisation system. The Mood Conductor app can be used from mobile devices or laptops. Two public performances were held with the Mood Conductor system during the Hack The Barbican festival at the Barbican Arts Centre in London in August 2013.</gtr:description><gtr:id>7216B245-EF4F-423A-8017-B2A5344561C2</gtr:id><gtr:impact>These two performances contributed to make the Hack The Barbican festival a very successful event gathering a lot of visitors. Audience members enjoyed the novel experience made possible by the Mood Conductor system allowing them to interact with the performers over the course of the concert. The performances also helped improving the system to prepare another performance which was held in Switzerland (Geneva) as part of the International Conference on Affective Computing.</gtr:impact><gtr:outcomeId>54635485a548a4.05875179</gtr:outcomeId><gtr:title>Mood Conductor Performances at the Barbican Arts Centre</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>http://lanyrd.com/2013/hackthebarbican/scmwgx/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>World preimere at London's Cadogan Hall of the five compositions created through the Inspired by Digital project by young composers using technologies developed by Centre for Digital Music researchers. Performers included members of the London Chamber Orchestra.</gtr:description><gtr:id>13644781-A4CA-48B6-B787-C30370F2D4AC</gtr:id><gtr:impact>Impact included technology outreach to young composers and to the general public.</gtr:impact><gtr:outcomeId>54630be79f5a42.05932878</gtr:outcomeId><gtr:title>Inspired by Digital: Final Concert</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:url>http://www.qmul.ac.uk/publicengagement/activities/items/89531.html</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Several C4DM creative projects were shown in public for the first time at Sonar+D 2016. Amongst these was Tape.pm by CDT-funded PhD student Victor Loux, an interactive object which explores novel ways to record and share an improvisation that is created on a musical instrument (at S&amp;oacute;nar this was Collidoscope, another C4DM research project). 

Report from Victor Loux:
&amp;quot;Tape.pm is a project that developed during my Masters' advanced placement at b00t consultants, that links with the Collidoscope (developed by Ben Bangler and Fiore Martin) which was also shown at S&amp;oacute;nar and I helped with setting up. Tape.pm explores novel ways to record and share an improvisation that is created on a musical instrument (at S&amp;oacute;nar this was Collidoscope). Tape.pm was praised by attendants at the conference for its simple and creative approach to recording - and that it encouraged further exploration by the attendants. As such, the design of the object is purposefully ambiguous to encourage this exploration.

The exhibition of Tape.pm at S&amp;oacute;nar+D allowed me to gain first-hand access to public feedback about this piece and to gauge general interest and reactions and to observe how people engaged with it as well as to take notes about how to improve its design. Importantly, doing so at S&amp;oacute;nar+D allowed me to place myself in a professional context at an international event and to engage with a much more focussed audience - people attending were specifically interested in audio and interaction.

Outcomes: this initial testing with an international audience at S&amp;oacute;nar+D, which was largely composed of non-musicians, allowed me to formulate more specific questions and testing goals for a further formal study in a controlled environment that happened a few weeks later. Because Tape.pm gives a take-away &amp;quot;souvenir&amp;quot; code, I was able to use web analytics to analyse the actual usage and timings of people using Tape.pm even after the event finished, and use this data as a basis from which to form more realistic conditions for a controlled &amp;quot;lab&amp;quot; study. The outcomes of the observations S&amp;oacute;nar+D will be used in the APP report for this project.&amp;quot;</gtr:description><gtr:id>345F058D-84E1-4475-896F-0F1D5F173C54</gtr:id><gtr:impact>Tape.pm was praised by attendants at the conference for its simple and creative approach to recording - and that it encouraged further exploration by the attendants.</gtr:impact><gtr:outcomeId>58ac4b87661971.84081384</gtr:outcomeId><gtr:title>C4DM project Tape.pm at Sonar+D 2016.</gtr:title><gtr:type>Artistic/Creative Exhibition</gtr:type><gtr:url>https://sonar.es/en/2016/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>This event organised by C4DM is part of the C4DM Concert series 2015-2016. 

The performance was part of &amp;quot;Some Things Rich and Strange&amp;quot;, an Inside Out Festival and Music@QMUL event. The performance lasted 40 mins. Arno Babajanian's epic Piano Trio in F# minor performed by Elaine Chew (piano), Hilary Sturt (violin), and Ian Pressland (cello), with music-driven spectral art created by Alessia Milo. Babajanian's dark, ethereal, and wild trio was illuminated by specially designed vectorial drawings created in Processing, playing with time, light, darkness, colours, frequencies, and their spectral energy. The spectral art was generated in real-time in response to the music and projected on the musicians.</gtr:description><gtr:id>44EBD8DD-8D32-4B47-A106-60BA4E742D69</gtr:id><gtr:impact>C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrates the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.</gtr:impact><gtr:outcomeId>56ddb5441a17d9.12556742</gtr:outcomeId><gtr:title>Art Inside Music (curated by Paul Edlin, QMUL music director)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://sites.google.com/site/c4dmconcerts1516/home/classical/artinsidemusic</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Public evening concert at QMUL with a 'hackable' digital musical instrument featuring 10 performers of varying backgrounds. Showed a range of creative techniques including circuit bending. Repeated in part one month later at a regular concert series at a pub near Goldsmiths College, London.</gtr:description><gtr:id>C7E80772-C8D5-48C1-B8EC-F714B8887076</gtr:id><gtr:impact>Musicians will use the instruments in further performances of their own (including internationally, e.g. Japan tour by one musician). Interest by audiences in learning more about instrument design and technical platform.</gtr:impact><gtr:outcomeId>545bb3da438890.77795333</gtr:outcomeId><gtr:title>D-Box concerts</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>At the Study Day, C4DM researchers Elaine Chew (MuSA_RT visualisations of tonal space), Andrew McPherson (Magnetic Resonator Piano), and Andrew Robertson (B-Keeper and its family of beat trackers) presented their technologies and underlying research and theories. Contemporary composers Lisa Bielawa, Cheryl Frances-Hoad, Eduardo Miranda, and Gabriel Prokofiev led creative discussions of ways emerging composers could integrate these technologies in a new work. The programme concluded with a showcase of performances featuring these technologies. Composition proposals were received from ~40 young composers, from which 5 were selected for the ensuing workshop and performance.</gtr:description><gtr:id>A4E432DB-792A-486F-B8A6-054A85C48AB8</gtr:id><gtr:impact>Young composers were introduced to new music technologies, and given an opportunity to write new music for the technologies.</gtr:impact><gtr:outcomeId>54630de08d0761.03491499</gtr:outcomeId><gtr:title>Inspired by Digital: Study Day</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:url>http://elainechew-research.blogspot.co.uk/2012/11/1112-inspired-by-digital-call-for.html?q=Inspired+by+Digital</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Piraeus</gtr:collaboratingOrganisation><gtr:country>Greece, Hellenic Republic</gtr:country><gtr:description>MIREX Music/ Speech Classification and Detection Task</gtr:description><gtr:id>B7A9EFEC-9F9E-49B2-92C9-F032ECFB76FB</gtr:id><gtr:impact>The following outputs and outcomes were recorded:
1) Successfully organised a public evaluation campaign through the Music Information Retrieval Evaluation eXchange (MIREX) framework, with two tasks: music/speech classification and music/speech detection.
2) Datasets: (i) Curated a private dataset for music/speech classification. (ii) Curated a public dataset using recordings from the Internet Archive for music/speech detection (http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection#Dataset_2) (iii)
Curated a private dataset using recordings from the British Library Sound Archive for music/speech detection.
3) Received 22 code submissions from 12 international teams, and 11 extended abstracts (2-3 pages each) describing the submitted methods.
4) Task description website: http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection
5) Task results website: http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection_Results
6) 2 posters presenting submitted systems for the proposed task were presented in the MIREX poster session, taking place at the International Society for Music Information Retrieval Conference (ISMIR 2015), taking place in Malaga, Spain on 25-30 October 2015.

The collaboration is in the field of music technology / music informatics, hence from the C4DM perspective it is not interdisciplinary (from another perspective, anything related to music technology can be considered interdisciplinary!).</gtr:impact><gtr:outcomeId>56d9ab72140be1.10401399-3</gtr:outcomeId><gtr:partnerContribution>- Music Informatics Research Group of City University London (Dr Tillman Weyde, Dr Dan Tidhar): task announcement; led the creation of the training and test datasets for the music/speech detection task; gave feedback on task design and organisation.
- Department of Informatics, University of Piraeus, Greece (Dr Aggelos Pikrakis): led the creation of the test dataset for the music/speech classification task; gave feedback on the design of aforementioned task.
- The British Library, Digital Scholarship Department &amp;amp; British Library Labs (Dr Aquiles Alencar-Brayner): provided recordings from the BL Sound Archive, which were used for creating the music/speech detection test dataset.</gtr:partnerContribution><gtr:piContribution>The research team at C4DM/QMUL led the organisation of the MIREX task in music/speech classification and detection, including: task website creation and maintenance, software development for evaluation metrics, receiving and checking submissions and extended abstracts, debugging code submissions, evaluating submitted systems, and publishing results. In addition the team contributed in creating the music/speech detection training and test datasets.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>New York University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Research visit by Maria Pantelli, New York University (NYU)</gtr:description><gtr:id>94827C98-B3F3-4577-B16D-8D07A63B25A2</gtr:id><gtr:impact>For testing the algorithms developed in this research, music/speech annotations were created for a corpus of 360 world music recordings and melody contours were annotated for a set of 30 world music recordings. Other outcomes of this visit included the representation of music metadata using graphs, in particular hypergraphs, and the application of community detection algorithms to identify meaningful partitions of the graph. 

Maria and collaborators developed a software package and an interactive visualisation for characterisation of singing styles in world music. She also developed methods for the representation of music metadata using graphs, in particular hypergraphs, and the application of community detection algorithms to identify meaningful partitions of the graph. 

Maria collaborated with faculty and students in the Music Research Lab at NYU, as evidenced by the co-authored paper listed below:

M. Panteli, R. Bittner, J. P. Bello, and S. Dixon, &amp;quot;Towards the Characterization of Singing Styles in World Music,&amp;quot; IEEE International Conference on Acoustics, Speech and Signal Processing, submitted.</gtr:impact><gtr:outcomeId>58ab0a8e5a79b1.02005569-1</gtr:outcomeId><gtr:partnerContribution>The purpose of this research visit was to increase knowledge and resources from collaboration with the music technology group at New York University (NYU). 

Maria and collaborators developed a software package and an interactive visualisation for characterisation of singing styles in world music. She also developed methods for the representation of music metadata using graphs, in particular hypergraphs, and the application of community detection algorithms to identify meaningful partitions of the graph.

Maria collaborated with faculty and students in the Music Research Lab at NYU, as evidenced by the co-authored paper listed below:

M. Panteli, R. Bittner, J. P. Bello, and S. Dixon, &amp;quot;Towards the Characterization of Singing Styles in World Music,&amp;quot; IEEE International Conference on Acoustics, Speech and Signal Processing, submitted.</gtr:partnerContribution><gtr:piContribution>Pantelli's PhD is part of a bigger collaboration, namely the &amp;quot;Deep History of Music (DHOM)&amp;quot; project, involving researchers from six universities. This research visit was expected to contribute to new data resources collected from NYU and affiliated institutions and is also linked to a grant proposal, written by professor Armand Leroi, Imperial, in collaboration with Queen Mary.

 During Maria's stay at New York University between 22 May and 15 August 2016 she had the opportunity to meet with people from both academia and the industry. In particular she attended a meeting between academics of New York University and NYU Abu Dhabi discussing potential collaborations between their institutions and Queen Mary. Following this meeting she has been invited to attend the 3rd workshop on &amp;quot;Cross-disciplinary and Multicultural Aspects of Musical Rhythms&amp;quot;, Abu Dhabi, 17-20 March 2017, as a way to keep the NYU-QM collaboration going. What is more, she attended local events held at Spotify New York and she presented her work to industry-related audience promoting research of Queen Mary University of London. 

The findings of this work are summarized in a research paper (see next section) that has been submitted to the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing 2017. The work focuses on the characterisation of singing styles in world music and includes the development of a software package and an interactive visualization. 

For testing the algorithms developed in this research, music/speech annotations were created for a corpus of 360 world music recordings and melody contours were annotated for a set of 30 world music recordings.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Drake Music</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Andrew McPherson's collaboration with Drake Music (Bella platform)</gtr:description><gtr:id>CB6CBA35-EBDD-4398-BBF8-C48778139B4D</gtr:id><gtr:impact>Bela, the high-performance embedded platform for creating interactive audio systems, has had a major impact on researchers, makers and musicians alike. On 29 February 2016, McPherson launched a Kickstarter campaign for Bela which has been extremely successful, exceeding its &amp;pound;5k fundraising target it less than 4 hours and reaching 350% of its goal in 24 hours. This campaign is still underway but will result in the shipment of Bela kits to hundreds of individuals worldwide. It has also led to a substantial amount of social media attention and offers to collaborate with several major music technology platform developers.</gtr:impact><gtr:outcomeId>56ded92f04fd79.78759820-1</gtr:outcomeId><gtr:partnerContribution>Drake Music (charity for music making for the disabled) - held an Accessible Music Hackathon at QMUL on 6 February 2016, attendance 35 people (full capacity). Possible future host for MAT internship projects. Ongoing collaboration in their monthly DMlab instrument design sessions.</gtr:partnerContribution><gtr:piContribution>Bela is a high-performance embedded platform for creating interactive audio systems. Andrew McPherson has run a half dozen public workshops and hackathons using the platform, all of which have been well attended (most filled to capacity). One of the hackathons was a collaboration with the charity Drake Music, which focuses on creating musical instruments for disabled individuals. Following these events, he built up a network of over 20 collaborators using the hardware.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Aalborg University</gtr:collaboratingOrganisation><gtr:country>Denmark, Kingdom of</gtr:country><gtr:description>Computational Creativity, Perception, and Music collaboration (Dr. K. Agres, Prof Geraint Wiggins)</gtr:description><gtr:id>40B3DF54-662C-4EBC-A8CA-6C1FF0F07EE6</gtr:id><gtr:impact>This is an ongoing collaboration and paper in progress with David Meredith (Aalborg University, Denmark), and with Moshe Bar (Bar-Ilan University, Israel). Both of these projects are multi-disciplinary, involving music cognition and computer science, and music perception and visual perception, respectively. 

The uplifting trance paper has been published in Frontiers in Psychology: Cognitive Science and the authors have been asked to submit a book chapter for an Oxford University Press volume on Music and Consciousness (this has been submitted and is now under review). The trance work is also multi-disciplinary, involving aspects of music theory, machine learning, psychology, and music production.

The following papers are still in preparation:

Agres, K., Bar, M., &amp;amp; Pearce, M. (in preparation). A Cross-Modal Comparison of Veridical and Schematic Expectations in Music and Vision.

Agres, K., &amp;amp; Meredith, D. (in preparation). A computational model of musical change detection and memory performance.

Agres, K., McGregor, S., Rataj, K., Purver, M., &amp;amp; Wiggins, G. (in preparation). Modeling metaphor perception with distributional semantics vector space models.</gtr:impact><gtr:outcomeId>58c2dd70c7f147.84863693-1</gtr:outcomeId><gtr:partnerContribution>Collaborators on all of the projects have contributed to the experimental design and have helped write or revise the manuscripts. In the trance music study, musical change detection project, and distributional semantics experiment, the collaborators contributed methods relating to the computational modelling.</gtr:partnerContribution><gtr:piContribution>Dr. Agres led the experimental design, empirical data collection, data analysis, and writing up of the Uplifting Trance music experiments. Dr. Agres is leading the studies/papers with David Meredith, with Marcus Pearce and Moshe Bar, and with QMUL researchers (Stephen McGregor, Matthew Purver, and Geraint Wiggins).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Bar-Ilan University</gtr:collaboratingOrganisation><gtr:country>Israel, State of</gtr:country><gtr:description>Computational Creativity, Perception, and Music collaboration (Dr. K. Agres, Prof Geraint Wiggins)</gtr:description><gtr:id>1AD5D4B8-11DF-4AE4-8185-9D0AB5CBBFA0</gtr:id><gtr:impact>This is an ongoing collaboration and paper in progress with David Meredith (Aalborg University, Denmark), and with Moshe Bar (Bar-Ilan University, Israel). Both of these projects are multi-disciplinary, involving music cognition and computer science, and music perception and visual perception, respectively. 

The uplifting trance paper has been published in Frontiers in Psychology: Cognitive Science and the authors have been asked to submit a book chapter for an Oxford University Press volume on Music and Consciousness (this has been submitted and is now under review). The trance work is also multi-disciplinary, involving aspects of music theory, machine learning, psychology, and music production.

The following papers are still in preparation:

Agres, K., Bar, M., &amp;amp; Pearce, M. (in preparation). A Cross-Modal Comparison of Veridical and Schematic Expectations in Music and Vision.

Agres, K., &amp;amp; Meredith, D. (in preparation). A computational model of musical change detection and memory performance.

Agres, K., McGregor, S., Rataj, K., Purver, M., &amp;amp; Wiggins, G. (in preparation). Modeling metaphor perception with distributional semantics vector space models.</gtr:impact><gtr:outcomeId>58c2dd70c7f147.84863693-2</gtr:outcomeId><gtr:partnerContribution>Collaborators on all of the projects have contributed to the experimental design and have helped write or revise the manuscripts. In the trance music study, musical change detection project, and distributional semantics experiment, the collaborators contributed methods relating to the computational modelling.</gtr:partnerContribution><gtr:piContribution>Dr. Agres led the experimental design, empirical data collection, data analysis, and writing up of the Uplifting Trance music experiments. Dr. Agres is leading the studies/papers with David Meredith, with Marcus Pearce and Moshe Bar, and with QMUL researchers (Stephen McGregor, Matthew Purver, and Geraint Wiggins).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>City, University of London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>MIREX Music/ Speech Classification and Detection Task</gtr:description><gtr:id>B4C2F2E1-2E02-4180-812B-F446A6B1AF3E</gtr:id><gtr:impact>The following outputs and outcomes were recorded:
1) Successfully organised a public evaluation campaign through the Music Information Retrieval Evaluation eXchange (MIREX) framework, with two tasks: music/speech classification and music/speech detection.
2) Datasets: (i) Curated a private dataset for music/speech classification. (ii) Curated a public dataset using recordings from the Internet Archive for music/speech detection (http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection#Dataset_2) (iii)
Curated a private dataset using recordings from the British Library Sound Archive for music/speech detection.
3) Received 22 code submissions from 12 international teams, and 11 extended abstracts (2-3 pages each) describing the submitted methods.
4) Task description website: http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection
5) Task results website: http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection_Results
6) 2 posters presenting submitted systems for the proposed task were presented in the MIREX poster session, taking place at the International Society for Music Information Retrieval Conference (ISMIR 2015), taking place in Malaga, Spain on 25-30 October 2015.

The collaboration is in the field of music technology / music informatics, hence from the C4DM perspective it is not interdisciplinary (from another perspective, anything related to music technology can be considered interdisciplinary!).</gtr:impact><gtr:outcomeId>56d9ab72140be1.10401399-2</gtr:outcomeId><gtr:partnerContribution>- Music Informatics Research Group of City University London (Dr Tillman Weyde, Dr Dan Tidhar): task announcement; led the creation of the training and test datasets for the music/speech detection task; gave feedback on task design and organisation.
- Department of Informatics, University of Piraeus, Greece (Dr Aggelos Pikrakis): led the creation of the test dataset for the music/speech classification task; gave feedback on the design of aforementioned task.
- The British Library, Digital Scholarship Department &amp;amp; British Library Labs (Dr Aquiles Alencar-Brayner): provided recordings from the BL Sound Archive, which were used for creating the music/speech detection test dataset.</gtr:partnerContribution><gtr:piContribution>The research team at C4DM/QMUL led the organisation of the MIREX task in music/speech classification and detection, including: task website creation and maintenance, software development for evaluation metrics, receiving and checking submissions and extended abstracts, debugging code submissions, evaluating submitted systems, and publishing results. In addition the team contributed in creating the music/speech detection training and test datasets.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Science Museum Group</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>The Science Museum</gtr:department><gtr:description>Marcus Pearce's collaboration with the Science Musium in London</gtr:description><gtr:id>9D0B9D89-D810-41C9-A4B0-6A3332AF9C73</gtr:id><gtr:impact>Pearce's (project lead) and Delval's (researcher assistant employed on the project) Live Science Residency took place in November and December 2015. Between 11/11/15 and 11/12/15, the Residency attracted 371 visitors ranging in age from 8 to 78, who often came as families specifically to take part in our research and find out more about musical preferences.</gtr:impact><gtr:outcomeId>56dee6551f56a1.37910962-1</gtr:outcomeId><gtr:partnerContribution>Live Science Residency placement at the Live Science Museum.</gtr:partnerContribution><gtr:piContribution>The project consisted of a Live Science Residency in the Science Museum - the Queen Mary team (Marcus Pearce and L&amp;eacute;na Delval) spent 5 weeks in the Museum running experiments on musical preference with visitors to the gallery. The purpose was twofold: first, to facilitate data collection and second, to engage the public with our research. 

The project employed L&amp;eacute;na Delval as a research assistant on a fixed term contract for 4 months. L&amp;eacute;na is now applying for PhD positions.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Catholic University of Portugal</gtr:collaboratingOrganisation><gtr:country>Portugal, Portuguese Republic</gtr:country><gtr:description>Brecht de Man's collaboration with CITAR</gtr:description><gtr:id>F1B91949-494B-43F4-98B8-1F4E7E7BBE75</gtr:id><gtr:impact>The findings of this award are contributing to a deeper understanding of music mixing practices. This is informing music technology education. The multitracks, mixes of those multitracks and evaluations of those mixes are all used in a large multitrack audio testbed. This testbed is used by music technology researchers, educators and students, in order to improve music and audio mixing.

The C4DM researchers found out how and why reverberation is added to tracks in a mix. We have devised a metric, the equivalent impulse response, which is strongly correlated with how multitrack reverberation is perceived, and is meaningful for analysis and interpretation.

The C4DM researchers are also investigating how mixes are made and evaluated by the students and educators at the Catholic University of Portugal, and comparing the results with similar experiments performed at other institutions across the world. This ongoing work will help us understand how music mixing practices are influenced by culture and region.

A C4DM open source software tool, the Web Audio Evaluation Tool, was improved and updated based on its use in this project.

Data gathered in the experiment has made a major contribution to C4DM's multitrack testbed.</gtr:impact><gtr:outcomeId>56ddad5bbd18f2.18113695-1</gtr:outcomeId><gtr:partnerContribution>The collaboration with CITAR was developed in particular with its Digital Creativity group and focused on sound design and audio engineering. The lead UCP researcher was Dr Pedro Duarte Pestana, assistant professor and researcher in audio engineering at UCP, and a recording engineer with over 15 years of experience. The students and educators at the Catholic University of Portugal participated in the experiments conducted by Brecht de Man. They have provided mixes on a relatively large number of songs, thereby providing a necessary 'mirror' set to the other large corpus of mixes that de Man has built at McGill University. Besides a direct contribution towards de Man's doctoral thesis, this experiment will lead to a Journal of the Audio Engineering Society paper.</gtr:partnerContribution><gtr:piContribution>The grant funded a one week placement of Brecht De Man at the Catholic University of Portugal. Brecht de Man gave a seminar on his research that was attended by students, educators and researchers from the Catholic University of Portugal. In the process of experiement participation, the student learned about C4DM technologies and research.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Amsterdam</gtr:collaboratingOrganisation><gtr:country>Netherlands, Kingdom of the</gtr:country><gtr:description>Marcus Pearce's collaboration with Bastiaan van der Weij: A probabilistic model of musical metre perception</gtr:description><gtr:id>0A7C3F1E-79ED-4DA1-9639-B1D981B15810</gtr:id><gtr:impact>The work has demonstrated that probabilistic methods can predict metrical categories in music perception with significant accuracy and identified advanced modelling methods that further increase accuracy. This lays the ground for investigating probabilistic models of metre perception as a way of simulating and understanding the effects of cross-cultural exposure on musical metre perception.

The collaboration strengthened ties between the Music Cognition Group at the UvA and the Music Cognition Lab at QMUL.</gtr:impact><gtr:outcomeId>56d9c7c8ef5962.75955725-1</gtr:outcomeId><gtr:partnerContribution>The research internship served as an opportunity for van der Weij to become strongly familiar with Pearce's research and to be able to exchange ideas in person on a regular basis. The internship provided a valuable opportunity for broadening van der Weij's experience and extending his network. Similarly, the visit to Queen Mary provided an opportunity for researchers at the Music Cognition Lab to be introduced to the research done at the Music Cognition Group in Amsterdam.

Thus, the objectives of the research internship were: 
1) Integrating a novel model of rhythm perception into the IDyOM framework, under supervision and guidance of Marcus Pearce, who originally developed this framework;
2) Kicking-off a partnership between Bastiaan van der Weij and Marcus Pearce, who acted as co-supervisor during van der Weij's stay at C4DM.</gtr:partnerContribution><gtr:piContribution>Funding from the Platform Grant has allowed C4DM to offer a three month research internship as part of the &amp;quot;Internship scheme to attract excellent PhD students and RAs from other universities (from a few days up to 3 months).&amp;quot; Bastiaan van der Weij's stay at C4DM was partly covered by the University of Amsterdam and partly by C4DM.

For his PhD research, van der Weij is working on a probabilistic model of rhythm perception, using methodologies very similar to those used by Marcus Pearce in his IDyOM framework. Both models are probabilistic and incremental models whose parameters are estimated from a large body of data. Both models aim to model the flux of listener's expectation as a piece of music unfolds, as a function of that listener's earlier musical experience. Both models use predictive distributions and information content to quantify these expectations.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Hunan Agricultural University, Changsha.</gtr:collaboratingOrganisation><gtr:country>China, People's Republic of</gtr:country><gtr:description>China Summer Camp Field Research (Nick Bryan-Kinns)</gtr:description><gtr:id>14FA9F42-5D2A-4755-8CC2-7486DF707F25</gtr:id><gtr:impact>Data from field work - video recordings, collaborative artworks created and ethnographic observations of local peoples' interaction with them. Interactive artefacts for exhibition, study, and research design inspiration.

1) Public Exhibitions:
18-25 Aug 2016 Sino-Italy design center, Huaihua, China;
9-16 Sep 2016 Inter/sections, London, UK.
2) Social Media Impact:
Tweets by Nick Bryan-Kinns about the Summer Camp (with #hengling16) received 3.4k impressions in Aug 2016.
3) Reported
Reported in Xinhua, China's premier news agency, and other national Chinese news outlets.
Xinhua Net, China Daily, China.org.
4) Website
http://eecs.qmul.ac.uk/~nickbk/hengling16/
Including:
? Design Diary
? Detailed descriptions, images, and pdf presentations of each artefact and the design process
? Videos of the artefacts

The C4DM team lead gathered detailed feedback from C4DM students on what is necessary for future rural China design field trips. Also, strengthening of existing collaboration with researchers Hunan University.</gtr:impact><gtr:outcomeId>58ad98956608a6.71685188-1</gtr:outcomeId><gtr:partnerContribution>It was a collaboration with Hunan University (China) to create interactive artifacts inspired by the Dong ethnic minority culture in rural China, and was part of The New Channel Program 2016 (Design for Social Innovation and Sustainability Network) led by Hunan University. 

New relationships with researchers and design students from Hunan University were established leading to potential future research collaborations.</gtr:partnerContribution><gtr:piContribution>It was a collaboration with Hunan University (China) to create interactive artifacts inspired by the Dong ethnic minority culture in rural China, and was part of The New Channel Program 2016 (Design for Social Innovation and Sustainability Network) led by Hunan University. The QMUL team created a popup maker space in Hengling Village, Tongdao County, Hunan Province with locally-sourced found objects and materials. They explored ethnic musical box design using local materials in a remote location, off grid, building on local cultural elements to inspire our interactive designs. We worked with low-cost/ low-fi making equipment with engagement with local population, students, and designers. Four interactive artifacts were created in the rural village, exhibited in China, and returned to the UK for exhibition in London: KeepMake; Doye Boxes; Dong Shine; and Dong Tunes.

Participants included two academics and three students from Hunan University, one academic and three students from QM, and one UK company.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>The British Library</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>MIREX Music/ Speech Classification and Detection Task</gtr:description><gtr:id>A03BA299-6F3F-45B8-B79E-851FCAE6309D</gtr:id><gtr:impact>The following outputs and outcomes were recorded:
1) Successfully organised a public evaluation campaign through the Music Information Retrieval Evaluation eXchange (MIREX) framework, with two tasks: music/speech classification and music/speech detection.
2) Datasets: (i) Curated a private dataset for music/speech classification. (ii) Curated a public dataset using recordings from the Internet Archive for music/speech detection (http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection#Dataset_2) (iii)
Curated a private dataset using recordings from the British Library Sound Archive for music/speech detection.
3) Received 22 code submissions from 12 international teams, and 11 extended abstracts (2-3 pages each) describing the submitted methods.
4) Task description website: http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection
5) Task results website: http://www.musicir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection_Results
6) 2 posters presenting submitted systems for the proposed task were presented in the MIREX poster session, taking place at the International Society for Music Information Retrieval Conference (ISMIR 2015), taking place in Malaga, Spain on 25-30 October 2015.

The collaboration is in the field of music technology / music informatics, hence from the C4DM perspective it is not interdisciplinary (from another perspective, anything related to music technology can be considered interdisciplinary!).</gtr:impact><gtr:outcomeId>56d9ab72140be1.10401399-1</gtr:outcomeId><gtr:partnerContribution>- Music Informatics Research Group of City University London (Dr Tillman Weyde, Dr Dan Tidhar): task announcement; led the creation of the training and test datasets for the music/speech detection task; gave feedback on task design and organisation.
- Department of Informatics, University of Piraeus, Greece (Dr Aggelos Pikrakis): led the creation of the test dataset for the music/speech classification task; gave feedback on the design of aforementioned task.
- The British Library, Digital Scholarship Department &amp;amp; British Library Labs (Dr Aquiles Alencar-Brayner): provided recordings from the BL Sound Archive, which were used for creating the music/speech detection test dataset.</gtr:partnerContribution><gtr:piContribution>The research team at C4DM/QMUL led the organisation of the MIREX task in music/speech classification and detection, including: task website creation and maintenance, software development for evaluation metrics, receiving and checking submissions and extended abstracts, debugging code submissions, evaluating submitted systems, and publishing results. In addition the team contributed in creating the music/speech detection training and test datasets.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>New York University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Center for Data Science</gtr:department><gtr:description>Outgoing visiting research to New York University (K Choi)</gtr:description><gtr:id>125BAA38-B2DC-4F2A-B60D-BA4F825AA918</gtr:id><gtr:impact>The most visible outcome of this collaboration is the conference paper by K. Choi, M. Sandler, G. Fazekas and K. Cho that has been accepted to the 42nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).

The visiting enabled Choui to collaborate closely with researchers outside Queen Mary and gain knowledge of areas other than signal processing side of music information retrieval. He mainly worked with Dr. Brian McFee and Prof. Kyunghyun Cho from the Center for Data Science (CDS), New York University. He discussed many implementation details of his work with Dr. McFee who has deep knowledge and intensive experience of MIR and Python. As a result, Choi reported he improved his programming techniques. Working with Prof. Cho and other students who are doing their research on deep learning and natural language processing deepened his understanding on deep learning in general.</gtr:impact><gtr:outcomeId>58ac3b3f92cb25.71298503-1</gtr:outcomeId><gtr:partnerContribution>Kyunghyun Cho supervised Keunwoo Choi for deep learning knowledge and implementations. In addition, CDS provided K Choi high-performance GPU servers. His work benefited from those facilities as it involves computationally extensive experiments.</gtr:partnerContribution><gtr:piContribution>Mark Sandler, George Fazekas, and Keunwoo Choi collaborated with Kyunghyun Cho for their research. They had regular Skype meetings to discuss research details and co-write the paper submitted to the 42nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Ecole des Mines de Saint-etienne (ENSMSE)</gtr:collaboratingOrganisation><gtr:country>France, French Republic</gtr:country><gtr:description>Collaboration with Tristan Begueria on D-Box (Andrew Mcpherson)</gtr:description><gtr:id>D75782A2-33D3-49B5-817B-66B947B7985F</gtr:id><gtr:impact>This project and collaboration with Tristan developed the capabilities of the D-Box hackable musical instrument arising from previous support from the EPSRC Platform Grant and the EPSRC Hackable Instruments project (EP/K032046/1), producing a new way to interact with live-sampled sounds. 

The extra 1-month funded period for Tristan stay's at C4DM resulted in a finished instrument and a partial draft of a paper, and submission of a final manuscript will take place after we conduct a user study.</gtr:impact><gtr:outcomeId>56d98a0d950358.78269663-1</gtr:outcomeId><gtr:partnerContribution>Andrew Mcpherson from C4DM has integrated results of Tristan Begueria's summer research project into the D-Box hackable instrument, which will in turn be used in workshops, performances and an upcoming crowdfunding campaign.</gtr:partnerContribution><gtr:piContribution>Tristan spent a 6-month placement at C4DM, Queen Mary, working on the D-Box musical instrument, which comes out of the Hackable Instruments project originally funded by Andrew McPherson's EPSRC First Grant. The project placement lasted from 1 April to 30 September 2015. The Platform Grant funded allowed to cover Tristan's stagy for an extra month to continue working on the D-Box musical instrument project with Andrew Macpherson.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Max Planck Society</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:department>Max Planck Institute for Ornithology</gtr:department><gtr:description>Collaboration with MPIO Seewiesen (Dan Stowell)</gtr:description><gtr:id>B81EA5A2-F2D4-4B7F-8DE6-1041FB59389D</gtr:id><gtr:impact>Method developed by Stowell and applied to Gill et al's data finds increased detail of animal communication patterns, specifically patterns related to whether a pair of animals is a breeding pair or not.

A journal article was accepted for IEEE/ACM Trans Audio Speech and Language Processing. Preprint: https://arxiv.org/abs/1612.05489

Plans in 2016 were made for future collaboration have been with Lisa Gill and (supervisor) Manfred Gahr, Max Planck Institute for Ornithology, Seewiesen.</gtr:impact><gtr:outcomeId>56d980d9e20354.55496876-1</gtr:outcomeId><gtr:partnerContribution>MPIO research member Lisa Gill collaborated with Dan Stowell on the development of a method by Stowell and applied to Gill et al's data finds, something which increased detail of animal communication patterns, specifically patterns related to whether a pair of animals is a breeding pair or not.</gtr:partnerContribution><gtr:piContribution>Lisa Gill visited Dan Stowell from C4DM at Queen Mary in November for five days. The joint work and collaboration led to MPIO members contributing to a publication and to further grant proposals in future.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>London Sinfonietta</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Steve Reich's Clapping Music application (Marcus Pearce, Sam Duffy)</gtr:description><gtr:id>C41F1875-390F-43A2-A04E-FB54230F135E</gtr:id><gtr:impact>Duffy, S. and Pearce, M. (in review). What makes rhythms hard to perform? An investigation using Steve Reich's Clapping Music. Submitted to PLoS One. 
 
Duffy, S., Palmer, T., Burke, A., Palczynski, B. &amp;amp; Pearce, M. (in prep.). Using Digital Technology to Stimulate Engagement with a New Music Genre: Steve Reich's Clapping Music App. 

The collaboration is multi-disciplinary - Tim Palmer is a professional musicians and senior lecturer at Trinity Laban. Andrew Burke and Barbara Palczynski work at the London Sinfonietta, an orchestra specialising in contemporary classical music.

Winner of the 2017 Public Engagement Involve Award at the QMUL Engagement and Enterprise awards.</gtr:impact><gtr:outcomeId>58ad87d560c199.49087234-1</gtr:outcomeId><gtr:partnerContribution>The workshops in schools were conducted by Sam Duffy (PG funded) and Tim Palmer. Tim has been contributing to the journal articles we are writing. The London Sinfonietta were involved in the original development of the app (not funded by the PG) and have contributed to the journal articles in terms of proof reading, commenting and supplying images and information.</gtr:partnerContribution><gtr:piContribution>The C4DM members Sam Duffy and Marcus Pearce have been writing up the research previously conducted as journal articles for publication.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Guildhall School of Music &amp; Drama</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Open Symphony (Mathieu Barthet &amp;amp; Kate Hayes)</gtr:description><gtr:id>BE50FB98-4E6A-4599-93F3-C78774822E56</gtr:id><gtr:impact>This multidisciplinary collaboration allowed to innovate combining art, science and technology:
- Delivered public engagement events in UK and abroad reaching a wide audience (from school students to seniors from various background)
- Publication in high impact factor journal
- Award of other funding (QMUL Centre for Public Engagement)</gtr:impact><gtr:outcomeId>58aee2e87b4336.53196515-1</gtr:outcomeId><gtr:partnerContribution>The musician partner, Kate Hayes, acted as music director of the project and developed some of the artistic components of the system. She led the artistic part in music rehearsals and performances.</gtr:partnerContribution><gtr:piContribution>M. Barthet and his team designed and developed technological and artistic components for the creative participation of audiences of live music performances. They organised interactive concerts and conducted evaluation studies with performers and audiences. They disseminated the research through academic publications and online blog, photo and video resources.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>Participation in BBC Audio Research Partnership</gtr:description><gtr:id>699E8D9E-A821-4F4A-ABB3-EE63F53DBE9C</gtr:id><gtr:impact>Research grants, from both EPSRC and TSB - see web site for details.</gtr:impact><gtr:outcomeId>56dffbfa6e00a9.28399344-1</gtr:outcomeId><gtr:partnerContribution>Contributions to research in EP/L019981/1, especially in writing the grant. Hosting internship students from our CDT in Media and Arts Technology, and from the Centre for Digital Music.</gtr:partnerContribution><gtr:piContribution>Prof Sandler attends meetings of the steering board of the BBC Audio Research Partnership. Other activities include open research events, e.g. at BBC Salford or New Broadcasting House</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>London Chamber Orchestra</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>LCO New: Explore Inspired by Digital</gtr:description><gtr:id>A9113BD5-5E70-4914-AE90-87058ED8B127</gtr:id><gtr:impact>These are listed under Artistic Outputs, and include new music compositions by young composers, workshops, and concerts.</gtr:impact><gtr:outcomeId>54632792311fe7.58886212-1</gtr:outcomeId><gtr:partnerContribution>The London Chamber Orchestra provided organisational expertise, expert musicians, and a platform (concert series and education/outreach programmes) through which to create the series of Inspired by Digital events: a study day, a workshop, and a final concert.</gtr:partnerContribution><gtr:piContribution>The Centre for Digital Music provided technologies, access to QMUL locations, and technical and artistic support for the events. There were frequent (2-3 times each week) meetings to coordinate the various details of the events.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Live Science Museum Residency (Marcus Pearce): Why do we like the music we like?</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>DE679E61-1C34-4EAF-95BB-F4F2CD43E927</gtr:id><gtr:impact>The project is ongoing, so no significant impact or outcome has been generated from this activity yet.

The project employed L&amp;eacute;na Delval as a research assistant on a fixed term contract for 4 months. L&amp;eacute;na is now applying for PhD positions. The project consisted of a Live Science Residency in the Science Museum - Pearce and Delval spent 5 weeks in the Museum running experiments on musical preference with visitors to the gallery. The purpose was twofold: first, to facilitate data collection and second to engage the public with our research. Between 11/11/15 and 11/12/15, the Live Science Residency attracted 371 visitors ranging in age from 8 to 78, who often came as families specifically to take part in our research and find out more about musical preferences. 

Pearce and Delval intend to publish their data which includes a large and well-validated public dataset of emotion and preference ratings for music in a range of different genres. This may be used to support future research in music emotion recognition and modelling of musical emotions and preference decisions. They also expect this work to lead to one or more journal articles. Work on these will commence once the analysis of the data has been completed.</gtr:impact><gtr:outcomeId>56dee3d939a226.62799424</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BeagleRT workshop C4DM</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>45F947E4-36FE-4FF7-8FFF-D61461A3DA8C</gtr:id><gtr:impact>We organised a workshop for PhD students in the Centre for Digital Music and those on the Media &amp;amp; Arts Technology programme introducing the new BeagleRT real-time embedded audio environment. It showed how the environment could be used for the students' research and introduced basic concepts of real-time audio programming.

Many participants expressed interest in using the platform in their own work.</gtr:impact><gtr:outcomeId>545f86aac41db6.07343531</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>HORSE2016 workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A99A9DFE-2EB2-4395-8010-C3A3D697A64C</gtr:id><gtr:impact>HORSE 2016 was a one-day workshop/debate devoted to &amp;quot;horses and Potemkin villages in applications of machine learning&amp;quot; that took place on the 19 September 2016. 60 people attended the event. The key finding of HORSE2016 is that many people are interested in the topics it covered. The organiser (Bob Sturm, Centre for Digital Music, Queem Mary) posted a blogpost featuring the videos of the event on Sep 25 (https://highnoongmt.wordpress.com/2016/09/25/horse2016-success/), and it received 426 views from around the world, and was featured on reddit. The next day (yesterday) it received 258 views. All videos on the HORSE2016 YouTube Channel (https://www.youtube.com/channel/UCKazfHUF5QmT55EKT5Wem1Q) have had 1131 views so far (Feb 15 2017).</gtr:impact><gtr:outcomeId>58a478ef269d52.61697719</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>https://highnoongmt.wordpress.com/2016/09/25/horse2016-success/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Princeton University Seminar by the Centre for DIgital Music</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>305EF6D5-A846-4DA6-8C9F-903680987EDB</gtr:id><gtr:impact>This was a seminar at the Department of Mathematics, Princeton University, by PhD student Francisco Rodr&amp;iacute;guez Algarra (supervisor: Bob Sturm), Centre for Digital Music.Among the attendees to the seminar at Princeton, there was great interest in the call for employing proper experimental procedure, and not blindly trusting standard benchmarks. Though its specific examples are in MIR, the Centre made efforts to connect with general machine learning methods. Before the presentation, a private conversation was held with the host, Dr. Joakim Anden. He was very happy to pursue building upon our work to improve his methodology. 

The Centre have sowed the seeds for a future collaboration with Dr. And&amp;eacute;n, specifically exploiting our analysis to improve the scattering-based music content analysis systems he developed. The interest show by parties not currently involved in MIR shows great promise of future collaboration.</gtr:impact><gtr:outcomeId>58ab05abcfa111.36060969</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>D-Box presentation FabLab Cardiff</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>7CE4A766-17C8-4C17-8088-59163D8B361A</gtr:id><gtr:impact>Presented the D-Box hackable instrument at FabLab Cardiff (Cardiff School of Art and Design), with an audience including arts students and members of the &amp;quot;maker&amp;quot; community. Followed by questions, discussions and hands-on demos.

Introduced our research to a broader maker community outside science and engineering, and may lead to further collaborative projects e.g. a placement project for a PhD student.</gtr:impact><gtr:outcomeId>546214b718aed8.69408964</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshop on Auditory Neuroscience, Cognition and Modelling 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>2B291180-9CC2-4C00-8A92-ADDC2E24EC76</gtr:id><gtr:impact>The main aim of the organisers (Marcus Pearce, Emmanouil Benetos, Yvonne Blockland) was to organise and host at QMUL a workshop on Auditory Neuroscience and Computation, which took place on Wednesday 17 February 2016, at the Charterhouse Square campus. The workshop brought together auditory neuroscientists, cognitive scientists, as well as researches in music and audio signal processing and related fields. New insights on the cognitive and neural underpinnings of speech, music and sound processing were presented, with a large focus on EEG and MEG data analysis. The workshop focused on academic impact, however it had industry participation (from Audio Analytic, Red Apple Creative). Following an open call for abstracts and selection process., 32 presentations were accepted: 3 keynote talks, 6 oral presentations and 23 poster presentations. 

The workshop website includes a PDF book of abstracts and video recorded oral presentations (see website below). Post-workshop summaries are being written by both workshop organizers and attendees, to be published to several venues: Psychomusicology, Young Acousticians Network newsletter, Audio Analytics Lab blog. Future plans: discussions are underway on applying for an EPSRC Research Network grant, for funding future workshops and network activities. 

Post-workshop summaries are being written by both workshop organizers and attendees, to be published to several venues: Psychomusicology, Young Acousticians Network newsletter, Audio Analytics Lab blog.

- Psychomusicology report: 
o Workshop on Auditory Neuroscience, Cognition and Modeling. Agres, Kat; Sauv&amp;eacute;, Sarah Anne Psychomusicology: Music, Mind, and Brain, Vol 26(3), Sep 2016, 288-292. http://dx.doi.org/10.1037/pmu0000151
- Audio Analytic Lab blog post:
http://www.audioanalytic.com/aa-labs-impressions-on-the-workshop-on-auditory-neuroscience-cognition-and-modelling-2016/
- Young Acousticians Network newsletter:
https://euracoustics.org/activities/yan/yan-newsletter-folder/2016/march-2016/at_download/file

Preliminary discussions have been made with Iris Mencke and Elvira Brattico on hosting the next workshop.</gtr:impact><gtr:outcomeId>56dd95b943a2f3.06662405</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://c4dm.eecs.qmul.ac.uk/wancm2016/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>115th MPEG Meeting, Geneva (CH), 30 May - 3 Jun 2016</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D21D0925-5E17-4B6F-B8E0-36CE8A651AC2</gtr:id><gtr:impact>The work by C4DM on MVCO AMD 1 (Time Segments &amp;amp; Multi-track Audio) reached PDAM 1 stage (June 2016) and a ballot was issued. It is envisaged (depending on the ballot results) to be published as an ISO/IEC standard in June 2017. A meeting report is available at: 115th MPEG Meeting, Geneva (CH), 30 May - 3 Jun 2016.</gtr:impact><gtr:outcomeId>58ad8c5e6b2462.86066661</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://code.soundsoftware.ac.uk/attachments/download/2142/1PanosGeneva115MPEGMeetingReport.pdf</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Music Cognition Seminar (Aalborg, Denmark)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>2D7BF845-8F8D-4AF5-8777-41A898FAEECC</gtr:id><gtr:impact>Dr K. Agres (Centre for Digital Music, QMUL) delivered a lecture to Aalborg University music researchers in November 2016. This was a lecture for PhD and postdoc students, most of whom did not have a background in music psychology. The lecture sparked lively debate, questions and discussion of a multi-disciplinary nature, and helped inspire a collaborative research project. The most significant outcome / impact was the comparison of behavioural with computational results, and the manuscript in progress describing this work.</gtr:impact><gtr:outcomeId>58c2dea55778a5.40461000</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Mathematics and Computation in Music 2015 (organiser: Elaine Chew)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C3EE7433-A25B-4CA7-8EF5-B9E5B2D2048B</gtr:id><gtr:impact>The Fifth Biennial International Conference on Mathematics and Computation in Music (MCM2015) was held 22-25 June-2015 at Queen Mary University of London (QMUL) in the United Kingdom. MCM is the flagship conference of the Society for Mathematics and Computation in Music (SMCM), whose official publication is the Journal of Mathematics and Music (JMM). The conference brought together researchers from around the world who combine mathematics or computation with music theory, music analysis, composition and performance. MCM 2015 provided a dedicated platform for the communication and exchange of ideas amongst researchers in mathematics, informatics, music theory, composition, musicology, and related disciplines. The 2015 event was co-hosted by the Schools of Electronic Engineering and Computer Science (Centre for Digital Music) and the School of Mathematical Sciences, and was promoted by QMUL in association with the Society for Mathematics and Computation in Music, London Mathematical Society, Institute of Musical Research, and the EPSRC.

Videos of the keynotes can be found under the KEYNOTES tab and through bit.ly/mcm2015-keynotes.</gtr:impact><gtr:outcomeId>56ddb2487053b8.36613681</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://mcm2015.qmul.ac.uk/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Listening in the Wild 2015 (organiser: Dan Stowell)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>1DE50F40-E28F-41B2-8260-6E194DC18633</gtr:id><gtr:impact>The Listening in the Wild: Animal and machine audition in multi source environments research workshop took place at Queen Mary, London, 28th August 2015. The workshop aimed to address the following questions: a) How do animals recognise sounds in noisy multisource environments?, b) How should machines recognise sounds in noisy multisource environments?

The workshop brought together researchers in engineering disciplines (machine listening, signal processing, computer science) and biological disciplines (bioacoustics, ecology, perception and cognition), to discuss complementary perspectives on making sense of natural and everyday sound.</gtr:impact><gtr:outcomeId>56dd9e2ac36990.75560846</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://www.eecs.qmul.ac.uk/events/view/listening-in-the-wild-2015</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Andrew Macpherson's D-Box presentation, Humanising Music Technology workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>C8222FF8-24AF-492F-9A93-7AD68743A462</gtr:id><gtr:impact>The Humanising Music Technology workshop was held at Kingston University, Monday 16 November 2015. The event focused on the role of technology in our engagement with music and the intricate relationship between technology and creativity. The event covered a broad range of approaches ranging from art music to the fan-based Chiptune phenomena. As part of the event, Andrew Mcpherson and Alan Chamberlain (from Nottingham and working on the FAST project, also funded by EPSRC) presented musical hacking. After the presentation, participants had the opportunity to make music using a 'hackable' electronic instrument.</gtr:impact><gtr:outcomeId>56d98e30c6bfb2.85890878</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.kingston.ac.uk/events/item/1856/16-nov-2015-humanising-music-technology/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Trip to Shenzen, Hong Kong and Brisbane</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>1E58075B-D4E9-46CE-A182-94EDB4891C46</gtr:id><gtr:impact>Bela was presented at the Baoan Maker Alliance and at the Shenzhen DIY Night in Shenzhen. A pop-up Bela workshop was held at the New Interfaces for Musical Expression conference in Brisbane. While not originally in the schedule, the Centre for Digital Music received a last-minute invitation to host the workshop given the interest of many of the participants of the conference. During the workshop, the 20 participants were introduced the basic usage of the platform and their specific questions about Bela as a teaching tool were answered.

Bela, an embedded platform for audio and sensor processing, was developed at C4DM. During this trip Giulio Moro from the Centre for Digital Music got in touch with manufacturers, to plan for future orders and with the community of users, notably gathering interest for it being used as a teaching tools in universities around the world. Meeting manufacturers SeeedStudio and Elecrow in Shenzhen allowed to plan for 
the future of the hardware manufacturing of Bela. Useful feedback was gathered from the participants of the workshop in order to improve the ongoing hardware and software development of Bela, which is carried on at C4DM. 

The Centre is now close to creating an Augmented Instruments Lab spin-off company with the support of QM Innovation, with the main aim of commercializing Bela and support other projects from the lab. 

The Centre for Digital Music gathered interest from teachers at University of Virginia (Travis Thatcher, Peter Bussigel), Griffith University of Brisbane (John Ferguson, Andrew Brown, Toby Gifford), University of West of England (Chris Nash), Staffordshire University (Si Waite) who are interested in the use of Bela as a teaching tool for their classes in sound design and digital musical instruments.</gtr:impact><gtr:outcomeId>58ab02f65eeea7.06580354</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>C4DM visit to Universitat Pompeu Fabra, Barcelona</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8626AF55-22F1-4720-AA27-CC7CBBF6778A</gtr:id><gtr:impact>On 24 March 2016 a group of students and academics from C4DM visited the Universitat Pompeu Fabra in Barcelona to take part in the annual Doctoral Student Workshop held by the Department of Information and Communication Technologies, UPF. Three (3) academics and ten (10) students attended the workshop. 

The following posters were presented from C4DM:
* Doing it Wrong: Towards an understanding of the roles of error, risk and fiasco in DMI design for performance by Astrid Bin
* Measuring Affective, Physiological and Behavioural Differences in Solo, Competitive and Collaborative Games by Daniel Gabana Arellano
* Intelligent Machine Music Listening: An Experimental Approach by Francisco Rodr&amp;iacute;guez-Algarra
* Automatic Machine Transcription of Wildlife Bird Sound Scenes by Gnostothea-Veroniki Morfi
* Automatic Expressive Feature Analysis by Luwei Yang
* Automatic detection of outliers in world music collections by Maria Panteli
* Physically-Musically Inspired Probabilistic Models for Audio Content Analysis by Pablo A. Alvarado
* Building self-contained digital musical instruments with active tactile feedback using Bela embedded audio platform by Robert Jack
* Robust and Efficient Joint Alignment of Multiple
* Musical Performances by Siying Wang

After the morning poster session, the group got a chance to see the latest research of the Music Technology Group including designs of new musical instruments, analysis techniques for non-Western music, and computer-assisted monitoring of a musician's movements during performance.

A group of three (3) students and two (2) academics also had the chance to visit Hangar (https://hangar.org/en/), an arts centre in Barcelona. Marta Garcia, the Director of the Research Programmes, showed the group around the making facilities, studios, video editing suite, and audio synth studio.

The most significant impact was potential research collaborations formed between UPF and QM students who met each other at the workshop. At least one student from QM is considering taking a research visit to UPF. There were also discussions of ongoing exchanges such as inviting UPF students to take part in future EECS research showcases.</gtr:impact><gtr:outcomeId>58ac2b2f151999.17334183</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://portal.upf.edu/web/etic_doctoral_workshop/home</gtr:url><gtr:year>2006,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Interactive Workshop at QMUL's A Mind of Music festival: Open Symphony (M. Barthet, C4DM)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>F3E241D2-4ED8-4486-A24D-F2A84BA2CCC5</gtr:id><gtr:impact>Western performing arts practices have traditionally restricted creative interactions from audiences. Open Symphony was designed to explore audience-performer interaction in live music performance assisted by digital technology. The Open Symphony system transforms the traditional unidirectional musical chain by adding creative communication in the reverse direction, from the audience to performers.

Approximately 80 local East London school students attended this creative workshop at Queen Mary University of London during which they learned about music making and conducted performers using the Open Symphony mobile app. Students actively engaged in the activity and their tutors reported it had fostered a lot of interest and discussions in class after the event.

The most significant outcome from this activity is to have sparked the interest of young people in contemporary music helping them to understand music better and bridge the gap between them and performers.</gtr:impact><gtr:outcomeId>58aed83d38c0c7.69809573</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Research visit, New York University (NYU) by Maria Pantelli, Centre for DIgital Music</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FC44FC90-1A6B-47E4-BBBB-E32FC0FD9350</gtr:id><gtr:impact>During her research visit at the NYU, the PhD student Maria Pantelli from the Centre for Digital Music was invited to give an interview for the &amp;quot;I care if you listen&amp;quot; music blog (https://www.icareifyoulisten.com/2016/07/monthly-music-hackaton-science-music/) and give a talk at the corresponding social event held at Spotify New York in conjunction with the Monthly Music Hackathon: Science of Music. The audience of this blog and hackathon involves mostly people from the music and tech industries and some educational institutions including universities and high schools.

Maria also collaborated with faculty and students in the Music Research Lab at NYU, something that is reported seperately under the Collaborations/Partnerships.</gtr:impact><gtr:outcomeId>58ab07d1cc9c95.39303295</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>https://www.icareifyoulisten.com/2016/07/monthly-music-hackaton-science-music/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Ben Bengler's  presentation at the ACM Creativity and Cognition conference</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>44BFE047-7D5C-413E-93CF-F01A05955876</gtr:id><gtr:impact>Ben Bengler is currently funded by Platform Grant for additional post-phd research to maximise impact of his research including paper writing (intended to be interim bridging funding). In this time Ben has written a paper that was presented at ACM Creativity and Cognition conference (Glasgow, June 2015) which is the &amp;quot;premier forum for presenting the world's best new research investigating computing's impact on human creativity in a broad range of disciplines including the arts, design, science, and engineering&amp;quot;. This was the presentation of a study of why people get bored of interactive exhibits, even though they report that they find them interesting.

Outcome: Identification of key features of public interactive pieces which retain an audience's interest, or not.</gtr:impact><gtr:outcomeId>56d9b9ca1cb889.16029137</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Mailing of the SC4FN magazine to Primary schools</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>1D7CEB95-7ED9-4F1A-A853-CFA68252CECD</gtr:id><gtr:impact>This &amp;quot;Computing Sounds Wild&amp;quot; issue contained articles contributed by various C4DM people (Paul Curzon, Dan Stowell, Jane Waite and Peter W McOwan, Jo Brodie) which promoted the &amp;quot;machine listening&amp;quot; research topics studied in C4DM - how to use computational thinking to understand natural sound and related topics. Articles included:

 * How machines recognise birdsong
 * What makes music sound evil?
 * A visual way to explore soundscapes
 * Delia Derbyshire
 * Strange sounds from Apollo 11
 * How far can you hear?
 * Controlling music with gesture

Further mailing of magazines was planned, but was stopped as the funds allocated from the grant were used in full.</gtr:impact><gtr:outcomeId>58ac2d0bbd7502.48202670</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:url>http://www.cs4fn.org/magazine/magazine21.php</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>116th MPEG Meeting, Chengdu (CN), 17 - 21 Oct 2016</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>019CF655-A830-42A0-B37F-6D2E53B36B92</gtr:id><gtr:impact>C4DM's work on MVCO AMD 1 (Time Segments &amp;amp; Multi-track Audio) promoted to DAM 1 stage (Oct. 2016) with 'no comments'. It will be published as an ISO/IEC standard in June 2017. A meeting report is available at: 116th MPEG Meeting, Chengdu (CN), 17 - 21 Oct 2016.</gtr:impact><gtr:outcomeId>58ad8d19d18aa8.33503147</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://code.soundsoftware.ac.uk/attachments/download/2163/1PanosChengdu116MPEGMeetingReport.pdf</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>C4DM Concert Series 2015-2016 (organisers: Elaine Chew &amp; Andrew Mcpherson)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>B038E2C6-823E-4003-ACA2-0A94F45C289E</gtr:id><gtr:impact>Designed to challenge the ways we think about and make music, the concert series of the Centre for Digital Music at Queen Mary University of London presents cross-cutting performances drawing upon ongoing research in the themes of music cognition, creativity, and expression, new interfaces and augmented instruments, music informatics, and audio engineering at C4DM. This inaugural series was made possible, in part, by generous support from the centre's EPSRC Platform Grant for Digital Music.

The Classical music stream, directed by pianist and operations researcher Professor Elaine Chew, offers boundary-crossing concerts juxtaposing classical music with C4DM technologies and other art forms. The New Interfaces stream, directed by composer-violist and new instruments designer Dr Andrew McPherson showcases augmented instruments and interfaces developed at C4DM in concert. The Fixed Media stream, directed by computer music composer and electrical and computer engineer Dr Bob Sturm highlights electro-acoustic compositions enhanced by digital music technologies.

C4DM already has a world-leading reputation in research. Through the concerts, which are free for the public to attend, C4DM research and researchers have benefited from public exposure and being able to test and validate their technology &amp;quot;in the wild&amp;quot;. The concert series demonstrates the potential for C4DM to be a driver for musical innovation in London and to have a more prominent profile in the larger arts community. Working with external musicians to present these concerts presents scope for future collaborations.

These are the concerts that have taken place so far:

1) Art Inside Music (curated by Paul Edlin, QMUL music director), Thursday 22 October 2015, full performance: 40 mins (part of Some Things Rich and Strange, an Inside Out Festival and Music@QMUL event)
Arno Babajanian's epic Piano Trio in F# minor performed by Elaine Chew (piano), Hilary Sturt (violin), and Ian Pressland (cello), with music-driven spectral art created by Alessia Milo. Babajanian's dark, ethereal, and wild trio is illuminated by specially designed vectorial drawings created in Processing, playing with time, light, darkness, colours, frequencies, and their spectral energy. The spectral art was generated in real-time in response to the music and projected on the musicians.

2) New Interfaces Concert (curated by Andrew McPherson and Astrid Bin), Wednesday 4 November 2015, full performance: 120 mins
Laurel Pardue performed on an augmented violin with live visuals. Dianne Verdonk performs on &amp;quot;La Diantenne&amp;quot;, a new metal-sheet instrument capturing rich gestural interactions. Tim Exile performed on his &amp;quot;Flow Machine&amp;quot;, a hardware/software setup for live sampling and transformation of sound. Bogdan Vera and friends performed on the D-Box. The performance was coupled with a study of audience response conducted by C4DM/MAT PhD student Astrid Bin. The audience was introduced to the instruments before the performance and record their responses during the performance using a web-based system developed by Astrid.

3) Computer Music! by Stephen Travis Pope and other deep neural networks (curated by Bob Sturm), Monday 23 November 2015, full performance: 90 mins
Featuring music by Stephen Travis Pope, Oded Ben-Tal, Dan Stowell, and Bob L. Sturm

4) Musical Banditry (curated by Elaine Chew), Monday 21 December 2015, full performance: 60 mins
As Stravinsky famously said, &amp;quot;Lesser artists borrow, great artists steal.&amp;quot; The art of stealing was illustrated by Bob Sturm's Concatenative Variations of a Passage by Mahler and by Paul Lansky's Etudes and Parodies (winner of the 2005 International Horn Society competition) for horn trio as performed by Joel Ashford, Hilary Sturt, and Elaine Chew. Part of Lansky's piece was compared to the Beatles' While My Guitar Gently Weeps, as performed by Florian Guillaume, Giulio Moro, Keunwoo Choi, and Elio Quenton, culminating in a concurrent performance of the two. Musical similarities were made explicit using C4DM research in music informatics.
 
5) Chinese New Year Concert (curated by Luwei Yang), Monday 1 February 2016, full performance: 120 mins
The Chinese New Year Concert was organised jointly by the Chinese Students' Association (CSSA) and Confucius Institute at Queen Mary and not only celebrates Chinese New Year, but celebrated the fusion of arts and science at Queen Mary, combining music and performance with digital instruments. The event featured award-winning Chinese instrumentalists, C4DM members and the Magnetic Resonator Piano and the Mood Rater music technology.</gtr:impact><gtr:outcomeId>56ddb45d60d078.31033417</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://sites.google.com/site/c4dmconcerts1516/</gtr:url><gtr:year>2006,2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BeagleRT workshop Imperial</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>C7BEC336-1663-4CAD-8624-EF21CB5DD260</gtr:id><gtr:impact>Organised audio programming workshop at Imperial College Advanced Hackspace, to happen 14 November.

n/a (upcoming)</gtr:impact><gtr:outcomeId>545bb16f153302.30516383</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Centre for Digital Music promotional film</gtr:description><gtr:form>Engagement focused website, blog or social media channel</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6BD1A5ED-165E-4056-9464-B6ECB8A6312F</gtr:id><gtr:impact>This film was made for promoting C4DM activities to a general audience. Despite being a world-leading research group, C4DM did not have a media influence at the image of its academic and scientific excellence. The production of an inspiring promotional film was a first contribution towards a better outreach effort from C4DM. More specifically, it is formulated in accessible plain English and is displayed on the home page of C4DM website in order to generate public awareness, communicate research outcomes, disseminate knowledge and encourage public engagement and dialogue. Given its accessible character, it is also a valuable asset to reach potential partners outside of our academic niche as well as prospective students and researchers. 

The production of this films grants C4DM the ability to engage with a non-expert audience in a systematic way. An interesting by-product of this is that it also rose awareness about public engagement paradigms within C4DM researchers.

Note: At the time of reporting, the film is still in the process of being preparted for publication for a variety of legal and administrative reasons; these have been cleared and it will be posted very soon on Youtube.</gtr:impact><gtr:outcomeId>58ac376c1506f3.46024423</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Exhibition of Collidoscope at the Ars Electronica Festival 2016, Liz, Austria</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>28E92D3C-E8CD-4E0E-B691-6D39182DDFFD</gtr:id><gtr:impact>Ars Electronica, founded in 1979, is one of the world's foremost media art festivals and attracts 158,000+ visitors a year, hosted by Ars Electronica Centre - the Museum of the Future. Exhibiting at such a high profile international venue was one of the key objectives for the presentation of Collidoscope. Collidoscope was shown at Ars Electronica as part of the Ars Electronica u19 Festival 2016. Exhibitions were open from 10am to 7pm on 8 September 2016 to 12 September 2016 (5 full days). Ars Electronica requested &amp;quot;hands-on sessions&amp;quot; and that the participants would &amp;quot;experiment their musical creations&amp;quot;. The target audience was under 19s, so the sessions were highly interactive, and required substantial advanced planning (it was not meant to be a demo or showcases). 

One of the outcomes of this activity is a future ongoing collaboration with Ars Electronica. 

More about Collidoscope can be seen here: http://collidoscope.io/</gtr:impact><gtr:outcomeId>58ac42c3b367d1.65561707</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.aec.at/radicalatoms/en/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Hunan University Summer Camp Public Engagement</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8C2A4008-A832-4BAA-AA8C-F2957FB0DC65</gtr:id><gtr:impact>Ben Bengler and Nick Bryan-Kinns from C4DM, Queen Mary, took part in the two week Hunan University Summer interdisciplinary project on Social Innovation in which they led a team of QM and Chinese students to develop interactive audio and visual pieces with local Dong minority musicians for public performance in rural China. It was a highly adventurous piece of research work - creative interactive objects in a remote rural location in China with minority cultural groups. It was quite a significant piece of public engagement and interdisciplinary co-creation with rural Chinese participants. The key findings of the research were the development of methods to engage rural populations in co-creation.

The Platform Grant funded Ben Bengler for 3 months to produce high quality video and website to promote the work of the Summer Camp and to catalogue and collate the video and images collected during the Summer Camp to help with writing of research papers. Ben Bengler also contributed to the writing of a full paper for SIG-CHI 2016 conference (the leading conference in the field), and to our submission to the QM Public Engagement Award 2015.</gtr:impact><gtr:outcomeId>56d9be19c3eed8.64232923</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://eecs.qmul.ac.uk/~nickbk/NewChannel2015/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Yang L.'s talk to musicians at the Turkish Music Conservatory</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E0E1147E-F83F-4710-A0A1-41C0C9428FDC</gtr:id><gtr:impact>The talk &amp;quot;Cross-cultural analysis of vibrato: A case study comparing erhu and violin playing styles&amp;quot; was delivered as part of the CompMusic Seminar at the Istanbul Technical University's Turkish Music Conservatory. The seminar focused on Culture specific approaches in music technology.

Practicing musicians and music educators were exposed to technological and scientific methodologies in music research.</gtr:impact><gtr:outcomeId>54631f80aa0e84.17174414</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://compmusic.upf.edu/tr/node/219</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>C4DM at Sonar MarketLab 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6593D040-4D3D-4E5E-909C-B466529D8152</gtr:id><gtr:impact>C4DM presented a public exhibition of its research at the Sonar+D 2016 festival in Barcelona, 16-18 June 2016. Sonar+D is a high-profile annual exhibition of music technology attended by thousands of industry professionals, musicians and members of the general public. C4DM was chosen by competitive application for a booth on the exhibition floor. We showed 9 recent research projects supported by EPSRC funding (EP/K009559/1, EP/K032046/1, EP/G03723X/1). The booth attracted significant interest from a broad cross-section of the Sonar audience: it was nearly full for most of the three days, and anecdotally appeared to have more traffic than most of the booths at the exhibition. In parallel with this in-person exhibition, Sonar+D featured C4DM in their social media feeds, and we were interviewed for a broadcast on Spanish television. Aside from public engagement, the exhibition led to several new industry contacts, and it provided valuable feedback to the attending PhD students and postdocs on how to showcase their research in public.

The projects shown were:

1) Bela, an open-source platform for ultra-low-latency audio and sensor processing, which launched on Kickstarter in 2016;
2) TouchKeys, a transformation of the piano-style keyboard into an expressive multi-touch control surface, launched on Kickstarter in 2013 and spun out into a company in 2016;
3) Collidoscope, a collaborative audio-visual musical instrument which was a viral hit online with over 10M views;
4) RTSFX (Real-Time Sound Effects), an online library of real-time synthesised (rather than sampled) sound effects for a variety of different objects and environmental sounds;
5) Moodplay, a mobile phone-based system that allows users to collectively control music and lighting effects to express desired emotions;
6) Augmented Violin, a sensor-based extension of the traditional violin to give students constructive feedback on their playing
7) Tape.pm, an interactive object which explores novel ways to record and share an improvisation that is created on a musical instrument;
8) Aural Character of Places, an interactive online demo of soundwalks conducted around London;
9) MixRights, a demo of how content reuse is enabled by emerging MPEG standards, such as IM AF format for interactive music apps and MVCO ontology for IP rights tracking, driving a shift of power in the music value chain.

Several potential collaborations may result from this visit, including commercial opportunities related to the TouchKeys and Bela projects, and possible joint projects with other university labs who attended (e.g. IRCAM and the Music Technology Group at Universitat Pompeu Fabra in Barcelona). The publicity surrounding this exhibition may also serve as a recruitment tool for future research students in the Music and Acoustic Technology research area.

A particularly valuable outcome, especially for the PhD student attendees, was experience in showcasing their research to the public. Several students commented that they received valuable feedback on their projects and learned general skills in public engagement over the course of the three days. In one case (Tape.pm), interactions with the public helped generate a dataset which will be analysed in future research.

Finally, attendees at Sonar+D also had the opportunity to see other booths and talks at the event, generating new ideas and connections for future projects.</gtr:impact><gtr:outcomeId>58ac49984fea09.58330026</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>https://sonar.es/en/2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>ACM Conference on Human Factors in Computing Systems 2016</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B527C438-749D-493E-84F1-4BC84D13EB02</gtr:id><gtr:impact>The C4DM team (M. Barthet et al.) presented our work on the 'Participatory Music Performance in a Digital Age: Open Symphony' project. We organised interactive music performances during the Interactivity track of the ACM CHI 2016 conference, engaging both local performers and international audiences. Approximately 150 participants took part in the interactive live music performances and evaluation. Overall feedback was very positive with a wide majority of participants expressing interest in taking part again. It enabled M Barthet et al. to conduct a second evaluation of the system which was essential to be able to publish their work in a high impact factor journal.</gtr:impact><gtr:outcomeId>58aed43b0507e9.95021375</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Second workshop on Intelligent Music Production (WIMP2)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6004F883-4B0C-4836-B102-64238890433F</gtr:id><gtr:impact>Audio Engineering and Music Production are inherently technical disciplines, often involving extensive training and the investment of time. The processes involved implicitly require knowledge of signal processing and audio analysis, and can present barriers to musicians and non-specialists. The emerging ?eld of Intelligent Music Production addresses these issues via the development of systems that map complex processes to intuitive interfaces and automate elements of the processing chain.The first workshop on Intelligent Music Production was hosted by Birmingham City University last year, with support from QMUL's Centre for Digital Music. There were no plans or strategy for continued activity. With the support of the Platform Grant, the organisers hosted and organized the second workshop, greatly expanded the activities, and laid the groundwork for future events, funding, dissemination and collaborative activities. 

The 2nd Workshop was hosted by the Centre for Digital Musi, Queen Mary University of London. It took place on the 13 September 2016, It had over a hundred participants, including some from Japan, USA, Norway, Germany and France. It included people from academia, industry and some from the wider public. It was promoted widely. It had a twitter feed (still active) and videos from the event have been uploaded to two YouTube channels. The event provided an overview of some of the tools and techniques currently being developed in the ?eld. It also delivered insight for audio engineers, producers and musicians looking to gain access to new technologies. The day consisted of presentations from leading academics, industry keynotes, and additional posters and demonstrations.

Activities in the workshop included further promotion, development and update of our Open Multitrack Testbed. Some talks in the workshoo, both by others and by C4DM people, made use of this testbed.

There were 23 publications, 5 of which were from C4DM;
? Sean Enderby, Thomas Wilmering, Ryan Stables and Gy&amp;ouml;rgy Fazekas, 'A semantic architecture for knowledge representation in the digital audio workstation,' 2nd AES Workshop on Intelligent Music Production, London, UK, 13 September 2016.
? B. De Man, N. Jillings, D. Moffat, J. D. Reiss and R. Stables, &amp;quot;Subjective comparison of music production practices using the Web Audio Evaluation Tool,&amp;quot; 2nd AES Workshop on Intelligent Music Production, London, UK, 13 September 2016.
? B. De Man, J. D. Reiss, &amp;quot;The Open Multitrack Testbed: Features, content and use cases,&amp;quot; 2nd AES Workshop on Intelligent Music Production, London, UK, 13 September 2016.
? D. Ward, J. D. Reiss, &amp;quot;Loudness algorithms for automatic mixing,&amp;quot; 2nd AES Workshop on Intelligent Music Production, London, UK, 13 September 2016.
? Adib Mehrabi, Simon Dixon and Mark Sandler, 'Towards a comprehensive dataset of vocal imitations of drum sounds,' 2nd AES Workshop on Intelligent Music Production, London, UK, 13 September 2016.

Within the 23 publications, there were over 40 citations of previous C4DM work.
One workshop presentation has already led to 1 further C4DM publication;
? Jillings, Nicholas and Wang, Yonghao and Reiss, Joshua D. and Stables, Ryan, 'JSAP: A Plugin Standard for the Web Audio API with Intelligent Functionality,', Audio Engineering Society Convention 141, Sept. 2016

Several publications by others included demonstrations of creative projects. Notably,
? Francois Pachet of Sony unveiled his 'Flow machines,' which does automatic music composition. Soon after the Workshop, Sony released a press release about the work, which has now been covered widely in the international press and on social media.
? Owen Campbell demonstrated the ADEPT framework for adaptive audio effects
? Iver Jordal demonstrated artificial Neural Networks for Cross-Adaptive Audio Effects
? Gerard Roma showcased new source separation and upmixing technologies

Overall, this workshop was a success. As mentioned, future workshops, grants, placements etc are expected.</gtr:impact><gtr:outcomeId>58a484db91a315.41112079</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.aes-uk.org/forthcoming-meetings/wimp2/</gtr:url><gtr:year>2014,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>D-Box performance at the launch event of the Being Human Festival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>636A7104-6FB1-42A8-9245-2594CC1FFDC5</gtr:id><gtr:impact>To be confirmed by Andrew.</gtr:impact><gtr:outcomeId>56d98c553b46a0.20049260</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://beinghumanfestival.org/event/being-human-festival-launch-day/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>&quot;Steve Reich's Clapping Music&quot; (Marcus Pearce and Sam Duffy)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>AE992213-BDCF-436A-A099-964A2113F429</gtr:id><gtr:impact>&amp;quot;Steve Reich's Clapping Music&amp;quot; was a project funded by the Digital R and D fund for the Arts to examine whether a game-based app could stimulate engagement with a new music genre (minimalism) and improve ensemble rhythmic performance. During the project an adult focus group of 100 people was recruited. 59 participants completed the study, submitting all three surveys over a 6-week period whilst also playing the game and exploring the app content. In addition, over 3,000 short surveys were collected from players around the world. 35GB of Gameplay data was collected from over 45,000 users of the app. 

An important part of the continuation and development of this project is outreach. The team are still in the early stages of this work but have already contacted a number of schools, with some positive response, to present a workshop on the app that they have developed &amp;quot;Steve Reich's Clapping Music&amp;quot;. The workshop will aim to engage young students (11-16) in a musical genre they may not yet have experienced (Minimalism) which we hope they will continue to explore through the app. They also plan to run sessions on the process of developing the app at events such as 'Get into Tech' which aim to encourage young adults to consider a career in technology. The app was developed through an interdisciplinary project team of researchers, technologists and musicians, to transform a piece of music into a game to encourage learning and skills acquisition. The research team hopes to stimulate thinking on how to transform an idea into a compelling game. 

The team are working with The London Sinfonietta, an orchestra specialising in contemporary music, with a strong relationship with Steve Reich, to develop the workshops that will be offered.</gtr:impact><gtr:outcomeId>56dd9943ac2204.88752800</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>RA position for three years for Brecht de Man</gtr:description><gtr:end>2017-09-02</gtr:end><gtr:fundingOrg>Yamaha Motors</gtr:fundingOrg><gtr:id>1F696909-6192-4FB7-A6AB-ADCF76E14BB6</gtr:id><gtr:outcomeId>58aaeef2b45963.91668461</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2016-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>897000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Early Career Fellowship</gtr:description><gtr:end>2020-12-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N005112/1</gtr:fundingRef><gtr:id>56665420-7C10-485F-851A-7C6C750E1C5C</gtr:id><gtr:outcomeId>56deda7832a736.53010589</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>QMUL Centre for Public Engagement small award (PI: Mathieu Barthet)</gtr:description><gtr:fundingOrg>Queen Mary University of London</gtr:fundingOrg><gtr:id>87006A8A-F7C4-4FF6-8FD2-5E4352D5C02E</gtr:id><gtr:outcomeId>58aedabce7fe71.90909191</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>676850</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>Horizon 2020 (H2020-ICT-2015)</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>Proposal number 688382</gtr:fundingRef><gtr:id>D781F833-5CC7-4A3D-B4C4-2F78D0C566CC</gtr:id><gtr:outcomeId>56dff7a55117e4.60000148</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>This software analyses musical audio files and divides them into musically meaningful segments, typically 4-10 per song.</gtr:description><gtr:grantRef>EP/K009559/1</gtr:grantRef><gtr:id>3378B417-DC73-4659-BA44-E2D6D12A0C80</gtr:id><gtr:impact>It is often downloaded with Sonic Visualiser - see other IP entry</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>m-7794994560.810732766576a2</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Audio Segmentation VAMP plugin</gtr:title><gtr:yearProtectionGranted>2008</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Sonic Visualiser is an application for viewing and analysing the contents of music audio files. This is an open source software framework that works with its own plugin architecture called VAMP. Sonic Visualiser is Free Software, distributed under the GNU General Public License (v2 or later) and available for Linux, OS/X, and Windows.

Sonic Visualiser is intended for use by people in various academic disciplines, but also by non-academics, professional audio users, and interested hobbyists. It supports loading additional analysis plugins that other developers or institutions can publish, and at least 10 institutions (besides QM) have chosen to publish plugins in this format. Sonic Visualiser was first published by the Centre for Digital Music in 2007 and has been continuously developed at the Centre and maintained as free, cross-platform open-source software ever since.</gtr:description><gtr:grantRef>EP/K009559/1</gtr:grantRef><gtr:id>90C17E0B-8F37-4646-9538-B6DBE6035174</gtr:id><gtr:impact>According to our ongoing user survey that has been carried out since the start of 2014 (having 1053 responses to date, all from people who were actually using Sonic Visualiser at the time):

 * 49% of respondents use Sonic Visualiser primarily in a personal capacity, 10% for professional work, 33% in academia at some level (the rest &amp;quot;other&amp;quot;);
 * 46% use it primarily in the field of musicology or music analysis, 19% in music composition or production, 16% in audio engineering or signal processing, 5% in software development, 2% speech processing (the rest &amp;quot;other&amp;quot;);
 * 55% said they found it &amp;quot;very enjoyable&amp;quot; to use and 41% &amp;quot;moderately enjoyable&amp;quot;;
 * 27% said they found it &amp;quot;very easy&amp;quot; to use and 62% &amp;quot;moderately easy&amp;quot;;
 * 59% of respondents had some additional analysis plugins installed on top of the default configuration; among these respondents, the mean number of additional plugin sets installed was 3.4.

Sonic Visualiser's version-update-checker logs keep a count of distinct IP addresses from which SV has been used (for users who have agreed to this). They show over 138,000 distinct addresses during the past year, with around 4,000 distinct addresses per week on average.

An impression of the extent of usage can be gained by viewing illustrative Youtube videos about sound analysis made by third parties:
at a quick scan I count at least 100 that show Sonic Visualiser, of varying quality. Sonic Visualiser has also been used in online course material, such as the popular Coursera &amp;quot;Audio Signal Processing for Music Applications&amp;quot;, as well as in courses for academics in non-technical disciplines, such as the Digital Humanities at Oxford Summer School.

This is a free, open source project; it was used as an Impact Case study for REF 2014.

See the Sonic Visualiser website for details: http://sonicvisualiser.org 

Link to a page about the new release: http://sonicvisualiser.org/new-in-v3.html

Sonic Visualiser has been available for about a decade now, and this is one of the most substantial updates it's ever had (hence the updated major release number).</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>m-5840728334.93419776656fcc</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Sonic Visualiser</gtr:title><gtr:yearProtectionGranted>2006</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Soundbite is an iTunes plugin developed under this project. It is currently available from www.isophonics.net as freeware. Commercial licences are under negotiation. Under the Platform Grant it was adapted for an Android platform and reconstructed as a client-server architecture.

2017: A commercial licence for the core technology that was signed with Music XRay, a company based in the USA, several years ago is starting to yield royalties.</gtr:description><gtr:grantRef>EP/K009559/1</gtr:grantRef><gtr:id>14C1BA0F-B3F2-4FB1-9ABE-9EAE2037DF01</gtr:id><gtr:impact>It has been downloaded to hundreds of (unregistered) users. It was deployed in various grants to analyse collections of music for recommendation, navigation and play listing. It has been deployed in several commercial music aggregators workflows, enabling trials of large scale music recommendation systems.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>r-2240247775.0929523d6870ae</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Soundbite</gtr:title><gtr:yearProtectionGranted>2008</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>In the context of his PhD research, Elio Quinton is annotating the meter of the GTZAN dataset. There is no publicly available dataset of commercial music annotated with meter, to his knowledge. The only dataset having meter information is the Meetrens Tune Collection, which consist of Dutch folk tunes and songs either as MIDI files, audio recordings or score, but in either case are strictly monophonic melodies. As a consequence, the annotations would increase the value of a dataset that has already been used extensively in the community (more than a 100 published papers) and annotated with tempo and key. So far Elio has annotated the dataset in its entirety. However, in order to have a measure of the reliability of the annotations, the dataset should be annotated two or more times by different annotators. To complete this annotation task, Elio recruited professional musicians to carry it out.</gtr:description><gtr:id>F4F36290-D7A7-49C6-8FE8-139CFEFB42A1</gtr:id><gtr:impact>The initial aim has been completed. New questions generated during the first leg of the project are being investigated. The creation of this dataset has enabled two major streams of findings. On one hand, the creation of this dataset has enabled the validation of my research work, as stated in the original grant application. On the other hand, the analysis of the corpus itself and its comparison with other existing annotation corpuses (this is ongoing work) leads to valuable findings. Some of the findings are: enabling the assessment of the quality of other publicly available annotations datasets, augmenting the value of existing corpuses (including the one created under this grant).</gtr:impact><gtr:outcomeId>56d9a4b9df25d5.25375719</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Annotation of the meter of the GTZAN dataset (Elio Quinton)</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.isophonics.net/content/metrical-structure-annotations-gtzan-dataset</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>As a result of employing the PhD student Luwei Yang (under the supervision of Dr Elaine Chew, Queen Mary) and in collaboration with Dr Chew, a Hidden Markov Model has been designed, with different observation likelihood distributions (Gaussian Model and Gaussian Mixture Model), for portamento detection. 

As part of this research, a new dataset of fiddle music has been created for evaluation of portamento detection. The dataset comprises of 20 fiddle performances by Benny Martin, Mark O'Connor, Johnny Gimble, Bonnie Rideout and Ian Walsh.</gtr:description><gtr:id>4B5476EE-BC29-4D5C-81F8-33C019F5B3F3</gtr:id><gtr:impact>This research project is still ongoing.</gtr:impact><gtr:outcomeId>58c2d9cd508d93.09546525</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>New dataset of fiddle music (Elaine Chew&amp; Luwei Yang)</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Media Value Chain Ontology model for time-segments and multi-track audio including reference software and conformance datasets will be published by ISO/IEC MPEG in due time as: &amp;quot;ISO/IEC 21000-19 AMD 1 Extensions on Time-Segments and Multi-Track Audio&amp;quot;.</gtr:description><gtr:id>1684567C-A180-42DB-81D2-9661B37A1375</gtr:id><gtr:impact>1) Research tools (incl. conformance bitstreams aiming for achieving interoperability among emerging music services) for music interaction, learning, creation and sharing using IM AF are made available at: https://code.soundsoftware.ac.uk/projects/mpegdevelopments;

2) Software (incl. conformance bitstreams aiming for achieving interoperability among emerging music services) for music interaction, learning, creation and sharing using IM AF is made available at: https://code.soundsoftware.ac.uk/projects/mpegdevelopments; 

3) Public Engagement activities / Creative &amp;amp; artistic projects: inspired artists such as Emilio Molina Mart&amp;iacute;nez (ES), Tracy Redhead (AU) &amp;amp; Imogen Heap (UK) are releasing raw music material (i.e., multi-track audio, lyrics, album cover and credits) suitable for IM AF encoding. Creative projects also reported experimenting with different IM AF layouts focusing on its different features (i.e. remixing, Karaoke, chords, groups and rules).</gtr:impact><gtr:outcomeId>58ac50063d6172.48336659</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>The Media Value Chain Ontology model (Panos Kudumakis, C4DM)</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/mpegdevelopments</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The CHiME-Home dataset is a collection of annotated domestic environment audio recordings, described in:
P. Foster, S. Sigtia, S. Krstulovic, J. Barker, M. D. Plumbley. &amp;quot;CHiME-Home: A Dataset for Sound Source Recognition in a Domestic Environment,&amp;quot; in Proceedings of the 11th Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015; and available from https://archive.org/details/chime-home</gtr:description><gtr:id>A74E563E-FE44-41E8-86D6-0810AEAD03CF</gtr:id><gtr:impact>The dataset is being used in task 4 of the DCASE2106 Challenge, for performance evaluation of systems for the detection and classification of sound events. The challenge is organised by the Audio Research Group of Tampere University of Technology, by the QMUL Centre for Digital Music and by IRCCYN, and by the University of Surrey, under the auspices of the Audio and Acoustic Signal Processing (AASP) technical committee of the IEEE Signal Processing Society. See http://www.cs.tut.fi/sgn/arg/dcase2016/task-audio-tagging</gtr:impact><gtr:outcomeId>56d8cad2264c65.03464443</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>CHiME-Home</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://archive.org/details/chime-home</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The goal of this project is to populate the recently developed Open Multitrack Testbed (multitrack.eecs.qmul.ac.uk), an online platform for storing, searching and browsing multitrack audio and mixtures thereof. It is a resource intended for but not limited to users such as researchers in MIR and intelligent music production, as well as sound engineering students and audio software developers. The researchers' aim is to realise the testbed's potential impact by including the most popular multitrack datasets around, complemented with extensive metadata. The idea of publishing a multi-track database has been raised several times and a few prior attempts have been made within the Queen Mary researchers' group. The project is led by Brecht De Man, who collected audio files for a number of multi-track recordings. Gy&amp;ouml;rgy Fazekas (FAST IMPACt) advised on semantic technologies, having developed the Studio Ontology Framework which is being used in the testbed. Mariano Mora-Mcginity (also a FAST IMPACt team member) finished developing the testbed (barring solving any issues that may arise during the annotation) to a state in which it 
1. shows all content for the user to browse through; 
2. has a search interface to search and filter using any of the metadata fields; 
3. allows an authorised user (password protected with customisable permission) to enter new metadata associated with audio on the testbed server (c4dm.eecs.qmul.ac.uk/ multitrack) or elsewhere on the web. 

The C4DM team has annotated over 600 multitracks have been annotated, comprising over 3000 individual tracks. Full details have been maintained about the instrumentation, genre, recording process, artifacts, attribution.... This is a sufficient number for this data to now be used in machine learning and data mining applications. It is contributing to research on music mixing practices and multitrack audio processing.</gtr:description><gtr:id>59A80E46-4E8E-41FE-83F1-EAFBA17D04BF</gtr:id><gtr:impact>Several researchers have referenced the testbed in their papers since its inception, and it was well received during a session on multitrack audio datasets of the Unconference at 15th Conference of the International Society for Music Information Retrieval in Taipei, Taiwan, November 2014. Furthermore, within C4DM the local multitrack repository is extensively used for research, but as the licenses are not open - or unclear - disseminating data and results is less than straightforward. This is especially a problem when funding bodies, journals or conferences require open data for sustainability and reproducibility purposes. The research team is positive that this resource, the first of its kind, has the potential of becoming the main resource for any researcher, student or developer who needs multitrack audio and mixtures thereof, provided it has the critical mass to become appealing as a go-to platform. It is evident that there is already a strong support base for this idea, and already extensive usage of the still limited dataset. 

Multitrack audio is a valuable resource for a wide range of audio professionals, educators, students, developers, and researchers. Many of these communities are not that aware of C4DM and its activities. Notably, this project is expected to have significant impact with music technology students and educators, who will use the content in learning and teaching music production skills. R&amp;amp;D areas in which large and diverse sets of high quality, multi-stream audio are essential in music information retrieval (MIR) research, development of improved music production tools, and tasks like audio source separation, audio segmentation and research in auditory perception.</gtr:impact><gtr:outcomeId>56d8048a94bb36.58325250</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Multitrack Testbed (Team: QMUL)</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://multitrack.eecs.qmul.ac.uk</gtr:url></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>Research tools (incl. conformance bitstreams aiming for achieving interoperability among emerging music services) for music interaction, learning, creation and sharing using IM AF are made available at: https://code.soundsoftware.ac.uk/projects/mpegdevelopments</gtr:description><gtr:id>ACB353FB-E753-499F-859F-FAAF31879AB7</gtr:id><gtr:impact>? The work on MPEG-A: Interactive Music Application Format (IM AF) has inspired not only commercial products such as STEMS by Native Instruments (2015) and LANDR by MixGenius (2016) but also artists such as Emilio Molina Mart&amp;iacute;nez (ES), Tracy Redhead (AU) &amp;amp; Imogen Heap (UK) releasing raw music material (i.e., multi-track audio, lyrics, album cover and credits) suitable for IM AF encoding. Creative projects also reported experimenting with different IM AF layouts focusing on its different features (i.e. remixing, karaoke, chords, groups and rules).

? The work on MPEG-M: Multimedia Service Platform Technologies resulted on MixRights: Fair Trade Music Ecosystem launched at Mycelia Hack Weekend, Sonos Studios, London, 1-3 Apr. 2016 and more recently it further presented at Interactive Music Hack-Fest, MAT &amp;amp; C4DM Studios, QMUL, London, 11 June 2016, Sonar+D, Barcelona, 16-18 June 2016 and Mycelia Hack Weekend, Sonos Studios, London, 8-10 July 2016.</gtr:impact><gtr:outcomeId>58ac515ef31d95.26816088</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Research tools for music interaction, learning, creation and sharing using IM AF</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/mpegdevelopments</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>AccessibleSpectrumAnalyser is a real time spectrum analysis plug-in that allows visual impaired users to inspect spectrograms using the same sonification as the accessible peak meter. Instead of monitoring the amplitude of the audio signal though, you will be monitoring the frequency components of the signal within a customizable selection of frequencies.

It is released under the Cokos WDL license, which in short means you can alter it and redistribute it freely, even without providing the source code of your derivative work.</gtr:description><gtr:id>9E7E8698-0446-4650-91B8-F4E27AA656AF</gtr:id><gtr:impact>AccessibleSpectrumAnalyser is a real time spectrum analysis plug-in that allows visual impaired users to inspect spectrograms using the same sonification as the accessible peak meter. Instead of monitoring the amplitude of the audio signal though, you will be monitoring the frequency components of the signal within a customizable selection of frequencies.</gtr:impact><gtr:outcomeId>56dd9035027a39.98951437</gtr:outcomeId><gtr:title>Accessible Spectrum Analyser</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/apm/spectrum.html</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>AVA is an automatic vibrato and portamento detection and analysis tool. It accepts raw audio and automatically tracks the vibrato and portamento to display their expressive parameters for inspection and further statistical analysis.

The applications of AVA include music education and expression analysis, and its outputs provide a useful base for expression synthesis and transformation.
A free application for automatic vibrato and portamento analysis has been developed an made available online-see link below. An appropriate copyright agreement accompanies each download.</gtr:description><gtr:id>3787E4AF-4C4F-4823-AB3A-95579C5CE4E4</gtr:id><gtr:impact>Not applicable at this time.</gtr:impact><gtr:outcomeId>58c2da80183959.50548869</gtr:outcomeId><gtr:title>AVA - automatic vibrato and portamento detection and analysis tool (Elaine Chew &amp; Luwei Yang).</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://luweiyang.com/research/ava-project/</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>During her stay at NYU, the PhD student Maria Pantelli from the Centre for Digital Music collaborated with Rachel Bittner from NYU and together they implemented the software package MOTIF: a melody description framework for audio recordings. The software is available at https://github.com/rabitt/motif .</gtr:description><gtr:id>C7702F53-1328-46D9-BAB6-B48669157D2C</gtr:id><gtr:impact>The software is available at https://github.com/rabitt/motif. 

Maria also created an interactive demo for the investigation of music outliers which she demonstrated at the Monthly Music Hackathon, Spotify New York. The demo is available at http://eecs.qmul.ac.uk/~mp305/Outliers_world_tabs.html.</gtr:impact><gtr:outcomeId>58ab0be3de7d46.32722740</gtr:outcomeId><gtr:title>Software package MOTIF</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/rabitt/motif</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Software is being developed to port company-specific data structures into more generic formats accepted by relevant research groups, mostly via the use of widely accepted Semantic Web (RDF) ontologies.

A triple store of audio track metadata is under development, which has SPARQL endpoints at:
http://138.37.95.193:8080/openrdf-workbench/repositories/iso-rec/query
http://138.37.95.193:8080/openrdf-sesame/repositories/iso-rec?query 
(requires authentication, and urls subject to change in the future)</gtr:description><gtr:id>C7FF3F5C-35DF-4AAA-96D6-AB631BA0B74E</gtr:id><gtr:impact>It is expected that the project will help to inform decisions regarding how to store, process and share metadata and feature data from licensed audio collections.

The project supports ongoing development of a Proof of Concept mobile app which creates playlists and music recommendations by interacting with a server back end.

Development of server middleware which utilizes the SPARQL endpoint rather than a company-specific database is ongoing. Using the SPARQL endpoint will enable the Proof of Concept mobile app (see 12) to access audio feature data from multiple collections, rather than the single collection used to date.</gtr:impact><gtr:outcomeId>56dedffb4b69c1.75738293</gtr:outcomeId><gtr:title>Isophonics triple store for music metadata</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This application is based on international MPEG standards for achieving interoperability such as IM AF format for interactive music services (remixing, Karaoke &amp;amp; collaborative music creation) and MVCO ontology for IP rights tracking aims to enable both a rich user experience and a royalties fair and transparent music ecosystem.</gtr:description><gtr:id>50E1B90A-D7CC-4B6F-BAD6-58223489A612</gtr:id><gtr:impact>MPEG-A: Interactive Music Application Format (IM AF) offers rich user experience by supporting, among other features, multi-track audio with volume sliders for DJ mixing and lyrics for Karaoke applications, while from the other side, MPEG-21: Media Value Chain Ontology (MVCO) facilitates transparent content governance by supporting in a machine proccesable way the relationships' representation among user roles and their permissible actions on a particular IP entity. 

Therefore, this project/demonstrator based on international MPEG standards for achieving interoperability such as IM AF format for interactive music services (remixing, karaoke &amp;amp; collaborative music creation) and MVCO ontology for IP rights tracking aims to enable both a rich user experience and a royalties fair and transparent music ecosystem. 

Both IM AF &amp;amp; MVCO standards involved contributions by C4DM.

This work is taking place under the auspices of ISO/IEC MPEG and it has already been approved and supported by both MPEG Reqs &amp;amp; Systems groups. Publication stage:
? &amp;quot;Text of ISO/IEC 21000-19 PDAM 1 Extensions on Time-Segments and Multi-Track Audio&amp;quot;, Ref. as ISO/IEC JTC1/SC29/WG11/N16178, Geneva, Switzerland, June 2016.
? &amp;quot;Requirements for Media Value Chain Ontology&amp;quot;, Ref. as ISO/IEC JTC1/SC29/WG11/N15352, Warsaw, Poland, June 2015.</gtr:impact><gtr:outcomeId>56deedba5fd254.40279330</gtr:outcomeId><gtr:title>MixRights: Fair Trade Music Ecosystem</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/mpegdevelopments</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>- Node.js web application for creative participation of audiences
- Processing module to dynamically generate graphic scores for performers
- Web application for audiences to conduct performers</gtr:description><gtr:id>6F849202-3E26-4A9D-82AA-59AF6F87B871</gtr:id><gtr:impact>not known</gtr:impact><gtr:outcomeId>58aedeada5c5d3.93836726</gtr:outcomeId><gtr:title>Open Symphony web-based creative participation system</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Bela (formerly called BeagleRT) is an ultra-low-latency real-time audio platform for the BeagleBone Black embedded computer. It can be used to build musical instruments, including the D-Box hackable musical instrument developed for the EPSRC Hackable Instruments project. The software is of particular interest to audio and music researchers, especially those building real-time systems.

With less than 1ms of latency between action and sound, Bela performs faster than any other computer-based environment on the market, including high-spec laptops. It also features audio-rate sampling of every analog and digital input which makes design of sensor systems convenient. In 2015, a built-in browser-based IDE was added along with support for the Pure Data graphical computer music language widely used in the digital music community. Since 2016, we have continued to add support for other programming languages and hardware accessories, as well as extending the documentation and online resources available to the community.</gtr:description><gtr:id>A5503F9C-7177-4721-B366-DBE05BEB54B6</gtr:id><gtr:impact>Bela has gained significant traction amongst researchers, musicians and hobbyists in the 2014-17 period, with the number of users and community members growing every month.

On 29 February 2016, we launched a Kickstarter crowdfunding campaign for Bela, seeking &amp;pound;5k to build and distribute the hardware. We exceeded our funding goal in less than 4 hours (out of a 32-day campaign), ultimately raising &amp;pound;55k from over 500 backers. Following the campaign, we received invitations to collaborate from major academic institutions (IRCAM in France, STEIM in the Netherlands, University of Virginia, University of Edinburgh, University of Glasgow, Goldsmiths University and several others) and industry (Cycling74, makers of the popular Max/MSP software). 

Meanwhile, since 2014 we have held 10+ workshops on Bela and/or the D-Box instrument including 2 workshops at the Sonar music festival, a workshop at the NIME 2015 conference (Baton Rouge, LA, USA), an Audio Music Hackathon sponsored by Harman Audio, and an Accessible Music Hackathon with the charity Drake Music (held at QMUL). Following the successful Kickstarter campaign, we spun out a new company, Augmented Instruments Ltd, to commercialise the platform, and relaunched a public web shop in October 2016. We maintain several community resources, including a blog, wiki, web forum and several social media accounts. Our users actively contribute to these resources and frequently share projects they make using Bela. Community development itself has become an area of research for us (e.g. Morreale et al., &amp;quot;Building a maker community around an open hardware platform&amp;quot;, Proc. CHI 2017), and we have received a grant from Innovate UK (Feb-Oct 2017) to further develop the platform and build our community.</gtr:impact><gtr:outcomeId>545bb694961050.49097753</gtr:outcomeId><gtr:title>Bela Audio Platform</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://bela.io</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>SoundBite is the quickest and easiest way to create great-sounding playlists in iTunes. Once installed, SoundBite will get to know your music collection and when it's done, you're ready to create playlists to suit your mood with just a couple of mouse clicks! This updated version of the SoundBite app for iTunes works with the new Mac Operating Systems. The technology is now again accessible to many potential desktop users.</gtr:description><gtr:id>5995F130-A80B-4D56-A65A-DC9A231AF2ED</gtr:id><gtr:impact>From a commercial perspective, the app provides a slick demonstrator for potential investors without the overhead of digging deep into mobile platforms. The app was presented during Queen Mary University of London's School of Elec. Eng. and Computer Sc. Research Showcase (3 April 2014) which attracted a lot of visitors including many industrials. The app raised the interest of our partners Omniphone and Abbey Road Studios we collaborate with on research projects. We demonstrated the app to Rocket Music, a company founded by Elton John, and are currently in discussion with them to create a new digital music product.
From a pedagogical perspective, the SoundBite app represents a useful way to introduce UG and PG students to some of the Music Information Retrieval technologies developed by Queen Mary University of London.</gtr:impact><gtr:outcomeId>54635a92e50e57.72074731</gtr:outcomeId><gtr:title>SoundBite Application for iTunes</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>http://www.isophonics.net/LatestMacVersion</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>ShapeTones is an audiovisual memory game for iOS (iPhone/iPad). A sequence of 3 shapes and tones (&amp;quot;ShapeTones&amp;quot;) is played, and the player tries to reproduce it. Tapping different areas of the screen trigger different ShapeTones. The game starts with 3 ShapeTones. As the game evolves, more ShapeTones become available. When a new ShapeTone is added, a trial screen is shown to demonstrate where each ShapeTone is triggered. Some surprises happen along the way. As a one player game, the sequence is created automatically. As a two player game, one player creates the sequence, passes the device to the other player, who tries to repeat it. They then swap the roles.</gtr:description><gtr:id>189C2810-FE52-4B4E-9EE6-FB6BF2B2ED17</gtr:id><gtr:impact>Not currently known.</gtr:impact><gtr:outcomeId>56dd905af3c746.47676619</gtr:outcomeId><gtr:title>iOS Crossmodal memory game: ShapeTones</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/?q=shapetones</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Peak level meters, along with other components which rely on the sense of sight for their use, are inherently inaccessible to people living visual impairments. Our AccessiblePeakMeter is the first plug-in which makes these previously inaccessible meters completely accessible. It uses real-time sonification to deliver information to the user about audio levels and peaks in audio signals, and so supports core activities in audio production.

The AccessiblePeakMeter comes as a VST, AU or AAX plug-in, two of the main industry standards for the deployment of digital audio effects into professional DAWs (e.g. Cakewalk Sonar, Cockos Reaper, Ableton Live). The plug-in can be run on both Windows (32/64) and Mac platforms and it is free for download and completely open source.

It is released under the Cokos WDL license, which in short means you can alter it and redistribute it freely, even without providing the source code of your derivative work.</gtr:description><gtr:id>DD8B45BD-9AC6-4FE5-9658-35B0BFE7D1F3</gtr:id><gtr:impact>AccessiblePeakMeter (http://depic.eecs.qmul.ac.uk/apm/downloads.html#download) received the Best Solution by a Large Organization award in the AT&amp;amp;T and New York University Connect Ability Challenge, a global software competition for software technologies that improve the lives of people living with disabilities. The winner were announced on Sunday July 26 2015, the day of the 25th anniversary of the Americans with Disabilities Act.

AccessiblePeakMeter was also featured in the issue 19 of the Computer Science for Fun magazine, which explains the AccessiblePeakMeter in a bright, simple and effective way.

AccessiblePeakMeter was shortlisted for QM Public Engagement Award 2015.</gtr:impact><gtr:outcomeId>56dd901609e928.71637001</gtr:outcomeId><gtr:title>Accessible Peak Level plugin</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/apm/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>ofxEnveloper is an openFrameworks addon that converts gestures into an envelope type of graphical information, by recognising inflection points from the gesture. Thresholds for horizontal and vertical lines can also be defined. It is particularly aimed for animation and sound applications, and for use with drawing surfaces (such as Wacom tablets or touchscreen interfaces).

Possible uses:
1 audio effect or EQ envelopes;
2 animation curves;
3 creation/manipulation of audio and/or visual sequences;
4 arpeggiators.
Compatibility: openFrameworks 0.9.0

It is released under the Cokos WDL license, which in short means you can alter it and redistribute it freely, even without providing the source code of your derivative work.</gtr:description><gtr:id>95BE7D8B-C195-4D40-B70F-D4F992161E7C</gtr:id><gtr:impact>Not known.</gtr:impact><gtr:outcomeId>56dd90745d5716.08101556</gtr:outcomeId><gtr:title>OpenFrameworks Add-on for gesture to envelope converter</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/?q=ofxenveloper</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The Collidoscope is a unique hardware device and real-time granular sampler/synthesizer for collaborative performance and exploration that was developed as a side project by C4DM researchers (Ben Bengler and Fiore Martin). Collidoscope's intuitive 'hands-on' approach to sampling and real-time sound manipulation captivates the
general public as well as professional musicians alike.</gtr:description><gtr:id>0552C58C-45EE-4FC2-A046-740F49631223</gtr:id><gtr:impact>Exhibition in public, including international exhibition. Engagement with public in the design process and at public events. Dissemination of information about the project through website, twitter, and micro-videos. 

Dissemination of all software, hardware, and 3D designs as open source.</gtr:impact><gtr:outcomeId>58ac44f59db866.20510728</gtr:outcomeId><gtr:title>Collidoscope</gtr:title><gtr:type>Physical Model/Kit</gtr:type><gtr:url>http://collidoscope.io/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>Augmented Instruments Ltd</gtr:companyName><gtr:description>On 8 September 2016, a spinout company Augmented Instruments Ltd was formed to lead the commercialisation of the Bela embedded audio platform. Bela was created within the Centre for Digital Music as an open-source hardware and software tool for creating musical instruments and interactive audio systems, targeted at the maker community. The spinout company now handles sales and support to the general public, and has received a &amp;pound;30k grant from Innovate UK to further develop the Bela technology.</gtr:description><gtr:id>89000922-54E7-4A9A-BE7C-1AA2454F2CC0</gtr:id><gtr:impact>The spinout company now handles sales and support to the general public, and has received a &amp;pound;30k grant from Innovate UK to further develop the Bela technology.</gtr:impact><gtr:outcomeId>58b5755201d384.19186929</gtr:outcomeId><gtr:yearCompanyFormed>2016</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>6B25DC16-99BB-4C6F-B65E-8D142A8034DD</gtr:id><gtr:title>An environment for submillisecond-latency audio and sensor processing on BeagleBone Black</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8684d6f92c31ca847100a52153e1c26"><gtr:id>a8684d6f92c31ca847100a52153e1c26</gtr:id><gtr:otherNames>McPherson, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ded190934526.16791143</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C2813BCA-1D9E-4F0D-991B-41DEA2AA952A</gtr:id><gtr:title>110th MPEG Meeting Report, Strasbourg, France, 20-24 Oct. 2014, BSI IST/37_14_0872</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/82e457ccce3cee3a6691ac0ddb9f51d7"><gtr:id>82e457ccce3cee3a6691ac0ddb9f51d7</gtr:id><gtr:otherNames>Kudumakis, P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545cef2eba5609.66009995</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7B37C32C-ED70-41B1-B41B-EA9F8111711A</gtr:id><gtr:title>APE: Audio Peceptual Evaluation Toolbox for MATLAB</gtr:title><gtr:parentPublicationTitle>136th Audio Engineering Society Convention</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/730c3648b50c50d98d234809b97f5950"><gtr:id>730c3648b50c50d98d234809b97f5950</gtr:id><gtr:otherNames>De Man B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545ce87c436d35.25944293</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>240B8206-93C8-42DD-8023-A3C84E97F2D0</gtr:id><gtr:title>The Digital Music Lab</gtr:title><gtr:parentPublicationTitle>Journal on Computing and Cultural Heritage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe4b41ae102.43566242</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D43CDC7F-4974-4C23-8CAE-09001674F702</gtr:id><gtr:title>Demo paper: The BBC Desktop Jukebox music recommendation system: A large scale trial with professional users</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2fd53c70d717490415b8aa5cf9c39f79"><gtr:id>2fd53c70d717490415b8aa5cf9c39f79</gtr:id><gtr:otherNames>Fazekas G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546341d466a9d2.24351109</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FEE620BB-4154-4824-9C18-C0EF820C7905</gtr:id><gtr:title>Request of ISO/IEC 21000-19 AMD 1 Extensions on Time-Segments and Multi-Track Audio, Ref. as ISO/IEC JTC1/SC29/WG11/N16177, Geneva, Switzerland, June 2016.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62bf602dedc27c01e3a375e514f62402"><gtr:id>62bf602dedc27c01e3a375e514f62402</gtr:id><gtr:otherNames>N/A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ad8a8c97acb3.36997249</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A980CEF-945A-45A1-97E7-58447C6033FF</gtr:id><gtr:title>Open Symphony: Creative Participation for Audiences of Live Music Performances</gtr:title><gtr:parentPublicationTitle>IEEE MultiMedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e1ae91a136cc1808e002adb45e242f69"><gtr:id>e1ae91a136cc1808e002adb45e242f69</gtr:id><gtr:otherNames>Wu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58aed034222b51.05214220</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>004C72EB-9527-4CAC-802A-9B4966B3EB1B</gtr:id><gtr:title>Semantic models of musical mood: Comparison between crowd-sourced and curated editorial tags</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7c92816b2a582aaba721e22aa41459e"><gtr:id>d7c92816b2a582aaba721e22aa41459e</gtr:id><gtr:otherNames>Saari P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546342d9289ff5.20157486</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8EBFBBA9-89C0-4174-97A7-54F8955F04D3</gtr:id><gtr:title>Using community engagement to drive co-creation in rural China</gtr:title><gtr:parentPublicationTitle>International Journal of Design</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1f12ef0f1b6927e46397672c109434ab"><gtr:id>1f12ef0f1b6927e46397672c109434ab</gtr:id><gtr:otherNames>Wang, W.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ac63653dd6c0.85136205</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E51E03CF-0401-474A-B7D3-BBE9F953E288</gtr:id><gtr:title>Text of ISO/IEC 21000-19 PDAM 1 Extensions on Time-Segments and Multi-Track Audio, Ref. as ISO/IEC JTC1/SC29/WG11/N16178, Geneva, Switzerland, June 2016.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62bf602dedc27c01e3a375e514f62402"><gtr:id>62bf602dedc27c01e3a375e514f62402</gtr:id><gtr:otherNames>N/A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aaf57b270160.69508408</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F79DCB49-F5B4-4256-AE56-3A28ECE6F969</gtr:id><gtr:title>An Interlanguage Study of Musical Timbre Semantic Dimensions and Their Acoustic Correlates</gtr:title><gtr:parentPublicationTitle>Music Perception: An Interdisciplinary Journal</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f28ef150e55728edf9a0f23f5ab108b0"><gtr:id>f28ef150e55728edf9a0f23f5ab108b0</gtr:id><gtr:otherNames>Zacharakis A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545cdf39ce9c77.80481129</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3DB1D87D-1643-4C31-BAE4-F5E9AF78A4FB</gtr:id><gtr:title>Automatic subgrouping of multitrack audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d0c3b1e19a127525da1b7c3123e28c8"><gtr:id>0d0c3b1e19a127525da1b7c3123e28c8</gtr:id><gtr:otherNames>Ronan, D. M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8a832bb2a3.43275796</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A2E616E-7B4D-4C62-BC88-942AB18EB6CA</gtr:id><gtr:title>Demo hour</gtr:title><gtr:parentPublicationTitle>interactions</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/31640edfba40e3616bee251920d351a5"><gtr:id>31640edfba40e3616bee251920d351a5</gtr:id><gtr:otherNames>Kim S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546225cc1a8542.09132556</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>458F0F62-A579-4FE2-A9CD-2E76AA2AFF84</gtr:id><gtr:title>The Mood Conductor System: Audience and Performer Interaction using Mobile Technology and Emotion Cues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/81ab13dd87a821207a4ebb226adc9306"><gtr:id>81ab13dd87a821207a4ebb226adc9306</gtr:id><gtr:otherNames>Fazekas, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>97822909669236</gtr:isbn><gtr:outcomeId>54634a53737844.74388490</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B4E4E5C9-6051-462B-80D2-EECF263782C1</gtr:id><gtr:title>A study of instrument-wise onset detection in Beijing Opera percussion ensembles</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d4295036b925d78acf53ac26e33a695b"><gtr:id>d4295036b925d78acf53ac26e33a695b</gtr:id><gtr:otherNames>Tian M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460b59d60ed95.02101069</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0C2873E7-7B4E-4504-A361-1BC619B78A7C</gtr:id><gtr:title>Computational Analysis of the Live Music Archive</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2eb4f439cbab291121cb1f4342c03c5e"><gtr:id>2eb4f439cbab291121cb1f4342c03c5e</gtr:id><gtr:otherNames>Bechhofer S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460b6f0b26321.41233617</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>680E00FB-E86C-40AA-8278-01B9CC85ED0F</gtr:id><gtr:title>Cross-cultural comparisons of expressivity in recorded erhu and violin music: Performer vibrato styles</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9cd3eb1bf88accc501e5f42973e8397"><gtr:id>c9cd3eb1bf88accc501e5f42973e8397</gtr:id><gtr:otherNames>Yang L.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54630898bf9c49.92018834</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>148E0F4C-A764-46EB-BDF5-A517B9924FA5</gtr:id><gtr:title>Divergence in dialogue.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4f0dcc93fe8265846527178f721680be"><gtr:id>4f0dcc93fe8265846527178f721680be</gtr:id><gtr:otherNames>Healey PG</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>545d19a75799a7.05055340</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E364022A-517D-4C47-B7D2-3B01AFC16745</gtr:id><gtr:title>A perceptual audio mixing device</gtr:title><gtr:parentPublicationTitle>134th Audio Engineering Society Convention 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/194174a88a6c4c9b2f2f1b7e726493c7"><gtr:id>194174a88a6c4c9b2f2f1b7e726493c7</gtr:id><gtr:otherNames>Terrell M.J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460b5e070bd36.33121265</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>805209A6-D41C-4A7D-B6B9-AC0904052706</gtr:id><gtr:title>Adaptive control of amplitude distortion effects</gtr:title><gtr:parentPublicationTitle>Proceedings of the AES International Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/46ebf25a1be8db47b851659f2bb7b107"><gtr:id>46ebf25a1be8db47b851659f2bb7b107</gtr:id><gtr:otherNames>De Man B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545ce1d72e1628.55483126</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>24FA2C9E-8A49-4D4D-BEEF-FB143FB30E94</gtr:id><gtr:title>Acoustic Scene Classification: Classifying environments from the sounds they produce</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95bd38a0450f7e3f704c0e410abd17d5"><gtr:id>95bd38a0450f7e3f704c0e410abd17d5</gtr:id><gtr:otherNames>Barchiesi D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568abac3bcd003.31029280</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CD22C562-25F1-4D92-8208-D92FD3402CB3</gtr:id><gtr:title>A knowledge-engineered autonomous mixing system</gtr:title><gtr:parentPublicationTitle>135th Audio Engineering Society Convention 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/46ebf25a1be8db47b851659f2bb7b107"><gtr:id>46ebf25a1be8db47b851659f2bb7b107</gtr:id><gtr:otherNames>De Man B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545ce1d7ca06b1.06761768</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>66EAC2B0-9909-4064-882C-05718352C100</gtr:id><gtr:title>Towards the Characterization of Singing Styles in World Music</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d149bcb8bf2438ed045758669ba414c"><gtr:id>2d149bcb8bf2438ed045758669ba414c</gtr:id><gtr:otherNames>Panteli, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58b57677c1be27.84041354</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39E9E145-F00B-4F22-ADC0-F7779F39FAA1</gtr:id><gtr:title>Perceptual Evaluation of Music Mixing Practices</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8b71dc7cc6.19966332</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7924F0C3-F4A0-454F-8690-27E1506F05A2</gtr:id><gtr:title>Identification of drum overhead-microphone tracks in multi-track recordings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f25e435dbce332942b61ba948152393f"><gtr:id>f25e435dbce332942b61ba948152393f</gtr:id><gtr:otherNames>Arimoto, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adbb0f03da84.70404767</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB472BB1-5B63-4666-BD9B-9E9DD7264A98</gtr:id><gtr:title>A semantic architecture for knowledge representation in the digital audio workstation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2dff7b9e4ee8bdffea46b7710ec42c7d"><gtr:id>2dff7b9e4ee8bdffea46b7710ec42c7d</gtr:id><gtr:otherNames>Enderby, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aaf9123c5c21.09820869</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A0FEF1F-C273-4E06-AB30-923CB1288231</gtr:id><gtr:title>An analysis and evaluation of audio features for multitrack music mixtures</gtr:title><gtr:parentPublicationTitle>15th Int. Society for Music Information Retrieval Conference (ISMIR-14)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/730c3648b50c50d98d234809b97f5950"><gtr:id>730c3648b50c50d98d234809b97f5950</gtr:id><gtr:otherNames>De Man B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545ce826348ac5.00823097</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>171713AE-7FDD-4432-8356-F30E335DAC64</gtr:id><gtr:title>Do Online Social Tags Predict Perceived or Induced Emotional Responses to Music?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af2cd3e9a7b35dd64f17b00fd3b093fb"><gtr:id>af2cd3e9a7b35dd64f17b00fd3b093fb</gtr:id><gtr:otherNames>Y. Song</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54614646254809.07774017</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9CF111BA-0D60-4DB7-A4CB-720AA383BF6F</gtr:id><gtr:title>Vibrato Performance Style: A Case Study Comparing Erhu and Violin</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9cd3eb1bf88accc501e5f42973e8397"><gtr:id>c9cd3eb1bf88accc501e5f42973e8397</gtr:id><gtr:otherNames>Yang L.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54630829277484.19085806</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7527138F-EF21-4EAF-9225-21EF864A4F09</gtr:id><gtr:title>Intelligent Multitrack Dynamic Range Compression</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/51bf4dfa986916ecad0537b64e1e1c57"><gtr:id>51bf4dfa986916ecad0537b64e1e1c57</gtr:id><gtr:otherNames>Ma Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58b6efbf1d3d03.06254202</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>683D10A6-19B3-43EC-B2C7-535945C134D3</gtr:id><gtr:title>Implementation and Assessment of Joint Source Separation and Dereverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/88a2bf3a49c9153f619135eaace64319"><gtr:id>88a2bf3a49c9153f619135eaace64319</gtr:id><gtr:otherNames>Moffat, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd8bfca43ce9.74015973</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3877EBAE-6C47-472A-89F6-703B56677DA1</gtr:id><gtr:title>Harmonic Structure Predicts the Enjoyment of Uplifting Trance Music.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91b74dcbc9e6275c2c5c2139ca368338"><gtr:id>91b74dcbc9e6275c2c5c2139ca368338</gtr:id><gtr:otherNames>Agres K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>58c2dae064a6b4.86826889</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95FCB680-5232-4733-ABE2-6CAC14D8BD11</gtr:id><gtr:title>Effect of Spatial Sampling Approaches on Virtual High Order Ambisonics,</gtr:title><gtr:parentPublicationTitle>Proceedings of the 21st International Congress on Sound and Vibration (ICSV21)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/28693029455850ec2b6ff4cf5ebb7163"><gtr:id>28693029455850ec2b6ff4cf5ebb7163</gtr:id><gtr:otherNames>Yu G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545ce607e4ada4.63535003</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>67D89E41-D988-42F0-926E-963E67A23D13</gtr:id><gtr:title>Automatic Control of a Digital Reverberation Effect using Hybrid Models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/18a23c5c75e9505791bc6081775e580b"><gtr:id>18a23c5c75e9505791bc6081775e580b</gtr:id><gtr:otherNames>Chourdakis, E.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd8bbe90d5d1.35507490</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3F75798A-8261-41B2-B8BC-E7B6FD519C7A</gtr:id><gtr:title>New sonorities for jazz recordings: Separation and mixing using deep neural networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5744161248cb24efa38c26348adbbed3"><gtr:id>5744161248cb24efa38c26348adbbed3</gtr:id><gtr:otherNames>Mimilakis, S. I.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adba317a5623.89231894</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D84D5CD-61A9-470C-A94A-DA6E139493EA</gtr:id><gtr:title>Collidoscope: Let's Ride the Waves of Sound</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/12837e34387f4aaa8325c922c2e9b6a2"><gtr:id>12837e34387f4aaa8325c922c2e9b6a2</gtr:id><gtr:otherNames>Bengler, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:isbn>n/a</gtr:isbn><gtr:outcomeId>58aaf3f88050c8.68723326</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5FDD8610-1BB8-49C8-9641-7FA83817BBAA</gtr:id><gtr:title>From Interactive to Adaptive Mood-Based Music Listening Experiences in Social or Personal Contexts</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/14262211abdfa8e8ffb73d446196413a"><gtr:id>14262211abdfa8e8ffb73d446196413a</gtr:id><gtr:otherNames>Bartet M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b57bafcdc220.62254291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>451A892E-CD75-4B3C-9E57-213384979753</gtr:id><gtr:title>Perceptual Evaluation and Analysis of Reverberation in Multitrack Music Production</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e72cb7b44cf4c6c84c06036531451e5"><gtr:id>2e72cb7b44cf4c6c84c06036531451e5</gtr:id><gtr:otherNames>de Man B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ada864a8fa24.08227530</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA1EB9EF-BF7D-4731-AC6A-E6BCAADD8244</gtr:id><gtr:title>Non-negative matrix factorisation incorporating greedy hellinger sparse coding applied to polyphonic music transcription</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845955cb409b459776fd0fe8bfe01bf7"><gtr:id>845955cb409b459776fd0fe8bfe01bf7</gtr:id><gtr:otherNames>O'Hanlon K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b8ef4d34f57.79536908</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A06205B1-822C-432D-A662-2A18DE6A903A</gtr:id><gtr:title>The Open Multitrack Testbed: Features, content and use cases</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aafd2b934cc7.05062250</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A65150A3-9C9D-4835-AC8D-25E8A5EF0946</gtr:id><gtr:title>Requirements for Media Value Chain Ontology&amp;quot;, Ref. as ISO/IEC JTC1/SC29/WG11/N15352</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62bf602dedc27c01e3a375e514f62402"><gtr:id>62bf602dedc27c01e3a375e514f62402</gtr:id><gtr:otherNames>N/A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58aaf5f73cdbc3.37508581</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F938824-A71C-4DCE-9B7D-F010B70F8FD1</gtr:id><gtr:title>Subjective comparison of music production practices using the Web Audio Evaluation Tool</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aaf9cd8de797.87951397</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>75090152-1F20-4AA6-A402-62B3063CD011</gtr:id><gtr:title>Exposing the scaffolding of digital instruments with hardware-software feedback loops</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8684d6f92c31ca847100a52153e1c26"><gtr:id>a8684d6f92c31ca847100a52153e1c26</gtr:id><gtr:otherNames>McPherson, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ded1c4a785d2.38509429</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D6B6E8A-3689-4200-BDE3-BEE8FF746FD9</gtr:id><gtr:title>Design and Use of a Hackable Digital Instrument</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0501129a8811ffaa9434777b3c0b57e9"><gtr:id>0501129a8811ffaa9434777b3c0b57e9</gtr:id><gtr:otherNames>Zappi, V.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545b9754a92814.81308051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>37E53F9E-F7CC-4273-8ACD-3E0AD6DD7806</gtr:id><gtr:title>Towards a comprehensive dataset of vocal imitations of drum sounds</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/946a1d83374b2eab922704d0e99fba23"><gtr:id>946a1d83374b2eab922704d0e99fba23</gtr:id><gtr:otherNames>Mehrabi, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aafe03e585d3.28499912</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D06024F1-77BA-4E63-8D57-F4711E667902</gtr:id><gtr:title>Learning about structural analysis from structural analyses</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1036c1921e3a074a3bf5358f0da0c742"><gtr:id>1036c1921e3a074a3bf5358f0da0c742</gtr:id><gtr:otherNames>Smith J. B. L.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546305993f8774.34954352</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EEEE569A-93E9-4B92-B712-4CD3B077F535</gtr:id><gtr:title>Workshop on Auditory Neuroscience, Cognition and Modeling.</gtr:title><gtr:parentPublicationTitle>Psychomusicology: Music, Mind, and Brain</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91b74dcbc9e6275c2c5c2139ca368338"><gtr:id>91b74dcbc9e6275c2c5c2139ca368338</gtr:id><gtr:otherNames>Agres K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ad9f4c183b15.11076077</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B376E77D-F5C7-44CF-ACB5-D7878BD8196E</gtr:id><gtr:title>A Semantic Approach To Autonomous Mixing</gtr:title><gtr:parentPublicationTitle>Journal on the Art of Record Production (JARP)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/730c3648b50c50d98d234809b97f5950"><gtr:id>730c3648b50c50d98d234809b97f5950</gtr:id><gtr:otherNames>De Man B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545ce68a694405.46234285</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>388F4FF1-F5BA-44B0-864A-E778581F3D31</gtr:id><gtr:title>Spectral characteristics of popular commercial recordings 1950-2010</gtr:title><gtr:parentPublicationTitle>135th Audio Engineering Society Convention 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/09bc51b9a351612285a0b7e8b13f790a"><gtr:id>09bc51b9a351612285a0b7e8b13f790a</gtr:id><gtr:otherNames>Pestana P.D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545ce1d36e7694.20511396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>78C55148-252C-4F9B-BB49-19FBBC4287DB</gtr:id><gtr:title>Skip the pre-concert demo: how technical familiarity and musical style affect audience response</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d5b2d623f1d1658f9afb4bb3576596ba"><gtr:id>d5b2d623f1d1658f9afb4bb3576596ba</gtr:id><gtr:otherNames>Astrid, B. S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ad998672cca7.33752271</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>28B71FC7-D991-49B7-9379-B8C842C70EBD</gtr:id><gtr:title>The impact of subgrouping practices on the perception of multitrack mixes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d0c3b1e19a127525da1b7c3123e28c8"><gtr:id>0d0c3b1e19a127525da1b7c3123e28c8</gtr:id><gtr:otherNames>Ronan, D. M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8adaa1efa4.59749715</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E04EAA7-0F69-4223-BD14-0181BC0639F6</gtr:id><gtr:title>The Open Multitrack Testbed</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58b582f5095d48.37333942</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8837EA40-C3AB-413A-90B4-A2CE64A6C8BD</gtr:id><gtr:title>User Preference on Artificial Reverberation and Delay Time Parameters</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1cbf9ed7ab9f474a900c4c88d7b0aaf5"><gtr:id>1cbf9ed7ab9f474a900c4c88d7b0aaf5</gtr:id><gtr:otherNames>Pestana P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ada34ebc6900.45877324</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0C94FCCF-8056-43B6-A930-1FC0E469A1C5</gtr:id><gtr:title>Multichannel High-Resolution NMF for Modeling Convolutive Mixtures of Non-Stationary Signals in the Time-Frequency Domain</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6068d3617f1ae8cde762f03977b6bf83"><gtr:id>6068d3617f1ae8cde762f03977b6bf83</gtr:id><gtr:otherNames>Badeau R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f982982f38f2cc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9C19F7DD-3539-4C73-B76F-980373133CB0</gtr:id><gtr:title>Evaluation and Improvement of the Mood Conductor Interactive System</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef496efdb44c02f91c91e2c7933bebb5"><gtr:id>ef496efdb44c02f91c91e2c7933bebb5</gtr:id><gtr:otherNames>Lou, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54634ceb0ab428.76095913</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>16244164-7B1D-420B-A93E-70322C7FA8D9</gtr:id><gtr:title>A Machine-Learning Approach to Application of Intelligent Artificial Reverberation</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/90da4ed181c4d86807d80eba2f5db50f"><gtr:id>90da4ed181c4d86807d80eba2f5db50f</gtr:id><gtr:otherNames>Chourdakis E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ada7ee2efd74.04268662</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>85C7430B-9D27-4EDC-8D05-E58DFBF59BA7</gtr:id><gtr:title>Perceptual Evaluation and Analysis of Reverberation in Multitrack Music Production</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e72cb7b44cf4c6c84c06036531451e5"><gtr:id>2e72cb7b44cf4c6c84c06036531451e5</gtr:id><gtr:otherNames>de Man B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ada4491da353.67301994</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E58734A-B92A-4790-8CE3-7B48475A7761</gtr:id><gtr:title>Learning Incoherent Subspaces: Classification via Incoherent Dictionary Learning</gtr:title><gtr:parentPublicationTitle>Journal of Signal Processing Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95bd38a0450f7e3f704c0e410abd17d5"><gtr:id>95bd38a0450f7e3f704c0e410abd17d5</gtr:id><gtr:otherNames>Barchiesi D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55faa0aa0f245544</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>955C9103-27A8-4918-A29D-880F550FA8FF</gtr:id><gtr:title>Audio Commons: Bringing Creative Commons Audio Content to the Creative Industries</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d488e1e88d0ac61b8a7050bff49a909"><gtr:id>0d488e1e88d0ac61b8a7050bff49a909</gtr:id><gtr:otherNames>Font, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adbb864be4e9.19234226</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A3F1A487-59F5-4E51-AC38-0F4A922D0576</gtr:id><gtr:title>Chime-home: A dataset for sound source recognition in a domestic environment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9b5f55896bb7295e62c00aa27f2162d4"><gtr:id>9b5f55896bb7295e62c00aa27f2162d4</gtr:id><gtr:otherNames>Foster P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b8263d91d48.94002022</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>514A7B77-A1AD-4EB0-95FB-EEF8F30DC43D</gtr:id><gtr:title>Modeling meter induction as perceptual inference using IDyOM</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7f66515b6791ece038367d007c343b56"><gtr:id>7f66515b6791ece038367d007c343b56</gtr:id><gtr:otherNames>Van der Weij, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddbb6b755773.76370769</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>104C2545-4E4A-41D4-BD05-7BB71304D350</gtr:id><gtr:title>Towards complex matrix decomposition of spectrograms based on the relative phase offsets of harmonic sounds</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef93cb8fbcbe9b1c58b38d1d340a4eaf"><gtr:id>ef93cb8fbcbe9b1c58b38d1d340a4eaf</gtr:id><gtr:otherNames>Kirchhoff H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f9449441c0388d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2506D843-34C2-4B8E-9CB2-9F8304C6C14B</gtr:id><gtr:title>Using Tags to Select Stimuli in the Study of Music and Emotion</gtr:title><gtr:parentPublicationTitle>Proceedings of the 3rd International Conference on Music and Emotion</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af2cd3e9a7b35dd64f17b00fd3b093fb"><gtr:id>af2cd3e9a7b35dd64f17b00fd3b093fb</gtr:id><gtr:otherNames>Y. Song</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54647ac54d84a9.38300901</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87EF8F63-9CF8-4AE6-9B54-A3D32206FA8B</gtr:id><gtr:title>Convolutional Recurrent Neural Networks for Music Classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd01de688471425898aee5aedaee41ee"><gtr:id>cd01de688471425898aee5aedaee41ee</gtr:id><gtr:otherNames>Choi, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>58ac38db718c61.38477491</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>279664CE-CE10-480F-9758-A9E50F6B5C9D</gtr:id><gtr:title>Why do listeners disagree about large-scale formal structure? A case study</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1036c1921e3a074a3bf5358f0da0c742"><gtr:id>1036c1921e3a074a3bf5358f0da0c742</gtr:id><gtr:otherNames>Smith J. B. L.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463053f05a1e4.35815828</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0FF488A-BA87-4260-8FA1-E5C21D2844C6</gtr:id><gtr:title>The mathematics of mixing</gtr:title><gtr:parentPublicationTitle>AES: Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4210bb825d5c869e24d244aa7bf50dab"><gtr:id>4210bb825d5c869e24d244aa7bf50dab</gtr:id><gtr:otherNames>Terrell M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>15494950</gtr:issn><gtr:outcomeId>5460b6192aacd6.14812523</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C8614C0D-D995-497B-AF76-7F06481C4E56</gtr:id><gtr:title>On-bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4193fed805f732438d2edd0ab039c35e"><gtr:id>4193fed805f732438d2edd0ab039c35e</gtr:id><gtr:otherNames>Stowell Dan</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ad9ddc857ac9.62544418</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DE2963AE-5372-4F13-B9E0-13E327205C79</gtr:id><gtr:title>Harmonic Motion: A Toolkit for Processing Gestural Data for Interactive Sound</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/470a8b0324678cd2516daace017af908"><gtr:id>470a8b0324678cd2516daace017af908</gtr:id><gtr:otherNames>Murray-Browne T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>568becba32f3d0.23425816</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C10AA39-5A56-4278-ACD0-0AF507AB1EF2</gtr:id><gtr:title>On the relation between gesture, tone production and perception in classical cello performance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95c2edd7cda2be84ae6d851d27349fe4"><gtr:id>95c2edd7cda2be84ae6d851d27349fe4</gtr:id><gtr:otherNames>Chudy M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546478fa8e5942.13164204</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9550470E-AB0A-43C0-A0D0-57524FD02A5D</gtr:id><gtr:title>JSAP: A Plugin Standard for the Web Audio API with Intelligent Functionality</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e08762d3026398f3061df5d7216b816b"><gtr:id>e08762d3026398f3061df5d7216b816b</gtr:id><gtr:otherNames>Jillings, N.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aaf7994a2ed3.72592170</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D2870183-8C1C-41E3-BDCE-712792110AE9</gtr:id><gtr:title>Tap the ShapeTones: Exploring the effects of crossmodal congruence in an audio-visual in- terface</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e77dac7a6c74d7d3a33e15faee434237"><gtr:id>e77dac7a6c74d7d3a33e15faee434237</gtr:id><gtr:otherNames>Metatla, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dda63b5e12f3.64390508</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2E5D320-BED0-4659-9A77-6C65B079C9D8</gtr:id><gtr:title>&amp;quot;I could play here for hours..&amp;quot; (thinks the visitor and leaves): Why People Disengage from Public Interactives</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/12837e34387f4aaa8325c922c2e9b6a2"><gtr:id>12837e34387f4aaa8325c922c2e9b6a2</gtr:id><gtr:otherNames>Bengler, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9bb60525de4.83525551</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E943A15D-D450-4880-8CEC-9A49A1AD09B0</gtr:id><gtr:title>Audio Commons: Bringing Creative Commons Audio Content to the Creative Industries</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d488e1e88d0ac61b8a7050bff49a909"><gtr:id>0d488e1e88d0ac61b8a7050bff49a909</gtr:id><gtr:otherNames>Font, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd8c39d86000.39848123</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5F08A1FD-446F-459F-968E-2131B20DB6F9</gtr:id><gtr:title>Evaluation of the Mood Conductor Interactive System Based on Audience and Performers' Perspectives</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef496efdb44c02f91c91e2c7933bebb5"><gtr:id>ef496efdb44c02f91c91e2c7933bebb5</gtr:id><gtr:otherNames>Lou, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>97822909669236</gtr:isbn><gtr:outcomeId>54634b93c998c1.70175219</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD18032A-E08C-47BD-BC7F-2DD7ACFB7721</gtr:id><gtr:title>Non-Negative Group Sparsity with Subspace Note Modelling for Polyphonic Transcription</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845955cb409b459776fd0fe8bfe01bf7"><gtr:id>845955cb409b459776fd0fe8bfe01bf7</gtr:id><gtr:otherNames>O'Hanlon K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56ddd531ceef31.53891935</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>512C553B-6C7C-415D-9EE7-B3AB959A7419</gtr:id><gtr:title>A Comparison of Extended Source-Filter Models for Musical Signal Reconstruction</gtr:title><gtr:parentPublicationTitle>Proceedings of the 17th International Conference on Digital Audio Effects</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de90e0a11f6247b1fa56ba283933afeb"><gtr:id>de90e0a11f6247b1fa56ba283933afeb</gtr:id><gtr:otherNames>T. Cheng</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54647b8762f205.46812857</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95DDD43D-CA25-4AA8-A821-57A8E9B52538</gtr:id><gtr:title>Polymetros</gtr:title><gtr:parentPublicationTitle>interactions</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/44277654a770f794c782025d57e9000c"><gtr:id>44277654a770f794c782025d57e9000c</gtr:id><gtr:otherNames>Bengler B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546223b989dd88.13381330</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B56FDD4F-283A-4DE7-81EF-0ADB13E2396C</gtr:id><gtr:title>Identification of cover songs using information theoretic measures of similarity</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9b5f55896bb7295e62c00aa27f2162d4"><gtr:id>9b5f55896bb7295e62c00aa27f2162d4</gtr:id><gtr:otherNames>Foster P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5461458d2e3833.77731155</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EC9C70D-6181-4D02-823D-D6691A31854C</gtr:id><gtr:title>Loudness algorithms for automatic mixing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd43aad7c5ff37c0694d19d95050b719"><gtr:id>bd43aad7c5ff37c0694d19d95050b719</gtr:id><gtr:otherNames>Ward, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aafda4e5a4c5.60263395</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K009559/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>