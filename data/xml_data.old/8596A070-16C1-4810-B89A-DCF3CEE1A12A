<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/62B693D8-12F4-4EEE-A622-010188252868"><gtr:id>62B693D8-12F4-4EEE-A622-010188252868</gtr:id><gtr:name>North Bristol NHS Trust</gtr:name><gtr:address><gtr:line1>Trust Headquarters</gtr:line1><gtr:line2>Frenchay Hospital</gtr:line2><gtr:line3>Beckspool Road, Frenchay</gtr:line3><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS16 1JE</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/196EA8E7-21D8-459A-A838-6B89B533DBAD"><gtr:id>196EA8E7-21D8-459A-A838-6B89B533DBAD</gtr:id><gtr:name>easyJet Airline Company Limited</gtr:name><gtr:address><gtr:line1>Hangar 89</gtr:line1><gtr:line2>London Luton Airport</gtr:line2><gtr:postCode>LU2 9PF</gtr:postCode><gtr:region>East of England</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF6D3974-FE0F-4CB3-89FF-589DF8759D56"><gtr:id>EF6D3974-FE0F-4CB3-89FF-589DF8759D56</gtr:id><gtr:name>Nara Institute of Science &amp; Technology</gtr:name><gtr:address><gtr:line1>8916.6 Takayama cyo</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/0A73F6B3-EE27-4627-B3A4-6F6BFA7772AA"><gtr:id>0A73F6B3-EE27-4627-B3A4-6F6BFA7772AA</gtr:id><gtr:firstName>Walterio</gtr:firstName><gtr:otherNames>Wolfgang</gtr:otherNames><gtr:surname>Mayol-Cuevas</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3F5FF400-3C1E-434C-A2E6-73F6536F2ECA"><gtr:id>3F5FF400-3C1E-434C-A2E6-73F6536F2ECA</gtr:id><gtr:firstName>Iain</gtr:firstName><gtr:otherNames>Donald</gtr:otherNames><gtr:surname>Gilchrist</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/1B1F2323-F669-465D-A55F-643C58962339"><gtr:id>1B1F2323-F669-465D-A55F-643C58962339</gtr:id><gtr:firstName>Casimir</gtr:firstName><gtr:surname>Ludwig</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8AFF7C0F-ECFC-4E6A-8CE6-6E1570F288BD"><gtr:id>8AFF7C0F-ECFC-4E6A-8CE6-6E1570F288BD</gtr:id><gtr:firstName>Dima</gtr:firstName><gtr:surname>Damen</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN013964%2F1"><gtr:id>8596A070-16C1-4810-B89A-DCF3CEE1A12A</gtr:id><gtr:title>GLANCE: GLAnceable Nuances for Contextual Events</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N013964/1</gtr:grantReference><gtr:abstractText>This project will develop and validate exciting novel ways in which people can interact with the world via cognitive wearables -intelligent on-body computing systems that aim to understand the user, the context, and importantly, are prompt-less and useful. Specifically, we will focus on the automatic production and display of what we call glanceable guidance. Eschewing traditional and intricate 3D Augmented Reality approaches that have been difficult to show significant usefulness, glanceable guidance aims to synthesize the nuances of complex tasks in short snippets that are ideal for wearable computing systems and that interfere less with the user and that are easier to learn and use.

There are two key research challenges, the first is to be able to mine information from long, raw and unscripted wearable video taken from real user-object interactions in order to generate the glanceable supports. Another key challenge is how to automatically detect user's moments of uncertainty during which support should be provided without the user's explicit prompt.

The project aims to address the following fundamental problems:
1. Improve the detection of user's attention by robustly determining periods of time that correspond to task-relevant object interactions from a continuous stream of wearable visual and inertial sensors.
2. Provide assistance only when it is needed by building models of the user, context and task from autonomously identified micro-interactions by multiple users, focusing on models that can facilitate guidance.
3. Identify and predict action uncertainty from wearable sensing in particular gaze patterns and head motions.
4. Detect and weigh user expertise for the identification of task nuances towards the optimal creation of real-time tailored guidance.
5. Design and deliver glanceable guidance that acts in a seamless and prompt-less manner during task performance with minimal interruptions, based on autonomously built models.

GLANCE is underpinned by a rich program of experimental work and rigorous validation across a variety of interaction tasks and user groups. Populations to be tested include skilled and general population and for tasks that include: assembly, using novel equipment (e.g. an unknown coffee maker), and repair tasks (e.g. replacing a bicycle gear cable). It also tightly incorporates the development of working demonstrations.
And in collaboration with our partners the project will explore high-value impact cases related to health care towards assisted living and in industrial settings focusing on assembly and maintenance tasks. 

Our team is a collaboration between Computer Science, to develop a the novel data mining and computer vision algorithms, and Behavioral Science to understand when and how users need support.</gtr:abstractText><gtr:potentialImpactText>Receiving on-the spot training, being able to automatically document industrial processes and being able to better understand users of advanced assistive technology has important and wide spread positive implications across many industries.

It is often ignored the fact that most things that are made, repaired or maintained involve non scripted processes that are performed by a small number of expert, or at least, skilled individuals. This is of concern in many economies and in the UK, lack of training and the ability to increase productivity compared to other economies is of great concern according to the Bank of England [4].

There are many industries within the manufacturing sector that can benefit from methods that with no interruption or extra burden on their workers, allow for the transfer of know-how. Systems that observe, learn and extract the relevant from the irrelevant will be of great help. Training is a prime example of where this process can play a significant role.

High technology sectors such as aerospace have a number of situations where word of mouth is a commonplace yet clearly brittle knowledge transfer method. Consider the case of a turbine engine manufactured by one company, sold to another and maintained by a third party. It is not uncommon that when an engine fails for the first time six years or so after its construction, the small team of about a dozen individuals that built it wont be available to repair it. It can also be the case that no one has repaired a specific issue with such an engine before and thus the first time this is needed will take a substantial amount of time, skill and exploration and thus, high cost. Passing these first and precious few instances of knowledge for training others that will require it is of great potential benefit. The benefit increases further if it simplifies the transmission of this knowledge through geographical locations and across languages and cultures. We have long term aspirations on the impact of the work we are proposed but our industry partner sees some potential benefits in a number of situations in the short to mid term horizon (5-10 years).

Being able to distill from observations of people doing things what is important and being able to present it in manners that are intuitive and unobtrusive is what our project aims to build the foundations for and this has applications beyond industry.

Many perfectly physically able and keen individuals of many ages sharing knowledge that can support one another in-situ will have huge impact on how empowered people feel. This can be hinted by the number of times people resort to trying to find videos on the web on how to do things, and in many cases only to be faced by a difficult sifting process trying to separate experts from amateurs from the irrelevant. Systems that can do this separation automatically will have great social and even cultural implications as people will feel more confident on trying and doing things themselves.

In the longer term, cognitive wearable systems could be of benefit in supporting memory or other neurological conditions. Reminding patients on how to do things in their daily routines and or for supporting their rehabilitation can have positive effects on self esteem and recovery. 

Overall, devising more useful methods that understand people uncertainty better and that build models to gain insight on issues such as task relevance or expertise will have many applications anywhere anyone needs help to do or to document actions.</gtr:potentialImpactText><gtr:fund><gtr:end>2021-04-02</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-04-04</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>806993</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8C84AB1C-FEF5-4F20-AC3C-066227D2770E</gtr:id><gtr:title>Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f6375a7b323eaa2c164d75bb65d5ae9b"><gtr:id>f6375a7b323eaa2c164d75bb65d5ae9b</gtr:id><gtr:otherNames>Moltisanti D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>15505499</gtr:issn><gtr:outcomeId>5a90aafbf0f368.08348727</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N013964/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>