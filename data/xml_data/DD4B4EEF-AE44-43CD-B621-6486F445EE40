<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM507088%2F1"><gtr:id>DD4B4EEF-AE44-43CD-B621-6486F445EE40</gtr:id><gtr:title>Audio Data Exploration: New Insights and Value</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M507088/1</gtr:grantReference><gtr:abstractText>The &amp;quot;Audio Data Exploration: New Insights and Value&amp;quot; project is a collaboration between Audio Analytic Ltd. and the
Centre for Digital Music &amp;amp; Centre for Intelligent Sensing at Queen Mary University of London (QML). Compared to
mathematical, textual or visual data, audio data has remained largely underexploited and undervalued, and thus represents
an opportunity to grow innovation and to develop new markets. While Automatic Speech Recognition and Music Analysis
are now creating some industrial value, R&amp;amp;D needs to be conducted to tackle the challenges posed by Automatic
Environmental Sound Recognition defined in a broader sense. The newly developed advanced audio data analysis and
modelling techniques will create value across a variety of applicative domains. While proven markets include Professional
Security and Home Security, a range of novel markets can be developed in relation to Multimedia Database Indexing,
Environmental and Industrial Monitoring, the Internet of Things and more. The project will gather the newly developed
audio analysis and modelling techniques into a demonstrator instantiated as a &amp;quot;Personal Audio Space Indexer&amp;quot;.</gtr:abstractText><gtr:potentialImpactText>ECONOMIC IMPACTS
Audio Analytic, the lead partner in this project, will directly benefit from the transfer of state-of-the-art academic research on
sound recognition into their product range. This will enable Audio Analytic to reinforce its position as a world leading
supplier of sound recognition applications. (Medium term time scale.) The company is planning to quadruple its size after
entering the consumer market, thus creating new employment in the UK (Medium term).
A wide range of players from of diversity of markets spanning professional security, home security, media indexing,
environmental/industrial monitoring and the internet of things will be able to offer a distinctive feature in the form of reliable
and smarter sound event detection as part of their products and applications. These impacts will come through both
existing and newly developed commercial relationships between Audio Analytic and such companies (Medium to long
term).
SOCIAL IMPACTS
Improved public safety: Members of the public looking to protect their safety and security will benefit from the deployment
of the new smart sound recognition as part of home security solutions and smart city solutions, to protect against events
such as, and not limited to: break-ins (detection of glass break sound, burglar alarm sound), aggressions (gunshots, calls
for help etc.), fire (detection of smoke alarm sound), domestic accidents (detection of SOS keywords, specific accident
sounds) etc. Companies and organisations looking to improve security and safety in the workplace, transport systems or
public spaces will benefit from the availability of solutions for sound-based safety and security monitoring.
Environment and public expense: In the case of accident detection, the faster response time enabled by the system may
mean shorter hospital stays and related reductions in NHS expense and CO2 emissions. In the case of environmental
monitoring: the proposed system will make it possible to compute indicators of environmental health through acoustic
monitoring.
Wellbeing of vulnerable adults: On a longer timescale, vulnerable adults in the home such as the elderly, and their families
or carers, will benefit from future enhancements to include sound monitoring for assistance calls to respond to SOS
keywords, or identification of departure from regular sound activity patterns in the home to provide reassurance and
support. Deaf members of the public may also see their quality of life improved by sound indicators that would describe
particular audio events arising in their environment. While some of these impacts are outside the scope of the current
project, the demonstrator will cover enough general sound recognition cases to enable the long term expansion of market
segments by Audio Analytic.
ADDITIONAL IMPACTS
People: The postdoctoral researcher (PDRA) will benefit through gaining skills in knowledge transfer and application of
research technology in a commercial setting, applicable to their future career. (Short term.)
Transfer of technology best practice: Academic research in the domain of sound recognition will benefit from access to a
real-life platform and real-life data to evaluate novel sound recognition algorithms. This project will set a precedent for best
practice in algorithm evaluation, maths optimisation and transfer of technology between the academia and the industry.
(Medium to long term.)</gtr:potentialImpactText><gtr:fund><gtr:end>2015-09-02</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-11-03</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>77442</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2980000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>H2020-ICT-2015 Audio Commons</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>688382</gtr:fundingRef><gtr:id>0175F8F9-E9A1-45F3-A740-FDCDE920EB17</gtr:id><gtr:outcomeId>568bdebdb40db2.04379356</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The findings are being further investigated by project partners Audio Analytic for use in their product range.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>A6FACE67-D22F-4DE3-93E6-0FD844458B86</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d86dacd7c983.43771091</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Our experiments demonstrate that including temporal information in the input representation helps improve the performance of classifiers for environmental audio data. Autoencoders appear to be effective at summarising temporal information over short time-scales. We observed that spherical k-means as a feature learning technique, is able to learn features that perform similarly to autoencoder features. This is a useful finding, since spherical k-means can be trained more efficiently on very large datasets than autoencoders.</gtr:description><gtr:exploitationPathways>The findings are being investigated further by project partners Audio Analytic for use in their product range.</gtr:exploitationPathways><gtr:id>ADEE893D-3785-4CEA-B57F-A4E51B73AC1C</gtr:id><gtr:outcomeId>56d86d120fea28.10808715</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The CHiME-Home dataset is a collection of annotated domestic environment audio recordings, described in:
P. Foster, S. Sigtia, S. Krstulovic, J. Barker, M. D. Plumbley. &amp;quot;CHiME-Home: A Dataset for Sound Source Recognition in a Domestic Environment,&amp;quot; in Proceedings of the 11th Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015; and available from https://archive.org/details/chime-home</gtr:description><gtr:id>71475B2C-B99F-4952-BE9A-CCA4BBAF55AF</gtr:id><gtr:impact>The dataset is being used in task 4 of the DCASE2106 Challenge, for performance evaluation of systems for the detection and classification of sound events. The challenge is organised by the Audio Research Group of Tampere University of Technology, by the QMUL Centre for Digital Music and by IRCCYN, and by the University of Surrey, under the auspices of the Audio and Acoustic Signal Processing (AASP) technical committee of the IEEE Signal Processing Society. See http://www.cs.tut.fi/sgn/arg/dcase2016/task-audio-tagging</gtr:impact><gtr:outcomeId>56d8cad2264c65.03464443</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>CHiME-Home</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://archive.org/details/chime-home</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2FE09C42-A38D-432C-9674-3D394D6BC584</gtr:id><gtr:title>Chime-home: A dataset for sound source recognition in a domestic environment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9b5f55896bb7295e62c00aa27f2162d4"><gtr:id>9b5f55896bb7295e62c00aa27f2162d4</gtr:id><gtr:otherNames>Foster P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b8263d91d48.94002022</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A88A00DA-A5DA-429F-9835-4F1DB4AC718E</gtr:id><gtr:title>Detection of overlapping acoustic events using a temporally-constrained probabilistic model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b98cc5c6adc7.35571051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E83A7C3-7633-4C2A-A0AB-D4B09FE386DA</gtr:id><gtr:title>Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0cd56c5d6f324ffe01776b7518642ba"><gtr:id>d0cd56c5d6f324ffe01776b7518642ba</gtr:id><gtr:otherNames>Xu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a86b7d8ea2f28.58568142</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D596FAB3-3104-4F53-B496-BE2644643DBE</gtr:id><gtr:title>Automatic Environmental Sound Recognition: Performance Versus Computational Cost</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f7b1878e971a0aeaa3ca61089c272023"><gtr:id>f7b1878e971a0aeaa3ca61089c272023</gtr:id><gtr:otherNames>Sigtia S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d40174f2cd8.17365487</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M507088/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>