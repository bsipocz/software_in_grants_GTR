<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9C10D78F-6430-4CA7-9528-B96B0762A4C6"><gtr:id>9C10D78F-6430-4CA7-9528-B96B0762A4C6</gtr:id><gtr:name>Cardiff University</gtr:name><gtr:address><gtr:line1>Research &amp; Consultancy</gtr:line1><gtr:line2>PO Box 923</gtr:line2><gtr:line4>Cardiff</gtr:line4><gtr:line5>South Glamorgan</gtr:line5><gtr:postCode>CF10 3TE</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C7510606-A36F-4725-A89B-9D592374972A"><gtr:id>C7510606-A36F-4725-A89B-9D592374972A</gtr:id><gtr:name>University of Stirling</gtr:name><gtr:department>Computing Science and Mathematics</gtr:department><gtr:address><gtr:line4>Stirling</gtr:line4><gtr:line5>Stirlingshire</gtr:line5><gtr:postCode>FK9 4LA</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C7510606-A36F-4725-A89B-9D592374972A"><gtr:id>C7510606-A36F-4725-A89B-9D592374972A</gtr:id><gtr:name>University of Stirling</gtr:name><gtr:address><gtr:line4>Stirling</gtr:line4><gtr:line5>Stirlingshire</gtr:line5><gtr:postCode>FK9 4LA</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9C10D78F-6430-4CA7-9528-B96B0762A4C6"><gtr:id>9C10D78F-6430-4CA7-9528-B96B0762A4C6</gtr:id><gtr:name>Cardiff University</gtr:name><gtr:address><gtr:line1>Research &amp; Consultancy</gtr:line1><gtr:line2>PO Box 923</gtr:line2><gtr:line4>Cardiff</gtr:line4><gtr:line5>South Glamorgan</gtr:line5><gtr:postCode>CF10 3TE</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5195F775-F0E4-443A-AE02-3E6B51BB33A7"><gtr:id>5195F775-F0E4-443A-AE02-3E6B51BB33A7</gtr:id><gtr:name>Phonak Hearing Systems</gtr:name><gtr:address><gtr:line1>Laubisrutistrasse 28</gtr:line1><gtr:line4>8712 Stafa</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Switzerland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C4145C68-58B8-441E-86DD-A4079B65B478"><gtr:id>C4145C68-58B8-441E-86DD-A4079B65B478</gtr:id><gtr:name>MRC Institute of Hearing Research</gtr:name><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/461244CD-E991-44D9-A2F7-318530121EC1"><gtr:id>461244CD-E991-44D9-A2F7-318530121EC1</gtr:id><gtr:firstName>Jon</gtr:firstName><gtr:surname>Barker</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/BB3AEC7D-AED7-46C8-9550-A63FFFF481DF"><gtr:id>BB3AEC7D-AED7-46C8-9550-A63FFFF481DF</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Abel</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/583A3078-B60E-4E34-9653-2A7547A30A5A"><gtr:id>583A3078-B60E-4E34-9653-2A7547A30A5A</gtr:id><gtr:firstName>Amir</gtr:firstName><gtr:surname>Hussain</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/48F05AEA-E340-41DB-AABE-1893D0B5DA88"><gtr:id>48F05AEA-E340-41DB-AABE-1893D0B5DA88</gtr:id><gtr:firstName>Roger</gtr:firstName><gtr:surname>Watt</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM026981%2F1"><gtr:id>B3086D1C-42D0-4AE5-ADBA-CF08A3E6A571</gtr:id><gtr:title>Towards visually-driven speech enhancement for cognitively-inspired multi-modal hearing-aid devices (AV-COGHEAR)</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M026981/1</gtr:grantReference><gtr:abstractText>Current commercial hearing aids use a number of sophisticated enhancement techniques to try and improve the quality of speech signals. However, today's best aids fail to work well in many everyday situations. In particular, they fail in busy social situations where there are many competing speech sources; they fail if the speaker is too far from the listener and swamped by noise. We have identified an opportunity to solve this problem by building hearing aids that can 'see'. 

This ambitious project aims to develop a new generation of hearing aid technology that extracts speech from noise by using a camera to see what the talker is saying. The wearer of the device will be able to focus their hearing on a target talker and the device will filter out competing sound. This ability, which is beyond that of current technology, has the potential to improve the quality of life of the millions suffering from hearing loss (over 10m in the UK alone).

Our approach is consistent with normal hearing. Listeners naturally combine information from both their ears and eyes: we use our eyes to help us hear. When listening to speech, eyes follow the movements of the face and mouth and a sophisticated, multi-stage process uses this information to separate speech from the noise and fill in any gaps. Our hearing aid will act in much the same way. It will exploit visual information from a camera (e.g.using a Google Glass like system), and novel algorithms for intelligently combining audio and visual information, in order to improve speech quality and intelligibility in real-world noisy environments. 

The project is bringing together a critical mass of researchers with the complementary expertise necessary to make the audio-visual hearing-aid possible. The project will combine new contrasting approaches to audio-visual speech enhancement that have been developed by the Cognitive Computing group at Stirling and the Speech and Hearing Group at Sheffield. The Stirling approach uses the visual signal to filter out noise; whereas the Sheffield approach uses the visual signal to fill in 'gaps' in the speech. The vision processing needed to track a speaker's lip and face movement will use a revolutionary 'bar code' representation developed by the Psychology Division at Stirling. The MRC Institute of Hearing Research (IHR) will provide the expertise needed to evaluate the approach on real hearing loss sufferers. Phonak AG, a leading international hearing aid manufacturer, will provide the advice and guidance necessary to maximise potential for industrial impact.

The project has been designed as a series of four workpackages that consider the key research challenges related to each component of the device's design. These questions have been identified by preliminary work at Sheffield and Stirling. Among the challenges are developing improved techniques for visually-driven audio-analysis; designing better metrics for weighting audio and visual evidence; developing techniques for optimally combining the noise-filtering and gap-filling approaches. A further key challenge is that, for a hearing aid to be effective, the processing cannot delay the signal by more than 10ms. 

In the final year of the project a full integrated, software prototype will be clinically evaluated using listening tests with hearing-impaired volunteers in a range of modern noisy reverberant environments. Evaluation will use a new purpose-built speech corpus that will be designed specifically for testing this new class of multimodal device. The project's clinical research partner, the Scottish Section of MRC IHR, will provide advice on the experimental design and analysis aspects throughout the trials. Industry leader Phonak AG will provide advice and technical support for benchmarking real-time hearing devices. The final clinically-tested prototype will be made available to the whole hearing community as a testbed for further research, development, evaluation and benchmarking.</gtr:abstractText><gtr:potentialImpactText>This mufti-disciplinary project has been designed to have impact beyond the academic environment:

*Sufferers of hearing loss*

The aim of the proposal is to demonstrate a totally new class of hearing device that, by using visual input, is able to deliver an unparalleled level of speech intelligibility in noisy situations where current audio-only hearing aids are known to fail. The proposal, by supplying the enabling research for this technology, has potential for significant long term societal impact. Reduced ability to understand speech in noise is one of the most debilitating symptoms of hearing loss. An effective hearing device would improve the quality of life of millions of hearing loss suffers (over 10m in the UK alone [1], receiving around &amp;pound;300M of treatment from the NHS annually [2]). Notably even mild age-related hearing loss (something which effects us all) can cause speech to become hard to understand in situations where many people are speaking at the same time (e.g., social gatherings); or where speech is heard from a distance and degraded by reverberation (e.g., classrooms). Even a small improvement in performance could be the difference that allows someone to continue in their job (e.g., a teacher in a noisy classroom) or to remain socially active, avoiding potential isolation and depression. Note that as the device will work by complementing the visual processing performed routinely in speech perception, it will be of particular benefit to hearing loss suffers who are also visually impaired.

*The hearing aid industry*

A new class of audio-visual (AV) hearing aid would have impact on the hearing aid industry itself: demand for AV aids would rapidly displace inferior audio-only devices. There are clear precedents for hearing science rapidly transforming hearing technology, e.g., multiple microphone processing and frequency compression have been commercialised to great effect. We foresee AV processing as the next step forward. Previous barriers to V processing are falling: reliable wireless technology frees the computation from having to be performed on the device itself; wearable computing devices are becoming sufficiently powerful to perform real-time face tracking and feature extraction. AV aids will also impact on industry standards for hearing aid evaluation and clinical standards for hearing loss assessment. Plans for realising these industrial impacts (including through an International Workshop, AV hearing device Challenge/Competition and open-source dissemination) are detailed in the Pathways to Impact document.

*Applications beyond hearing aids*

The project has potential impact in other speech processing applications, including,

-Cochlear implants (CI). CI users have even more severe problems coping with noise. With further research the technologies we are proposing could be used directly in CI signal processing. 

-Telecommunications. Here we imagine video signals captured at the transmission-end being used to filter and enhance acoustic signals arriving at the receiver-end. Note, this could be useful either in teleconferencing, or built into conventional audio-only receivers, or for people with visual impairment who are unable to see visual cues directly.

-Speech-enhancement for normal hearing. AV speech intelligibility enhancement may be useful for users with no hearing loss in certain situations, e.g., in situations where ear defenders are being worn - factories, emergency response, military, etc. 

[1] http://www.patient.co.uk/doctor/deafness-in-adults
[2] http://www.publications.parliament.uk/pa/cm201415/cmhansrd/cm140624/text/140624w0002.htm#140624w0002.htm_wqn4</gtr:potentialImpactText><gtr:fund><gtr:end>2018-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>418261</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Cardiff University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>MRC Network (Cardiff)</gtr:description><gtr:id>F3A22207-7211-477F-9CFD-BA84C03E8EC1</gtr:id><gtr:impact>No outputs yet.</gtr:impact><gtr:outcomeId>56d5d61f6d3c30.59603602-1</gtr:outcomeId><gtr:partnerContribution>Attendance, presentations at meetings, etc.</gtr:partnerContribution><gtr:piContribution>We are collaborators on a MRC Network grant for a Hearing Aid Research Network, contributing presentations and discussions on the topic of disruptive technologies for hearing aids.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>MRC Hearing Aid Network (Cardiff) Presentation</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3A040488-365D-4A1A-ABA1-754DDD167673</gtr:id><gtr:impact>Discussed work of Stirling project on behalf of Prof Hussain at Cardiff MRC Network meeting (held by Prof. Culling)</gtr:impact><gtr:outcomeId>56d5d4c0a704f5.84249039</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://www.mrc.ac.uk/documents/pdf/hearing-aid-research-networks/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>MRC-EPSRC Network workshop meeting on Microphone Technologies for Hearing Aids, organized at Stirling, 2 Feb 2017</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9E209410-0FD2-4DD1-8A5B-E152A1D2C6E7</gtr:id><gtr:impact>AV-COGHEAR PI: Prof. A. Hussain (Stirling), jointly with MRC Network PI: Prof J. Culling (Cardiff), organized the MRC-EPSRC Network meeting at Stirling, in the form of a one day interactive Workshop, attended by approximately 40 participants from multi-disciplinary backgrounds. All AV-COGHEAR project partners, including PI: Prof Hussain, CI: Dr J. Barker (Sheffield), project postdocs, Dr A. Ahsan, Dr R. Marxer, Dr A. Abel, the project's clinical partner: Dr W. Whitmer (MRC IHR Glasgow) and the industry partner, Dr P. Derleth (Phonak Hearing), attended the Workshop meeting and actively participated in discussions to explore and develop synergies between AV-COGHEAR and other related MRC Network and EPSRC project partners. In particular, plans for developing and sharing audio-visual speech data were discussed and agreed, along with a detailed proposal for jointly-organizing a first of its kind, international Workshop (chaired by A. Hussain, J. Barker, J. Culling and J. Hansen) on &amp;quot;Challenges for Hearing Assistive Technology (CHAT-2017)&amp;quot;, as part of INTERSPEECH'2017 at Stockholm, Sweden, on 19th August 2017.</gtr:impact><gtr:outcomeId>58c5bdbb030696.63271855</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>British Society of Audiology Conference</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>FF0EF81E-E8FF-4D80-AF4C-ABE7B3447EE4</gtr:id><gtr:impact>We will be attending the British Society of Audiology Conference, 25-27 April 2016, which is a mixture of researchers, clinical practitioners, and industry presenters, to present an initial poster, titled &amp;quot;Audiovisual Speech Processing: Exploiting visual features for joint-vector modelling&amp;quot;. The aim is to present the new project to a wider audience.</gtr:impact><gtr:outcomeId>56dd8e37e36e74.46845774</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>https://www.eventsforce.net/fitwise/frontend/reg/thome.csp?pageID=127483&amp;eventID=323&amp;eventID=323</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Impact for Access - Stirling University</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D581D180-B2FB-4B4A-93E5-982DEFD2712E</gtr:id><gtr:impact>Approximately 70 school pupils (of ages between 14 and 16) attended a visit to the research organisation (University of Stirling), to learn more about studying Computing Science. As part of this, an image processing tutorial and interactive demo was given to small groups, which resulted in questions and discussions about both the direct research topic (signal processing), and studying Computing Science more generally.</gtr:impact><gtr:outcomeId>56dd8fc95028c4.16117412</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>MRC-EPSRC Network workshop meeting on Microphone Technologies for Hearing Aids, Cardiff, September 21, 2017</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>97D21530-95F1-4622-8B84-FFC431B14576</gtr:id><gtr:impact>Along with other speakers (Daniel Robert, Allan Belcher, and John Culling), Ahsan Adeel presented the EPSRC AV-COGHEAR vision, objectives, ongoing-future work, immediate challenges, and video/audio-visual information in hearing aids: A brief review and some future directions. The meeting was attended by around 30-40 people consisting of academics from other funded projects and staff from the EPSRC and MRC. The meeting aimed to review grant impact mechanisms and discuss impact emphasis and requirements.</gtr:impact><gtr:outcomeId>58c688fa3322a8.49864079</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>MRC/EPSRC workshop on hearing aid technology research, June 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>2421A4E2-9D07-4E98-9061-5B0C1E9B214C</gtr:id><gtr:impact>Dr Jon Barker presented the aims and initial progress of the project at a joint EPSRC/MRC workshop on hearing aid technology research. The meeting was attended by around 60-70 people consisting of academics from other funded projects and staff from the EPSRC and MRC.

The meeting was useful in that it allowed us to identify links with ongoing projects working with audio-visual speech data.</gtr:impact><gtr:outcomeId>58c00ac4cdb365.79176950</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>CHAT-2017 Workshop, Stockholm, August 2017</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>68FB91B0-799F-4F96-9947-C8013E5BC168</gtr:id><gtr:impact>We have organised a 1-day international workshop entitled 'Challenges for Hearing Assistive Technology, CHAT-2016' that will run as a satellite event of the large week-long Interspeech Conference in Stockholm in August. The workshop has been granted recognition and financial support from the International Speech Communication Association (ISCA). We have recruited a Scientific Committee of 25 leading international researchers representing both academia and the hearing aid industry. The purpose of the workshop will be as a meeting place for the hearing aid industry and researchers working in speech technology. We hope that interaction between these communities can stimulate fresh ideas for new directions in hearing assistive technology. The workshop will also provide an opportunity to promote the work being conducted under our EPSRC-funded AV-COGHEAR project.</gtr:impact><gtr:outcomeId>58c008cf6e5647.73467186</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://spandh.dcs.shef.ac.uk/chat2017/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>This new publicly available dataset is based on the benchmark audio-visual GRID corpus, which was originally developed by our project partners at Sheffield for speech perception and automatic speech recognition. The new dataset contains a range of joint audiovisual vectors, in the form of 2D-DCT visual features, and the equivalent audio log-filterbank vector. All visual vectors were extracted by tracking and cropping the lip region of a range of Grid videos (1000 videos from five speakers, giving a total of 5000 videos), and then transforming the region with 2D-DCT. The audio vector was extracted by windowing the audio signal, and transforming each frame into a log-filterbank vector. The visual signal was then interpolated to match the audio, and a number of large datasets were created, with the frames shuffled randomly to prevent bias, and with different pairings, including multiple visual frames to estimate a single audio frame (from one visual to one audio pairings, to 28 visual to one audio pairings).</gtr:description><gtr:id>688450B0-F9F3-4940-9E3B-31855ECD5CFC</gtr:id><gtr:impact>This new publicly available dataset is developed as a benchmark for the speech enhancement community. It enables researchers to evaluate how well audio speech can be estimated using visual information only. Specifically, the application of novel speech enhancement algorithms (including those based on advanced machine learning), can be used to evaluate the potential of exploiting visual cues for speech enhancement.</gtr:impact><gtr:outcomeId>58c685c2ef9e00.50005870</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Audiovisual Dataset for audiovisual speech mapping using the Grid Corpus</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://hdl.handle.net/11667/81</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>A novel two-stage enhanced Visually-derived Wiener Filtering (EVWF) approach for speech enhancement is being developed. The first stage employs a neural network based data driven approach to approximate clean audio features using temporal visual only features (lip reading). The next stage proposes the novel use of inverse filterbank (FB) transformation in place of cubic spline interpolation employed in the state-of-the-art visually-derived Wiener Filtering (VWF) approach. The novel EVWF has demonstrated an enhanced capability to estimate the clean high-dimensional audio power spectrum, from low-dimensional visually-derived audio filterbank features, compared to the state-of-the-art VWF. approach</gtr:description><gtr:id>437415C0-FF3A-402C-9581-630BBCB393FC</gtr:id><gtr:impact>Ongoing performance evaluation in reverberant domestic environments with multiple real-world sound sources (using the benchmark CHiME2 and audio-visual GRID corpuses), has shown that the proposed EVWF method is more reliable compared to both state-of-the-art, visually-derived weiner filtering, and audio-only speech enhancement methods, such as spectral subtraction and log-minimum mean-square error, with significant performance improvements demonstrated in terms of quantitative and qualitative speech enhancement measures.</gtr:impact><gtr:outcomeId>58c67da0afeb20.88951517</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>A Novel Enhanced Visually-Derived Wiener Filtering Approach for Speech Enhancement</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type></gtr:researchMaterialOutput><gtr:researchMaterialOutput><gtr:description>A novel Long Short-Term Memory (LSTM) based data-driven approach has been developed to approximate clean audio features, using temporal visual features
(lip reading).</gtr:description><gtr:id>A301A5CF-1201-4235-9E91-D9843A3B1A8B</gtr:id><gtr:impact>We have carried out preliminary simulation experiments using a new audiovisual (AV) dataset developed for AV speech mapping, based on the benchmark AV GRID corpus (that was originally developed, by our project partners at Sheffield, for speech perception and automatic speech recognition). Comparative results show that the proposed LSTM AV mapping model can deliver significantly enhanced clean audio features estimation, compared to our previously reported, benchmark Multi-Layered Perceptron (MLP) based AV speech modelling approach.</gtr:impact><gtr:outcomeId>58c682062f2c46.53197412</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Novel Long Short-Term Memory based Lip-Reading For Speech Enhancement in Cognitively-Inspired Multi-Modal Hearing-Aids</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>6F59142E-87A5-4C10-A67B-687893565F8B</gtr:id><gtr:title>Extracting online information from dual and multiple data streams</gtr:title><gtr:parentPublicationTitle>Neural Computing and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/11911e4f32122bc57e3f32656f7711be"><gtr:id>11911e4f32122bc57e3f32656f7711be</gtr:id><gtr:otherNames>Malik Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c584a5852440.53513915</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F399BCB4-2252-4A6C-B9B1-146EFE148C6C</gtr:id><gtr:title>Cognitively Inspired Audiovisual Speech Filtering: Towards an Intelligent, Fuzzy Based, Multimodal, Two-Stage Speech Enhancement System</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/865ce726857741ef5288ec5249020f02"><gtr:id>865ce726857741ef5288ec5249020f02</gtr:id><gtr:otherNames>Abel Andrew</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>9783319135083</gtr:isbn><gtr:outcomeId>56e10212c9a363.47452688</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>03B74815-03AD-4E7A-8D55-0EDCB173CDDA</gtr:id><gtr:title>Biologically Inspired Progressive Enhancement Target Detection from Heavy Cluttered SAR Images</gtr:title><gtr:parentPublicationTitle>Cognitive Computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8e49ffed5fb0d02ad5f6ec5fce26614"><gtr:id>a8e49ffed5fb0d02ad5f6ec5fce26614</gtr:id><gtr:otherNames>Gao F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c58eca7c6828.41340896</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>71399CBB-6942-4735-BBA7-FBEDA655F954</gtr:id><gtr:title>Towards GPU-Based Common-Sense Reasoning: Using Fast Subgraph Matching</gtr:title><gtr:parentPublicationTitle>Cognitive Computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/caec3d7022318bd3505166efe4513122"><gtr:id>caec3d7022318bd3505166efe4513122</gtr:id><gtr:otherNames>Tran H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c585c7e6a1c6.26028519</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D562AB6-2491-4353-8313-A80E38C1C332</gtr:id><gtr:title>Guided Policy Search for Sequential Multitask Learning</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Systems, Man, and Cybernetics: Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b72cf3f004cea554f8a0a023c08d5e41"><gtr:id>b72cf3f004cea554f8a0a023c08d5e41</gtr:id><gtr:otherNames>Xiong F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a995aaa54b9d5.96399171</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5AC0361-35C6-48AB-BA3F-431CC65EAC2F</gtr:id><gtr:title>The impact of automatic exaggeration of the visual articulatory features of a talker on the intelligibility of spectrally distorted speech</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2237aeb6017cfdfa35f4dd0ef65ef091"><gtr:id>2237aeb6017cfdfa35f4dd0ef65ef091</gtr:id><gtr:otherNames>Alghamdi N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aaa562abfedc2.64158585</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CDF4ABB7-090B-45BC-B460-D0115C2B1DDE</gtr:id><gtr:title>A novel brain-inspired compression-based optimised multimodal fusion for emotion recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/347e8527307a12d61bc6ef52393153c6"><gtr:id>347e8527307a12d61bc6ef52393153c6</gtr:id><gtr:otherNames>Gogate M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9553e1b58565.19000022</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9F4897F8-888F-4460-BD98-D4AAF326B376</gtr:id><gtr:title>Multi-Layered Echo State Machine: A novel Architecture and Algorithm</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/504518e9cf34df2f85e33b27cbb84191"><gtr:id>504518e9cf34df2f85e33b27cbb84191</gtr:id><gtr:otherNames>Malik, Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e1010b42ab00.37478798</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B40F9676-73D7-4C78-BCBF-517FEABA1947</gtr:id><gtr:title>A Biologically Inspired Vision-Based Approach for Detecting Multiple Moving Objects in Complex Outdoor Scenes</gtr:title><gtr:parentPublicationTitle>Cognitive Computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec72f7723ec4fc2d1d06790a72d5169a"><gtr:id>ec72f7723ec4fc2d1d06790a72d5169a</gtr:id><gtr:otherNames>Tu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675fd2416340</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A642A119-D0A6-4CF8-9E95-BD573F3102DA</gtr:id><gtr:title>Comparing Oversampling Techniques to Handle the Class Imbalance Problem: A Customer Churn Prediction Case Study</gtr:title><gtr:parentPublicationTitle>IEEE Access</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2dae8b99a243b64dd03d66256b721408"><gtr:id>2dae8b99a243b64dd03d66256b721408</gtr:id><gtr:otherNames>Amin A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a2fe10df220c6.62398356</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A8C6DE55-9A8F-455F-87F3-BC3C096488CB</gtr:id><gtr:title>Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques.</gtr:title><gtr:parentPublicationTitle>Cognitive computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5c716247c0df5e1fef91adc29e1fde3f"><gtr:id>5c716247c0df5e1fef91adc29e1fde3f</gtr:id><gtr:otherNames>Dashtipour K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1866-9956</gtr:issn><gtr:outcomeId>58c586e2b79b62.00877680</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>073BA6DA-FEC1-47E1-A5C8-B555B380C8C9</gtr:id><gtr:title>Lung cancer detection using Local Energy-based Shape Histogram (LESH) feature extraction and cognitive machine learning techniques</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec436bdbc5c85f8e9dee259c7b225d73"><gtr:id>ec436bdbc5c85f8e9dee259c7b225d73</gtr:id><gtr:otherNames>Wajid S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7d72e5af253.20794095</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FEB9B410-7715-4ACF-832E-831086C5F2DA</gtr:id><gtr:title>Towards a Biologically Inspired Soft Switching Approach for Cloud Resource Provisioning</gtr:title><gtr:parentPublicationTitle>Cognitive Computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dff6eb16482989cf1b8239aeefdf75b1"><gtr:id>dff6eb16482989cf1b8239aeefdf75b1</gtr:id><gtr:otherNames>Ullah A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e102df8680a3.24517942</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1DA0F66D-0515-41D9-AB60-C4D872B6D10F</gtr:id><gtr:title>Computational Intelligence for Changing Environments [Guest Editorial]</gtr:title><gtr:parentPublicationTitle>IEEE Computational Intelligence Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b2c46c8258c06e4949fd9181e5986ea9"><gtr:id>b2c46c8258c06e4949fd9181e5986ea9</gtr:id><gtr:otherNames>Hussain A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e034a8441aa7.57957612</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>332EE8B3-0847-406C-8B9E-6835C1D1DC30</gtr:id><gtr:title>Genetic optimization of fuzzy membership functions for cloud resource provisioning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dff6eb16482989cf1b8239aeefdf75b1"><gtr:id>dff6eb16482989cf1b8239aeefdf75b1</gtr:id><gtr:otherNames>Ullah A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c57c1fb8aeb6.33151201</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E04F5B8B-93B1-4EBF-9225-E6BAC5CD1AA9</gtr:id><gtr:title>Distributed Reservoir Computing with Sparse Readouts [Research Frontier]</gtr:title><gtr:parentPublicationTitle>IEEE Computational Intelligence Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca7c4d8bab2ea9548175819388d1bfa5"><gtr:id>ca7c4d8bab2ea9548175819388d1bfa5</gtr:id><gtr:otherNames>Scardapane S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c58ba78f7df1.27779654</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4D40DE8E-43C4-44F5-9A1B-09A55F011A82</gtr:id><gtr:title>A New Spatio-Temporal Saliency-Based Video Object Segmentation</gtr:title><gtr:parentPublicationTitle>Cognitive Computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec72f7723ec4fc2d1d06790a72d5169a"><gtr:id>ec72f7723ec4fc2d1d06790a72d5169a</gtr:id><gtr:otherNames>Tu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1866-9956</gtr:issn><gtr:outcomeId>5a362666602145.21640998</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3167572-F892-4A18-B4EA-3237AD93E08F</gtr:id><gtr:title>Investigating the Impact of Artificial Enhancement of Lip Visibility on the Intelligibility of Spectrally-Distorted Speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4a1f56e8de5a7adc48f75f029ab3101d"><gtr:id>4a1f56e8de5a7adc48f75f029ab3101d</gtr:id><gtr:otherNames>Alghamdi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c00df007de30.24022205</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3FA658D1-D333-4707-BC3E-FD50ED30AFD5</gtr:id><gtr:title>A Novel Spatiotemporal Longitudinal Methodology for Predicting Obesity Using Near Infrared Spectroscopy (NIRS) Cerebral Functional Activity Data</gtr:title><gtr:parentPublicationTitle>Cognitive Computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/890daf9a1f83473e4569317fff264446"><gtr:id>890daf9a1f83473e4569317fff264446</gtr:id><gtr:otherNames>Abdullah A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a995943aef4a1.65991891</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>443CD9A0-51FB-479A-8CA6-948ED82CD840</gtr:id><gtr:title>Group sparse regularization for deep neural networks</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca7c4d8bab2ea9548175819388d1bfa5"><gtr:id>ca7c4d8bab2ea9548175819388d1bfa5</gtr:id><gtr:otherNames>Scardapane S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe6b0965c22.18140603</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M026981/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>