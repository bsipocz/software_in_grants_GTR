<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Dept of Computing</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E7410EF7-9051-4F95-8981-1D02ED062618"><gtr:id>E7410EF7-9051-4F95-8981-1D02ED062618</gtr:id><gtr:firstName>Ben</gtr:firstName><gtr:surname>Glocker</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN023668%2F1"><gtr:id>41F7EEDB-9E30-42A1-914C-BAF829758DD1</gtr:id><gtr:title>QuantifyTBI: A Machine Learning Approach to Automatic Segmentation and Quantification of Lesions in Traumatic Brain Injury Imaging</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N023668/1</gtr:grantReference><gtr:abstractText>Traumatic brain injury (TBI) has been characterised as the most complex disease in the most complex organ of our body. TBI is defined as a pathological change in brain function caused by strong external force, commonly induced by falls, assaults, car traffic accidents, sport injuries, or the blast of an explosion in military combat. TBI has been estimated to affect over 6.8 million people per year worldwide and is the leading cause of disability and death of young adults in developed countries. The high number of incidences puts a major socio-economical burden on public health. A recent estimate of the total costs of TBI in Europe, excluding non-hospitalised patients, produces a figure of 33 billion Euros. The biggest cost, of course, is paid by the millions of patients and their families, who live for years with long-term consequences of TBI. Medical imaging combined with advanced computational methods have the potential to improve TBI care by supporting the critical tasks of early diagnosis, prognosis, and treatment. Imaging has been established as the primary tool for visual, non-invasive assessment of TBI both in critical care, and short- and long-term follow-up. However, the current use of imaging for TBI assessment is limited to manual, qualitative and often subjective inspection of the images. This motivates the main objective of this project which is the development of software tools that enable the automatic extraction of clinically useful information to improve care for patients with TBI.

The project QuantifyTBI explores computational methods, in particular machine learning approaches, to analyse and quantify brain scans of patients with TBI. Specific algorithms and software tools are developed that allow doctors to more objectively and accurately assess the severity of head injuries and monitor the progression during treatment. The main focus is to develop software that allows to automatically derive quantitative measures about TBI lesions from the patient's brain scans. Such measures include the number of lesions, the type of lesions, their size, the location, and the ratio of affected brain tissue. An accurate and comprehensive image-based quantification is essential for developing personalised treatment strategies, supporting diagnosis and monitoring disease progression. It also helps to better understand TBI from a clinical perspective, and will eventually lead to better treatment and improved outcome of TBI patients.</gtr:abstractText><gtr:potentialImpactText>The following beneficiaries will benefit from the research that is carried out in the QuantifyTBI project.

Public healthcare/NHS
Public healthcare, such as the NHS, benefits from the development of effective tools that support a better understanding of diseases and have the potential to help to develop better treatment strategies. Our research on TBI image analysis has the potential to have a significant impact on TBI related healthcare. The analysis tools will help to inform patient management, monitor injury progression, and improve prediction of outcome. Early and accurate prognostication would allow informed decisions regarding life sustaining treatment in the acute phase, and will help to manage treatment costs. 

Clinicians
Clinicians will benefit from software tools that will support diagnosis, monitoring and treatment of TBI. The software has the potential to make decisions more informed and less subjective and dependent on experience. Less trained clinicians will benefit the most from a decision support system that could be built around our analysis software.

Patients
Patients suffering from TBI will benefit from improved care. In particular, new personalised treatment strategies that can be developed using our analysis tools have the potential to improve patient outcome.

General public
TBI can affect everyone, and one of the most frequent causes are road traffic collisions. Our research has potential to raise the awareness for TBI in the general public. Informing the public about consequences of TBI, and emphasise the importance of precautions (e.g. wearing bike helmets) could eventually lead to fewer cases of TBI. As part of this research, we will engage with the public and disseminate TBI related information material.

Industry/Pharmaceutical Companies
The analysis tools have commercial value, and medical imaging companies have strong interests in the possibility of using such tools to derive imaging biomarkers that enable patient stratification in clinical trials as well as new ways for diagnostics. Patient stratification is also of great interest to pharmaceutical companies, in particular in phase 2 and 3 of clinical trials, which can reduce costs by optimising patient selection and reduce the number of individuals needed in a particular trial.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>97533</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our work on using artificial intelligence and machine learning techniques to analyse brain scans has received notable attention in the public media. We have been interviewed by several newspapers and we have delivered non-academic presentations about our work to the public, for example, at the World Economic Forum's Annual Meeting of the New Champions 2016. In particular, our work has been discussed in the context of a changing healthcare system which in future will make use of artificial intelligence to provide optimal patient care.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>BD7A3BDD-8DB9-45D6-9F31-B019906E83EC</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>58a07f9ac227a4.48585611</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have shown that machine learning, and in particular, deep neural networks, can be successfully applied to quantify lesions in brain images.</gtr:description><gtr:exploitationPathways>We have released an open source implementation of our software for detecting brain lesions. Other researchers will be able to build upon our results and further improve automatic systems for brain image analysis. The software has already been used by others on similar problems.</gtr:exploitationPathways><gtr:id>80694DA5-B428-4278-BEDA-57D07072AD7C</gtr:id><gtr:outcomeId>58a07d2d6c20e5.60698240</gtr:outcomeId><gtr:sectors><gtr:sector>Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.smh.com.au/national/health/dr-google-will-see-you-now-20161212-gt9dr1.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9ED850C9-C8BE-41AF-98CF-FD36E2D83F79</gtr:id><gtr:title>Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth</gtr:title><gtr:parentPublicationTitle>IEEE Transaction on Medical Imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/98fd5a144dc5347a07bb4187c80e74d4"><gtr:id>98fd5a144dc5347a07bb4187c80e74d4</gtr:id><gtr:otherNames>Valindria V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58a07bb3535398.12430386</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>41F51A26-94B9-4FFE-8C21-2184819C1E31</gtr:id><gtr:title>Regional brain morphometry in patients with traumatic brain injury based on acute- and chronic-phase magnetic resonance imaging.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd150c42a6efe797143aa5dceb5a5f8d"><gtr:id>bd150c42a6efe797143aa5dceb5a5f8d</gtr:id><gtr:otherNames>Ledig C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a3626198a05d6.51011835</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>69628DD8-232D-425E-8F58-1AD81EDE17E9</gtr:id><gtr:title>Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on medical imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/983db3644cce56f5cab9d724090573f1"><gtr:id>983db3644cce56f5cab9d724090573f1</gtr:id><gtr:otherNames>Valindria VV</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0278-0062</gtr:issn><gtr:outcomeId>5a2febf468bff4.90881976</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B390CC4B-4785-42E9-B3D0-B78CF665DF3E</gtr:id><gtr:title>DeepMedic for Brain Tumor Segmentation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41a2d3b38067eac54f89019b9fd22d57"><gtr:id>41a2d3b38067eac54f89019b9fd22d57</gtr:id><gtr:otherNames>Kamnitsas K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a07df8c401c0.19143889</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1D40206F-720F-41BD-A33C-DA6647DB9CBA</gtr:id><gtr:title>Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41a2d3b38067eac54f89019b9fd22d57"><gtr:id>41a2d3b38067eac54f89019b9fd22d57</gtr:id><gtr:otherNames>Kamnitsas K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>585d6bad686ed7.55104908</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N023668/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>