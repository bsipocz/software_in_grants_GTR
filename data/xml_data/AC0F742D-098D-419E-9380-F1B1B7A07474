<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/597DF019-2D92-427D-8665-582CB31F5FC7"><gtr:id>597DF019-2D92-427D-8665-582CB31F5FC7</gtr:id><gtr:firstName>Jonathan</gtr:firstName><gtr:surname>Starck</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D0F82D5-F19E-4EA8-A0C7-C7F120102A69"><gtr:id>2D0F82D5-F19E-4EA8-A0C7-C7F120102A69</gtr:id><gtr:firstName>Adrian</gtr:firstName><gtr:surname>Hilton</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE001351%2F1"><gtr:id>AC0F742D-098D-419E-9380-F1B1B7A07474</gtr:id><gtr:title>Video-based animation of people</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E001351/1</gtr:grantReference><gtr:abstractText>The goal of this research project is to solve the fundamental problem of re-using multiple-view video capture of people to support interactive animation with the quality of the source video. Research will investigate the resampling of a multiple-view video database for interactive animation of people to enable user control of movement and viewpoint with video-quality rendering. The proposed research will address the underlying problem of representation of articulated and highly dynamic non-rigid structures in video to allow indexing, reuse and manipulation whilst maintaining the visual quality. Ultimately the challenge is to enable video-quality rending of real people by reuse of captured video sequences allowing user control of movement as in conventional animation. Recent research has demonstrated photo-realistic animation of faces and simple objects by resampling video sequences.This proposal aims to take video-based animation to complex articulated objects such as people without full 3D reconstruction which has been shown to result in loss of visual quality in previous work. The new challenge is to synthesise video sequences of novel movements from captured video of different motion. Animating people from video requires several key advances, including: a representation of human posture and movement which supports the generation of sequences of previously unseen movements; a representation of multiple-view video of articulated objects allowing efficient storage and indexing; algorithms to efficiently selecting the most useful examples for the synthesis of a new sequences from a large set of example sequences; and new video synthesis algorithms for articulated objects in previously unseen configurations. Animation of people directly from captured video has the potential to provide enabling technology for next-generation, video-quality content production in film, television and games. Video-based animation of people will allow powerful re-use and manipulation of captured video (e.g., generating novel body movements for an actor), together with seamless compositing within photo realistic scenes.</gtr:abstractText><gtr:fund><gtr:end>2010-08-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-04-02</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>349255</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>BBC Research and Development</gtr:description><gtr:id>C97E4F03-4D87-40B7-9811-E9A84C0D9449</gtr:id><gtr:impact>Multi-disciplinary collaboration involves Computer Vision, Video Analysis, Psychoacoustics, Signal Processing and Spatial Audio</gtr:impact><gtr:outcomeId>b9c5b39ab9c5b3ae-1</gtr:outcomeId><gtr:partnerContribution>In kind contribution (members of Steering/Advisory Boards) Use of the BBC lab and research/development facilities. 
Studentships (industrial case) funding and co-supervision of PhD students.</gtr:partnerContribution><gtr:piContribution>Research in Computer Vision for broadcast production and Audio.
Technologies for 3D production, free-view point video in sports, stereo production from monocular cameras, video annotation
Member of the BBC Audio Research Partnership - developing the next generation of broadcast technology.</gtr:piContribution><gtr:sector>Public</gtr:sector></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Video-based animation technology has underpinned the development of systems for free-viewpoint video and video quality animation content production used in the creative industries (film, TV, games, web).</gtr:description><gtr:firstYearOfImpact>2006</gtr:firstYearOfImpact><gtr:id>AC8C42E3-CEF6-4242-B92E-2762EF464FBD</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545cec8e8ce098.55685679</gtr:outcomeId><gtr:sector>Creative Economy,Education,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Key findings: 

- the first video-based creation of photo-realistic animated content from actor performance
- a new approach to animated content production based on a new surface motion graph representation
- high-accuracy reconstruction of dynamic surface shape of actors from multiple view video
- photo-realistic free-viewpoint rendering of actor performance
- a comprehensive evaluation of methods for measuring shape-similarity of people across different poses and the introduction of new validated methods
- a framework for photo-realistic animated content production and rendering
- fundamental advance demonstrating the principle of reuse of captured dynamic shape and appearance to produce animation by concatenation</gtr:description><gtr:exploitationPathways>Findings have been taken forward in:
(1) the development of a production pipeline for video-realistic animated content production
(2) interactive real-time animation tools developed in the EU Project RE@CT led by the BBC
(3) exploitation of the technology by SME Artifacto to develop and demonstrate animated content for educational museum applications and cultural heritage</gtr:exploitationPathways><gtr:id>E033B6BD-23D6-4BAF-97C9-40F60C82EA9C</gtr:id><gtr:outcomeId>545cef9528a7b8.28277688</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://cvssp.org/Personal/AdrianHilton</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2F6A7541-C2B4-4E2E-A799-F6962B42FD26</gtr:id><gtr:title>Shape Similarity for 3D Video Sequences of People</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a75ed317b835fa04bc2ece4fa173be04"><gtr:id>a75ed317b835fa04bc2ece4fa173be04</gtr:id><gtr:otherNames>Huang P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53cfdefdef5c1cd3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6CE7720B-BEBE-48DA-969C-0C2C1720C748</gtr:id><gtr:title>Hybrid Skeletal-Surface Motion Graphs for Character Animation from 4D Performance Capture</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a75ed317b835fa04bc2ece4fa173be04"><gtr:id>a75ed317b835fa04bc2ece4fa173be04</gtr:id><gtr:otherNames>Huang P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0730-0301</gtr:issn><gtr:outcomeId>56e0b0151aaa11.00433894</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>40CB3F27-E0AA-4B36-B06C-2468330D5DF0</gtr:id><gtr:title>Visual analysis of lip coarticulation in VCV utterances</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/878018765ff204e703952e36901f7484"><gtr:id>878018765ff204e703952e36901f7484</gtr:id><gtr:otherNames>Turkmani A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>56dd7be42e3614.87015577</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2E8F426-5AB6-486A-BFA1-21A894E2C0C7</gtr:id><gtr:title>Visual analysis of lip coarticulation in VCV utterances</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc7cbd23acfb72411c5b28b0df47ecb"><gtr:id>3cc7cbd23acfb72411c5b28b0df47ecb</gtr:id><gtr:otherNames>A Turkmani</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>m_550967380813d65232</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>12B82707-9C6B-4850-9E05-68B2F2F34E17</gtr:id><gtr:title>Surface Motion Graphs for Animation from 3D Video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/34f94d9e812f3416b533873f3b5a2f5f"><gtr:id>34f94d9e812f3416b533873f3b5a2f5f</gtr:id><gtr:otherNames>P Huang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_661217064113ffb014</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87C72138-8FEB-4E50-8B4E-69EBB1717952</gtr:id><gtr:title>Human motion synthesis from 3D video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/176f7f6464a54c3be7b9b99100193f52"><gtr:id>176f7f6464a54c3be7b9b99100193f52</gtr:id><gtr:otherNames>Peng Huang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-3992-8</gtr:isbn><gtr:outcomeId>56dd7be2954ca0.45714676</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>310F322E-C6A3-45DA-A827-69F095343F27</gtr:id><gtr:title>High-resolution Animation of Facial Dynamics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ff8a17960cadf2b330f10fe187d79985"><gtr:id>ff8a17960cadf2b330f10fe187d79985</gtr:id><gtr:otherNames>N Nadtoka</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>m_842691434913f17c88</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>74B9529C-46C6-4DCA-B7F7-CC0A603053EB</gtr:id><gtr:title>Visual speech synthesis from 3D video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f100048935b99890fcd8ec4ad5df48a8"><gtr:id>f100048935b99890fcd8ec4ad5df48a8</gtr:id><gtr:otherNames>Edge J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2006-01-01</gtr:date><gtr:outcomeId>56dd7be407eee0.58961769</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>30B5B995-46D4-4629-81C6-7334FBCEBA1F</gtr:id><gtr:title>Surface Capture for Performance-Based Animation</gtr:title><gtr:parentPublicationTitle>IEEE Computer Graphics and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/20a697234073ee0d30e2422a42f0d19f"><gtr:id>20a697234073ee0d30e2422a42f0d19f</gtr:id><gtr:otherNames>Starck J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53d05b05b7bd02bf</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0EC4C80E-AFA4-4D81-96E6-1A99F9C9D349</gtr:id><gtr:title>The Multiple-Camera 3-D Production Studio</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/20a697234073ee0d30e2422a42f0d19f"><gtr:id>20a697234073ee0d30e2422a42f0d19f</gtr:id><gtr:otherNames>Starck J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d05d05d6e52c1f</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E001351/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>