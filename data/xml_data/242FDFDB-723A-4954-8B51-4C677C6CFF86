<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:department>School of Biological Sciences</gtr:department><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/48D30CB7-32D3-4BFB-BE61-9B7D40D5B34F"><gtr:id>48D30CB7-32D3-4BFB-BE61-9B7D40D5B34F</gtr:id><gtr:firstName>Riccardo</gtr:firstName><gtr:surname>Storchi</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=NC%2FP001505%2F1"><gtr:id>242FDFDB-723A-4954-8B51-4C677C6CFF86</gtr:id><gtr:title>Interrogating the mouse visual system by automated analysis of voluntary behaviour</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>NC/P001505/1</gtr:grantReference><gtr:abstractText>Retinal degeneration and irreversible photoreceptor diseases affect ~220 million worldwide and are currently not curable. In these diseases the light sensitive neurons in the retina die, leaving the retina unable to respond to light and the patient blind. Several approaches for restoring vision to these people, arising from recent scientific and technological advances, are currently under development. The only way to measure the effectiveness of these potential therapies before trailing them on humans is to ask whether they can restore vision to animals that suffer the same blinding conditions as humans. At present almost all of that work is undertaken in laboratory mice because strains of mice suffering retinal degeneration are readily available. Knowing whether treatments have been effective in these animals boils down to answering a simple question: how well can the mice see? 
This question is, at the moment, difficult to answer. As you cannot ask mice what they have seen, researchers must use indirect indications that mice have detected visual features in their environment. In this proposal I tackle the problem of how to achieve this. I will start with an approach that is humane but at present cannot answer the question as effectively as we'd like. The test relies on the observation that when mice detect a change in the visual scene they often respond with a change in behaviour. At present the change in behaviour that is measured is how far mice move around their environment. I believe that I can make this test much more powerful if I can measure mouse behaviour better. 
The first step of my project is to develop an automatic technique to identify distinct types of behaviour. Why this is important? Imagine a man standing in a room. After some time a light is flashed from the outside and projected through the window. What happens next? The man might walk to the window or instead he might just turn his head to look at it from his current position. How can we understand whether the man has seen the light? The only way to do that is by looking at his movements. If we only rely on him walking to the window we might fail in the task, however, if we are also able to determine when he turns his head towards the window we will stand better chances. In order to implement this possibility I will track body points (such as head and upper and low back) and I will use an algorithm to identify when sudden movements of these points occur. This technique is called changepoint analysis and will allow me to identify start or stop times of different behaviours (such as walking or head movements) and relate them to the times when experimental manipulations in the visual environment occur. 
Sometimes single manipulations of the visual environment fail to elicit a spontaneous response and they have to be repeated several times to obtain a good estimate of visual detection. However this can be problematic. Why? Back to the man in the room imagine that the flash occurs multiple times at regular time intervals. After a while the man will expect them to happen and lose interest. However if the flash is presented at irregular intervals sufficiently distant from each other the man might continue to react to them. The second step of my project is to optimise the timing of these experimental manipulations in order to maximise spontaneous reactions. 
The final step of my project is to use these techniques to measure and compare the effectiveness of three of the most recent and promising gene therapy strategies for restoring vision following retinal degeneration. 
My project will deliver a more powerful approach to assess visual perception in mice and a benchmark dataset to compare the effects of gene therapies. It will enable to better extrapolate from outcomes of animal studies to improvements in human condition and reduce the number of mice used for testing.</gtr:abstractText><gtr:technicalSummary>My project has two main aims: 
1) AIM1: To improve current methods of assessing visual discrimination in mice to meet the 3Rs objectives
2) AIM2: To use this improved methodology to compare efficacy for three of the most promising therapies currently available
AIM1: I will focus on behavioural tests based on spontaneous mouse behaviour. They rely upon the hypothesis that when mice detect a change in their visual environment this should be associated with an alteration in their behaviour. These tests provide a humane approach to assess visual discrimination however they suffer from low statistical power. In order to improve it I will first develop automated techniques to track multiple body points and, by using multivariate temporal segmentation and classification, to identify the fundamental building blocks of complex exploratory behaviours. Then I will use these improved behavioural measures to optimise stimulus design in terms of exposure times to the visual stimuli. To dynamically choose how to modify these parameters I will use well-established search procedures based on information theory and genetic algorithms. 
AIM2: I will use these improved techniques to compare the efficacy of the most recent optogenetic treatments based on selective expression in ON bipolar cells of channelrhodopsin-2, optomGluR6 and human rhodopsin. These data will provide an important benchmark dataset to compare the effects of pre-clinical treatments.
My project will deliver a higher throughput approach to assess visual perception in mice that will enable better extrapolation from outcomes of pre-clinical animal studies to improvements in human condition. By increasing statistical power it will reduce the number of mice needed for testing. The techniques I will develop for automated analysis of spontaneous behaviour will also impact other fields where experimenters use these behaviours to measure anxiety, depression, attention and memory.</gtr:technicalSummary><gtr:fund><gtr:end>2020-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/66BB3CDB-84B4-41B5-AB21-FEC312E91CC8"><gtr:id>66BB3CDB-84B4-41B5-AB21-FEC312E91CC8</gtr:id><gtr:name>NC3Rs</gtr:name></gtr:funder><gtr:start>2017-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>218946</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">NC/P001505/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>