<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D70BE577-813F-4FB8-8325-D7829EC6988B"><gtr:id>D70BE577-813F-4FB8-8325-D7829EC6988B</gtr:id><gtr:name>Rutherford Appleton Laboratory</gtr:name><gtr:address><gtr:line1>STFC Rutherford Appleton
Laboratory
Harwell
Didcot</gtr:line1><gtr:city>Oxford</gtr:city><gtr:postCode>OX11 0QX</gtr:postCode><gtr:region>South East</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/8D49A403-7D6B-4C9E-9188-13616E6DDCCF"><gtr:id>8D49A403-7D6B-4C9E-9188-13616E6DDCCF</gtr:id><gtr:name>University of Oklahoma</gtr:name><gtr:address><gtr:line1>550 Parrington Oval</gtr:line1><gtr:line4>Norman</gtr:line4><gtr:line5>OK 73019-3032</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A9CA5E1C-DE53-4198-A3E4-F4764B5CB434"><gtr:id>A9CA5E1C-DE53-4198-A3E4-F4764B5CB434</gtr:id><gtr:name>Harper Adams University</gtr:name><gtr:address><gtr:line1>Harper Adams University</gtr:line1><gtr:line4>Newport</gtr:line4><gtr:line5>Shropshire</gtr:line5><gtr:postCode>TF10 8NB</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:department>Sch of Engineering and Informatics</gtr:department><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D70BE577-813F-4FB8-8325-D7829EC6988B"><gtr:id>D70BE577-813F-4FB8-8325-D7829EC6988B</gtr:id><gtr:name>Rutherford Appleton Laboratory</gtr:name><gtr:address><gtr:line1>STFC Rutherford Appleton
Laboratory
Harwell
Didcot</gtr:line1><gtr:city>Oxford</gtr:city><gtr:postCode>OX11 0QX</gtr:postCode><gtr:region>South East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8D49A403-7D6B-4C9E-9188-13616E6DDCCF"><gtr:id>8D49A403-7D6B-4C9E-9188-13616E6DDCCF</gtr:id><gtr:name>University of Oklahoma</gtr:name><gtr:address><gtr:line1>550 Parrington Oval</gtr:line1><gtr:line4>Norman</gtr:line4><gtr:line5>OK 73019-3032</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A9CA5E1C-DE53-4198-A3E4-F4764B5CB434"><gtr:id>A9CA5E1C-DE53-4198-A3E4-F4764B5CB434</gtr:id><gtr:name>Harper Adams University</gtr:name><gtr:address><gtr:line1>Harper Adams University</gtr:line1><gtr:line4>Newport</gtr:line4><gtr:line5>Shropshire</gtr:line5><gtr:postCode>TF10 8NB</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/ED3F72AB-DBB3-4540-9EA9-DEF85D5A7AA4"><gtr:id>ED3F72AB-DBB3-4540-9EA9-DEF85D5A7AA4</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Philippides</gtr:surname><gtr:orcidId>0000-0001-5503-0467</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI031758%2F1"><gtr:id>74E4DAC9-0015-4438-AB5E-06F338B84D74</gtr:id><gtr:title>Insect-inspired visually guided autonomous route navigation through natural environments</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I031758/1</gtr:grantReference><gtr:abstractText>Our overall objective is to develop algorithms for long distance route-based visual navigation through complex natural environments. Despite recent advances in autonomous navigation, especially in map-based simultaneous localisation and mapping (SLAM), the problem of guiding a return to a goal location through unstructured, natural terrain is an open issue and active area of research. Despite their small brains and noisy low resolution sensors, insects navigate through such environments with a level of performance that outstrips state-of-the-art robot algorithms. It is therefore natural to take inspiration from insects. There has been a history of bio-inspired navigation models in robotics but there are known components of insect behaviour yet to be incorporated into engineering solutions. In contrast with most modern robotic methods, to navigate between two locations, insects, use procedural route knowledge and not mental maps. An important feature of route navigation is that the agent does not need to know where it is at every point (in the sense of localizing itself within a cognitive map), but rather what it should do. Insects provide further inspiration for navigation algorithms through their innate behavioural adaptations which simplify navigation through unstructured, cluttered environments.One objective is to develop navigation algorithms which capture the elegance and desirable properties of insect homing strategies - robustness (in the face of natural environmental variation), parsimony (of mechanism and visual encoding), speed of learning (insects must learn from their first excursion) and efficacy (the simple scale over which insects forage). Prior to this we will bring together current insights regarding insect behaviour with novel technologies which allow us to recreate visual input from the perspective of foraging insects. This will lead to new tools for biologists and increase our understanding of insect navigation. In order to achieve these goals our Work Packages will be:WP1 Development of tools for reconstructing large-scale natural environments. We will adapt an existing panoramic camera system to enable reconstruction of the visual input experienced by foraging bees. Similarly, we will adapt new computer vision methods to enable us to build world models of the cluttered habitats of antsWP2 Investigation of optimal visual encodings for navigation. Using the world model developed in WP1, we will investigate the stability and performance of different ways of encoding a visual sceneWP3 Autonomous route navigation algorithms. We will test a recently developed model of route navigation and augment it for robust performance in natural environmentsOur approach in this project is novel and timely. The panoramic camera system has just been developed at Sussex. The methods for building world models have only recently become practical and have not yet been applied in this context. The proposed route navigation methodology is newly developed at Sussex and is based on insights of insect behaviour only recently observed. Increased knowledge of route navigation will be of interest to engineers and biologists. Parsimonious route-following algorithms will be of use in situations where an agent must reliably navigate between two locations, such as a robotic courier or search-and-rescue robot. Our algorithms also have potential broader applications such as improving guidance aids for the visually-impaired. Biologists and the wider academic community will be able to use the tools developed to gain an understanding of the visual input during behavioural experiments leading to a deeper understanding of target systems. There is specific current interest from Rothamsted Agricultural Institute who are interested in how changes in flight patterns affect visual input and navigational efficacy of honeybee foragers from colonies affected by factors like pesticides or at risk of colony collapse disorder.</gtr:abstractText><gtr:potentialImpactText>The cross-disciplinary work will be of interest to engineers and biologists in academia and industry as well as the general public and schools. It fits within EPSRCs Control Systems and Robot Engineering area and Cross-Disciplinary programme. Academia: Robust outdoor route navigation is an active area of research and our algorithms will be of interest to engineers and computer scientists working on ground-based and airborne guidance. Moreover, tools for reconstructing environments will allow these groups to test out their own models, while adaptation of Dense Scene Reconstruction will interest those working on 3D depth map reconstruction. Similarly, stable visual feature extraction in natural habitats will aid feature selection and data association problems in robotics and computer vision applications more generally. Biologists working on insect visual behaviour will benefit from software tools and data from the project by reconstructing sensory input experienced during their experiments, thus tying sensation to action. We will also produce testable hypotheses on the visual features and algorithms used for navigation. Specifically, Rothamsted Agricultural Institute will use our tools to interpret the effect of altered flight patterns of honeybees from infected colonies. More generally, as the navigational strategies of insects resemble, to a surprising extent, those of animals with much larger brains, a similarity likely to have arisen through convergent evolution of navigational mechanisms; understanding how insects operate and the cognitive processes involved is of interest to neuroscientists and psychologists. Industry: As their interests often align with academics, the novel technologies produced will be of interest to industrial robotic and computer vision applications in natural environments. Insect-inspired control systems are much used in autonomous robotics for autonomous navigation and exploration and in the control of unmanned air vehicles in particular. More generally, route navigation and visual feature selection will impact visual guidance systems (eg for the visually impaired). Finally, the games industry is a major driver of 3D mapping and adaptations of these technologies to large-scale environments will interest them. Through Rothamsted, the application of our work to pollinator flight paths will be of interest to farmers and industry concerned with crop pollination. Agro-chemical industries can assess the impact of altered flight patterns of bees exposed to pesticide (or disease) on the visual information perceived and whether this means, for instance, that they are unable to learn hive position, and plan mitigation strategies accordingly. Beekeepers are always fascinated by how their bees behave and will take great interest in the tools developed and the window they give on the sensory consequences of bee flight. General Public and Schools: The public is fascinated by insects and their lifestyles, and the ways in which we use robots and technology to study them. Moreover, the promise of autonomous travel always sparks interest in the benefits technology can bring to everyday life. Communicating our research will ensure we have an engaged generation who can see the benefits of technology in the study of the natural world. In particular, potential students will see the range of careers available to science students and the links between Biological and Computer Science. They will also see that studying computer science need not result in a career in systems admin, but can involve navigating flying robots and helping understand the brain. The team: The PDRA will gain useful transferable skills and expertise in 3D depth map reconstruction and insect behaviour. Summary: By disseminating our work through high-impact publications and international robotics and biology conferences and hosting specialist workshops, we will impact both UK and international groups.</gtr:potentialImpactText><gtr:fund><gtr:end>2013-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>102329</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Edinburgh</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Edinburgh University</gtr:description><gtr:id>CBBE804C-0528-48B9-8927-DD8B1944AECE</gtr:id><gtr:impact>Multi-disciplinary collaboration, Computer Science and Biology. Several publications:

Cheung, A., Collett, M., Collett, T. S., Dewar, A., Dyer, F., Graham, P, Mangan, M., Narendra, A., Philippides, A., St&amp;uuml;rzl, W., Webb, B. Wystrach, A. &amp;amp; Zeil, J. (2014). Still no convincing evidence for cognitive map use by honeybees. PNAS, 111 (42), E4396-E4397

Wystrach, A., Philippides, A., Aurejac, A., Cheng, K.,Graham, P. (2014) Visual scanning behaviours and their role in the navigation of the Australian desert ant Melophorus bagoti, J Comp Physiol A.,1-12

Wystrach, A., Mangan, M., Philippides, A. and Graham, P. (2013) Snapshots in ants? New interpretations of paradigmatic experiments. J Exp Biol, 216:1766-1770</gtr:impact><gtr:outcomeId>b9d08d24b9d08d38-1</gtr:outcomeId><gtr:partnerContribution>We have several joint publications and they have given talks at Sussex. They have given us access to field sites and so we have been able to get new data. They have trialled our algorithms. We have gained expertise in UV filter based vision</gtr:partnerContribution><gtr:piContribution>We have several joint publications and have given talks at Edinburgh. We have provided expertise in navigation algorithms and robotics. We have provided simulation code for their publications</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Harper Adams University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Harper Adams on autonomous agricultural robot navigation</gtr:description><gtr:id>4CF3C562-4441-42C3-BA65-DBA3915D2678</gtr:id><gtr:impact>Joint Newton Agritech funded project. Multi-disciplinary: engineering, bio-inspired robotics, Agri-tech</gtr:impact><gtr:outcomeId>545cc2bc5abba1.53387488-1</gtr:outcomeId><gtr:partnerContribution>We are partners on a Newton Agritech Fund project and have collaborated on other funding bids, for which they are dealing with GPS-based navigation and Agri-tech applications for our algorithms. They also wrote a letter of support for a RAEng/Leverhulme fellowship (unsuccessful)</gtr:partnerContribution><gtr:piContribution>We are partners on a Newton Agritech Fund project and have collaborated on other funding bids, for which my part is insect-inspired navigation algorithms</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oklahoma</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Collaboration with Prof Doug Gaffin</gtr:description><gtr:id>94812E99-1984-49A4-8DD0-D6C0DD262570</gtr:id><gtr:impact>one paper submitted, bench fees for Sussex</gtr:impact><gtr:outcomeId>b970bd54b970bd68-1</gtr:outcomeId><gtr:partnerContribution>Prof Doug Gaffin came to be a visiting professor with us from January-May 2013. We got 2000 bench fees plus Doug working free for us for 4 months (estimate 2k/month) but more importantly we have one publication submitted and are planning others. He has also introduced us to engineers at Oklahoma university interested in exploiting the algorithms</gtr:partnerContribution><gtr:piContribution>We hosted Prof Doug Gaffin as a visiting professor with us from January-May 2013 providing expertise in insect-inspired navigation algorithms</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Rutherford Appleton Laboratory</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Space Science and Technology Department</gtr:department><gtr:description>Collaboration with RAL Space on autonomous robotic for space exploration and AgriTech</gtr:description><gtr:id>F7DDE532-293D-4F3F-8DCB-131D06C6ED9D</gtr:id><gtr:impact>Newton Agritech funding. 

Multi-disciplinary collaboration involving Engineering, specifically Robotics, with Computational Biology and Artificial Life. It will be applied in Agri-tech and space exploration</gtr:impact><gtr:outcomeId>545cc1bbc7eb15.85922284-1</gtr:outcomeId><gtr:partnerContribution>We are partners on a Newton Agritech Fund project (which they are leading on) and have collaborated on other unsuccessful funding bids. 

Their part is that they provide the robotic platforms and also have helped me a lot with robotics. They also part-funded a trip to China as part of the Netwon project. They have also loaned me a robotic platform to test algorithms

In addition, they wrote a letter of support for a RAEng/Leverhulme fellowship (unsuccessful)</gtr:partnerContribution><gtr:piContribution>We are partners on a Newton Agritech Fund project and have collaborated on other unsuccessful funding bids (an ECHORD++ NSTP, Newton). IN all the bids, our part is insect-inspired visual navigation algorithms 

We are partners on a Newton Agritech Fund project and have collaborated on other funding bids, for which they are dealing with GPS-based navigation and Agri-tech applications for our algorithms. They also wrote a letter of support for a RAEng/Leverhulme fellowship (unsuccessful)</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Schools Outreach and science festivals</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>A485BA02-6C62-48AF-B8F3-517379CB083F</gtr:id><gtr:impact>I have undertaken a variety of schools outreach events based on the work including some talks, but mainly hands-on demonstrations: Can you navigate like an ant?; and robotic demonstrations, especially showcasing interdisciplinary research to school children and focussing on Widening Participation in particular. I have also participated in STEM events such as Big Bang as well as 3 exhibits in the Brighton Science Festival

People seem to be very interested in the cross-disciplinary nature, we have been asked back to many of the Schools (eg Dorothy Stringer, Brighton, Portslade Academy) and, as Admissions Tutor, I have seen applications from these schools, though a causal link is difficult to esetablish</gtr:impact><gtr:outcomeId>545cc9d15104c0.95805446</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:url>http://www.sussex.ac.uk/lifesci/insectnavigation/</gtr:url><gtr:year>2011,2012,2013,2014,2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1535884</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Programme grant</gtr:description><gtr:end>2021-12-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/P006094/1</gtr:fundingRef><gtr:id>AECFA014-3B73-4601-ADE7-A6601ECE68C7</gtr:id><gtr:outcomeId>58aef81baf8a88.55672085</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>70000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Newton Agritech fund</gtr:description><gtr:end>2016-03-02</gtr:end><gtr:fundingOrg>Science and Technologies Facilities Council (STFC)</gtr:fundingOrg><gtr:id>E5A31DD1-4F11-49F4-802F-AF78E7CC3FFD</gtr:id><gtr:outcomeId>563a0a5c7ca9d7.44278017</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>2500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Seedcorn Research Funding, University of Sussex</gtr:description><gtr:end>2012-05-02</gtr:end><gtr:fundingOrg>University of Sussex</gtr:fundingOrg><gtr:id>3471F375-E752-4370-80A9-9B40041FB01A</gtr:id><gtr:outcomeId>5ee388e05ee388f4</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The grant has had significant academic input with multiple articles, citations and over 10 (and counting)presentations at international conferences.

In 2014 I was contacted by researchers in RAL Space and Harper Adams University interested in using the algorithms developed in these grants in Space Exploration and Agricultural robots respectively. This has resulted in a joint project funded by the Newton Agritech fund.

Beneficiaries: Biologists and biomimetic engineers

Contribution Method: Through publications and presentations

We have given outreach sessions for schools based on this research and aimed at Widening Participation in particular. We have done ~15 sessions in total reaching 300 students.

Beneficiaries: schools children

Contribution Method: The research was the basis for the demo sessions we have done</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>05367E51-B984-4BDA-A7EC-FF97EEFD01EC</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>m-7667376052.278444de25ed62</gtr:outcomeId><gtr:sector>Agriculture, Food and Drink,Digital/Communication/Information Technologies (including Software),Other</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have developed a set of methods for reconstructing the visual input experienced by insects as they forage for food. These methods have been used to gather data sets from the natural habitats of ants during behavioural experiments so we can interpret insect behaviour. In collaboration with Barbara Webb and Michael Mangan (Edinburgh University) and Wolfgang Sturzl (DLR, Munich) we have used these methods to reconstruct the entire visual history of several ants of the whole course of their life outside the nest.



We have developed a novel insect-inspired algorithm for autonomous route navigation. This is the first complete model of visual route navigation in ants and is a new approach to modelling visual homing and route navigation. Our algorithm robustly navigates routes through complex environments and shows many characteristics of the behaviour of navigating ants. In particular it can also perform place search with the same mechanism and can explain paradigmatic experiments previously used as evidence for 'snapshot' based models. It has been very well-received by the insect-inspired navigation community. 



We have analysed a novel innate behaviour: visual scanning in ants. This behaviour prompted our novel navigation model but in turn we have been able to use the model and visual input reconstruction methods to analyse the behavioural consequences of this behaviour in the field.



We have undertaken a detailed analysis of the learning flights of bumblebees. We found that the learning flights are composed of nest-centric loops, while return flights are zigzags. WE showed that these different manoeuvres are variants of one another and that the key point of similarity between the two is when the bee both faces and flies towards the nest. This ties in with our familiarity based model of place homing. We also showed how these nest-centric manoeuvres are tied to geocentric information. 



We have started to analyse learning walks in ants using both the familiarity model and visual input reconstruction methods, through a part-EPSRC funded doctoral student. The main finding is that learning walks should be partly tailored to certain distant features of the world, but that they should also have some general features. The next stage of this work is to tie our simulation results to learning walks in different species of ants in different visual environments that we have recorded.</gtr:description><gtr:exploitationPathways>The navigation algorithms are of potential use in autonomous robotic engineering especially in Agriculture and in particular in the navigation of UAVs in GPS-denied environments. We are currently exploiting these avenues through collaborations with RAL Space and Harper Adams University, through a Newton agritech funded joint project.

There is also potential for the algorithms to be used as navigational aids for visually impaired people. 


Our algorithms will be used for schools outreach and widening participation in particular, as well as public engagement The main outcome of our work is to add to the body of knowledge on visual learning and memory. 


1) It is adding to our knowledge of what is encoded in a visual memory. Our group has a PhD student who will now try and tie this in to recordings from the insect eye to see what visual features are actually encoded


2) The behaviour is an example of active learning and in particular adds to our knowledge of how innate behaviours can scaffold learning


3) The scanning behaviour will be used to give us a measure of the ant's uncertainty potentially allowing us to investigate Bayesian combining of navigation cues.


4) Our image reconstruction methods and navigation algorithms have also been used both by ourselves and other groups to interpret the behaviour of foraging ants thus giving us a window into the algorithms they might use to navigate.

We have published multiple papers on this work and are planning to use the methods developed as the basis of grant applications (ERC and HFSP applications were submitted but these schemes are extremely competitive and we were unsuccessful). A programme grant to the EPSRC has made it through the first round and is to be submitted March 18, 2016. The work has also prompted new collaborations with Oklahoma University, RAL Space and Harper Adams University as well as strengthening ties with Edinburgh University.

The other major exploitation route is in engineering and robotics. Our navigation algorithms can be used for autonomous robotic navigation and UAVs in particular. We are testing our algorithm in a variety of natural habitats and contexts and using autonomous robots. For instance:



(1) we are developing methods by which a flying agent can navigate based on the view of the ground; We are planning grant applications to follow the work up (EPSRC) in the next 12 months.

(2) we have a prototype of the algorithm that can be run as a mobile phone app which will allow people to recapitulate a path taken by others. 

(3) Our familiarity based navigation algorithm could also be applied in a more abstract informational context. 


Our algorithms are very simple and attractive to the public and we are using them as the basis of demos for outreach</gtr:exploitationPathways><gtr:id>2D4BAB36-C350-4638-BF40-A6D9317DC3C1</gtr:id><gtr:outcomeId>r-6336999839.485623777eedd6</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Agriculture, Food and Drink,Digital/Communication/Information Technologies (including Software),Transport,Other</gtr:sector></gtr:sectors><gtr:url>http://www.sussex.ac.uk/lifesci/insectnavigation</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C794EB3C-0486-4A58-8E81-AA5D8B9A8845</gtr:id><gtr:title>SmartData</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/603bd27d26c352755090264333872bbe"><gtr:id>603bd27d26c352755090264333872bbe</gtr:id><gtr:otherNames>Tomko N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4614-6408-2</gtr:isbn><gtr:outcomeId>doi_53cfcbfcbc621a36</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3BA8E4F4-3922-471F-9E8F-9CCF097677D5</gtr:id><gtr:title>A neural network based holistic model of ant route navigation</gtr:title><gtr:parentPublicationTitle>BMC Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e115e0e9ff6306ba8105d235ff782fe2"><gtr:id>e115e0e9ff6306ba8105d235ff782fe2</gtr:id><gtr:otherNames>Baddeley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d07b07b82aba14</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA1B8ABD-E30F-42EA-80C4-E270D5601523</gtr:id><gtr:title>Still no convincing evidence for cognitive map use by honeybees.</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Academy of Sciences of the United States of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7e79eb694a13631a2abaed50a822cc21"><gtr:id>7e79eb694a13631a2abaed50a822cc21</gtr:id><gtr:otherNames>Cheung A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0027-8424</gtr:issn><gtr:outcomeId>545cbf4559b864.27290709</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>35CA8CA8-9D2F-48F9-8057-1CF3B0AFD3D5</gtr:id><gtr:title>Tool sequence optimization using synchronous and asynchronous parallel multi-objective evolutionary algorithms with heterogeneous evaluations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f56be03654e7137d0272eab9f4f63850"><gtr:id>f56be03654e7137d0272eab9f4f63850</gtr:id><gtr:otherNames>Churchill A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4799-0453-2</gtr:isbn><gtr:outcomeId>doi_53d056056ecc9094</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1A76F161-C76A-4A77-98F5-6CC4F7F460E5</gtr:id><gtr:title>Insect-inspired navigation algorithm for an aerial agent using satellite imagery.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/516bf1489f4d2184eeeedfd3ac8c2b54"><gtr:id>516bf1489f4d2184eeeedfd3ac8c2b54</gtr:id><gtr:otherNames>Gaffin DD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>56d5df5fb57c52.58782595</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7553342D-8CD2-4B54-9424-EAE4BB134B18</gtr:id><gtr:title>Bumblebee calligraphy: the design and control of flight motifs in the learning and return flights of Bombus terrestris.</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4cb15900d6debffb42c0e188dd3d22fe"><gtr:id>4cb15900d6debffb42c0e188dd3d22fe</gtr:id><gtr:otherNames>Philippides A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>doi_53d07d07d2d59c38</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F36F917-B946-4032-B74C-6BFBD02A1F2D</gtr:id><gtr:title>Do endothelial cells dream of eclectic shape?</gtr:title><gtr:parentPublicationTitle>Developmental cell</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b8b211becaf71125a1293ecc48aa145"><gtr:id>8b8b211becaf71125a1293ecc48aa145</gtr:id><gtr:otherNames>Bentley K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1534-5807</gtr:issn><gtr:outcomeId>doi_55f9749740669318</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B93F0AB1-DC96-46B4-B98E-15FCE93BAA3A</gtr:id><gtr:title>Dual coding with STDP in a spiking recurrent neural network model of the hippocampus.</gtr:title><gtr:parentPublicationTitle>PLoS computational biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/767588090a2d653c4e09007891dd6560"><gtr:id>767588090a2d653c4e09007891dd6560</gtr:id><gtr:otherNames>Bush D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1553-734X</gtr:issn><gtr:outcomeId>doi_53d0800808e2248a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>512F8E19-CF6F-4F49-A871-00655985993B</gtr:id><gtr:title>Navigation-specific neural coding in the visual system of Drosophila.</gtr:title><gtr:parentPublicationTitle>Bio Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/56bb069bfe13e92cc7245f8ece3e70e2"><gtr:id>56bb069bfe13e92cc7245f8ece3e70e2</gtr:id><gtr:otherNames>Dewar AD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0303-2647</gtr:issn><gtr:outcomeId>56d5df5f843b94.45201426</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3586C33C-FE8C-47F1-A7DF-A6F8379C3617</gtr:id><gtr:title>How do field of view and resolution affect the information content of panoramic scenes for visual navigation? A computational investigation.</gtr:title><gtr:parentPublicationTitle>Journal of comparative physiology. A, Neuroethology, sensory, neural, and behavioral physiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/067517a8b37e2c7e64572cc78363849b"><gtr:id>067517a8b37e2c7e64572cc78363849b</gtr:id><gtr:otherNames>Wystrach A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0340-7594</gtr:issn><gtr:outcomeId>56d5df5f579207.90853002</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B1FC7F07-3761-4E22-A566-93E3BF852FC6</gtr:id><gtr:title>Coordinating compass-based and nest-based flight directions during bumblebee learning and return flights.</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26ea0825b0a7c4a03fa9af4bececcfe5"><gtr:id>26ea0825b0a7c4a03fa9af4bececcfe5</gtr:id><gtr:otherNames>Collett TS</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>doi_53d07d07d2dda26e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26A26736-BB1A-4CEA-B68E-5A2CA9AEBFD8</gtr:id><gtr:title>Many hands make light work: further studies in group evolution.</gtr:title><gtr:parentPublicationTitle>Artificial life</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/603bd27d26c352755090264333872bbe"><gtr:id>603bd27d26c352755090264333872bbe</gtr:id><gtr:otherNames>Tomko N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1064-5462</gtr:issn><gtr:outcomeId>doi_53d076076c60896a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5A9FDEA5-35EB-4C37-8340-C05DDC076BB2</gtr:id><gtr:title>Using neural networks to understand the information that guides behavior: a case study in visual navigation.</gtr:title><gtr:parentPublicationTitle>Methods in molecular biology (Clifton, N.J.)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4cb15900d6debffb42c0e188dd3d22fe"><gtr:id>4cb15900d6debffb42c0e188dd3d22fe</gtr:id><gtr:otherNames>Philippides A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>978-1-4939-2238-3</gtr:isbn><gtr:issn>1064-3745</gtr:issn><gtr:outcomeId>56d5df5fdb3008.85674565</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53F7BABB-F46E-4E75-ACAB-8F59FA2205F6</gtr:id><gtr:title>The role of differential VE-cadherin dynamics in cell rearrangement during angiogenesis.</gtr:title><gtr:parentPublicationTitle>Nature cell biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b8b211becaf71125a1293ecc48aa145"><gtr:id>8b8b211becaf71125a1293ecc48aa145</gtr:id><gtr:otherNames>Bentley K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1465-7392</gtr:issn><gtr:outcomeId>doi_55f974974070ca0b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D7EA6232-064D-4E79-A72F-6292B0623272</gtr:id><gtr:title>Holistic visual encoding of ant-like routes: Navigation without waypoints</gtr:title><gtr:parentPublicationTitle>Adaptive Behavior</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e115e0e9ff6306ba8105d235ff782fe2"><gtr:id>e115e0e9ff6306ba8105d235ff782fe2</gtr:id><gtr:otherNames>Baddeley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d07a07a2fe3c8d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>227FCEA3-9E15-4529-BE29-07B93D2700AD</gtr:id><gtr:title>Snapshots in ants? New interpretations of paradigmatic experiments.</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/067517a8b37e2c7e64572cc78363849b"><gtr:id>067517a8b37e2c7e64572cc78363849b</gtr:id><gtr:otherNames>Wystrach A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>545cbf92191613.96055957</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6ABB6333-9902-4579-8C86-2F871A97EAC1</gtr:id><gtr:title>A model of ant route navigation driven by scene familiarity.</gtr:title><gtr:parentPublicationTitle>PLoS computational biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e115e0e9ff6306ba8105d235ff782fe2"><gtr:id>e115e0e9ff6306ba8105d235ff782fe2</gtr:id><gtr:otherNames>Baddeley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1553-734X</gtr:issn><gtr:outcomeId>545cbfd7aa9b17.88286396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>29C76CD6-8749-43D9-98F5-D8C77C3B7EB0</gtr:id><gtr:title>Visual scanning behaviours and their role in the navigation of the Australian desert ant Melophorus bagoti.</gtr:title><gtr:parentPublicationTitle>Journal of comparative physiology. A, Neuroethology, sensory, neural, and behavioral physiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/067517a8b37e2c7e64572cc78363849b"><gtr:id>067517a8b37e2c7e64572cc78363849b</gtr:id><gtr:otherNames>Wystrach A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0340-7594</gtr:issn><gtr:outcomeId>545cbf45344e29.36959860</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D8B9A32-00FA-45D9-B1BA-83B7E5E1B9AB</gtr:id><gtr:title>Snapshots in ants? New interpretations of paradigmatic experiments</gtr:title><gtr:parentPublicationTitle>Journal of Experimental Biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a5e1fa4a6868b36d5510940bfc4a6aec"><gtr:id>a5e1fa4a6868b36d5510940bfc4a6aec</gtr:id><gtr:otherNames>Antoine Wystrach (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>r_2960335746035e0f42</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>907B4CC1-DCA9-406B-ACBB-F87D6633DF95</gtr:id><gtr:title>How might ants use panoramic views for route navigation?</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4cb15900d6debffb42c0e188dd3d22fe"><gtr:id>4cb15900d6debffb42c0e188dd3d22fe</gtr:id><gtr:otherNames>Philippides A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>doi_53d07d07d1fe57ab</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E832584-FEAB-4800-883A-D35F8B900C4D</gtr:id><gtr:title>What is the relationship between visual environment and the form of ant learning-walks? An in silico investigation of insect navigation</gtr:title><gtr:parentPublicationTitle>Adaptive Behavior</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/428304844ba6aee993afadb8712f2df1"><gtr:id>428304844ba6aee993afadb8712f2df1</gtr:id><gtr:otherNames>Dewar A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545cbf4508a0b2.88381256</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I031758/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>53F27348-198B-4AEF-A34B-8307067F507C</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Systems engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>21CE2EA6-E7A2-4406-A045-0DA7CA19B695</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Control Engineering</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>