<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:department>Engineering</gtr:department><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C7FD454E-D359-48AF-95FD-3E1375802686"><gtr:id>C7FD454E-D359-48AF-95FD-3E1375802686</gtr:id><gtr:name>South Yorkshire Fire and Rescue</gtr:name><gtr:address><gtr:line1>Edlington Fire Station</gtr:line1><gtr:line2>Edlington Lane</gtr:line2><gtr:line3>Warmsworth</gtr:line3><gtr:postCode>DN12 1DA</gtr:postCode><gtr:region>Yorkshire and the Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4D6A8C0E-1BC0-4091-80A6-1A5E0463DCB4"><gtr:id>4D6A8C0E-1BC0-4091-80A6-1A5E0463DCB4</gtr:id><gtr:name>Thales Netherland</gtr:name><gtr:address><gtr:line1>Haakbergerstraat 49</gtr:line1><gtr:postCode>7554 PA</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Netherlands</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/25C0D19F-053F-4DDA-B059-B1B1E11C0696"><gtr:id>25C0D19F-053F-4DDA-B059-B1B1E11C0696</gtr:id><gtr:name>Guide Dogs</gtr:name><gtr:address><gtr:line1>Hillfields</gtr:line1><gtr:line2>Burghfield Common</gtr:line2><gtr:postCode>RG7 3YG</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E040FEDB-ACCB-4B09-AF0F-532F0E1A4550"><gtr:id>E040FEDB-ACCB-4B09-AF0F-532F0E1A4550</gtr:id><gtr:firstName>Thrishantha</gtr:firstName><gtr:surname>Nanayakkara</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/86AF6894-0368-482C-A4B5-76B06083C300"><gtr:id>86AF6894-0368-482C-A4B5-76B06083C300</gtr:id><gtr:firstName>Kaspar</gtr:firstName><gtr:surname>Althoefer</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/82DB6ACB-6A28-4751-8955-6B6C1EDD39B4"><gtr:id>82DB6ACB-6A28-4751-8955-6B6C1EDD39B4</gtr:id><gtr:firstName>Lakmal</gtr:firstName><gtr:otherNames>Dasarath</gtr:otherNames><gtr:surname>Seneviratne</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI028765%2F1"><gtr:id>F6739077-8FDF-44AD-A1EE-550B53757032</gtr:id><gtr:title>REINS</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I028765/1</gtr:grantReference><gtr:abstractText>The REINS project is to design and investigate haptic communicational interfaces (reins) between a human agent and a mobile robot guide. The reins will facilitate joint navigation and inspection of a space under conditions of low visibility (occurring frequently in fire fighting). The focus is on haptic and tactile human robot cooperation as it has been found that a limited visual field and obscured cameras adds to the distress of humans working under pressure. Humans naturally interact with animals using tactile feedback in scenarios such as working with guide dogs and horse riding; the REINS project aims to extend this practice to human robot interaction. Expertise from a number of different disciplines - design, engineering, robotics, and communication - will be brought to bear on the problem of designing a communicational interface which will be both sufficiently robust for the relevant physical environment and sufficiently flexible to allow for the on-the-spot exercise of human judgement and creativity.Inspired by the use of a harness for a guide dog and also the rein to ride or drive a horse, the REINS project will investigate and experiment with haptic interfaces for human-robot cooperation. The low/no visibility constraint ensures the focus is on the tactile and haptic aspects only. Currently, robots do not sufficiently enhance human confidence. In human-robot cooperation, the human (by nature) will try to 'read' the situation, and anticipate the movements of the robot companion. The robot is provided with an impedance filter and the rein enables the human to feel the robot's movements and behaviour. Experiences with remotely controlling a robot which is not directly visible show that 'operators spent significantly more time gathering information about the state of the robot and the state of the environment than they did navigating the robot'.The REINS project aims to map the communicational landscape in which humans (fire fighters, but also the visually impaired) might be working with robots, with the emphasis on tactile and haptic interaction. We adapt a semi-autonomous mobile robot for navigation in front of a human. The robot provides rich sensory data and is enabled to try the mechanical impedances of the objects it encounters. We also design and implement a soft rein (rope), a wireless rein and a stiff rein (inspired by the lead for guide dog) enabling the human to use the robot to actively probe objects. The project thus creates the means to explore the haptic Human-Robot Interaction landscape. We will work from an integrationist perspective in which the communicator is not a mere user of pre-existing signs but a sign-maker; the signs emerge in the ongoing coordination and integration of activities adapted to the particular circumstances. We review the communicational landscape occurring within a team of (human) fire fighters and in addition review literature on working guide dogs and horse riding. A research question is whether the information should be explicitly encoded as messages or can remain implicit.In the initial phase of the project the robot is adapted and the first prototypes of the reins are implemented; the emphasis in this phase is on providing rich data to the human. The second phase is dedicated to surveying the communicational landscape. The human-robot team will navigate a known environment with low visibility where unknown obstacles may occur. At least two different types of reins are applied: one requires that messages are explicitly coded, while the other propagates the information implicitly. Based on experiences in the first trials the reins might be adapted to improve their usability. Professional fire fighters will be the first group of subjects to try the reins, later on also volunteers experienced with guide dogs may join the experimentations.</gtr:abstractText><gtr:fund><gtr:end>2015-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>205057</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Trespass public event</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>229B8EA6-8B16-443C-8A59-CB72A19ADE5B</gtr:id><gtr:impact>Sponsored by the King's Cultural Institute, I worked with a choreographer to understand the parallels between programming the human body and programming robots to explore new meaning and opportunities in both fields. A public exposition was held on 31st March 2013, titled &amp;quot;translocations&amp;quot; (http://www.shobanajeyasingh.co.uk/works/translocations/ ), where my views were subjected to public discussion. This led to a wider collaboration including Bartlett School Architecture at UCL, and the above cultural partners to showcase human-robot dancing in a public exhibition titled &amp;quot;Trespass&amp;quot; (https://vimeo.com/133691929). I participated in a panel moderated by New Scientist journalist Simon Ings.</gtr:impact><gtr:outcomeId>56cb2e7f7fca19.05597294</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://vimeo.com/133691929</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Cultural impact:
Contributed to collaboration with King's Cultural Institute to develop a human-robot dancing systems and event.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>FC7D875D-1E5C-4FB9-A231-01B383393FF7</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54631b4f206236.40005196</gtr:outcomeId><gtr:sector>Creative Economy,Education</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>1. Human-human demonstrations of guiding showed that the guider uses on average, a 3rd order predictive policy to take current guiding actions based on predicted errors of the follower.

2. The follower uses on average, a 2nd order reactive policy to make current movements based on past commands given by the guider.

3. When the follower's movements in response to tug forces generated by the guider is modeled using a virtual damped inertial model (virtual model because, followers movements largely comes from voluntary forces than due to tug forces of the guider), the variability of the damping coefficient reliably reflects the variability of the follower's trust in the guider.

4. When control policy identified from human demonstration experiments was implemented on a planar 1-DoF robotic arm to perturb the blindfolded subjects' most dominant arm in leftward/rightward directions, it was found that naive subjects elicit a 2nd order reactive behaviour similar to human demonstration experiments. However, trained subjects developed 2nd order predictive following behaviour.

5. Naive and trained subjects' arm muscle activation is significantly different in leftward/rightward arm perturbation.

6. When humans are trained with primitive vibroactuator array patterns given in a sleeve worn in the arm, they can accurately recognize linear combinations of those patterns.


The low/no visibility constraint ensures the focus is on the tactile and haptic aspects only. The research aims to map the communicational landscape in which humans (fire fighters, but also the visually impaired) might be working with robots, with the emphasis on tactile and haptic interaction.



In the initial phase, we conducted human-human interaction experiments to find the salient features where one person with limited auditory and visual perception of the environment ( human), is guided by an agent with full perceptual capabilities (assume as robot/machine), via a hard rein along a given path. 



Key findings:



1. We defined the state as the relative orientation difference between the human guider (a guider) and the human follower (a follower), the angle of the rein relative to the guider as an action. Numerical novel auto regressive model of linear discrete control policy of the guider and the follower was proposed. The results show the guider learns 3rd order predictive control policy to guide the human follower, while the follower learns 2nd order reactive model to follow the guider.



2. While learning the control policy, we tried to understand gradual change in muscle activation of the guider across the trials to see optimality of total effort to generate actions in the actuation space by the guider. The results show that the total muscle activation increases to a maximum around the mid of the trials (20 trials, 10 pairs) and then decreases in last 10 trials. This suggests that effort optimization is a non-monotonic process. During the first 10 trials, participants may have given priority to order selection than optimization in the actuation space.



3. To study human (the follower) confidence towards the guiding agent, a confidence model is proposed (to study the virtual damping coefficient and the virtual mass with pushing/pulling force). Experiments were conducted on three different paths (straight / 60 degree turn / 90 degree turn). Our experimental results of human participants show that the variability of the virtual damping coefficients correlates more with the complexity of the path. Furthermore, When the follower drops confidence in 60 degree turn and 90 degree turn, the guider has to exert a higher tug force to take following agent into desired trajectory that leads to higher average values for the virtual mass and virtual damping coefficient.</gtr:description><gtr:exploitationPathways>At present we are planning to carry out an impact acceleration project in collaboration with the Fire Service College, Gloucestershire. This involves testing an outdoor version of the robot in a simulated fire-fighting scenario.</gtr:exploitationPathways><gtr:id>605C9D1D-1119-4B77-BB46-395732AB296C</gtr:id><gtr:outcomeId>r-8092862878.08126177752936</gtr:outcomeId><gtr:sectors><gtr:sector>Communities and Social Services/Policy,Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:sectors><gtr:url>http://thrish.org/research-team/anuradha-ranasinghe</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>1A70EC64-16D1-4BF5-9A48-0CD49CECE229</gtr:id><gtr:title>Can a Soft Robotic Probe Use Stiffness Control Like a Human Finger to Improve Efficacy of Haptic Perception?</gtr:title><gtr:parentPublicationTitle>IEEE transactions on haptics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2254e897f5fda55b4eb2cc6d9790c177"><gtr:id>2254e897f5fda55b4eb2cc6d9790c177</gtr:id><gtr:otherNames>Sornkarn N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1939-1412</gtr:issn><gtr:outcomeId>585d41d9d47162.50717246</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>10F3D767-2E49-4A1B-A233-1A8C1F107F5F</gtr:id><gtr:title>A two party haptic guidance controller via a hard rein</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7371b334f477ce33a42d02d26e98c820"><gtr:id>7371b334f477ce33a42d02d26e98c820</gtr:id><gtr:otherNames>Ranasinghe A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d3676c13600.21613156</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52DC5866-375D-48A3-8BA7-CCBC13805362</gtr:id><gtr:title>Identification of Haptic Based Guiding Using Hard Reins.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7371b334f477ce33a42d02d26e98c820"><gtr:id>7371b334f477ce33a42d02d26e98c820</gtr:id><gtr:otherNames>Ranasinghe A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5675e93234ba2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>74D7F34E-F8F2-41DA-9CCC-73EB9D1D11B3</gtr:id><gtr:title>A Geographic Primitive-Based Bayesian Framework to Predict Cyclone-Induced Flooding*</gtr:title><gtr:parentPublicationTitle>Journal of Hydrometeorology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a77683d2ac1266a6aaae46537bf4a26"><gtr:id>2a77683d2ac1266a6aaae46537bf4a26</gtr:id><gtr:otherNames>Wijesundera I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54676013660fa4.08203412</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A01BAB29-D212-4BC6-98EE-ACF196889E3D</gtr:id><gtr:title>A novel approach to determine the inverse kinematics of a human upper limb model with 9 degrees of freedom</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/39bf9d89f1753a2fa33a8782ef78bb17"><gtr:id>39bf9d89f1753a2fa33a8782ef78bb17</gtr:id><gtr:otherNames>Masinghe W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1664-4</gtr:isbn><gtr:outcomeId>5467606adff250.26695175</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CD72253C-D64A-4A82-A95E-1092C390922D</gtr:id><gtr:title>Stable Grip Control on Soft Objects With Time-Varying Stiffness</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Robotics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3a8cad1fc20bef44e419267908003639"><gtr:id>3a8cad1fc20bef44e419267908003639</gtr:id><gtr:otherNames>Nanayakkara T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4254d5ded7.63577414</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>33D4E5F3-6FEC-4494-8B81-9DA900A3B169</gtr:id><gtr:title>Morphological Computation of Haptic Perception of a Controllable Stiffness Probe.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2254e897f5fda55b4eb2cc6d9790c177"><gtr:id>2254e897f5fda55b4eb2cc6d9790c177</gtr:id><gtr:otherNames>Sornkarn N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a2fe84d9b9092.08016920</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3EAE2B68-626E-455A-96B0-D0991E4A2EAE</gtr:id><gtr:title>An Optimal State Dependent Haptic Guidance Controller via a Hard Rein</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7371b334f477ce33a42d02d26e98c820"><gtr:id>7371b334f477ce33a42d02d26e98c820</gtr:id><gtr:otherNames>Ranasinghe A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d36e2280465.69709247</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>319604C2-F371-43D8-9E7E-939519C6B2D5</gtr:id><gtr:title>Salient Feature of Haptic-Based Guidance of People in Low Visibility Environments Using Hard Reins.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7371b334f477ce33a42d02d26e98c820"><gtr:id>7371b334f477ce33a42d02d26e98c820</gtr:id><gtr:otherNames>Ranasinghe A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>5675e6ade96b4</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I028765/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>35</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>