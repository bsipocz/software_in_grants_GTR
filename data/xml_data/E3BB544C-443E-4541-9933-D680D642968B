<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6DB33BF0-3EA1-420D-9875-85B72E852F6A"><gtr:id>6DB33BF0-3EA1-420D-9875-85B72E852F6A</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Zisserman</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI012001%2F1"><gtr:id>E3BB544C-443E-4541-9933-D680D642968B</gtr:id><gtr:title>Learning to Recognise Dynamic Visual Content from Broadcast Footage</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I012001/1</gtr:grantReference><gtr:abstractText>This research is in the area of computer vision - making computers which can understand what is happening in photographs and video. As humans we are fascinated by other humans, and capture endless images of their activities, for example home movies of our family on holiday, video of sports events or CCTV footage of people in a town center. A computer capable of understanding what people are doing in such images would be able to do many jobs for us, for example finding clips of our children waving, fast forwarding to a goal in a football game, or spotting when someone starts a fight in the street. For Deaf people, who use a language combining hand gestures with facial expression and body language, a computer which could visually understand their actions would allow them to communicate in their native language. While humans are very good at understanding what people are doing (and can learn to understand special actions such as sign language), this has proved extremely challenging for computers.Much work has tried to solve this problem, and works well in particular settings for example the computer can tell if a person is walking so long as they do it clearly and face to the side, or can understand a few sign language gestures as long as the signer cooperates and signs slowly. We will investigate better models for recognising activities by teaching the computer by showing it many example videos. To make sure our method works well for all kinds of setting we will use real world video from movies and TV. For each video we have to tell the computer what it represents, for example throwing a ball or a man hugging a woman . It would be expensive to collect and label lots of videos in this way, so instead we will extract approximate labels automatically from subtitle text and scripts which are available for TV. Our new methods will combine learning from lots of approximately labelled video (cheap because we get the labels automatically), use of contextual information such as which actions people do at the same time, or how one action leads to another ( he hits the man, who falls to the floor ), and computer vision methods for understanding the pose of a person (how they are standing), how they are moving, and the objects which they are using.By having lots of video to learn from, and methods for making use of approximate labels, we will be able to make stronger and more flexible models of human activities. This will lead to recognition methods which work better in the real world and contribute to applications such as interpreting sign language and automatically tagging video with its content.</gtr:abstractText><gtr:fund><gtr:end>2016-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>500842</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>There two key developments: 

1. A method for predicting point sets in images. For example, to predict a person's 2D pose by localizing the points of their hands, elbows and shoulders; or to track eyes, nose and mouth on a video of a moving face. The method is based on deep learning of a convolutional neural network model.
Software has been made publically available

2. A method for learning to recognize human gestures, such as sign language, in videos. The method only requires a single example of the gesture to learn from, and then improves its recognition performance by finding other examples in video. The approach involves tracking human hands in video, and then detecting the gesture using machine learning techniques.</gtr:description><gtr:exploitationPathways>Can be used in any application that requires tracking human pose.</gtr:exploitationPathways><gtr:id>413F67F4-E8BD-4C96-A691-D983CB946CF7</gtr:id><gtr:outcomeId>56d76d2bb21c01.64127964</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.robots.ox.ac.uk/~vgg/research/sign_language_new/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A set of large video datasets annotated with human upper-body pose</gtr:description><gtr:id>2EFA1F18-F68C-47EE-97DC-C192E12A9396</gtr:id><gtr:impact>Several papers have used this as a benchmark.</gtr:impact><gtr:outcomeId>56d61685abb686.78180874</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Human Pose Estimation datasets</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/data/pose/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Convolutional networks (ConvNets) currently produce the state-of-the-art results for the task of human pose estimation. However, even ConvNets can still produce absurdly erroneous pose predictions in videos - particularly for unusual poses, challenging illumination or viewing conditions, self-occlusions or unusual shapes (e.g. when wearing baggy clothing, or unusual body proportions).

We address these issues with a method for automatically learning reliable, occlusion-aware, person specific pose estimators in videos. Using the fact that people tend not to change appearance over the course of a long video (same clothes, same body shape), we show that the large quantity of data in the video can be exploited to 'personalize' a ConvNet pose estimator, thereby improving performance for unusual poses.</gtr:description><gtr:id>93E942E1-678B-4A92-9F98-87BDE8795C51</gtr:id><gtr:impact>Too early</gtr:impact><gtr:outcomeId>58bdeb4e38c657.68563231</gtr:outcomeId><gtr:title>Software for Personalizing Human Video Pose Estimation</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/personalized_pose/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This code enables training of heatmap regressor ConvNets for the general problem of regressing (x,y) positions in images.</gtr:description><gtr:id>7B6438BD-85E3-417A-B713-5735E0F0B4C9</gtr:id><gtr:impact>Has already been used in several publications.</gtr:impact><gtr:outcomeId>56d61c261a1ec0.36519035</gtr:outcomeId><gtr:title>VGG CNN Heatmap Regressor</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/cnn_heatmap/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Software to accurately and efficiently detect configurations of one or more people in edited TV material. Such configurations often appear in standard arrangements due to cinematic style, and we take advantage of this to provide scene context.</gtr:description><gtr:id>AF9711E5-8C89-4BF3-BA8E-25F070EB0D09</gtr:id><gtr:impact>Available to be used.</gtr:impact><gtr:outcomeId>56d61b66bbe132.96140164</gtr:outcomeId><gtr:title>Software for Detecting Upper Body Configurations</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/ubc/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>70D7AB37-CF99-4F2E-BA5E-EA7860B5262F</gtr:id><gtr:title>Automatic and Efficient Human Pose Estimation for Sign Language Videos</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/215a3501ac12c0957a121af339f6bea5"><gtr:id>215a3501ac12c0957a121af339f6bea5</gtr:id><gtr:otherNames>Charles J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545bb11f2570a8.24018426</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F1D4E089-B84C-4C2C-851C-E200C7824288</gtr:id><gtr:title>Regularized Max Pooling for Image Categorization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4dc6801d5ad5170e73bea33d81ec70b6"><gtr:id>4dc6801d5ad5170e73bea33d81ec70b6</gtr:id><gtr:otherNames>Minh Hoai</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56b1ebc9d2df70.49001776</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1AF12872-F9DB-493E-A191-20045E8508B6</gtr:id><gtr:title>Thread-Safe: Towards Recognizing Human Actions Across Shot Boundaries</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4dc6801d5ad5170e73bea33d81ec70b6"><gtr:id>4dc6801d5ad5170e73bea33d81ec70b6</gtr:id><gtr:otherNames>Minh Hoai</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56b1ee00c4eff2.95561290</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8CB58A29-CEA9-44C8-B987-4393273C9D83</gtr:id><gtr:title>Upper Body Pose Estimation with Temporal Sequential Forests</gtr:title><gtr:parentPublicationTitle>N/A</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57e70663bc644ed52b371f14840efd8a"><gtr:id>57e70663bc644ed52b371f14840efd8a</gtr:id><gtr:otherNames>Charles, J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545bb925bf9129.28552687</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E4C7DCE-AA8F-4513-97C0-BCD713A2110E</gtr:id><gtr:title>Large-scale Learning of Sign Language by Watching TV (Using Co-occurrences)</gtr:title><gtr:parentPublicationTitle>N/A</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e9b5f784371a4ff1dd31fffd0fbd770"><gtr:id>6e9b5f784371a4ff1dd31fffd0fbd770</gtr:id><gtr:otherNames> Pfister T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545bafa3362c09.32220034</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>883C0239-B39D-4969-A530-5FEE7CE9DC2E</gtr:id><gtr:title>Domain Adaptation for Upper Body Pose Tracking in Signed TV Broadcasts</gtr:title><gtr:parentPublicationTitle>N/A</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/250b919baa4364e0192abc775244ce94"><gtr:id>250b919baa4364e0192abc775244ce94</gtr:id><gtr:otherNames>Charles J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545bb073786e22.62015862</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5AFC9B23-7FFE-4955-9446-4BC12DF72231</gtr:id><gtr:title>Flowing ConvNets for Human Pose Estimation in Videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ee713c0c273acab111d37c46bb13b77a"><gtr:id>ee713c0c273acab111d37c46bb13b77a</gtr:id><gtr:otherNames>Tomas Pfister</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b1eecd610474.94882961</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>345E1D5A-70C2-4966-813E-00404FA2806D</gtr:id><gtr:title>Personalizing Human Video Pose Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e7997bc42c37551711c39054811fd039"><gtr:id>e7997bc42c37551711c39054811fd039</gtr:id><gtr:otherNames>J. Charles</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>573c316c695114.33531300</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA99E083-17B4-430A-B741-7AAB55F52A08</gtr:id><gtr:title>Discriminative Sub-categorization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4dbd7e508ce919ea8f77d3b3276daa15"><gtr:id>4dbd7e508ce919ea8f77d3b3276daa15</gtr:id><gtr:otherNames>Hoai M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545bb1c400dbd1.57873377</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CE8B62D0-AAAC-421E-BC51-A8A1247600F6</gtr:id><gtr:title>Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts</gtr:title><gtr:parentPublicationTitle>N/A</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/794946e7dbca6021acaeea472626370a"><gtr:id>794946e7dbca6021acaeea472626370a</gtr:id><gtr:otherNames>Pfister, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_936588522813d80e4c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C52CDA85-076D-44CE-B241-E3FFC8FA61CF</gtr:id><gtr:title>Talking Heads: Detecting Humans and Recognizing Their Interactions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4dbd7e508ce919ea8f77d3b3276daa15"><gtr:id>4dbd7e508ce919ea8f77d3b3276daa15</gtr:id><gtr:otherNames>Hoai M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545bbc016c4d05.53687962</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>18346E02-F948-4269-91DB-0AF2BD07E8BC</gtr:id><gtr:title>Action Recognition From Weak Alignment of Body Parts</gtr:title><gtr:parentPublicationTitle>British Machine Vision Conference, 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd3fcb4b7049d4c550f9dc05e57b7b78"><gtr:id>dd3fcb4b7049d4c550f9dc05e57b7b78</gtr:id><gtr:otherNames>Hoai, M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545bbb8d5c5684.41295361</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0CDCB5A6-B9C3-4E0C-A62A-FDD9558F36E6</gtr:id><gtr:title>Signs in time: Encoding human motion as a temporal image</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d80ddce33601108e0f0f3076a6603a6a"><gtr:id>d80ddce33601108e0f0f3076a6603a6a</gtr:id><gtr:otherNames>Chung, J.S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd8f07339b73.83254341</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>18FB8779-F8F9-4681-B0BF-79C2D2B6CD23</gtr:id><gtr:title>Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos</gtr:title><gtr:parentPublicationTitle>Springer Lecture Notes on Computer Science (LNCS)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b19f49784ad002b6968f9010116856f5"><gtr:id>b19f49784ad002b6968f9010116856f5</gtr:id><gtr:otherNames>Pfister, T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545bbcc6cb7e78.05796735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26E5DAA8-6A27-43A3-9C96-338752646FCB</gtr:id><gtr:title>Improving Human Action Recognition using Score Distribution and Ranking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4dc6801d5ad5170e73bea33d81ec70b6"><gtr:id>4dc6801d5ad5170e73bea33d81ec70b6</gtr:id><gtr:otherNames>Minh Hoai</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56b1ed5ab5db39.57484255</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I012001/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>