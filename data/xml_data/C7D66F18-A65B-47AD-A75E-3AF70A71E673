<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B86BF310-2812-4FAC-8860-A72CF97C07C8"><gtr:id>B86BF310-2812-4FAC-8860-A72CF97C07C8</gtr:id><gtr:name>Cork Institute of Technology</gtr:name><gtr:address><gtr:line1>Cork Institute of Technology</gtr:line1><gtr:line2>Rossa Avenue</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E"><gtr:id>598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E</gtr:id><gtr:name>Friedrich-Alexander University</gtr:name><gtr:address><gtr:line1>Schlossplatz 4</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C01F36D5-8E9C-47AF-AA5E-6267C814CF59"><gtr:id>C01F36D5-8E9C-47AF-AA5E-6267C814CF59</gtr:id><gtr:name>INRIA</gtr:name><gtr:address><gtr:line1>Domaine de Voluceau</gtr:line1><gtr:line2>Rocquencourt BP-105</gtr:line2><gtr:postCode>F-78153</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FE1EC3EB-3D38-4ABB-94B2-8429AC475C8F"><gtr:id>FE1EC3EB-3D38-4ABB-94B2-8429AC475C8F</gtr:id><gtr:name>Northwestern University</gtr:name><gtr:address><gtr:line1>633 Clark Street</gtr:line1><gtr:postCode>60208</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9587265A-30C0-45F0-B974-BDB3DAED8E8D"><gtr:id>9587265A-30C0-45F0-B974-BDB3DAED8E8D</gtr:id><gtr:name>Telecom ParisTech</gtr:name><gtr:address><gtr:line1>46 Rue Barrault</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A3C215EE-5645-4543-ACD7-878BFB368AC7"><gtr:id>A3C215EE-5645-4543-ACD7-878BFB368AC7</gtr:id><gtr:firstName>Panos</gtr:firstName><gtr:surname>Kudumakis</gtr:surname><gtr:orcidId>0000-0003-0518-4198</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0C941CCA-2B6E-4ADC-B915-3543BAEF5164"><gtr:id>0C941CCA-2B6E-4ADC-B915-3543BAEF5164</gtr:id><gtr:firstName>Chris</gtr:firstName><gtr:surname>Cannam</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/67CAA1DB-31CD-4307-A2B8-B3A2A41082E0"><gtr:id>67CAA1DB-31CD-4307-A2B8-B3A2A41082E0</gtr:id><gtr:firstName>Sebastian</gtr:firstName><gtr:surname>Ewert</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/058B7945-94E9-450F-A970-E88BA39F33D9"><gtr:id>058B7945-94E9-450F-A970-E88BA39F33D9</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Johnathan</gtr:otherNames><gtr:surname>Bryan-Kinns</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL027119%2F1"><gtr:id>C7D66F18-A65B-47AD-A75E-3AF70A71E673</gtr:id><gtr:title>Musical Audio Repurposing using Source Separation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/L027119/1</gtr:grantReference><gtr:abstractText>Delivery of audio has become increasingly complex: originally in single channel (mono) or 2-channel stereo format, now surround sound in &amp;quot;5.1&amp;quot; format (5 main speakers plus one low frequency effects channel) is available in many home cinema systems, and many other multichannel audio formats are available (e.g. 6.1, 7.1, 10.2 and 22.2). In addition, new interactive apps allow users to remix musical audio, changing instrument volumes, and music games allow players to control individual instruments. Content creators therefore have to develop new ways to create and distribute their audio content to allow their content to be played back on these multichannel systems, or remixed by users to suit their own tastes.
However, much audio content is still in legacy formats, mainly 2-channel stereo. We therefore need ways to &amp;quot;repurpose&amp;quot; this legacy audio content, converting these into surround sound or to the separate &amp;quot;stems&amp;quot; needed for remixable audio.
The aim of this project is to develop a new approach to high quality audio repurposing, based on high quality musical audio source separation. To achieve this we will combine new high resolution separation techniques with information such as musical scores, instrument recognition, onset detection, and pitch tracking. Instead of aiming at generic source separation, we will develop algorithms designed to match the separation performance to the final target (upmixing or remixing). In parallel, we will investigate perceptual evaluation measures for source separation, remixing and upmixing, and develop new diagnostic evaluation techniques tailored to measure different aspects of the repurposed outcome.
The outcomes of this project will allow music consumers to enjoy their favourite songs in interactive remixing apps and games, even where the original separate &amp;quot;stems&amp;quot; are not available. It will also allow music companies, broadcasters and sound archive holders to provide high quality upmixed versions of their large archive content, for an increasing generation of listeners with surround sound systems in the home.</gtr:abstractText><gtr:potentialImpactText>(Non-academic beneficiaries are outlined here and in &amp;quot;Pathways to Impact&amp;quot;. For more on academic impact, see &amp;quot;Academic Beneficiaries&amp;quot; and the &amp;quot;Academic Impact&amp;quot; section in the Case for Support.)
Audio researchers in industry will benefit from new methods for upmixing and remixing emerging from the project. 
Manufacturers of audio upmixing equipment and plugins, and broadcasters wishing to upmix legacy 2-channel stereo content, will benefit from our new high-quality upmixing methods. Manufacturers of other musical audio effects boxes will benefit from new methods for remixing allowing repurposing of legacy audio content. 
Other holders of legacy audio and audiovisual archives, such as the British Library, BFI and regional sound archives, will benefit from the ability to upmix their content for modern audiences becoming increasingly used to surround sound audio. 
There is a strong interest amongst both professional and high-end consumer audio users in new methods for unmixing 2-channel stereo content to 5.1 surround sound, leading to a range of upmix (or &amp;quot;unwrap&amp;quot;) plugins for systems such as ProTools. These users will benefit from new upmix approaches emerging from this project, either through direct use of research prototypes, or through enhanced software or tools from audio equipment or plugin manufacturers. 
Sound artists and composers will benefit from our remixing methods, allowing them to use sounds from mixed audio signals as part of compositions. 
Remixing apps are becoming available for mobile devices, allowing users to remix and share audio tracks. Currently these are limited to use tracks where the separated sources are available from the original music label. These remix users and companies would benefit from the ability to remix from the stereo content that they already own. 
The staff employed on the project, including postdoctoral research assistants undertaking the research, will gain skills applicable to industrial problems such as advanced digital signal processing, research software development, and evaluation methodologies.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>887606</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2980000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>H2020-ICT-2015 Audio Commons</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>688382</gtr:fundingRef><gtr:id>2E0EF6B5-65E4-497B-8814-612F2F8C4C61</gtr:id><gtr:outcomeId>568bdebdb40db2.04379356</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Currently early-stage discussions on possible games applications.</gtr:description><gtr:id>038821C1-BE79-4FD8-A92F-94E35EF1D350</gtr:id><gtr:impactTypes/><gtr:outcomeId>568be773aa7e73.50248700</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2A161E51-87FD-455E-B803-599DC4273024</gtr:id><gtr:title>Polyphonic Sound Event Tracking Using Linear Dynamical Systems</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fdf37cffef1.51411585</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2FC57610-60C1-42BD-8C61-A7816147CE1C</gtr:id><gtr:title>Music remixing and upmixing using source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99a0fbade35.65162612</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>16E428B2-3999-44A1-ADE9-D4D0039AD048</gtr:id><gtr:title>Singing Voice Separation Using Deep Neural Networks and F0 Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c701913fdd98.38264396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6985E7D1-EC9D-4026-94C1-4661F7D81609</gtr:id><gtr:title>Non-Negative Group Sparsity with Subspace Note Modelling for Polyphonic Transcription</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845955cb409b459776fd0fe8bfe01bf7"><gtr:id>845955cb409b459776fd0fe8bfe01bf7</gtr:id><gtr:otherNames>O'Hanlon K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d3ffe46ea79.48364266</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BF8FA9A-C9C1-4FC5-9EDF-1214D21D592B</gtr:id><gtr:title>Single-channel audio source separation using deep neural network ensembles</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6991d8abd65eefb1f983eb14593b94ad"><gtr:id>6991d8abd65eefb1f983eb14593b94ad</gtr:id><gtr:otherNames>Grais EM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b994e5da6951.36732602</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BF54508-4023-4FD4-85CD-1012E1E2E057</gtr:id><gtr:title>Untwist: A new toolbox for audio source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99a7c63f380.63542356</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EF281ABB-1A31-46CC-A7E5-5BB632AC2E89</gtr:id><gtr:title>Remixing musical audio on the web using source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99b0a668674.76797431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F5DDA73D-CDB7-45F1-AF21-82DCFAAAA25C</gtr:id><gtr:title>Two-Stage Single-Channel Audio Source Separation Using Deep Neural Networks</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/228946c0aeaa4e48b73a820d80916550"><gtr:id>228946c0aeaa4e48b73a820d80916550</gtr:id><gtr:otherNames>Grais E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe09b6da7a2.31812638</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F47D897A-9DA2-4943-895F-3A175677BA7D</gtr:id><gtr:title>Non-negative matrix factorisation incorporating greedy hellinger sparse coding applied to polyphonic music transcription</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845955cb409b459776fd0fe8bfe01bf7"><gtr:id>845955cb409b459776fd0fe8bfe01bf7</gtr:id><gtr:otherNames>O'Hanlon K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b8ef4d34f57.79536908</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>32B0A519-A5E2-4752-82AF-7D895860313E</gtr:id><gtr:title>Detection of overlapping acoustic events using a temporally-constrained probabilistic model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b98cc5c6adc7.35571051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>07CDD8B1-6650-44D5-A8E5-F8451937C74B</gtr:id><gtr:title>Evaluation of audio source separation models using hypothesis-driven non-parametric statistical methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3fff870985131b893b7254f573f4031a"><gtr:id>3fff870985131b893b7254f573f4031a</gtr:id><gtr:otherNames>Simpson A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b995f28f1178.85372688</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L027119/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>C7D66F18-A65B-47AD-A75E-3AF70A71E673</gtr:id><gtr:grantRef>EP/L027119/1</gtr:grantRef><gtr:amount>887606.65</gtr:amount><gtr:start>2014-11-01</gtr:start><gtr:end>2014-12-31</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>1CEC4D98-AAD2-41EE-A306-4EF7F281D016</gtr:id><gtr:grantRef>EP/L027119/2</gtr:grantRef><gtr:amount>856793.48</gtr:amount><gtr:start>2015-04-01</gtr:start><gtr:end>2018-07-31</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>