<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:department>Computing Department</gtr:department><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C58BC45F-F8E2-4B71-BF2E-00DEF357D9C2"><gtr:id>C58BC45F-F8E2-4B71-BF2E-00DEF357D9C2</gtr:id><gtr:firstName>Rebecca</gtr:firstName><gtr:surname>Fiebrink</gtr:surname><gtr:orcidId>0000-0002-7609-2234</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FP024327%2F1"><gtr:id>C136B29F-811B-450A-95B3-88589CE283D6</gtr:id><gtr:title>Supporting Feature Engineering for End-User Design of Gestural Interactions</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/P024327/1</gtr:grantReference><gtr:abstractText>Sensors for analysing human gesture and activity (such as accelerometers and gyroscopes) are becoming increasingly affordable and easy to connect to existing software and hardware. There is great, unexplored potential for these sensors to support custom gestural control and activity recognition systems. Applications include include the creation of bespoke gestural control interfaces for disabled people, new digital musical instruments, personalised performance analysis systems for athletes, and new embodied interactions for gaming and interactive art. The ability to easily create novel interactions with motion sensors also benefits schoolchildren and university students who are learning about computing through the use of sensors with platforms such as BBC micro:bit and Arduino.

We have previously established methods for enabling people without programming expertise to build custom gesturally-controlled systems, using interactive machine learning. These methods allow people to easily create new systems by demonstrating examples of human actions, along with the desired label or computer response for each action. 

Unfortunately, many compelling applications of custom gesture and activity recognition require substantial pre-processing of raw sensor data (i.e., &amp;quot;feature engineering&amp;quot;) before machine learning can be applied successfully. Experts first apply a variety of signal processing techniques to sensor data in order to make machine learning feasible. Many people who would benefit from the ability to create custom gestural interactions lack the signal processing and programming expertise to apply those methods effectively or efficiently. It is not known how to successfully expose control over feature engineering to non-experts, nor what the trade-offs among different strategies for exposing control might be.

Our hypothesis is that it is possible to support creation of more complex gestural control and analysis systems by non-experts. We propose to develop and compare three methods for exposing control over feature engineering to non-experts, each requiring varying degrees of user involvement. 

The first method uses a fully automated approach, which attempts to computationally identify good features. The second method first elicits high-level information about the problem from the user, and then employs this information to better inform the automated approach. The third method directly involves the user in the feature engineering process. By leveraging users' ability to demonstrate new gestures, identify patterns in visualisations, and reason about the problem domain - as well as computers' ability to employ users' demonstrations to propose new relevant features - this interactive approach may yield more accurate recognisers. Such an approach may also help users learn about the utility of different features, enabling more efficient debugging of their systems and a better understanding of how to build other systems in the future. 

We are interested in understanding both the accuracy and usability of these methods. We will evaluate each method with people training several types of gesture and activity recognisers. We will compare each method in terms of the accuracy of the final system, the time required for users to build the system, users' subjective experiences of the design process and quality of the final system, and improvements in users' ability to reason about building gestural interactions with sensors. 

This research will enable a wider range of people to successfully use popular motion sensors to create bespoke gestural control and analysis systems, for use in a broader range of applications. Our techniques will be directly implemented in existing open-source software for interactive machine learning. The methods and study outcomes will also inform future work to support feature engineering for users creating real-time systems with other types of sensors.</gtr:abstractText><gtr:potentialImpactText>This project puts at its heart user-centred design and stakeholder engagement, in order to ensure that benefits are recognised in a number of different target audiences. 

1. User groups in the general public:

New groups of people will gain the ability to create compelling, customised interactions with motion sensors, including:
- People with disabilities who can create new, customised gestural interfaces for controlling computers and games;
- Musicians creating new digital musical instruments;
- Athletes and trainers creating personalised performance monitoring and analysis systems;
- Artists creating new interactive experiences;
- &amp;quot;Quantified self&amp;quot; practitioners seeking new ways to understand or change their habits; 
- &amp;quot;Hackers&amp;quot; and &amp;quot;makers&amp;quot; developing new interactions with sensing hardware for practical or creative use. 

The ability to create more sophisticated gesture and activity recognition systems can enhance such people's quality of life, providing an increased sense of agency, new creative outlets, and a better understanding of themselves. We will integrate our techniques for end-user feature engineering directly into existing free software that provides GUIs for use by non-programmers. This means the above users will immediately be able to employ these techniques. 

2. Users in the creative and digital industries

Individuals who currently design interactions as part of their professional practice-including many artists, musicians, and indie game designers-will be able to use sensors more effectively and efficiently in their work. Other people who currently employ sensors for non-commercial, personal use, such as hackers/makers, may become more likely to commercialise their designs as the sophistication of interactions they can create improves.

3. School students using sensors to learn about computing. 

The introduction of 1 million BBC micro:bits into UK classrooms this year reflects recognition that work with sensors can engage students in learning computing, encouraging creative exploration of projects that connect computing to the physical world. Our project greatly expands the set of interactions that non-experts can easily build using micro:bit's accelerometers, and our findings will directly inform the design of future tools for students experimenting and creating interactions with sensors. The software tools released by our project will also be immediately usable by university students in physical computing classrooms who are building new interactions with sensors and platforms such as Arduino and Raspberry Pi. Increased student engagement with electronics presents opportunities for additional, longer-term economic benefits: students who build solid foundations in STEM skills will be better prepared to contribute to the 21st century workforce and to become technology innovators themselves.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-12-25</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2017-06-26</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>99991</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/P024327/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>