<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C754D612-4D8B-42CB-922C-07E7F5BE4B49"><gtr:id>C754D612-4D8B-42CB-922C-07E7F5BE4B49</gtr:id><gtr:name>Washington University in St. Louis</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0CF6353C-24C9-4866-99CF-FB1DB633AF6F"><gtr:id>0CF6353C-24C9-4866-99CF-FB1DB633AF6F</gtr:id><gtr:name>University of North Carolina at Chapel Hill</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Ear Institute</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C754D612-4D8B-42CB-922C-07E7F5BE4B49"><gtr:id>C754D612-4D8B-42CB-922C-07E7F5BE4B49</gtr:id><gtr:name>Washington University in St. Louis</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0CF6353C-24C9-4866-99CF-FB1DB633AF6F"><gtr:id>0CF6353C-24C9-4866-99CF-FB1DB633AF6F</gtr:id><gtr:name>University of North Carolina at Chapel Hill</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/1318ADD8-85A3-4FDE-816A-99386CE74212"><gtr:id>1318ADD8-85A3-4FDE-816A-99386CE74212</gtr:id><gtr:firstName>Jennifer</gtr:firstName><gtr:otherNames>Kim</gtr:otherNames><gtr:surname>Bizley</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FH016813%2F2"><gtr:id>78B4512B-DDAE-4A23-8B88-3C4FD5460E16</gtr:id><gtr:title>Identifying the signal in the noise: a systems approach for examining invariance in auditory cortex</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/H016813/2</gtr:grantReference><gtr:abstractText>We are able to recognize and understand speech across many different speakers, voice pitches and listening conditions. However, the acoustic waveform of a sound (e.g. the vowel 'ae') will vary considerably depending on the individual speaker, and the 'ae' may be embedded in a cacophony of other, background sounds in our often noisy environments. Despite this, we have no difficulty recognizing an 'ae' as an 'ae', suggesting that the brain is capable of forming a representation of the vowel sound which is invariant to these 'nuisance' variables. For vowel sounds, the timbre, or vowel identity, is determined by the spectral envelope. Filtering by the mouth, lips and tongue results in energy peaks, or 'formants' in the spectrum, and it is the location of these formants which differentiates vowel sounds from one another. Thus, the fact that we are able to discriminate 'ae' from 'ih' irrespective of the gender, age or accent of a speaker suggests that we are able to form an invariant representation of the formant relations independently of the fundamental frequency, room reverberations, or spatial location in both quiet and noisy conditions. The aim of this research program is to discover where and how such invariant representations arise in the central auditory system and how these representations are maintained in noisy environments. Forming invariant representations is one of the greatest challenges for sensory systems, and understanding where and how such representations are read out is crucial for the design of any neuroprosthetic device. Our research uses ferrets as their hearing range spans a very similar range of frequencies to ours. Moreover, ferret vocalizations share many similarities with human vowel sounds. Ferrets rapidly learn to discriminate vowel sounds and we are able to record the activity of their nerve cells whilst they perform such listening tasks. By probing the circumstances under which the ferret is able to discriminate vowel sounds, and measuring the neural activity, we can look for where in the auditory brain invariant vowel representation might occur. The second part of this project involves reversibly silencing individual brain areas by cooling them. The principle of this technique is much the same was as using an ice pack to cool pain neurons in a bruised piece of skin. Small 'cryoloops' are implanted above auditory cortex in trained animals.This technique allows us to test whether particular brain areas are causally involved in vowel discrimination. The final part of this project investigates the role of visual information in auditory perception. It is well known that seeing a persons mouth movements while they talk to you enhances your ability to understand them - especially if you are listening in a very noisy room. When trying to pick out a quiet sound in a noisy background knowing when the sound is likely to occur also enhances your ability to correctly identify it. It has recently been shown that visual information is integrated into the very earliest auditory cortical areas. However, quite how this visual information shapes our auditory perception is unknown. The work in this proposal seeks to examine how visual information helps a trained animal to identify vowel sounds more accurately, whilst simultaneously examining how the visual stimulus influences the behaviour of neurons in auditory cortex. Inappropriate integration of auditory and visual information is postulated to underlie schizophrenic symptoms and understanding how informative visual stimuli influence auditory cortical activity will provide valuable insight into how sensory integration occurs in the healthy brain.Hearing impaired individuals most frequently suffer from an inability to effectively identify speech in noisy environments. Understanding how neurons are able to represent vowel identity robustly across a variety of listening conditions and noise environments will enhance hearing aid and cochlear implant design.</gtr:abstractText><gtr:technicalSummary>Failure to understand speech in a noisy environment is one of the principal complaints of the hearing impaired. Our remarkable ability to recognize and understand speech across many different speakers, voice pitches and listening conditions likely depends on the auditory brain extracting those acoustic cues which provide reliable information about a particular sound feature in order to form a neural representation which robustly identifies the stimulus regardless of task-irrelevant 'nuisance' variables. This proposal employs a systems neuroscience approach to examine how auditory cortex forms invariant neural representations of vowel identity enabling a listener to differentiate vowels such as /ae/ from /ih/ irrespective of the voice pitch or location in space, and how this is maintained despite changes in the background noise environment. Three specific questions will be addressed: (1) How (with what neural code) and where (in which cortical field) do neurons support invariant perception of vowel identity? (2) Which cortical fields are necessary for an animal to perform invariant vowel recognition? (3) What role does visual information play in helping us to 'hear better'? By simultaneously measuring spiking activity, local field potentials and neural oscillations in trained ferrets performing a vowel identification task we will examine the incidence and location of invariant vowel timbre encoding. We will explore the neural codes which might support invariant coding and use reversible inactivation techniques examining causal relationships between the activity in specific cortical fields and vowel discrimination behaviour. Lastly we will examine how visual information is integrated within auditory cortex in order to aid listening in difficult conditions. Whilst multisensory integration has been documented in early sensory cortices very few studies have sought to correlate physiological measures with simultaneous assessment of any multisensory behavioural advantage.</gtr:technicalSummary><gtr:potentialImpactText>The proposed work is a fundamental neuroscience research project and will produce key insights into the functional organisation of mammalian sensory pathways and the processing of sounds by biological systems. Discriminating speech sounds in a noisy environment is one of the principle complaints of hearing impaired listeners. Understanding the neural processing which underlies this ability in normal hearing brains will offer key insights into how to better design signal processors in cochlear implants and hearing aids. Similar advantages will be afforded to communication technologies; we need only consider the very substantial shortcomings of even the most artificial speech recognition systems to be reminded of the remarkable sophistication of the auditory system. Collaboration with ENT surgeons and audiologists both within and beyond Oxford University ensures that our work maintains a clinical focus. Dr Hartley, a clinician-scientist has recently developed a ferret cochlear implant model meaning that the neural processing insights we gain from our efforts to relate perception to neural firing can be used to generate testable hypothesis which can be implemented within this animal model. We have established collaborations within Oxford with ENT clinicians. We maintain active collaborations with the growing number of sensory neuroscience groups, both in the UK and USA, who use ferrets as an animal model. Through regular dialogue we will continue to share new data, techniques and methods to the benefit of all. An example of such collaborative endeavour is the ferret brain atlas which is being developed by neuroanatomist Dr Sussane Radtke-Schuller (based in Munich), in collaboration with the University of Maryland based group of Dr Shihab Shamma, and the Oxford group. This atlas is the first of its kind for this species and will provide a free online resource containing cytoarchitectonic data and high resolution structural MRI scans which will be available to assist the increasing number of scientists who are working with ferrets. We increase the impact of our work by participate in the Deafness Research UK's public outreach programs. In the past year we have had a number of school work experience students, and a Nuffield bursary student spend time in the lab in an attempt to encourage more school leavers to consider pursuing a career in biomedical science. We maintain an uptodate website which details our most recent work as well as our research goals and includes routes through which members of the public can contact us. The research in this proposal addresses fundamental neuroscience issues with little immediate commercial exploitation potential. Its benefits will be in enhancing our scant knowledge of how the healthy brain operates to process sounds. This knowledge will be, through publication and dissemination at international meetings, made available to engineers and others who will be able to apply what we learn about neural coding to improving the signal processing capabilities of cochlear implants, hearing aids and telecommunication devices. Whilst cochlear implants have been tremendously successful they are only an option for those individuals with an intact auditory nerve. A knowledge of the neural coding mechanisms within auditory cortex will guide the stimulation strategies and design of neural prosthesis which targets auditory centres in the brainstem or midbrain.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-10-09</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2011-10-10</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>384541</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of North Carolina at Chapel Hill</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Department of Psychiatry</gtr:department><gtr:description>UNC Chapel Hill, North Carolina</gtr:description><gtr:id>81FF4195-EEB6-4D5D-8FBB-28636BEAE661</gtr:id><gtr:impact>None yet.</gtr:impact><gtr:outcomeId>545be94dc4f766.87628131-1</gtr:outcomeId><gtr:partnerContribution>We have a collaborative project which has been funded by HFSP and which commences this month.</gtr:partnerContribution><gtr:piContribution>We have a collaborative project which has been funded by HFSP and which commences this month.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Washington</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Prof Adrian KC Lee, University of Washington, Seattle</gtr:description><gtr:id>4F339008-DD66-4D6A-9353-079D88AE69E3</gtr:id><gtr:impact>Publications: Bizley, Shinn-Cunningham and Lee 2012, J.Neurosci.
Manuscript under review (Maddox, Atilgan, Bizley, Lee).</gtr:impact><gtr:outcomeId>545be8b89f1445.39420020-1</gtr:outcomeId><gtr:partnerContribution>Our collaborators designed the original psychophysical paradigm which was run in parallel in our two labs. We are performing animal studies while our collaborators perform imaging - in both cases using the same stimuli</gtr:partnerContribution><gtr:piContribution>We have collaborated on a psychophysics project (currently under review - Maddox, Atilgan, Bizley and Lee), and continue to collaborate using invasive recordings in our animal model and non-invasive functional imaging in humans.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Science showoff</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>56178E9B-A1F9-4359-94B4-3DDFC8D49641</gtr:id><gtr:impact>Talk generated lots of interest and discussion afterwards</gtr:impact><gtr:outcomeId>545a4ee6994f36.71426823</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TEDx talk</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E653F6EE-F282-46FD-8388-C07784EB304F</gtr:id><gtr:impact>Lots of discussion, &amp;gt;1000 views on youtube</gtr:impact><gtr:outcomeId>545a4f659144d2.19819773</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.youtube.com/watch?v=GWUVRjtlg1Y</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>SoapBox Science</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>67DB2830-52BF-421A-8372-2EF493BEC2BA</gtr:id><gtr:impact>Many people (adults and children) listened for 20 mins or more.

People seemed interested and to have appreciated new things about hearing and the brain</gtr:impact><gtr:outcomeId>545a4e83994997.07897403</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://soapboxscience.org.uk</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Auditory Scene Analysis Workshop HWK, Delmenhorst</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A65AB761-ED3B-4D69-83E3-C148F2AD3307</gtr:id><gtr:impact>Discussion and continuing collaboration</gtr:impact><gtr:outcomeId>545be33e915d28.84885996</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>375000</gtr:amountPounds><gtr:country>France, French Republic</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Human Frontiers Science Foundation</gtr:description><gtr:end>2017-11-02</gtr:end><gtr:fundingOrg>Human Frontier Science Program (HFSP)</gtr:fundingOrg><gtr:fundingRef>RGY0068</gtr:fundingRef><gtr:id>C43AB27D-68BC-4838-8AE3-B2A34978CE10</gtr:id><gtr:outcomeId>545a4d600d43c2.66935749</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2014-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1500000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Society / Wellcome Trust Sir Henry Dale Fellowship</gtr:description><gtr:end>2018-08-02</gtr:end><gtr:fundingOrg>Sir Henry Dale Fellowships</gtr:fundingOrg><gtr:fundingRef>WT098418RR</gtr:fundingRef><gtr:id>D5FAB648-C291-4230-A1A4-91305DD56088</gtr:id><gtr:outcomeId>545be66ea89d66.68426836</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2013-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>65000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Defeating Deafness PhD studentship</gtr:description><gtr:end>2017-04-02</gtr:end><gtr:fundingOrg>Action on Hearing Loss</gtr:fundingOrg><gtr:id>AFCA15AA-62C8-4264-8E4A-13E69DE5A9B4</gtr:id><gtr:outcomeId>545be6c72f1a18.49031851</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2013-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>12000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>International Award</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:id>CC7FA0F8-7B0D-4E76-9DC5-5BDF743D99D6</gtr:id><gtr:outcomeId>545be760c92315.29705621</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-07-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings have been presented at a variety of national and international meetings. We have published two review papers and have a paper detailing some behavioural work undergoing a second revision. Our neural findings are currently being prepared for submission for peer review and subsequent publication.
Our software and methods have been shared with a number of other researchers. In addition the methods that we developed for freely moving recordings have led to me being able to assisted another research group in setting up a ferret lab as an alternative to primate work.</gtr:description><gtr:id>19785CFE-247F-4BE1-AC9E-E7EFDD09289C</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545bcabe0cc925.50824932</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare,Pharmaceuticals and Medical Biotechnology</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We are able to recognize and understand speech across many different speakers, voice pitches and listening conditions. However, the acoustic waveform of a sound (e.g. the vowel /a/) will vary considerably depending on the individual speaker, and the /a/; may be embedded in a cacophony of other, background sounds in our often noisy environments. Despite this, we have no difficulty recognizing an /a/; as an /a/, suggesting that the brain is capable of forming a representation of the vowel sound which is invariant to these nuisance variables. Thus, the fact that we are able to discriminate /a/ from /e/; irrespective of the gender, age or accent of a speaker suggests that we are able to form an invariant representation of the formant relations independently of the fundamental frequency, room reverberations, or spatial location in both quiet and noisy conditions. The aim of this research program was to discover where and how such invariant representations arise in the central auditory system and how these representations are maintained in noisy environments. We have recorded neural activity during sound discrimination and used machine learning approaches to demonstrate how neurons are capable of achieving perceptual invariance across voice pitch and in different noise backgrounds. We have further documented how auditory cortical neurons are shaped by experience - both short term (for example when passively listening as opposed to actively discriminating) and long term (trained versus naive subjects). These findings are being prepared for publication.</gtr:description><gtr:exploitationPathways>We hope that our findings will provide the foundation for work exploring how the auditory brain makes sense of complicated sound scenes - such as listening to conversation in a busy restaurant. Our data provide insight into how the brain achieves this challenge and may in the future provide biologically inspired solutions to computer listening devices or signal processing devices in hearing prosthetics.</gtr:exploitationPathways><gtr:id>D17316F9-F386-49EA-B43C-0DA1912CD4FF</gtr:id><gtr:outcomeId>545cfda1ddf3e6.91466350</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Analysis suite (written by Dr Stephen Town) for collection and analysis of neural data. Provided to 4 research groups on request.</gtr:description><gtr:id>F9CA9719-BEBC-46D0-B4EE-B581E13B48A9</gtr:id><gtr:impact>Provided to 4 research groups on request.</gtr:impact><gtr:outcomeId>545be51af25050.92102174</gtr:outcomeId><gtr:title>Neural analysis software</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Software to interface with TDT equipment to control behavioural testing apparatus. This was developed by the post-doc and PI named on this grant (Town and Bizley) and has been distributed to at least two other laboratories.</gtr:description><gtr:id>113D51E4-7902-40F2-9588-68E44DF7F41B</gtr:id><gtr:impact>No actual Impacts realised to date</gtr:impact><gtr:outcomeId>r-3513990625.2864666fd1dec4</gtr:outcomeId><gtr:title>GoFerret</gtr:title><gtr:type>software</gtr:type><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>E8A79E3C-46AB-499F-A2EC-6C94D3FDB821</gtr:id><gtr:title>Auditory cortex represents both pitch judgments and the corresponding acoustic cues.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b7cc714e4bb39b109bfedce9a878dce6"><gtr:id>b7cc714e4bb39b109bfedce9a878dce6</gtr:id><gtr:otherNames>Bizley JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>545a4a03959be8.71332340</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1E229ABE-E5BB-460E-8950-616BC13959EA</gtr:id><gtr:title>Acute Inactivation of Primary Auditory Cortex Causes a Sound Localisation Deficit in Ferrets.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bdc6edb43181df7897f4bb166462cda"><gtr:id>4bdc6edb43181df7897f4bb166462cda</gtr:id><gtr:otherNames>Wood KC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>58c28752795d08.02470048</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>646CE09C-09A4-4FC3-A8AD-56CD0DB7765F</gtr:id><gtr:title>Integration of Visual Information in Auditory Cortex Promotes Auditory Scene Analysis through Multisensory Binding.</gtr:title><gtr:parentPublicationTitle>Neuron</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35d44c68f698c137bb4763bec46ca810"><gtr:id>35d44c68f698c137bb4763bec46ca810</gtr:id><gtr:otherNames>Atilgan H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>0896-6273</gtr:issn><gtr:outcomeId>5a9e72162a7ea1.25463788</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06C72CDC-9673-4BA1-87FF-3B0367454FD7</gtr:id><gtr:title>Auditory cortical processing in real-world listening: the auditory system going real.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c0e3d88ec34f0ede1e31b15375e2c767"><gtr:id>c0e3d88ec34f0ede1e31b15375e2c767</gtr:id><gtr:otherNames>Nelken I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>545cfefd0573e3.66999294</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE07F215-8C11-489A-A0AD-FE30E5A03430</gtr:id><gtr:title>Spectral timbre perception in ferrets: discrimination of artificial vowels under different listening conditions.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b7cc714e4bb39b109bfedce9a878dce6"><gtr:id>b7cc714e4bb39b109bfedce9a878dce6</gtr:id><gtr:otherNames>Bizley JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>545a4a036e8c70.15205051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5CF2F67B-1573-4CE2-8C3F-77C9515717F4</gtr:id><gtr:title>Neural and behavioral investigations into timbre perception.</gtr:title><gtr:parentPublicationTitle>Frontiers in systems neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08fbf809767ea3e68e53e6e784d018f6"><gtr:id>08fbf809767ea3e68e53e6e784d018f6</gtr:id><gtr:otherNames>Town SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1662-5137</gtr:issn><gtr:outcomeId>545a4a03e2c576.65028770</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F2590364-B43E-458B-91F4-8CDA65FBEFA9</gtr:id><gtr:title>Egocentric and allocentric representations in auditory cortex.</gtr:title><gtr:parentPublicationTitle>PLoS biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08fbf809767ea3e68e53e6e784d018f6"><gtr:id>08fbf809767ea3e68e53e6e784d018f6</gtr:id><gtr:otherNames>Town SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1544-9173</gtr:issn><gtr:outcomeId>5a9e7238586087.94609410</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AC502130-1075-43FB-9126-EC890B479FFF</gtr:id><gtr:title>The what, where and how of auditory-object perception.</gtr:title><gtr:parentPublicationTitle>Nature reviews. Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b7cc714e4bb39b109bfedce9a878dce6"><gtr:id>b7cc714e4bb39b109bfedce9a878dce6</gtr:id><gtr:otherNames>Bizley JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1471-003X</gtr:issn><gtr:outcomeId>545a4a03ba5250.57781754</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C982B92A-C0F4-4A46-A0FF-CAFC10D978B2</gtr:id><gtr:title>The role of spectral cues in timbre discrimination by ferrets and humans.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08fbf809767ea3e68e53e6e784d018f6"><gtr:id>08fbf809767ea3e68e53e6e784d018f6</gtr:id><gtr:otherNames>Town SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>5675e4e614a19</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/H016813/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>E84B386D-B2B0-4E06-820D-A4BC2165AF83</gtr:id><gtr:grantRef>BB/H016813/1</gtr:grantRef><gtr:amount>384541.36</gtr:amount><gtr:start>2011-01-01</gtr:start><gtr:end>2011-01-01</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>78B4512B-DDAE-4A23-8B88-3C4FD5460E16</gtr:id><gtr:grantRef>BB/H016813/2</gtr:grantRef><gtr:amount>384541.36</gtr:amount><gtr:start>2011-10-10</gtr:start><gtr:end>2014-10-09</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>