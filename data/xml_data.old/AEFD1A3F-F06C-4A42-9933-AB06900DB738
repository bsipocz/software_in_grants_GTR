<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:department>Computing</gtr:department><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CC8726CC-3BC9-41BE-9A45-1D0063038804"><gtr:id>CC8726CC-3BC9-41BE-9A45-1D0063038804</gtr:id><gtr:name>National Museums of Scotland</gtr:name><gtr:address><gtr:line1>Chambers Street</gtr:line1><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH1 1JF</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/12431908-778C-4717-A19A-6C0FDAA0463E"><gtr:id>12431908-778C-4717-A19A-6C0FDAA0463E</gtr:id><gtr:name>Communication Matters</gtr:name><gtr:address><gtr:line1>Catchpell House</gtr:line1><gtr:line2>Carpet Lane</gtr:line2><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH6 6SP</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/64FB9A5F-B0E6-4BDB-A56E-84E2F21A51A8"><gtr:id>64FB9A5F-B0E6-4BDB-A56E-84E2F21A51A8</gtr:id><gtr:name>Capability Scotland</gtr:name><gtr:address><gtr:line1>11 Ellersly Road</gtr:line1><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH12 6HY</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9E47EDE7-EC95-438D-BDF5-9A0486653362"><gtr:id>9E47EDE7-EC95-438D-BDF5-9A0486653362</gtr:id><gtr:name>Edesix Ltd</gtr:name><gtr:address><gtr:line1>24 Forth Street</gtr:line1><gtr:postCode>EH1 3LH</gtr:postCode><gtr:region>Scotland</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CF0A65E2-E726-4268-867D-9DF9E4301642"><gtr:id>CF0A65E2-E726-4268-867D-9DF9E4301642</gtr:id><gtr:name>Arria NLG ltd</gtr:name><gtr:address><gtr:line1>Level1, 1 Beadon Road</gtr:line1><gtr:line2>Hammersmith</gtr:line2><gtr:postCode>W6 0EA</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/246C8EE0-BD14-477B-A661-8FF1695D2D12"><gtr:id>246C8EE0-BD14-477B-A661-8FF1695D2D12</gtr:id><gtr:name>Sensory Software International Ltd</gtr:name><gtr:address><gtr:line1>Smart House</gtr:line1><gtr:line2>4A Court Road</gtr:line2><gtr:postCode>WR14 3BL</gtr:postCode><gtr:region>West Midlands</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BB8587AB-CD96-4852-9EC5-E324AAB22BD0"><gtr:id>BB8587AB-CD96-4852-9EC5-E324AAB22BD0</gtr:id><gtr:name>Scope</gtr:name><gtr:address><gtr:line1>6 Market Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>N7 9PW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7E7843B8-B885-4119-8BC3-88B43F44C496"><gtr:id>7E7843B8-B885-4119-8BC3-88B43F44C496</gtr:id><gtr:name>Tobii Technology AB</gtr:name><gtr:address><gtr:line1>Tobii Dynavox</gtr:line1><gtr:line2>Saltmatargatan 8A</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9F03121C-0FF5-4D75-B76B-ADC337768524"><gtr:id>9F03121C-0FF5-4D75-B76B-ADC337768524</gtr:id><gtr:name>Ninewells Hospital &amp; Medical School</gtr:name><gtr:address><gtr:line1>PO BOX 120</gtr:line1><gtr:line4>Dundee</gtr:line4><gtr:line5>Angus</gtr:line5><gtr:postCode>DD1 9SY</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6FF7A8E1-62E2-42C1-9502-44CECFDEB57C"><gtr:id>6FF7A8E1-62E2-42C1-9502-44CECFDEB57C</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>James</gtr:otherNames><gtr:surname>McKenna</gtr:surname><gtr:orcidId>0000-0003-0530-2035</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/BD2BCA2A-BDCB-4EFF-B43B-502080FE48AD"><gtr:id>BD2BCA2A-BDCB-4EFF-B43B-502080FE48AD</gtr:id><gtr:firstName>Rolf</gtr:firstName><gtr:surname>Black</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D39AEAB1-A444-419E-A2F0-D78D0F489CFE"><gtr:id>D39AEAB1-A444-419E-A2F0-D78D0F489CFE</gtr:id><gtr:firstName>Annalu</gtr:firstName><gtr:surname>Waller</gtr:surname><gtr:orcidId>0000-0002-3009-8040</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B3980ADD-58CF-4A26-B922-E005960588A6"><gtr:id>B3980ADD-58CF-4A26-B922-E005960588A6</gtr:id><gtr:firstName>Per Ola</gtr:firstName><gtr:surname>Kristensson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6134FA22-29FD-4B3B-945D-39DB0CC49CDC"><gtr:id>6134FA22-29FD-4B3B-945D-39DB0CC49CDC</gtr:id><gtr:firstName>Jianguo</gtr:firstName><gtr:surname>Zhang</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN014278%2F1"><gtr:id>AEFD1A3F-F06C-4A42-9933-AB06900DB738</gtr:id><gtr:title>ACE-LP: Augmenting Communication using Environmental Data to drive Language Prediction.</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N014278/1</gtr:grantReference><gtr:abstractText>Communication is the essence of life. We communicate in many ways, but it is our ability to speak which enables us to chat in every-day situations. An estimated quarter of a million people in the UK alone are unable to speak and are at risk of isolation. They depend on Voice Output Communication Aids (VOCAs) to compensate for their disability. However, the current state of the art VOCAs are only able to produce computerised speech at an insufficient rate of 8 to 10 words per minute (wpm). For some users who are unable to use a keyboard, rates are even slower. For example, Professor Stephen Hawking recently doubled his spoken communication rate to 2 wpm by incorporating a more efficient word prediction system and common shortcuts into his VOCA software. Despite three decades of developing VOCAs, face-to-face communication rates remain prohibitively slow. Users seldom go beyond basic needs based utterances as rates remain, at best, 10 times slower than natural speech. Compared to the average of 150-190 wpm for typical speech, aided communication rates make conversation almost impossible.

ACE-LP brings together research expertise in Augmentative and Alternative Communication (AAC) (University of Dundee), Intelligent Interactive Systems (University of Cambridge), and Computer Vision and Image Processing (University of Dundee) to develop a predictive AAC system that will address these prohibitively slow communication rates by introducing the use of multimodal sensor data to inform state of the art language prediction. For the first time a VOCA system will not only predict words and phrases; we aim to provide access to extended conversation by predicting narrative text elements tailored to an ongoing conversation.

In current systems users sometimes pre-store monologue 'talks', but sharing personal experiences (stories) interactively using VOCAs is rare. Being able to relate experience enables us to engage with others and allows us to participate in society. In fact, the bulk of our interaction with others is through the medium of conversational narrative, i.e. sharing personal stories. Several research projects have prototyped ways in which automatically gathered data and language processing can support disabled users to communicate easily and at higher rates. However, none have succeeded in harnessing the potential of such technology to design an integrated communication system which automatically extracts meaningful data from different sources, transforms this into conversational text elements and presents results in such a way that people with severe physical disabilities can manipulate and select conversational items for output through a speech synthesiser quickly and with minimal physical and cognitive effort. 

This project will develop technology which will leverage contextual data (e.g. information about location, conversational partners and past conversations) to support language prediction within an onscreen user interface which will adapt depending on the conversational topic, the conversational partner, the conversational setting and the physical ability of the nonspeaking person. Our aim is to improve the communication experience of nonspeaking people by enabling them to tell their stories easily, at more acceptable speeds.</gtr:abstractText><gtr:potentialImpactText>Communication is fundamental to quality of life. Having a voice enables disabled individuals to direct their lives; it impacts on their mental health and helps in finding employment and remaining in work for longer. Slow communication rates mean that many disabled people are lonely which results in poor quality of life. &amp;quot;The Right to Speak&amp;quot;, a 2012 report by the Scottish Government warns that poor communication affects delivery of care and puts people's lives at risk. Although estimates suggest that 0.05% of the UK population could benefit from current VOCAs, recent research by Communication Matters (CM), the largest charity for people who use AAC, has recognised that with changing demographics and improved technology, future generations of VOCAs could be used by an order of magnitude more people (0.5% of the UK population). 

This project will target nonspeaking literate people including people with cerebral palsy; locked-in syndrome, motor neuron disease (MND), head and neck cancer, and Parkinson's disease (PD). According to the CM research, the target groups would account for around 25% of AAC users, approximately 315,000 people in the UK. The severity of disability and the progressive nature of some diseases impacts on families, carers, healthcare professionals, friends and all who interact with the AAC user, increasing the beneficiaries significantly.
This is a participatory research project and as such, at least 20 end users, their families, friends and caregivers will have direct benefit from the project. From experience, we know that participants benefit socially and their self-esteem and communication skills improve when involved in a research project.

We will extend our existing AAC Research Group online community to engage interested parties throughout the project to reduce the chasm between innovation and adoption. In addition, we will present workshops at Communication Matters Symposia throughout the project to present our results to nonspeaking people, their families, speech and language therapists and other professionals interested in AAC.

We have a strong track record working with industry to transfer research into commercial products. We will carry out a professionally-run industrial workshop in the form of a technology roadmapping session in year 3 to identify routes to impact. We have identified two possible routes (an open source non-profit organisation and a technology start-up) which will be the starting point for our discussions with University advisors, our Advisory Group and our Industrial partners. Our three industrial partners anticipate that they will benefit from the outcomes of the research, both in terms of AAC technology and within mainstream surveillance data analysis.

Publicity features in newspapers, radio and television will be used to raise the public awareness of the abilities and needs of people who use AAC; building on our success in obtaining media publicity for our previous projects. We will also collaborate with the Scottish National Museum to create an installation as part of their new Communications exhibition.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1007562</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>487B0E66-28AF-4654-BACE-79C7A0750CF8</gtr:id><gtr:title>The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d51aeb0f18e1631b34f303df3ae9e2a1"><gtr:id>d51aeb0f18e1631b34f303df3ae9e2a1</gtr:id><gtr:otherNames>Jameson A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa91879425218.60349028</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2F82150C-D8B1-4116-87C4-D39495A48F51</gtr:id><gtr:title>Finding Time Together: Detection and Classification of Focused Interaction in Egocentric Video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f7e4c36e5e4e496cb8104517c97bb54"><gtr:id>3f7e4c36e5e4e496cb8104517c97bb54</gtr:id><gtr:otherNames>Bano S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa924b6524ad1.99144181</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3D882B6-CCFB-4097-8D49-08FC90BB04D1</gtr:id><gtr:title>ACE-LP: Augmenting Communication using Environmental Data to drive Language Prediction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/56a575eda02f1b852bde8f00ee2125b9"><gtr:id>56a575eda02f1b852bde8f00ee2125b9</gtr:id><gtr:otherNames>Black R</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56d45bd57fdf38.11439995</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N014278/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>45</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>15BC6F17-6453-42B4-836A-01286E6D8068</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Comput./Corpus Linguistics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>