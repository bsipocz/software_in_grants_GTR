<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1F1D5088-5B97-4904-B35D-B2B965641A9A"><gtr:id>1F1D5088-5B97-4904-B35D-B2B965641A9A</gtr:id><gtr:name>Think-A-Move, LLC</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Mechanical Engineering</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1F1D5088-5B97-4904-B35D-B2B965641A9A"><gtr:id>1F1D5088-5B97-4904-B35D-B2B965641A9A</gtr:id><gtr:name>Think-A-Move, LLC</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/9DD9EF24-13EA-4368-AB17-C826159E7BF9"><gtr:id>9DD9EF24-13EA-4368-AB17-C826159E7BF9</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:otherNames>Edward</gtr:otherNames><gtr:surname>Lutman</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/740F7D2C-5FEC-46D2-9CC2-C9A8701B478D"><gtr:id>740F7D2C-5FEC-46D2-9CC2-C9A8701B478D</gtr:id><gtr:firstName>Maria</gtr:firstName><gtr:surname>Stokes</gtr:surname><gtr:orcidId>0000-0002-4204-0890</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/54E40CDC-078F-4EA4-8F09-146C4019C2AF"><gtr:id>54E40CDC-078F-4EA4-8F09-146C4019C2AF</gtr:id><gtr:firstName>Ravi</gtr:firstName><gtr:surname>Vaidyanathan</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF01869X%2F1"><gtr:id>E54ADD67-84C2-41DC-9016-172F287FB2C8</gtr:id><gtr:title>A Tongue Movement Command and Control System Based on Aural Flow Monitoring</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F01869X/1</gtr:grantReference><gtr:abstractText>Although there is a well-recognized need in society for effective tools which will enable the physically impaired to be more independent and productive, existing technology still fails to meet many of their needs. In particular, nearly all mechanisms designed for human-control of peripheral devices require users to generate the input signal through bodily movements, most often with their hands, arms, legs, or feet. Such devices clearly exclude individuals with limited appendage control. Spinal cord injuries, repetitive strain injuries, severe arthritis, loss of motion due to stroke, and central nervous system (CNS) disorders all represent examples of these impairments. Past work has attempted to address this need through recognition of the potential of the oral cavity (victims of stroke, spinal damage, and arthritis can often move their tongue and mouth) for control input. Developed mechanisms include: track balls, small joysticks, and retainers inserted in the mouth to be manipulated by the tongue, or sip and puff tubes which respond to concentrated exhaling and inhaling. Despite their numerous successful implementations, these devices can be difficult to operate, problematic if they fall from or irritate the mouth, may impair verbal communication, and present hygiene issues since they must be inserted within the mouth.The objective of this programme is to surmount these issues through the development of a unique tongue-movement communication and control strategy in a stand-alone device that can be calibrated for patient-use with all manner of common household devices and tailored to control assistive mechanisms. The strategy is based on detecting specific tongue motions by monitoring air pressure in the human outer ear, and subsequently providing control instructions corresponding to that tongue movement for peripheral control. Our research has shown various movements within the oral cavity create unique, traceable pressure changes in the human ear, which can be measured with a simple sensor (e.g. a microphone) and analyzed to produce commands, that can in turn be used to control mechanical devices or other peripherals. Results from this programme will enable patients with quadriplegia, arthritis, limited movement due to stroke, or other conditions causing limited or painful hand/arm movement to interface with their environment and control all manner of equipment for increased independence and quality of life.</gtr:abstractText><gtr:fund><gtr:end>2011-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>269165</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Think-A-Move, LLC</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Think-A-Move</gtr:description><gtr:id>BDBF96FC-C32E-4A05-8BE5-EEE0DAA092EB</gtr:id><gtr:impact>Production of the first ever tongue controlled wheelchair that did not require any oral cavity devices
Production of the human-robot interface that did not require any gesture control or oral interface that has been written into the next-generation US Army specifications for control of unmanned vehicles</gtr:impact><gtr:outcomeId>54648b890f5383.94816316-1</gtr:outcomeId><gtr:partnerContribution>Hardware design and earpiece fabrication for speech and tongue based aural interface</gtr:partnerContribution><gtr:piContribution>Development of algorithms and mathematical architectures supporting commercialization of speech and tongue based aural interface.</gtr:piContribution><gtr:sector>Private</gtr:sector></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Public Press Resulting from Project</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>0BE800BE-2ED3-49B2-B049-E7D5FE2E5370</gtr:id><gtr:impact>The New Scientist article, &amp;quot;Tongue clicks to control wheelchairs&amp;quot; was written about this research highlighting its assistive potential for prosthetic limb control research and wheelchair control was published on Dec 4, 2010 (pg 24 (link)

A youtube video posted by New Scientist about the article citing this research had over 6,000 views within a week of posting on the New Scientist website.</gtr:impact><gtr:outcomeId>5464c93e1d5b67.51043830</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.newscientist.com/article/dn19790</gtr:url><gtr:year>2010</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Television Programme</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>06EEBE39-3443-44B4-AE33-684635A93B38</gtr:id><gtr:impact>The research was featured on the television program &amp;quot;&amp;quot;Assisted Living&amp;quot;, as a part of &amp;quot;BBC Inside Out&amp;quot;. A demonstration of the first ever tongue controlled prosthetic hand was shown on the television show, which reached an audience in the thousands. Broadcast by BBC West, broadcast Nov 8, 2010

Members of the general public expressed interest in visiting my lab and reported an increased awareness of assistive technology.</gtr:impact><gtr:outcomeId>5464cace3349e7.53728267</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:year>2010</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Public Press Resulting from Project</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>DA3EF18F-5424-4E38-B460-80E740BEBE71</gtr:id><gtr:impact>The New Scientist article, &amp;quot;Tongue clicks to control wheelchairs&amp;quot; was written about this research highlighting its assistive potential for prosthetic limb control research and wheelchair control was published on Dec 4, 2010 (pg 24 (link)

A youtube video posted by New Scientist about the article citing this research had over 6,000 views within a week of posting on the New Scientist website.</gtr:impact><gtr:outcomeId>5464c93c3134d4.98037380</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.newscientist.com/article/dn19790</gtr:url><gtr:year>2010</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Royal Society Summer Science Showcase 2009</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>DDCF7B91-2164-4B48-8A62-4419AD51AFF8</gtr:id><gtr:impact>A booth showing parts of the system at a 'Biologically Inspired Systems' display at the 2009 Royal Society Summer Science Exhibition was constructed and demonstrated. Over 1000 people attending the event came to the booth.

Members of the lay public and even the royal family observed the display, which resulted in very wide dissemination.</gtr:impact><gtr:outcomeId>5464c7aea3e070.49161862</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2009</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>20000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>UK Engineering and Physical Sciences Research Council (EPSRC), Kick Start Grant</gtr:description><gtr:end>2013-04-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>5EB13989-55AC-44BA-A69B-C65A03FD73D5</gtr:id><gtr:outcomeId>5464ae303d3282.14283707</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>375000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>US Office of Naval Research NICOP Program</gtr:description><gtr:end>2017-12-02</gtr:end><gtr:fundingOrg>US Office of Naval Research Global</gtr:fundingOrg><gtr:id>1E6FA739-B63C-403C-82A4-90E9ADC627A0</gtr:id><gtr:outcomeId>54648e95f1dab7.25423582</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2014-07-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>160000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Dyson Foundation Research</gtr:description><gtr:end>2016-10-02</gtr:end><gtr:fundingOrg>Dyson Foundation</gtr:fundingOrg><gtr:id>8C958B9A-6252-4471-82E8-DE37C01EACCA</gtr:id><gtr:outcomeId>5464adb8b0d774.23512880</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2011-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Findings from this program created the first ever tongue based machine interface system that required no oral insertion. It led to the awards: &amp;quot;A Teleoperative Sensory-Motor Control System&amp;quot; (Imperial College-NUS Singapore), &amp;quot;Integrated motion, muscle, and neural activity monitoring&amp;quot; (EPSRC kick-start internal award), and &amp;quot;Multimodal Hands'-Free Interfaces for Robotic Teleoperation&amp;quot; (US Army to Think-A-Move (TAM), Ltd for commercialization; reference: W56HZV-05-C-103) worth &amp;pound;800,000 collectively. It spawned new products in human-robot interface being currently adopted by the US military. Industrial adoption also resulted in the first tongue controlled wheelchair with no oral obstruction. 

The work was also was also highlighted in a keynote video for the 'New Technology Foundation Award' recognizing highest innovation at the 20 year anniversary IEEE IROS conference. The Engineer (2008) and New Scientist (2010) featured the research; a New Scientist video showing the first tongue controlled prosthetic hand received 6,000+ views its first week.</gtr:description><gtr:firstYearOfImpact>2009</gtr:firstYearOfImpact><gtr:id>A02330AE-A671-4036-828A-8EF189D1BFFF</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5464c595157651.55449970</gtr:outcomeId><gtr:sector>Aerospace, Defence and Marine,Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The goal of the project was to demonstrate a new concept for human-machine interface in assistive device control. While extensive research has been performed in human-machine interface (HMI) for command and control, to date, nearly all interfaces are limited in their utility outside controlled environments due to the need for operator motion, lack of portability, cumbersome interface with other approaches, and equipment expense. In this program, we present the first system we are aware of that addresses all these issues. The system is capable of tracking tongue movement to indicate operator desire by monitoring airflow in the ear canal, thus no external operator movement is required. The system is unobtrusive, trivial to don, and leaves the patient free to execute any other activity while wearing/using the system. The only sensor necessary is a simple microphone and earpiece housing which is small enough and comfortable enough to be worn in the ear indefinitely. To our knowledge, our research team is the only group that has investigated the aural cavity as a monitoring venue for machine interface, and has proposed the only system whereby tongue movement may be tracked without insertion of any device in the oral cavity. We have observed tongue movement to be faster, quieter, and (in most cases) more intuitive to the user for direct device motion control compared to speech.

We have further investigated the synergy of the system with existing neural interfaces, with specific focus on brain implants and surface brain electrodes which could be combined with the tongue motion system to form a in comprehensive interface, and expanded our research results to address pattern classification of movement with respect to a range of biosignals. In summary, new contributions in this program include:



1. A new sensor for monitoring airflow in the aural cavity

2. Implementation of signal capture and recognition algorithms to accurately identify and classify tongue movement through monitoring of airflow in the aural cavity 

3. A strategy for detecting and classifying simple and compound tongue movements based on airflow in the aural cavity for robust hands-free HMI

4. New multi-channel pattern classification algorithms to accurately identify physiological signals and correlate them with user intent; the implementation of these algorithms on both tongue movement and neural signals

5. A novel signal capture and segmentation system to identify physiological signals and separate them from other interfering signals in the body; the implementation of these signals for real-time extraction, segmentation, and disturbance rejection of tongue movement ear pressure signals and off-line classification of neural signals

6. The first ever published real-time results of a tongue human-machine interface with no insertion of any device in the oral cavity. The system has been demonstrated for real-time control of a robotic (prosthetic) assist arm, and in simulation on a power assist wheelchair. 

7. The development of a software system including a graphic user interface (GUI) that a user may use to calibrate the system to recognize their unique tongue signals

Research output and impact for the programme has been prolific. This research has produced 8 refereed journal publications, with another 2 currently in preparation, and more than 20 refereed conference publications. Public impact for this work includes major media coverage in magazines including New Scientist, the Association of Computing Machinery (ACM), and The Engineer, and being featured on a 2010 BBC West television episode of &amp;quot;Inside Out&amp;quot;. Videos associated with BBC Inside Out and New Scientist featured the first ever tongue controlled prosthetic hand.</gtr:description><gtr:exploitationPathways>Commercial applications include prosthetic limbs and rehabilitation/assist equipment, including interfaces for power wheelchair control. Although the device is not universally applicable for any situation (e.g. when force feedback is required) we believe it represents a very significant contribution to assistive technology. We believe this work will lay the foundation for a new generation of hands-free human machine interface systems. 



At this time, the company Think-A-Move has been given the results and is consolidating them for use with a new tongue-controlled wheelchair. The prosthetic company RSL Stepper has also given in-kind contributions to allow the system to be demonstrated on their Bebionic prosthetic hand. Possible venues of commercialization in this regard are also being considered.
 Future work could involve synergizing tongue movement and neural modes of interface to develop a cohesive, robust human/robot interface. A foundation for two distinct modes of operation is envisioned whereby several devices (e.g. a power wheelchair, prosthetic limbs, household appliances, stationary mechanical assist devices, etc.) may all be directed given the possibilities for control input.</gtr:exploitationPathways><gtr:id>0C0A7C44-482F-4B5C-9643-FD54F7B8F99E</gtr:id><gtr:outcomeId>r-4037544973.22956567764a98a</gtr:outcomeId><gtr:sectors><gtr:sector>Electronics,Healthcare</gtr:sector></gtr:sectors><gtr:url>http://faculty.nps.edu/ravi/HMI/Bristol%20Tongue%20Control%20Video.avi</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs><gtr:productOutput><gtr:description>The company Think-a-Move, Ltd, USA created a new wheelchair control system based on voice and tongue movement drawing from our academic research.</gtr:description><gtr:id>04095BB9-2C33-4367-9F58-46686BA1D6E8</gtr:id><gtr:impact>This system was the first ever wheelchair control system that did not require oral cavity insertion or bodily movement.</gtr:impact><gtr:outcomeId>5464b12576f848.39524240</gtr:outcomeId><gtr:stage>Late clinical evaluation</gtr:stage><gtr:status>On hold</gtr:status><gtr:title>Oral Controlled Wheelchair</gtr:title><gtr:type>Therapeutic Intervention - Medical Devices</gtr:type><gtr:yearDevCompleted>2011</gtr:yearDevCompleted></gtr:productOutput></gtr:productOutputs><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C75CFB1C-B19D-4651-B60C-3712771C47D5</gtr:id><gtr:title>A DCT-Gaussian classification scheme for human-robot interface</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ddff01fbb9a0d5ed614ddf5a3349266e"><gtr:id>ddff01fbb9a0d5ed614ddf5a3349266e</gtr:id><gtr:otherNames>Kota S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-3803-7</gtr:isbn><gtr:outcomeId>546486aed997b6.76399504</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A797D97C-528F-40CD-A379-39E3BB429EBE</gtr:id><gtr:title>A Feature Ranking Strategy to Facilitate Multivariate Signal Classification</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/84aee1b65297373b8851862d56b0ea9a"><gtr:id>84aee1b65297373b8851862d56b0ea9a</gtr:id><gtr:otherNames>Gupta L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d05f05f83a23a1</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>27C338B5-3EBE-4A77-BF4F-2490B52ECBD4</gtr:id><gtr:title>A heterogeneous framework for real-time decoding of bioacoustic signals: Applications to assistive interfaces and prosthesis control</gtr:title><gtr:parentPublicationTitle>Expert Systems with Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aea8d2a6606e66d946fd03fe9ec8c491"><gtr:id>aea8d2a6606e66d946fd03fe9ec8c491</gtr:id><gtr:otherNames>Mace M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53cff3ff32ad44a6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3CE67C9B-D566-427A-8FEA-5F21DB2A3ED7</gtr:id><gtr:title>Abstracts of the British Society of Audiology Short Papers Meeting on Experimental Studies of Hearing and Deafness</gtr:title><gtr:parentPublicationTitle>International Journal of Audiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f17cdb3b8df0f84cd0e505c9a3fbced"><gtr:id>3f17cdb3b8df0f84cd0e505c9a3fbced</gtr:id><gtr:otherNames>Furness D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>5464756b1ffc73.77920958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7CA82C5E-B719-43D7-8813-F7B3BC2334E4</gtr:id><gtr:title>Biomimetics: Nature-Based Innovation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d59313b4a6d7fdd21f19e777712e6142"><gtr:id>d59313b4a6d7fdd21f19e777712e6142</gtr:id><gtr:otherNames>A. G. Pipe; R. Vaidyanathan; C. Melhuish;</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>10-1439834768</gtr:isbn><gtr:outcomeId>5464899084d343.37450391</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>09C0B569-FCA9-4443-A6E7-C4ACD2CF34CF</gtr:id><gtr:title>An automated approach towards detecting complex behaviours in deep brain oscillations.</gtr:title><gtr:parentPublicationTitle>Journal of neuroscience methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aea8d2a6606e66d946fd03fe9ec8c491"><gtr:id>aea8d2a6606e66d946fd03fe9ec8c491</gtr:id><gtr:otherNames>Mace M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0165-0270</gtr:issn><gtr:outcomeId>54647933d656d9.15762979</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A1C85867-9C1A-4804-8AD8-5AD2B22C9F59</gtr:id><gtr:title>Augmenting neuroprosthetic hand control through evaluation of a bioacoustic interface</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aea8d2a6606e66d946fd03fe9ec8c491"><gtr:id>aea8d2a6606e66d946fd03fe9ec8c491</gtr:id><gtr:otherNames>Mace M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464877c12f1b2.03438226</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6DBDFFC9-A019-45C9-BE96-13430675BAA6</gtr:id><gtr:title>Real-time implementation of a non-invasive tongue-based human-robot interface</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aea8d2a6606e66d946fd03fe9ec8c491"><gtr:id>aea8d2a6606e66d946fd03fe9ec8c491</gtr:id><gtr:otherNames>Mace M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-6674-0</gtr:isbn><gtr:outcomeId>5464859300e361.34973409</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E800536E-ED8D-4692-AB89-4537934613F0</gtr:id><gtr:title>Ensemble classification for robust discrimination of multi-channel, multi-class tongue-movement ear pressure signals.</gtr:title><gtr:parentPublicationTitle>Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aea8d2a6606e66d946fd03fe9ec8c491"><gtr:id>aea8d2a6606e66d946fd03fe9ec8c491</gtr:id><gtr:otherNames>Mace M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4244-4121-1</gtr:isbn><gtr:issn>1557-170X</gtr:issn><gtr:outcomeId>5464862038a4f6.80025635</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3A654FE1-58CB-4CE1-91A5-A31DC699C3E6</gtr:id><gtr:title>Pattern classification of deep brain local field potentials for brain computer interfaces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/826eb36f856525a1dc08956c40f32660"><gtr:id>826eb36f856525a1dc08956c40f32660</gtr:id><gtr:otherNames>Mamun K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-4833-1</gtr:isbn><gtr:outcomeId>54648800aa0f68.55987948</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E0AA88BA-457B-4043-A999-279EF6B32B47</gtr:id><gtr:title>Ensemble classification for multi-channel multi-class ear pressure signals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f35df85e2d31e702d31e278d41a1eb12"><gtr:id>f35df85e2d31e702d31e278d41a1eb12</gtr:id><gtr:otherNames>Michael Mace (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_699418291213f07036</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>51620811-B353-4041-8619-AC0FD24999E7</gtr:id><gtr:title>A robust strategy for decoding movements from deep brain local field potentials to facilitate brain machine interfaces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/826eb36f856525a1dc08956c40f32660"><gtr:id>826eb36f856525a1dc08956c40f32660</gtr:id><gtr:otherNames>Mamun K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4577-1199-2</gtr:isbn><gtr:outcomeId>54648605674a48.91758833</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C39BF49-1D8B-4F25-B15F-AA4D4C7B8060</gtr:id><gtr:title>Tongue in cheek: a novel concept in assistive human machine interface</gtr:title><gtr:parentPublicationTitle>Journal of Assistive Technologies</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aea8d2a6606e66d946fd03fe9ec8c491"><gtr:id>aea8d2a6606e66d946fd03fe9ec8c491</gtr:id><gtr:otherNames>Mace M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_53279965821382e232</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>025621E8-EC64-449D-AC08-624511198DFE</gtr:id><gtr:title>Multi-layer neural network classification of tongue movement ear pressure signal for human machine interface</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/826eb36f856525a1dc08956c40f32660"><gtr:id>826eb36f856525a1dc08956c40f32660</gtr:id><gtr:otherNames>Mamun K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-8496-6</gtr:isbn><gtr:outcomeId>5464867b9909e0.46573009</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>35468FE9-2A46-42A3-AC9E-B3DDE5283400</gtr:id><gtr:title>Pairwise diversity ranking of polychotomous features for ensemble physiological signal classifiers.</gtr:title><gtr:parentPublicationTitle>Proceedings of the Institution of Mechanical Engineers. Part H, Journal of engineering in medicine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/84aee1b65297373b8851862d56b0ea9a"><gtr:id>84aee1b65297373b8851862d56b0ea9a</gtr:id><gtr:otherNames>Gupta L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0954-4119</gtr:issn><gtr:outcomeId>doi_53d079079da8e60d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1A700124-BFCF-43B1-BF5A-049C4E0E4674</gtr:id><gtr:title>Decoding movement and laterality from local field potentials in the subthalamic nucleus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/826eb36f856525a1dc08956c40f32660"><gtr:id>826eb36f856525a1dc08956c40f32660</gtr:id><gtr:otherNames>Mamun K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4244-4140-2</gtr:isbn><gtr:outcomeId>doi_53d05b05bcf267d2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0B3A7B53-D156-4D4A-9F7F-ABBAA448DDF2</gtr:id><gtr:title>A wavelet denoising approach for signal action isolation in the ear canal.</gtr:title><gtr:parentPublicationTitle>Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a83c0749bbc0948d53a93f272dbf707d"><gtr:id>a83c0749bbc0948d53a93f272dbf707d</gtr:id><gtr:otherNames>Vaidyanathan R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-1-4244-1814-5</gtr:isbn><gtr:issn>1557-170X</gtr:issn><gtr:outcomeId>5464873ce7ac79.52889247</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E57A917-D654-4D0E-B75C-BC11660BFB7C</gtr:id><gtr:title>Robust real-time identification of tongue movement commands from interferences</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/826eb36f856525a1dc08956c40f32660"><gtr:id>826eb36f856525a1dc08956c40f32660</gtr:id><gtr:otherNames>Mamun K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d00200229aacd0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7A0B2AA3-A5DC-4E55-B35D-8FEDFF5953DA</gtr:id><gtr:title>Movement decoding using neural synchronization and inter-hemispheric connectivity from deep brain local field potentials.</gtr:title><gtr:parentPublicationTitle>Journal of neural engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de9ccb822d1211033370220072275b1e"><gtr:id>de9ccb822d1211033370220072275b1e</gtr:id><gtr:otherNames>Mamun KA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1741-2552</gtr:issn><gtr:outcomeId>585d4656654d61.55342992</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA43D13A-1BE7-483C-9AF0-F11CAE5D9A14</gtr:id><gtr:title>Assessment of human response to robot facial expressions through visual evoked potentials</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df7cafe74fb1c8e92c2a5c3364418e31"><gtr:id>df7cafe74fb1c8e92c2a5c3364418e31</gtr:id><gtr:otherNames>Craig R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-8688-5</gtr:isbn><gtr:outcomeId>5464856a63f4b2.46450311</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A0FCC674-ADE9-4153-878B-09F7AFEAA8EA</gtr:id><gtr:title>Multivariate Bayesian classification of tongue movement ear pressure signals based on the wavelet packet transform</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/826eb36f856525a1dc08956c40f32660"><gtr:id>826eb36f856525a1dc08956c40f32660</gtr:id><gtr:otherNames>Mamun K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7875-0</gtr:isbn><gtr:outcomeId>546485eb045fd0.47658825</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F01869X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>