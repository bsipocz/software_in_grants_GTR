<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/44160F04-5CBF-4E8E-A6C6-C0EF61A5865C"><gtr:id>44160F04-5CBF-4E8E-A6C6-C0EF61A5865C</gtr:id><gtr:name>Lancaster University</gtr:name><gtr:department>Computing &amp; Communications</gtr:department><gtr:address><gtr:line1>University House</gtr:line1><gtr:line4>Lancaster</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>LA1 4YW</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/44160F04-5CBF-4E8E-A6C6-C0EF61A5865C"><gtr:id>44160F04-5CBF-4E8E-A6C6-C0EF61A5865C</gtr:id><gtr:name>Lancaster University</gtr:name><gtr:address><gtr:line1>University House</gtr:line1><gtr:line4>Lancaster</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>LA1 4YW</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/51CCFFB1-6CE4-46FE-A088-941812CFF4E2"><gtr:id>51CCFFB1-6CE4-46FE-A088-941812CFF4E2</gtr:id><gtr:name>SensoMotoric Instruments GmbH</gtr:name><gtr:address><gtr:line1>Warthestr. 21</gtr:line1><gtr:postCode>14513</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E5ADA650-EF3D-45B8-AA99-0D75FD297260"><gtr:id>E5ADA650-EF3D-45B8-AA99-0D75FD297260</gtr:id><gtr:firstName>Jamie</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Ward</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/9CCD79AA-7EB2-4DC0-9C7F-BB94BF0FF609"><gtr:id>9CCD79AA-7EB2-4DC0-9C7F-BB94BF0FF609</gtr:id><gtr:firstName>Hans</gtr:firstName><gtr:surname>Gellersen</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH005064%2F1"><gtr:id>EB4EBE97-E4D8-4B76-9444-50FD7211ED21</gtr:id><gtr:title>EAR: Eye-based Activity Recognition</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H005064/1</gtr:grantReference><gtr:abstractText>Activity recognition is concerned with inference of human activity from observation of the user's actions, to facilitate systems that proactively support users in everyday activity. Application areas include health, safety, cognitive assistance, lifestyle monitoring, entertainment, and energy management. In this project, we propose to advance activity recognition by investigating patterns of eye movement as a novel contextual cue. What makes eye movement a very distinct source of activity-related information is their direct link to visual cognition. This means that eye movement data has the potential to provide an online indication not only of what activity occurs but also of underlying cognitive processes, for example attention. Eye movements, including their measurement and analysis, have been studied extensively in clinical ophthalmology and cognitive science, but their use for on-line activity recognition in everyday settings is a novel proposition. Significant challenges include the adaptation of measurement techniques fit for everyday settings, the quality of information that can be mined under realistic conditions, and the demonstration of practical utility of eye-based context in real-world application.This project is designed as an initial investigation of the feasibility and potential of eye-based activity recognition. The research will include an assessment of eye tracking techniques, specifically of optical methods that track features on the eye, and of electrooculography (EOG) measuring eye movement with skin electrodes placed near the eyes. Based on the assessment, we will conduct explorative studies of eye patterns in daily life. The aim here is to gain insight into what eye movement reveals about everyday activity, and to develop methods for extraction of features from eye movement patterns that may be useful for activity and context recognition. A third part of the work will investigate application of eye-based context information in interactive systems. The plan is to develop two practical demonstrators, a first one exploring how longitudinal eye movement analysis may contribute to health and lifestyle monitoring, and a second one showing how eye movements might be used for task assistance (for example, search assistance when the eyes indicate visual search activity).</gtr:abstractText><gtr:fund><gtr:end>2012-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-03-29</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>146112</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>950403</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>SENSING AND IMAGING FOR DIAGNOSIS OF DEMENTIAS (SIDD)</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/M006255/1</gtr:fundingRef><gtr:id>8C4FC198-296D-43CB-A4A6-1A133F75CFF5</gtr:id><gtr:outcomeId>54649d338b8763.00214878</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>504000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Marie Curie Initial Training Network</gtr:description><gtr:end>2014-12-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>264738</gtr:fundingRef><gtr:id>122F63D0-AECB-4FC1-90D1-532DD52F8AFC</gtr:id><gtr:outcomeId>546005c5918d25.51794347</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>70273</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Doctoral Training Award</gtr:description><gtr:end>2014-06-02</gtr:end><gtr:fundingOrg>Lancaster University</gtr:fundingOrg><gtr:id>1699B0AE-8BEA-4D9C-A42D-74E431F7B58D</gtr:id><gtr:outcomeId>5460074a0c5692.68287899</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>59686</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Faculty PhD Scholarship</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>Lancaster University</gtr:fundingOrg><gtr:id>9D96C8CD-3CA6-48FB-9166-6F7282563BBC</gtr:id><gtr:outcomeId>546007e8981dc9.39530452</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The way our eyes move is strongly linked to what we do. As we engage in certain activities, our eyes will move in quite specific and often predictable ways. That makes the eyes a rich source of information about human activity. Would it then be possible to automatically infer what we do, from analysis of our eye movements in daily life? This is a fundamental question as eye movement analysis had never before been considered for activity inference, and yet there is great application potential for systems that are able to recognize and support their user's activity. Therefore, the aim of the project was to conduct a first exploration of eye-based activity recognition, to establish the technical feasibility, study eye movement in daily life situations, and develop novel methods for eye movement pattern analysis. 

The most significant findings of our research are on eye-based activity classification and activity spotting. Activity classification is the problem of predicting what a person is doing, given a predefined set of activities that we anticipate. In an experiment we had users perform five distinct activities in front of a computer screen (reading, writing, copying text, web browsing, and watching video), and demonstrated that these could be classified with good precision and recall solely based on eye movement analysis. This means that a machine can predict an activity by looking the user in the eyes, without knowing what they look at. Moreover, we showed that it is possible to do this in a user-independent manner; meaning that we can train an activity classification system with eye movements of some users, and then readily deploy it with other users. The results have been published in IEEE Trans. Pattern Analysis and Machine Intelligence, the highest impact journal in Computer Science and Artificial Intelligence, and are already cited over 100 times by research groups around the world. In other experiments we demonstrated that it is possible to &amp;quot;spot&amp;quot; an activity of interest with a wearable eye-tracker, even when the activity occurs in different situations including when people are mobile and out in the streets (published in ACM Trans Applied Perception). 

We developed a number of novel methods for eye movement pattern analysis. One of the ideas we developed was to code eye movements with symbols that represent direction and amplitude of movement, so that recurring movement patterns can be captured as words of variable length. Using machine learning and minimum redundancy maximum relevance feature selection, we found that &amp;quot;wordbook encoding&amp;quot; contributed significantly to characterization of activities. We also developed novel &amp;quot;shape features&amp;quot; to model the shape of eye movement signals over consecutive time windows, and showed that this enabled robust detection of smooth pursuit eye movements (these movements occur when our eyes follow moving objects). This is a landmark development as smooth pursuits have previously been ignored in research on eye tracking for human-computer interaction.</gtr:description><gtr:exploitationPathways>The findings can be taken forward in development of eye tracking systems and applications, in particular for mining of eye movement data for patterns that correlate with activities and mental health. They also can inform new creative uses of eye-tracking for human-computer interaction.</gtr:exploitationPathways><gtr:id>B7BE74B9-2107-4439-B2F4-C2D43B60D969</gtr:id><gtr:outcomeId>54649ee4e77199.97781063</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare,Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>6B380633-4E0E-4023-961F-4ADD64E2934E</gtr:id><gtr:title>Multimodal recognition of reading activity in transit using body-worn sensors</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Applied Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6f62264f285d5503ad6f234be9e0e513"><gtr:id>6f62264f285d5503ad6f234be9e0e513</gtr:id><gtr:otherNames>Bulling A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545ec1d59d9ee2.64156887</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4949E655-CB39-4F0E-9114-DA825ACD9A0B</gtr:id><gtr:title>Toward Mobile Eye-Based Human-Computer Interaction</gtr:title><gtr:parentPublicationTitle>IEEE Pervasive Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6f62264f285d5503ad6f234be9e0e513"><gtr:id>6f62264f285d5503ad6f234be9e0e513</gtr:id><gtr:otherNames>Bulling A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>545ec1cf477bc1.80263785</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>965FA5C4-CF36-4024-8688-EBBA09E67F32</gtr:id><gtr:title>Wearable eye tracking for mental health monitoring</gtr:title><gtr:parentPublicationTitle>Computer Communications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/10b3d8ca84ce3d0c09b2b0ad2bfdcef6"><gtr:id>10b3d8ca84ce3d0c09b2b0ad2bfdcef6</gtr:id><gtr:otherNames>Vidal M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545ec1cbc20a49.11148366</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4B374D4A-C554-4473-9F0D-88583064F367</gtr:id><gtr:title>Performance metrics for activity recognition</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Intelligent Systems and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08df285307de7473b5634cdc0739fbb5"><gtr:id>08df285307de7473b5634cdc0739fbb5</gtr:id><gtr:otherNames>Ward J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545ec1d44a8b89.25688526</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>36328C9B-0794-4083-A0DA-B7CDDAB81AC4</gtr:id><gtr:title>Eye movement analysis for activity recognition using electrooculography.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6f62264f285d5503ad6f234be9e0e513"><gtr:id>6f62264f285d5503ad6f234be9e0e513</gtr:id><gtr:otherNames>Bulling A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>545ec1d062ffc0.09130406</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H005064/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>