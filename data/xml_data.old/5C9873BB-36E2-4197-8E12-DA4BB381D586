<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Informatics</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/57C1419F-ADC3-4215-AC70-0DC50AB032C5"><gtr:id>57C1419F-ADC3-4215-AC70-0DC50AB032C5</gtr:id><gtr:firstName>Sharon</gtr:firstName><gtr:surname>Goldwater</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FR006660%2F1"><gtr:id>5C9873BB-36E2-4197-8E12-DA4BB381D586</gtr:id><gtr:title>Modeling the Development of Phonetic Representations</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/R006660/1</gtr:grantReference><gtr:abstractText>Listeners differ cross-linguistically in the cues, or perceptual dimensions, they rely on when perceiving speech. For example, Japanese listeners categorizing English [l] and [r] do not rely on the same acoustic features of the speech signal (e.g., the third formant) that native English listeners do. These cross-linguistic differences are typically attributed to listeners' knowledge of sound categories. For example, English listeners know that [l] and [r] are two categories, whereas Japanese listeners know that [l] and [r] are part of the same category; and this knowledge is hypothesized to affect their reliance on dimensions.

The proposed research tests the hypothesis that category knowledge is not necessary for perceptual dimension learning to occur. Drawing on representation learning methods that have performed well in low-resource automatic speech recognition, where extensive labeled training data are not available, two models are proposed that learn dimensions without relying on knowledge of sound categories. The first relies on temporal information as a proxy for category knowledge, while the second relies on top-down information from similar words, which infants have been shown to use. These models are evaluated on their ability to predict listeners' discrimination judgments from speech perception experiments on native and non-native contrasts, when trained on the same language background as the listeners.</gtr:abstractText><gtr:potentialImpactText>Building models of perceptual dimension learning can significantly impact science and health. For example, greater insight into dimension learning can help us to understand the difficulties that adults face when acquiring a second language, and possibly to develop better methods for teaching second languages. In addition, research suggests that dyslexia and some language learning impairments can be caused by problems with low-level speech processing. A better understanding of perceptual learning could lead to better diagnosis and treatment in these cases. 

Our work could also lead to improved speech technology for low-resource languages. State-of-the-art systems rely on hundreds of hours of hand-transcribed data, which is time-consuming and expensive to create. Consequently, high-quality systems are only available for a few languages, and speech recognition researchers are increasingly looking for ways to develop systems that learn from audio alone. Our proposed work draws on existing methods, but explores these in ways that have not been done within the ASR community, including more carefully controlled comparisons between methods and comparisons against human perceptual data. We anticipate that our investigation will lead to insights and perhaps new techniques that can transfer directly to the field of speech recognition and ultimately lead to systems that learn more effectively using little or no transcribed audio. Such systems could become important tools for documenting and analyzing endangered and minority languages, and could help make speech technology more universally available, not just to majority language speakers in rich countries, as most systems are today.</gtr:potentialImpactText><gtr:fund><gtr:end>2021-01-14</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2018-01-15</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>295140</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">ES/R006660/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E3D10FCE-1658-4591-88D0-80A32BF192D0</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Computational Linguistics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>4685997F-6C82-4108-9425-1593E206E9F2</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Language Acquisition</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>78C47607-4818-4A9C-B510-D5D9A368C83F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Phonetics</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>