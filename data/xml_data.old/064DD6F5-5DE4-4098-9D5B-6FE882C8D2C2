<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/CE43EBFA-3FC9-44BC-B6FF-001F11664C46"><gtr:id>CE43EBFA-3FC9-44BC-B6FF-001F11664C46</gtr:id><gtr:name>Aarhus University</gtr:name><gtr:address><gtr:line1>Nordre Ringgade 1</gtr:line1><gtr:line4>Aarhus C</gtr:line4><gtr:line5>DK-8000</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CE43EBFA-3FC9-44BC-B6FF-001F11664C46"><gtr:id>CE43EBFA-3FC9-44BC-B6FF-001F11664C46</gtr:id><gtr:name>Aarhus University</gtr:name><gtr:address><gtr:line1>Nordre Ringgade 1</gtr:line1><gtr:line4>Aarhus C</gtr:line4><gtr:line5>DK-8000</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/98BF5121-C99E-4246-971D-E7C4C3783EB4"><gtr:id>98BF5121-C99E-4246-971D-E7C4C3783EB4</gtr:id><gtr:name>University of Texas at Austin</gtr:name><gtr:address><gtr:line1>One University Station</gtr:line1><gtr:postCode>78712</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8A621CD6-4DE9-4B35-992C-631871D7793E"><gtr:id>8A621CD6-4DE9-4B35-992C-631871D7793E</gtr:id><gtr:name>University of Rochester</gtr:name><gtr:address><gtr:line1>PO Box 270001</gtr:line1><gtr:line4>Rochester</gtr:line4><gtr:line5>New York</gtr:line5><gtr:postCode>NY 14627</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2A80FFDA-3B8B-43BA-80C3-3AA850B49BA1"><gtr:id>2A80FFDA-3B8B-43BA-80C3-3AA850B49BA1</gtr:id><gtr:name>University of the West of England</gtr:name><gtr:address><gtr:line1>Coldharbour Lane</gtr:line1><gtr:line2>Frenchay</gtr:line2><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS16 1QY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FFD80473-A5D1-43FF-B423-C25E30E16BD4"><gtr:id>FFD80473-A5D1-43FF-B423-C25E30E16BD4</gtr:id><gtr:name>Max Planck</gtr:name><gtr:address><gtr:line1>Stuhlsatzenhausweg 85</gtr:line1><gtr:line4>Saarbruecken</gtr:line4><gtr:line5>D-66123</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/906D1A88-BD93-4530-8B20-ED28F16EBECB"><gtr:id>906D1A88-BD93-4530-8B20-ED28F16EBECB</gtr:id><gtr:name>Aardman Animations Ltd</gtr:name><gtr:address><gtr:line1>Gas Ferry Road</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS1 6UN</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7BC6FCF2-358C-45C5-A5A7-E859DD754711"><gtr:id>7BC6FCF2-358C-45C5-A5A7-E859DD754711</gtr:id><gtr:name>Lund University</gtr:name><gtr:address><gtr:line1>Box 117</gtr:line1><gtr:line2>S-221 00</gtr:line2><gtr:line4>Lund</gtr:line4><gtr:postCode>S-221 00</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Sweden</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C8EEEFEF-593E-4880-BD7C-14D9E2D6BBC5"><gtr:id>C8EEEFEF-593E-4880-BD7C-14D9E2D6BBC5</gtr:id><gtr:name>Purdue University</gtr:name><gtr:address><gtr:line1>West Lafayette</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/27812A1D-D032-4A2A-8CC1-4EEE59C345C4"><gtr:id>27812A1D-D032-4A2A-8CC1-4EEE59C345C4</gtr:id><gtr:name>Thales Research and Technology UK Ltd</gtr:name><gtr:address><gtr:line1>Worton Drive</gtr:line1><gtr:line2>Worton Grange Business Park</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG2 0SB</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8B5A412B-27F8-4A77-A736-0890D6DE34C6"><gtr:id>8B5A412B-27F8-4A77-A736-0890D6DE34C6</gtr:id><gtr:name>ARRI Group</gtr:name><gtr:address><gtr:line1>Hohenzollerndamm 150</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EB6BC5F0-7CE0-4A35-8DDC-CBA698D07510"><gtr:id>EB6BC5F0-7CE0-4A35-8DDC-CBA698D07510</gtr:id><gtr:name>Academy of Motion Picture</gtr:name><gtr:address><gtr:line1>1313 Vine Street</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5FA451D-4FA7-423E-93F8-046141371F35"><gtr:id>D5FA451D-4FA7-423E-93F8-046141371F35</gtr:id><gtr:name>University of Western Australia</gtr:name><gtr:address><gtr:line1>35 Stirling Highway</gtr:line1><gtr:line4>Crawley</gtr:line4><gtr:line5>WA 6009</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Australia</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E4F53ABD-8ECF-4261-A666-0358CD9BCCBD"><gtr:id>E4F53ABD-8ECF-4261-A666-0358CD9BCCBD</gtr:id><gtr:name>Qinetiq Ltd</gtr:name><gtr:address><gtr:line1>St Andrews Road</gtr:line1><gtr:line4>Malvern</gtr:line4><gtr:line5>Worcestershire</gtr:line5><gtr:postCode>WR14 3PS</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/60D5ACA5-CD46-4D4F-B79D-0F574E16F291"><gtr:id>60D5ACA5-CD46-4D4F-B79D-0F574E16F291</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Bull</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F47B5EEA-B32F-40B5-84BA-681344D7422B"><gtr:id>F47B5EEA-B32F-40B5-84BA-681344D7422B</gtr:id><gtr:firstName>J</gtr:firstName><gtr:surname>Burn</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/9A170476-CABE-435B-BAB0-6267BB7125C8"><gtr:id>9A170476-CABE-435B-BAB0-6267BB7125C8</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>Roberts</gtr:surname><gtr:orcidId>0000-0002-4540-6683</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3F5FF400-3C1E-434C-A2E6-73F6536F2ECA"><gtr:id>3F5FF400-3C1E-434C-A2E6-73F6536F2ECA</gtr:id><gtr:firstName>Iain</gtr:firstName><gtr:otherNames>Donald</gtr:otherNames><gtr:surname>Gilchrist</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/1B1F2323-F669-465D-A55F-643C58962339"><gtr:id>1B1F2323-F669-465D-A55F-643C58962339</gtr:id><gtr:firstName>Casimir</gtr:firstName><gtr:surname>Ludwig</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/1F479B68-0318-44DF-A742-B05D939C4CA8"><gtr:id>1F479B68-0318-44DF-A742-B05D939C4CA8</gtr:id><gtr:firstName>Innes</gtr:firstName><gtr:otherNames>Cameron</gtr:otherNames><gtr:surname>Cuthill</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/861DC744-FFFA-4D15-B827-C652F7F7BB58"><gtr:id>861DC744-FFFA-4D15-B827-C652F7F7BB58</gtr:id><gtr:firstName>Nishan</gtr:firstName><gtr:surname>Canagarajah</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM000885%2F1"><gtr:id>064DD6F5-5DE4-4098-9D5B-6FE882C8D2C2</gtr:id><gtr:title>Vision for the Future</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M000885/1</gtr:grantReference><gtr:abstractText>Approximately half the cortical matter in the human brain is involved in processing visual information, more than for all of the other senses combined. This reflects the importance of vision for function and survival but also explains its role in entertaining us, training us and informing our decision-making processes. However, we still understand relatively little about visual processes in naturalistic environments and this is why it is such an important research area across such a broad range of applications.

Vision is important: YouTube video accounts for 25% of all internet traffic and in the US, Netflix accounts for 33% of peak traffic; by 2016 video is predicted by CISCO to account for 54% of all traffic (86% if P2P video distribution is included) where the total IP traffic is predicted to be 1.3 zettabytes. Mobile network operators predict a 1000 fold increase in demand over the next 10 years driven primarily by video traffic. At the other extreme, the mammalian eye is used by cheetahs to implement stable locomotion over natural terrain at over 80km/h and by humans to thread a needle with sub-millimetre accuracy or to recognise subtle changes in facial expression. The mantis shrimp uses 12 colour channels (humans use only three) together with polarisation and it possesses the fastest and most accurate strike in the animal kingdom. 

Vision is thus central to the way animals interact with the world. A deeper understanding of the fundamental aspects of perception and visual processing in humans and animals, across the domains of immersion, movement and visual search, coupled with innovation in engineering solutions, is therefore essential in delivering future technology related to consumer, internet, robotic and environmental monitoring applications.

This project will conduct research across three interdisciplinary strands: Visual Immersion, Finding and Hiding Things, and Vision in Motion. These are key to understanding how humans interact with the visual world. By drawing on knowledge and closely coupled research across computer science, electronic engineering, psychology and biology we will deliver radically new approaches to, and solutions in, the design of vision based technology. 

We recognise that it is critical to balance high risk research with the coherence of the underlying programme. We will thus instigate a new sandpit approach to ideas generation where researchers can develop their own mini-projects. This will be aligned with a risk management process using peer review to ensure that the full potential of the grant is realised. The management team will periodically and when needed, seek independent advice through a BVI Advisory panel. 

Our PDRAs will benefit in ways beyond those on conventional grants. They will for example be mentored to: 
i) engage in ideas generation workshops, defining and delivering their own mini-projects within the programme; 
ii) develop these into full proposals (grants or fellowships) if appropriate; 
iii) undertake secondments to international collaborator organisations, enabling them to gain experience of different research cultures; 
iv) lead the organisation of key events such as the BVI Young Researchers' Colloquium; v) be trained as STEM ambassadors to engage in outreach activities and public engagement; and 
vii) explore exploitation of their intellectual property. 
Finally we will closely link BVI's doctoral training activities to this grant, providing greater research leverage and experience of research supervision for our staff.</gtr:abstractText><gtr:potentialImpactText>Vision is central to the way humans interact with the world. A deeper understanding of the fundamental aspects of human perception and visual processing in humans and animals, will lead to innovation in engineering solutions. Our programme will therefore be instrumental in delivering future technology related to consumer, internet, robotic and environmental monitoring applications.

Through a closely coupled research programme across engineering, computer science, psychology and biology, this grant will deliver in each of these areas. Firstly, this research will be relevant to research communities across disciplines: it will benefit psychologists in generating realistic real world scenarios and data sets and results which help us to understand the way humans interact with the visual world; It will benefit biologists in providing visual models for understanding the evolution and ecology of vision; it will benefit engineers and computer scientists in providing radically new approaches to solving technology problems.

The research in Visual Immersion will be of great significance to the ICT community commercially in terms of future video acquisition formats, new compression methods, new quality assessment methods and immersive measurements. This will inform the future of immersive consumer products - 'beyond 3D'. In particular the project will deliver an understanding of the complex interactions between video parameters in delivering a more immersive visual experience. This will not only be relevant to entertainment, but also in visual analytics, surveillance and healthcare. Our results are likely to inform future international activity in video format standardisation in film, broadcast and internet delivery, moving thinking from 'end to end solutions' to the 'creative continuum' where content creation, production delivery, display, consumption and quality assessment, are all intimately interrelated. Our work will also help us to understand how humans interact with complex environments, or are distracted by environmental changes - leading to better design of interfaces for task based operations and hence improved situational awareness.

In terms of Finding and Hiding Things - impact will be created in areas such as visual camouflage patterns, offering a principled design framework which takes account of environmental factors and mission characteristics. It will also provide enhanced means of detecting difficult targets, through better understanding of the interactions between task and environment. It will provide benefits in application areas such as situational awareness and stealthy operation - highly relevant to surveillance applications. The work will also contribute in related areas such as environmental visual impact of entities such as windfarms, buildings or pylons. Hence the relevance of the research to energy providers and civil engineers. Finally, visual interaction with complex scenes is a key enabler for the 'internet of things'.

In the case of Vision in Motion, the research will deliver impact in the design of truly autonomous machines, exploiting our understanding of the way in which animals and humans adapt to the environment. The beneficiaries in this case will be organisations in the commercial, domestic and surveillance robotics or UAV sectors. Furthermore, understanding the interactions between motion and camouflage has widespread relevance to environmental applications and to anomaly detection. Through a better understanding of the effects of motion we can design improved visual acquisition methods, better consumer interfaces, displays and content formats. This will be of broad benefit across the ICT sector, with particular relevance to designers of visual interfaces and to content providers in the entertainment sector. Furthermore the research will benefit those working in healthcare - for example in rehabilitation or in the design of point of care systems incorporating exocentric vision systems</gtr:potentialImpactText><gtr:fund><gtr:end>2020-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1362874</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>BBC Immersive Technology Laboratory</gtr:description><gtr:id>877B1FA8-B68D-48B0-B9C4-279588789952</gtr:id><gtr:impact>New method of perceptual quantisation for HDR HEVC
Analysis of BBC archive in terms of feature classification</gtr:impact><gtr:outcomeId>56d87f77c26c43.72504528-1</gtr:outcomeId><gtr:partnerContribution>Provision of REDUX
Support for PhD students
Collaboration on perceptual quantisation
Secondment of BBC employees</gtr:partnerContribution><gtr:piContribution>High Dynamic range coding optimisation for HEVC
Perceptual video compression results
REDUX database analytics</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Aarhus University</gtr:collaboratingOrganisation><gtr:country>Denmark, Kingdom of</gtr:country><gtr:description>Immersive Assessments</gtr:description><gtr:id>96E40386-A4A9-427A-8BB9-A2DFDC59DBEC</gtr:id><gtr:impact>None yet - ongoing</gtr:impact><gtr:outcomeId>58bd88ac1c2810.19197257-1</gtr:outcomeId><gtr:partnerContribution>Ongoing collaboration</gtr:partnerContribution><gtr:piContribution>Collaboration with Aarhus Univ on development of Immersive assessment methods.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Keynote: IET ISP</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>47ADC3A7-A5C6-4EE0-BBA9-3345A0C41ADD</gtr:id><gtr:impact>Keynote Lecture IET ISP- Perceptual Video coding</gtr:impact><gtr:outcomeId>56d87bf131bfb2.75410079</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote: EPSRC VIHM Workshop</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D47825AE-29A9-4753-BD60-19028FF013FC</gtr:id><gtr:impact>Keynote lecture EPSRC Vision in Humans and Machines Workshop Bath 2016</gtr:impact><gtr:outcomeId>56d87c41e8f6f1.00898324</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Exploited in spin off Azul optics</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>9F1D4B4A-A2FA-43F4-8A65-A090C90554A0</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d8743eec34f4.20454013</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This grant has been active for two years of its 5 year duration. It has already provided partial funding for 10 RAs: Fan Zhang, Felix Mercer Moss, Guarav Malhotra, David Gibson, Shelby temple, Pui Anantrasirichai, Paul Hill, Ilse Daly, Steve Hinde and Henry Knowles. 

The early phase of the grant has already made significant contributions: 

1. Insight into the psychovisual durations required for subjective video assessment - showing that durations can be reduced from the current 10 secs to 3 seconds without statistically significant changes to mean opinion scores.
2. Modifications to the Rate Quality Optimisation Process for future video standards using a content-adaptive approach to the selection of Lagrangian multipliers and QP offsets
3. The development of a new transform (The undecimated dual tree complex wavelet transform) that is showing enhanced performance for image feature description for classification and detection applications.
4. New computational camera technology that supports flexible parameterisation during video acquisition. 
5. New perceptual approaches to image denoising and image fusion. This includes analysis of the frequently used contrast sensitivity function in the context of spectral weighting. We have found that alternatives to the usual Gabor-based CSF may be more appropriate when processing images formed using other transforms (e.g wavelets)
6. We have undertaken a study collaboratively with BBC R&amp;amp;;D that has analysed all video content broadcast over the past 7 years on BBC. Preliminary results show that we can define descriptors that characterise the content. 
7. Temple has demonstrated the link between human sensitivity to polarized light and macular degeneration, leading to a spin off Azul Optics.
8. Salience and priority estimation for the human visual system during locomotion. We have created a robust priority map from probabilities of gaze fixations. Texture-based features and CNN based features are employed. Two training streams are used for two fixation types: i) homogeneous area for a safe foot placement and ii) area around edge for awareness. The results show significant promise and much higher correlation with eye tracked data than the prior art.
9. New ways of mitigating the effects of atmospheric turbulence on long range imagery.</gtr:description><gtr:exploitationPathways>1. Modification of video presentation durations in future subjective testing methodologies
2. Contributions to future compression standards such as H.266
3. Enhanced more robust features (based on the Undecimated DT-CWT) for classification and detection applications.
4. Computational camera technology being developed in collaboration with DSTL. 
5. New perceptual approaches to image denoising and image fusion that are currently under development
6. Our work on Content classification will be useful in defining reduced test data sets and for online media analytics. 
7. Our work on predicting macular degeneration is now being taken to larger scale trials and could have major impact.
8. Salience and priority estimation for the human visual system during locomotion could have major impact on the design of control and navigation systems for autonomous vehicles, especially biped robots. 
9. Mitigating the effects of Atmospheric Turbulence in long range surveillance video.</gtr:exploitationPathways><gtr:id>78DF2056-C0B4-4C91-902F-52FEC43050EA</gtr:id><gtr:outcomeId>56d873efb27b71.05705968</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Manufacturing, including Industrial Biotechology,Retail,Security and Diplomacy,Transport</gtr:sector></gtr:sectors><gtr:url>http://www.bristol.ac.uk/vision-institute/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Collection of static and dynamic video textures for compression testing</gtr:description><gtr:id>6C657AB8-B003-477B-876D-EA1436B7EDFE</gtr:id><gtr:impact>Used by several groups around the world</gtr:impact><gtr:outcomeId>56d8762701f7c4.88706249</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>BVI Texture database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://data.bris.ac.uk/datasets/1if54ya4xpph81fbo1gkpk5kk4/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Collection of high frame rate clips with associated metadata for testing and developing future immersive video formats</gtr:description><gtr:id>2ECC9CC7-C47D-4EC7-87C5-2E7F8DBDCF6B</gtr:id><gtr:impact>None at present</gtr:impact><gtr:outcomeId>56d8776c3ac756.55836741</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>BV High frame rate database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://data.bris.ac.uk/data/dataset/k8bfn0qsj9fs1rwnc2x75z6t7</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>Azul Optics</gtr:companyName><gtr:description>Exploiting polarisation vision in humans to detect Age Related Macular Degeneration. Established by BVI Platform Grant researcher Shelby Temple based on work partially completed under the grant.</gtr:description><gtr:id>69E5F55E-D215-4696-960B-6724535363F6</gtr:id><gtr:impact>None yet - product under development</gtr:impact><gtr:outcomeId>58bd8a62c649b8.37260850</gtr:outcomeId><gtr:url>http://azuloptics.com/</gtr:url><gtr:yearCompanyFormed>2016</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>90A426D2-0882-4F30-B791-EBA83AEB6E9F</gtr:id><gtr:title>Compressive imaging using approximate message passing and a Cauchy prior in the wavelet domain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6347e7c549c8b91ab9bdc2334ae3018"><gtr:id>e6347e7c549c8b91ab9bdc2334ae3018</gtr:id><gtr:otherNames>Hill P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58bd7cb2dc6d59.50469017</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F9F82EB5-4903-4088-91F1-58C0249695AE</gtr:id><gtr:title>Fixation identification for low-sample-rate mobile eye trackers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58bd7edb712e09.54272021</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5867510-69D3-4BBD-A945-630291BEECE6</gtr:id><gtr:title>HEVC enhancement using content-based local QP selection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1809274402a5f43e1a80a137e1986a33"><gtr:id>1809274402a5f43e1a80a137e1986a33</gtr:id><gtr:otherNames>Zhang F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58bd7db2485775.20012230</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C567F54D-E566-4ED9-A89A-B0CDEA7579A5</gtr:id><gtr:title>Predicting video rate-distortion curves using textural features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4c412788d3ddf7e12bfe77607c942ee5"><gtr:id>4c412788d3ddf7e12bfe77607c942ee5</gtr:id><gtr:otherNames>Katsenou A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a9bb271833cc7.29292452</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BAEEB6DC-675F-4BE7-A617-090DF8464BD1</gtr:id><gtr:title>High Dynamic Range Video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d895db59e431e55ec84105d0da4fab0d"><gtr:id>d895db59e431e55ec84105d0da4fab0d</gtr:id><gtr:otherNames>Zhang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd7db2825182.81199937</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>339C2E09-5171-427C-BBB5-AFBC95A2059A</gtr:id><gtr:title>The visibility of motion artifacts and their effect on motion quality</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a4e7ed976e0e31d84ae2e5d14a361e7"><gtr:id>6a4e7ed976e0e31d84ae2e5d14a361e7</gtr:id><gtr:otherNames>Mackin A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58bd7f7a86d356.45616921</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>715C5AE5-B269-4317-9406-B7A98EED2142</gtr:id><gtr:title>Visual salience and priority estimation for locomotion using a deep convolutional neural network</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58bd7edba698d9.45087436</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D66FA0C5-7D91-4D92-9025-D3EA8B949F32</gtr:id><gtr:title>A Perception-Based Hybrid Model for Video Quality Assessment</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1809274402a5f43e1a80a137e1986a33"><gtr:id>1809274402a5f43e1a80a137e1986a33</gtr:id><gtr:otherNames>Zhang F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d86443a97353.24124431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>44F45DCE-8B85-4D9A-A865-6D8D81129D39</gtr:id><gtr:title>Improved illumination invariant homomorphic filtering using the dual tree complex wavelet transform</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6347e7c549c8b91ab9bdc2334ae3018"><gtr:id>e6347e7c549c8b91ab9bdc2334ae3018</gtr:id><gtr:otherNames>Hill P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15206149</gtr:issn><gtr:outcomeId>58bd7cb29977e7.06920051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C2066D70-FE9D-44F8-B92F-8CF9AD81267D</gtr:id><gtr:title>Robust texture features based on undecimated dual-tree complex wavelets and local magnitude binary patterns</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d8752910c207.00535201</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>235D8869-9F39-468A-85CF-A3FE202963B0</gtr:id><gtr:title>Contrast Sensitivity of the Wavelet, Dual Tree Complex Wavelet, Curvelet and Steerable Pyramid Transforms.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6347e7c549c8b91ab9bdc2334ae3018"><gtr:id>e6347e7c549c8b91ab9bdc2334ae3018</gtr:id><gtr:otherNames>Hill P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>585d416409d682.03212843</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38D39C45-AE07-4045-B03F-692320E0EC46</gtr:id><gtr:title>High Frame Rates and the Visibility of Motion Artifacts</gtr:title><gtr:parentPublicationTitle>SMPTE Motion Imaging Journal</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a4e7ed976e0e31d84ae2e5d14a361e7"><gtr:id>6a4e7ed976e0e31d84ae2e5d14a361e7</gtr:id><gtr:otherNames>Mackin A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9bafef1f8c38.37428446</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>81DBE578-23F9-4645-994B-7AB16FD32953</gtr:id><gtr:title>Support for reduced presentation durations in subjective video quality assessment</gtr:title><gtr:parentPublicationTitle>Signal Processing: Image Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e6dccd3d8651e8bdb660107cce52df1"><gtr:id>2e6dccd3d8651e8bdb660107cce52df1</gtr:id><gtr:otherNames>Mercer Moss F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>09235965</gtr:issn><gtr:outcomeId>58bd7d3b452c42.84185605</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1E96BC89-6685-455E-AC2C-443CC0B7EB3B</gtr:id><gtr:title>Fixation Prediction and Visual Priority Maps for Biped Locomotion.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>5a2fea3e2d65e0.55917614</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4FBCBC66-E1A5-49C8-BDE3-C5EDE5222934</gtr:id><gtr:title>Perceptual Image Fusion Using Wavelets.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6347e7c549c8b91ab9bdc2334ae3018"><gtr:id>e6347e7c549c8b91ab9bdc2334ae3018</gtr:id><gtr:otherNames>Hill P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5a9bb271221ac7.67909780</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FE64FA8-2ABB-47F5-BF14-DFFEC5D77A64</gtr:id><gtr:title>Line Detection as an Inverse Problem: Application to Lung Ultrasound Imaging.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on medical imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0278-0062</gtr:issn><gtr:outcomeId>5a2fecdec325f1.36815324</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>985B7D70-DDD0-4F2A-8309-609299359D98</gtr:id><gtr:title>Dynamic polarization vision in mantis shrimps.</gtr:title><gtr:parentPublicationTitle>Nature communications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f8a6b357aab5664757c4dec541a485a9"><gtr:id>f8a6b357aab5664757c4dec541a485a9</gtr:id><gtr:otherNames>Daly IM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2041-1723</gtr:issn><gtr:outcomeId>5a35f7930dd2a7.73203980</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2D2D05D1-5146-46BB-A6FF-F0037967411C</gtr:id><gtr:title>Line detection in speckle images using Radon transform and l1 regularization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>15206149</gtr:issn><gtr:outcomeId>5a9babb90ae4f9.85946680</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CB3717DD-E24E-4D87-89E5-1D7FF4CA0495</gtr:id><gtr:title>On the Optimal Presentation Duration for Subjective Video Quality Assessment</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e6dccd3d8651e8bdb660107cce52df1"><gtr:id>2e6dccd3d8651e8bdb660107cce52df1</gtr:id><gtr:otherNames>Mercer Moss F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d86443d0db23.46948676</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1FFCDD56-E672-489E-8538-99A904FD7881</gtr:id><gtr:title>Undecimated Dual-Tree Complex Wavelet Transforms</gtr:title><gtr:parentPublicationTitle>Signal Processing: Image Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6347e7c549c8b91ab9bdc2334ae3018"><gtr:id>e6347e7c549c8b91ab9bdc2334ae3018</gtr:id><gtr:otherNames>Hill P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0923-5965</gtr:issn><gtr:outcomeId>5a9bac78910a25.53395584</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4DF0AB26-272B-4BF7-B4F3-59847CCA7F92</gtr:id><gtr:title>What's on TV: A large scale quantitative characterisation of modern broadcast video content</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d5064487467d82ced29a3b992cd68786"><gtr:id>d5064487467d82ced29a3b992cd68786</gtr:id><gtr:otherNames>Moss F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58bd7d3b7d1e18.45348405</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EE9D74F9-F73B-4926-BD1D-CD58AD37E5C2</gtr:id><gtr:title>Time-varying decision boundaries: insights from optimality analysis.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e38d193b66fdac33c12eb3071cb09939"><gtr:id>e38d193b66fdac33c12eb3071cb09939</gtr:id><gtr:otherNames>Malhotra G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>5a3bbf84232623.23492604</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A410E3F0-B2DE-4E3F-B827-F22B867D1531</gtr:id><gtr:title>Video texture analysis based on HEVC encoding statistics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd5e2b29cf09cc5abc73e6a61fa23a83"><gtr:id>fd5e2b29cf09cc5abc73e6a61fa23a83</gtr:id><gtr:otherNames>Afonso M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a9bb27159a2d2.34348333</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M000885/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>