<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/A715CB4E-6888-4FE7-92F3-7C01A22A4C29"><gtr:id>A715CB4E-6888-4FE7-92F3-7C01A22A4C29</gtr:id><gtr:name>STFC - Laboratories</gtr:name><gtr:department>Scientific Computing Department</gtr:department><gtr:address><gtr:line1>Rutherford Appleton Laboratory</gtr:line1><gtr:line2>Harwell Science and Innovation Campus</gtr:line2><gtr:line3>Chilton</gtr:line3><gtr:line4>Didcot</gtr:line4><gtr:postCode>OX11 0QX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A715CB4E-6888-4FE7-92F3-7C01A22A4C29"><gtr:id>A715CB4E-6888-4FE7-92F3-7C01A22A4C29</gtr:id><gtr:name>STFC - Laboratories</gtr:name><gtr:address><gtr:line1>Rutherford Appleton Laboratory</gtr:line1><gtr:line2>Harwell Science and Innovation Campus</gtr:line2><gtr:line3>Chilton</gtr:line3><gtr:line4>Didcot</gtr:line4><gtr:postCode>OX11 0QX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E5CA8043-BC60-4750-95DC-E141FA711291"><gtr:id>E5CA8043-BC60-4750-95DC-E141FA711291</gtr:id><gtr:firstName>David Robert</gtr:firstName><gtr:surname>Emerson</gtr:surname><gtr:orcidId>0000-0002-6085-5049</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK038451%2F1"><gtr:id>E34B0738-27E5-4A64-B4A8-07BFB5B4C5D6</gtr:id><gtr:title>Future-proof massively-parallel execution of multi-block applications</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K038451/1</gtr:grantReference><gtr:abstractText>For many years, increasing the clock frequency of microprocessors has led to steady improvements in performance of computer applications. This gave an almost free performance boost to the speed of applications without having to re-write software for each new generation of processors. However, increasing the performance of processors in this manner led to an unsustainable increase in energy consumption. Thus, to gain higher performance chip developers now rely on multiple cores operating in parallel. The latest CPUs have up to 10 cores, each with a vector unit producing up to 8 single precision floating point results per clock cycle, while the latest graphics processors (GPUs) have up to 2688 much simpler cores operating in groups of 32.

This move into manycore computing has led to considerable hardware innovation, and it is likely that the next 10 years will see further rapid evolution in computer architectures. This poses huge challenges to application developers who naturally wish to concentrate on their engineering and scientific applications and how best to model them, without having to worry about the details of modern computer architectures. To address this, there are a range of efforts within scientific computing to develop high-level software packages or frameworks so that the application developer can specify what they want to be computed at a high level, and then the package takes care of the implementation details.

Building on prior EPSRC-funded research to develop a framework called OP2 for unstructured grid applications, this proposal aims to develop a future-proof extension called OPS to handle the needs of multi-block structured grid applications. Developers' applications can be written in FORTRAN or C, using a carefully-designed application programming interface (API), and then OPS generates customised code for the implementation on different hardware target platforms.

As well as customising for the different hardware, two other optimisation approaches will be adopted. One is the use of ``tiling'' to overlap the execution of parallel loops which are usually executed sequentially. This improves both performance and energy efficiency by reusing data within the cache, cutting down on the number of times data is moved between the processor and the main memory. This is something which is becoming increasingly important on modern architectures because the energy cost and time taken for data movement is much greater than for floating point operations. 

The other optimisation is the use of run-time optimisation for applications which execute for a long time. The backend implementations are parameterised, with parameters controlling aspects such as the number of threads in a thread block, or the size of a ``tile'' in the tiling optimisation. The optimal values for these parameters are not known a priori, and it could significantly affect the performance. By dynamically varying the values, and timing the consequential changes in performance, we can implement heuristics to iteratively improve the parameter values during the execution.

The new OPS framework will be assessed, both for performance and ease-of-use, by applying it to two important academic CFD codes, ROTOR developed at Bristol by Prof. Chris Allen, and SBLI developed by at Southampton by Prof. Neil Sandham. As well as being important codes in their own right, these are also representative of the needs of other codes within CCP12 (Computational Engineering), the UK Turbulence Consortium, and the UK Applied Aerodynamics Consortium.</gtr:abstractText><gtr:fund><gtr:end>2016-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>102640</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>High End Computing Call</gtr:description><gtr:end>2020-05-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/P022243/1</gtr:fundingRef><gtr:id>0D796687-054C-426D-BD81-72B5A5D9FA8D</gtr:id><gtr:outcomeId>58c2f32f19d4d8.80134583</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work on OPS to aid portable parallel computing of a lattice Boltzmann code led to a successful bid for further fundin as part of the UKCOMES activity.</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>565EEE4B-3873-4C5F-A633-4CA3B73CBBD8</gtr:id><gtr:impactTypes/><gtr:outcomeId>56e2be59e7ed83.15656222</gtr:outcomeId><gtr:sector>Aerospace, Defence and Marine,Energy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>362E78CB-4F60-452F-915B-77D3661C7EDE</gtr:id><gtr:title>Analysis of non-physical slip velocity in lattice Boltzmann simulations using the bounce-back scheme</gtr:title><gtr:parentPublicationTitle>Journal of Computational Science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e84e3707839d90302345838b73f672b7"><gtr:id>e84e3707839d90302345838b73f672b7</gtr:id><gtr:otherNames>Meng J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fee4cab8c82.49644868</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B1584BDC-CCD9-4F36-B1FA-6E7DE1A56F5E</gtr:id><gtr:title>Block Structured Lattice Boltzmann Simulation Using OPS High-Level Abstraction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e84e3707839d90302345838b73f672b7"><gtr:id>e84e3707839d90302345838b73f672b7</gtr:id><gtr:otherNames>Meng J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6cb2571f6e9.64392647</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AFD309F9-93BD-479C-8C31-13D5E6774942</gtr:id><gtr:title>A comparative study of boundary conditions for lattice Boltzmann simulations of high Reynolds number flows</gtr:title><gtr:parentPublicationTitle>Computers &amp; Fluids</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1e13da6ac111891dbc4771a045e4540b"><gtr:id>1e13da6ac111891dbc4771a045e4540b</gtr:id><gtr:otherNames>Hu K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe0c0bbb419.34467357</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21DA02B1-A5DD-4AD9-9D01-06630A6AD959</gtr:id><gtr:title>Block Structured Lattice Boltzmann Simulation Using OPS High-Level Abstraction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e84e3707839d90302345838b73f672b7"><gtr:id>e84e3707839d90302345838b73f672b7</gtr:id><gtr:otherNames>Meng J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6ca62428b36.14128964</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FF896E22-757F-4290-B6F8-039C80530539</gtr:id><gtr:title>Loop Tiling in Large-Scale Stencil Codes at Run-time with OPS</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Parallel and Distributed Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/14a678e355284efe1e4eb5f5e96036c7"><gtr:id>14a678e355284efe1e4eb5f5e96036c7</gtr:id><gtr:otherNames>Reguly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a352bd81b3379.93638119</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K038451/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>9EAAD5EA-2E54-4986-942F-2E204958FE29</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>High Performance Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>01AC56BC-45B4-434D-B015-7C879327F09F</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Parallel Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>