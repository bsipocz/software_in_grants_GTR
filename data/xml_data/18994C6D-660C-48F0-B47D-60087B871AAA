<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/8319F78A-DCBD-49F6-BE00-78E1CD75CDA9"><gtr:id>8319F78A-DCBD-49F6-BE00-78E1CD75CDA9</gtr:id><gtr:name>University of York</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Heslington</gtr:line1><gtr:line4>York</gtr:line4><gtr:line5>North Yorkshire</gtr:line5><gtr:postCode>YO10 5DD</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8319F78A-DCBD-49F6-BE00-78E1CD75CDA9"><gtr:id>8319F78A-DCBD-49F6-BE00-78E1CD75CDA9</gtr:id><gtr:name>University of York</gtr:name><gtr:address><gtr:line1>Heslington</gtr:line1><gtr:line4>York</gtr:line4><gtr:line5>North Yorkshire</gtr:line5><gtr:postCode>YO10 5DD</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B83892A3-8FDF-47C1-A48C-043BCF174A0B"><gtr:id>B83892A3-8FDF-47C1-A48C-043BCF174A0B</gtr:id><gtr:firstName>William</gtr:firstName><gtr:otherNames>Alfred</gtr:otherNames><gtr:surname>Smith</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN028481%2F1"><gtr:id>18994C6D-660C-48F0-B47D-60087B871AAA</gtr:id><gtr:title>Example-based Inverse Rendering</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N028481/1</gtr:grantReference><gtr:abstractText>&amp;quot;Rendering&amp;quot; is the computer graphics process of producing an image from a 3D model. Huge progress has been made on this problem over the past 50 years as evidenced by the photorealistic visual effects in Hollywood movies and highly engaging realtime graphics in video games. &amp;quot;Inverse rendering&amp;quot; is the much harder computer vision process of recovering shape and material properties of an object or scene from one or more images. Progress in this area has been much slower than for forward rendering and very few methods exist that are applicable to real world data. The exception is the estimation of 3D shape where techniques such as multiview stereo do allow accurate 3D models to be recovered from relatively uncontrolled imagery.

However, it is the material properties of a surface that determine the way in which light is reflected and, hence, its appearance. Once these are known, it is possible to predict what an object will look like under any illumination condition - a so called &amp;quot;relightable&amp;quot; model. This dramatically expands the range of applications for which the model could be used. In the specific case of faces, relightable models find application in many areas including animation, gaming, forensic imaging, biometrics and visual effects.

In this project, the PI will visit the world-leading Centre for Visual Computing at the University of California, San Diego to collaborate on an &amp;quot;example-based&amp;quot; approach to the problem of inverse rendering for faces. The idea is to use a database of faces for which the intrinsic reflectance properties have been measured. Given images of a face to inverse render, the approach will be to select patches from the database that are consistent with the observations (via a forward rendering process) and which are also locally consistent to ensure plausible face appearance.</gtr:abstractText><gtr:potentialImpactText>The research in this overseas visit could impact the following industries:

Video games and virtual reality: The methods we develop will allow real human characters (actors or players) to be imported and photo-realistically rendered into video games or virtual worlds using only a commodity camera. This will bring technology previously only available to high end visual effects studios to indy games studios and even gamers.

Animation and visual effects: Present face capture technologies are limited in a number of ways. Animators or visual effects directors must choose whether to capture dynamic models but with illumination baked-in to the texture maps or static models with intrinsic texture information (although this latter option requires complex and costly equipment that few studios would have). Our techniques offer the promise of: 1. reducing costs of equipment and artist time spent manually removing illumination effects from texture maps, 2. allowing relightable dynamic performance capture, 3. removing requirements on studio lighting during capture.

Face recognition and forensic face imaging: Facial inverse rendering is intrinsically tied to the problem of comparing face images across different illumination conditions. This is a major challenge in face recognition and forensic face imaging and our results could have impact here. For example, our approach could be used to compute an illumination invariant model of a face from, for example, a CCTV image sequence of a face moving through a scene. The relightable model could either provide illumination normalised input to a face recognition system or could be shown to human witnesses.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-04-05</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-01-06</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>12862</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The key finding in this project is that it is possible to compute the shape of an object and lighting in the scene from a single &amp;quot;polarisation&amp;quot; image. A polarisation image captures more information than is present in a normal RGB colour image, telling us something about the wave properties of the light. This can be used to infer properties of the shape and reflectance properties of the object and lighting in the scene (a problem sometimes called &amp;quot;inverse rendering&amp;quot;). It is possible to capture a polarisation image instantaneously using a special type of camera. Usually, shape-from-polarisation is used to measure the orientation of the object surface at each pixel. If the desired output is a mesh or depth map, then additional processing is needed (a problem called &amp;quot;surface integration&amp;quot;) and the solution is very sensitive to noise. The novelty in our approach was to solve directly for a depth map - the first time this had been done for this problem. As well as showing that this is possible, we showed how the approach can be formulated in a way that makes it very easy and efficient to solve. Finally, we also showed that there is enough information in a polarisation image to estimate the lighting in the scene. This makes our approach entirely uncalibrated and so can be used to capture 3D objects outdoors.</gtr:description><gtr:exploitationPathways>One shot shape and lighting estimation in uncontrolled conditions has numerous applications. After presenting the work at a conference, I was contacted by a researcher at the European Space Agency who was interested in using the work to analyse space debris. It is easy to imagine applications in areas such as cultural heritage (artefact digitisation), agriculture (polarisation is already used for food inspection), creative industries (capturing 3D content for games or animation).</gtr:exploitationPathways><gtr:id>19777C27-7474-41AB-9368-15616F4A913B</gtr:id><gtr:outcomeId>58b83dde23ea84.55191775</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Agriculture, Food and Drink,Creative Economy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/N028481/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>