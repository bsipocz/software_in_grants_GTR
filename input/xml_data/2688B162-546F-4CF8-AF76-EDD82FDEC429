<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/22C9FF8A-DD74-4CE6-B9AC-88CC35DAD198"><gtr:id>22C9FF8A-DD74-4CE6-B9AC-88CC35DAD198</gtr:id><gtr:name>Pompeu Fabra University</gtr:name><gtr:address><gtr:line1>Placa de la Merce, 10</gtr:line1><gtr:line2>Edifici Ta'nger</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/686D9BAA-DD97-47C0-80A0-26916C5FB515"><gtr:id>686D9BAA-DD97-47C0-80A0-26916C5FB515</gtr:id><gtr:name>Internet Archive</gtr:name><gtr:address><gtr:line1>300 Funston Avenue</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/5E2B04DD-4A03-45ED-9892-61C5CCB8AC68"><gtr:id>5E2B04DD-4A03-45ED-9892-61C5CCB8AC68</gtr:id><gtr:name>Newcastle University</gtr:name><gtr:address><gtr:line1>1 Park Terrace</gtr:line1><gtr:line4>Newcastle Upon Tyne</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>NE1 7RU</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/70B12895-CFF5-4BCD-B338-4A8A38085B55"><gtr:id>70B12895-CFF5-4BCD-B338-4A8A38085B55</gtr:id><gtr:name>B3 Media</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/F7080BBD-1315-4A1E-BD87-321DCED6C788"><gtr:id>F7080BBD-1315-4A1E-BD87-321DCED6C788</gtr:id><gtr:name>Solid State Logic</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9DC10C1B-2C89-4038-9DD1-2E23F0947072"><gtr:id>9DC10C1B-2C89-4038-9DD1-2E23F0947072</gtr:id><gtr:name>Rough Trade Records</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/457FE97D-03BD-490B-912A-009EB66ED085"><gtr:id>457FE97D-03BD-490B-912A-009EB66ED085</gtr:id><gtr:name>Sustrans</gtr:name><gtr:address><gtr:line1>National Cycle Network Centre</gtr:line1><gtr:line2>2 Cathedral Square</gtr:line2><gtr:line3>College Green</gtr:line3><gtr:line4>Bristol</gtr:line4><gtr:postCode>BS1 5DD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/BBEB8F79-64A4-425B-82F8-1F41990DA540"><gtr:id>BBEB8F79-64A4-425B-82F8-1F41990DA540</gtr:id><gtr:name>Omnifone</gtr:name><gtr:address><gtr:line1>50 Brook Green</gtr:line1><gtr:postCode>W6 7BJ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/7111EF10-B5DF-4DA9-8324-2B55178F7871"><gtr:id>7111EF10-B5DF-4DA9-8324-2B55178F7871</gtr:id><gtr:name>Uni of Illinois at Urbana Champaign</gtr:name><gtr:address><gtr:line1>Admissions and Records</gtr:line1><gtr:line2>901 West Illinois Street</gtr:line2><gtr:postCode>61801</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/22C9FF8A-DD74-4CE6-B9AC-88CC35DAD198"><gtr:id>22C9FF8A-DD74-4CE6-B9AC-88CC35DAD198</gtr:id><gtr:name>Pompeu Fabra University</gtr:name><gtr:address><gtr:line1>Placa de la Merce, 10</gtr:line1><gtr:line2>Edifici Ta'nger</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/686D9BAA-DD97-47C0-80A0-26916C5FB515"><gtr:id>686D9BAA-DD97-47C0-80A0-26916C5FB515</gtr:id><gtr:name>Internet Archive</gtr:name><gtr:address><gtr:line1>300 Funston Avenue</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5E2B04DD-4A03-45ED-9892-61C5CCB8AC68"><gtr:id>5E2B04DD-4A03-45ED-9892-61C5CCB8AC68</gtr:id><gtr:name>Newcastle University</gtr:name><gtr:address><gtr:line1>1 Park Terrace</gtr:line1><gtr:line4>Newcastle Upon Tyne</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>NE1 7RU</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/70B12895-CFF5-4BCD-B338-4A8A38085B55"><gtr:id>70B12895-CFF5-4BCD-B338-4A8A38085B55</gtr:id><gtr:name>B3 Media</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F7080BBD-1315-4A1E-BD87-321DCED6C788"><gtr:id>F7080BBD-1315-4A1E-BD87-321DCED6C788</gtr:id><gtr:name>Solid State Logic</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9DC10C1B-2C89-4038-9DD1-2E23F0947072"><gtr:id>9DC10C1B-2C89-4038-9DD1-2E23F0947072</gtr:id><gtr:name>Rough Trade Records</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BBEB8F79-64A4-425B-82F8-1F41990DA540"><gtr:id>BBEB8F79-64A4-425B-82F8-1F41990DA540</gtr:id><gtr:name>Omnifone</gtr:name><gtr:address><gtr:line1>50 Brook Green</gtr:line1><gtr:postCode>W6 7BJ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/457FE97D-03BD-490B-912A-009EB66ED085"><gtr:id>457FE97D-03BD-490B-912A-009EB66ED085</gtr:id><gtr:name>Sustrans</gtr:name><gtr:address><gtr:line1>National Cycle Network Centre</gtr:line1><gtr:line2>2 Cathedral Square</gtr:line2><gtr:line3>College Green</gtr:line3><gtr:line4>Bristol</gtr:line4><gtr:postCode>BS1 5DD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7111EF10-B5DF-4DA9-8324-2B55178F7871"><gtr:id>7111EF10-B5DF-4DA9-8324-2B55178F7871</gtr:id><gtr:name>Uni of Illinois at Urbana Champaign</gtr:name><gtr:address><gtr:line1>Admissions and Records</gtr:line1><gtr:line2>901 West Illinois Street</gtr:line2><gtr:postCode>61801</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E"><gtr:id>598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E</gtr:id><gtr:name>Friedrich-Alexander University</gtr:name><gtr:address><gtr:line1>Schlossplatz 4</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9"><gtr:id>F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9</gtr:id><gtr:name>Microsoft Research Ltd</gtr:name><gtr:address><gtr:line1>21 Station Road</gtr:line1><gtr:postCode>CB1 2FB</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5FD42B0E-EC04-46BD-8C3A-0B0DE24B0096"><gtr:id>5FD42B0E-EC04-46BD-8C3A-0B0DE24B0096</gtr:id><gtr:firstName>Kevin</gtr:firstName><gtr:otherNames>Richard</gtr:otherNames><gtr:surname>Page</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/62D3CD4B-2318-4A02-B23E-646358C34B6D"><gtr:id>62D3CD4B-2318-4A02-B23E-646358C34B6D</gtr:id><gtr:firstName>Geraint</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Wiggins</gtr:surname><gtr:orcidId>0000-0002-1587-112X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8A8DACC5-8800-4993-97BA-A4B756C2EAE4"><gtr:id>8A8DACC5-8800-4993-97BA-A4B756C2EAE4</gtr:id><gtr:firstName>Joe</gtr:firstName><gtr:surname>Marshall</gtr:surname><gtr:orcidId>0000-0001-9666-786X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/36A5E062-AB03-468C-BCE4-8DCB8962C34B"><gtr:id>36A5E062-AB03-468C-BCE4-8DCB8962C34B</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:otherNames>David</gtr:otherNames><gtr:surname>Tolmie</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6021DB09-4697-4A4C-B959-8E13F32297DA"><gtr:id>6021DB09-4697-4A4C-B959-8E13F32297DA</gtr:id><gtr:firstName>Stuart</gtr:firstName><gtr:surname>Reeves</gtr:surname><gtr:orcidId>0000-0001-7145-3320</gtr:orcidId><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/16B18A15-07CC-41DC-93A6-ECA584229D1C"><gtr:id>16B18A15-07CC-41DC-93A6-ECA584229D1C</gtr:id><gtr:firstName>Steve</gtr:firstName><gtr:surname>Benford</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/40AFD92D-0858-4127-AA8C-F0BA43DA964B"><gtr:id>40AFD92D-0858-4127-AA8C-F0BA43DA964B</gtr:id><gtr:firstName>Gyorgy</gtr:firstName><gtr:surname>Fazekas</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/AC23FEDF-B8E8-46CB-BEA0-3C2933633B40"><gtr:id>AC23FEDF-B8E8-46CB-BEA0-3C2933633B40</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>De Roure</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/67CAA1DB-31CD-4307-A2B8-B3A2A41082E0"><gtr:id>67CAA1DB-31CD-4307-A2B8-B3A2A41082E0</gtr:id><gtr:firstName>Sebastian</gtr:firstName><gtr:surname>Ewert</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL019981%2F1"><gtr:id>2688B162-546F-4CF8-AF76-EDD82FDEC429</gtr:id><gtr:title>Fusing Semantic and Audio Technologies for Intelligent Music Production and Consumption</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/L019981/1</gtr:grantReference><gtr:abstractText>Music is probably the most pervasive of the performing arts, and perhaps, the most abused (think of your recent shopping trips!). It has tremendous power to influence our emotions, often subliminally. The advent of recording in the 19th Century made it possible to enjoy music at a time, and in a place, different from the performance. Compression, broadband and the ever increasing capacity to aggregate large collections mean that the issues confronting music consumers have totally changed in nature: equally so for professionals, such as broadcasters (playlists for radio, music for documentaries, etc.) and those at the creative heart of the process: musicians, sound engineers and producers. The recorded music industry has grappled unsuccessfully with digital technology and the rate of adoption of new technologies has been slow, ironically, mostly in fear of piracy and loss of revenue. Given the social and economic importance of music, it is vital that the industry's crisis is averted and its decline reversed. Simple semantics and metadata are already helping (for example in recommendation and sharing services) but this is just the beginning. The next generation semantic technologies that are the focus of this proposal have the power to exact the turnaround that music (and other content industries) needs but this should be established via a fundamental and principled exploration of how semantic technologies underpin music throughout the value chain.

The proposal brings the very latest technologies to bear on the complete industry, end-to-end, producer to consumer, making the production process more fruitful, the consumption process more engaging, and the delivery and intermediation more automated and robust. In this project we will address 3 premises: (i) that Semantic Web technologies should be deployed throughout the content value chain from producer to consumer; (ii) that advanced signal processing should be employed in the content production phases to extract &amp;quot;pure&amp;quot; features of perceptual significance and represent these in standard vocabularies; (iii) that this combination of semantic technologies and content-derived metadata leads to advantages (and new products and services) at many points in the value chain, from recording studio to end-user (listener) devices and applications.

The project will work with partners from industry - BBC R&amp;amp;D, Microsoft Research Cambridge and Omnifone) as well as internationally - the International Audio Labs, a joint initiative of the Fraunhofer Institute in Erlangan and the local university, and the Internet Archive, one of the world's major on-line libraries. We will engage with other universities in the UK supported by a partnership fund and via the BBC Audio Research Partnership. 

This long term project will foster new ways for professionals to work with music in the studio and for consumers to engage in their homes. It will support new business models that emphasise the whole experience of musical involvement, and discover ways to monetise the metadata as well as the essential content. The technologies to be researched support new ways of learning (about and to play) music as well as new ways of teaching and performing. And because the project will encompass vast quantities of music data and metadata, from heterogeneous sources, and will stress test emerging principles of big data, distributed intelligence and future generation web, it also addresses key questions of wide significance to EPSRC's ICT Programme, particularly relating to Intelligent Information Systems and Working Together.</gtr:abstractText><gtr:potentialImpactText>By employing a 0.5 fte Outreach and Impact Officer (OIO), we expect to deliver significant impact throughout the project, by making impact part of the process from the start, rather than added in as an afterthought. Working closely with all the investigators, researchers and the programme manager, the OIO will help us capitalise on our industrial partnerships to develop specialised marketing, publicity and media strategies.

The project will deliver impact to academic beneficiaries in conventional ways, including papers in both journals and conferences, but will enhance this with more focussed provision in the form of special sessions, workshops and tutorials at selected important conferences including AES, ACM SIGGRAPH, CHI &amp;amp; Multimedia, and many others. 

Because media/content metadata is vital to the broadcast industry, we will continue to engage with the MPEG and EBU standardisation processes, as well as that of AES. Partners at Oxford are able to promote potential standards within W3C. This industrial engagement and impact is enhanced through our partners, BBC, Microsoft and Omnifone and, in turn, their partners reaching deep into the music industry eco-system. For industry we will also offer online training materials (free) and specialist on-site courses (charged) and we will organise concerts highlighting live use of semantic technologies.

We will provide Open Source software including building on the successful VAMP, Sonic Visualiser and Sonic Annotator products from Queen Mary, and the SoundSoftware project, that offers open code repositories as well as advice and training. Also with the assistance of the SoundSofware project, we will promote best practice in reproducible research. We will continue to enhance the Music Ontology, and its family of ontologies all of which are offered under Creative Commons licenses, and to grow the offering of Open Linked Data hosted from our servers, including dbTune and others that have been provided since the early days of the Semantic Web.

Other avenues for outreach include to schools, via cs4fn, audio! (two paper &amp;amp; online magazine offerings from QML) and TeenTech, an organisation coordinated by Maggie Philbin for STEM outreach into schools with which QML has undertaken several successful activities. It is highly likely that spin-outs will arise from this project, supported by the 3 universities' technology transfer teams.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-06-15</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-06-16</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>5199943</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>Performance integrator (Climb!) is a collaboration between David Weigl, Kevin Page (Oxford), Adrian Hazzard, Chris Greenhalgh, Steve Benford (Nottingham) and Maria Kallionp&amp;auml;&amp;auml; (visiting scholar at Nottingham). The collaboration supports the performance of Climb!, a gamified piece for virtuoso pianists composed by Maria. The collaboration involves the technical integration of the MELD dynamic semantic notation system developed at Oxford with Muzicodes, a music pattern recognition and interaction system developed at Nottingham.</gtr:description><gtr:id>99BFCAAE-645C-4D02-9B5E-A014459CC919</gtr:id><gtr:impact>The collaboration is still active; the ongoing composition of the piece interacting with ongoing technical developments. The collaborators have submitted a paper and performance notes for the project to NIME 2017, and further outputs are planned for the future, both in terms of submissions to academic conferences, and in performances of the piece. The collaboration is multi-disciplinary, drawing on aspects of music composition, music performance, audio engineering, computer science, and web science.</gtr:impact><gtr:outcomeId>58c2d4466d0b12.38935334</gtr:outcomeId><gtr:title>MELD supports &quot;Climb!&quot; virtuoso piano piece (see description also under &quot;collaborations and partnerships)</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Performance at the Mad Scientist festival in Bern, Switzerland, 9. September 2016, with acoustic musicians, using Audio Dream, a web-based performance software that uses clustering algorithms and recurrent neural networks to reorganize recorded audio from the performers.

https://github.com/florianthalmann/audio-dream</gtr:description><gtr:id>CDC4548E-0F85-4E20-8EE7-9E3D43C33FB9</gtr:id><gtr:impact>Not known.</gtr:impact><gtr:outcomeId>58b82f7e550f03.48919317</gtr:outcomeId><gtr:title>Performance at the Mad Scientist festival in Bern, Switzerland, 9. September 2016</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://github.com/florianthalmann/audio-dream</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>This is part of the FAST IMPACt research collaboration between Nottingham and Queen Mary that has pushed forward the ongoing development of the Semantic Player. The Nottingham and Queen Mary partners worked to extend and hone the existing functionality of the Semantic Player to enable the development, authoring and deployment of the B076 experience, which used iBeacons for indoor positioning.

The B076 audio experience engaged with 12 members of the public who signed up to take part. They were recruited via the Primary Arts Space and distributed flyers. It was hosted in the Primary Arts Space, Nottingham. It was an indoor interactive installation where visitors walked around an empty exhibition space. iBeacons were distributed around the room which tracked the proximity of visitors, thus determining their location in the room. Their position triggered playback of different audio streams which culminate in a continuous soundscape narrative.</gtr:description><gtr:id>9A93E83A-D206-4A9E-B62F-F083870E2D6D</gtr:id><gtr:impact>The most significant outcome from this creative project was extending the sensor functionality of the Semantic Player.</gtr:impact><gtr:outcomeId>58b80da329d788.38146395</gtr:outcomeId><gtr:title>The B076 audio experience</gtr:title><gtr:type>Artistic/Creative Exhibition</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>The Rough Mile experience engaged with 22 members of the public, who signed up to take part. They were recruited via Rough Trade records, posters and flyers distributed around Nottingham City. It consisted of two locative audio walks that took place on different days, for paris of friends to come and experience together. The first walk was a pre-defined narrative performance that in part captured information from the friends about music tracks choices they would like to gift to each other. The second walk then consisted of this gifted experience, where they listened to these music tracks as between pairs of friends. The experience was a research exercise, but also a standalone 'artistic engagement' in its own right.</gtr:description><gtr:id>5EF1D860-2A62-45BA-96E9-D5A429EB4E51</gtr:id><gtr:impact>Anecdotal evidence was captured from participants discussing their enjoyment and subsequent desire to seek out and engage with other similar experiences.

Successful deployment of a detailed locative audio experience with a universally positive reception was noted.</gtr:impact><gtr:outcomeId>58b80c5dc8c013.09336128</gtr:outcomeId><gtr:title>The Rough Mile audio experience</gtr:title><gtr:type>Artefact (including digital)</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>The project's goal is the development of a browser based system that enable composers and musicians to identify and perform musical phrases that acts as codes to trigger media interactions within their performance practice, such as: instance lighting cues, visuals, backing tracks, samples etc. Alongside development of this technical system, the research project aims to capture a deep understanding of the issues at play when using this system.

Development (on-going) of a contemporary composition intended for performance in international concert halls.</gtr:description><gtr:id>BEB6D341-370D-4B0D-A6AE-1B64A93DD1A9</gtr:id><gtr:impact>Not applicable at this time. The project is ongoing.

The completed composition, as supported by the developing Muzicodes and MELD technologies, intends for a series of high profile international performances. These include seeking out concert hall venues and performances connected to research engagements such as conferences and workshops.</gtr:impact><gtr:outcomeId>58b8020d698d79.37744480</gtr:outcomeId><gtr:title>Muzicodes: composing and performing musical codes (in progress)</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>&amp;quot;Ada Lovelace Numbers Into Notes&amp;quot; is an exercise in digital content capture and repurposing through a series of performances, as a case study in workflow and Digital Music Objects. It was conducted in collaboration with the AHRC Transforming Musicology project. The events were a performance of &amp;quot;Ada Sketches&amp;quot; by composer Emily Howard at the Science Museum, which was then performed in the Mathematical Institute in Oxford and captured together with audience data and front of house materials; this was then presented at the Ada Lovelace Symposium together additionally with music generated by a simulator of the Analytical Engine, and the combined efforts presented at the Digital Music Research network (December 2015) and the Centre for Digital Scholarship at the Bodleian Library (January 2016). The demonstration software is publicly available online.</gtr:description><gtr:id>F7C66C2A-3A2A-47A2-B013-74F2F05A12DC</gtr:id><gtr:impact>The event allowed for a number of engagement activities with both musicians and the general public to take place. It also led to the the generation of a variety of digital content that can be used to further develop the research and put similar events together in the future.</gtr:impact><gtr:outcomeId>56d80bd5b6a187.05032850</gtr:outcomeId><gtr:title>Ada Lovelace &quot;Numbers into Notes&quot; (Team: Oxford)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>https://dl.dropboxusercontent.com/u/15772302/NumbersIntoNotes/index.html</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Western performing arts practices have traditionally restricted audience interaction with performers. What if this were revisited - not to replace - but to create novel types of musical experiences? Open Symphony reimagines the music experience for a digital age, fostering alliances between performer and audience and our digital selves. The project investigates how to embrace digital technology in the live music environment to engage both audience and performers and develop new platforms for music composition, performance and listening.Open Symphony is an immersive performance system which explores the creativity and spontaneity of reactive interactions through an ensemble of performers and an audience using mobile technology and data visualisation. Open Symphony performances will be held at the workshop 'A Mind of Music' (March 2016, QMUL) and at the International conference on Computer Human Interaction (May 2016, USA).</gtr:description><gtr:id>68DA3686-924E-407E-8F1F-AAE2D5D927D4</gtr:id><gtr:impact>The Open Symphony project has been selected as one of six UK projects to join the Sound and Music organization's Audience Labs programme funded by Arts Council England&amp;quot;, which is an audience development programme designed to test new ways of communicating and presenting new music to audiences.</gtr:impact><gtr:outcomeId>56d80d7a89e115.34367081</gtr:outcomeId><gtr:title>Open Symphony (Team: QMUL)</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>http://isophonics.net/content/opensymphony</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>The Carolan guitar is an augmented acoustic guitar that has been constructed and deployed as a technology probe to explore the broad nature of and potential uses for the digital record of a music instrument. Carolan is an unusual instrument in that its digital record can be inspected by scanning its decorative inlay using a mobile phone. The guitar is called Carolan in honour of the legendary composer Turlough O'Carolan, the last of the great blind Irish harpers, and an itinerant musician who roamed Ireland at the turn of the 18th century, composing and playing beautiful celtic tunes. Like it's namesake, Carolan is a roving bard; a performer that passes from place to place, learning tunes, songs and stories as it goes and sharing them with the people it encounters along the way.

This is possible because of a unique technology that hides digital codes within the decorative patterns adorning the instrument. These act somewhat like QR codes in that you can point a phone or tablet at them in order to read their codes (see below) which in turn accesses information on the Internet. Unlike QR codes, however, they are aesthetically beautiful and form a natural part of instrument's decoration.

Scanning the different patterns on the Carolan guitar takes you to different information such as the history of how it was made, details of who has played it, videos of their performances and also the instrument's user guide and full technical specification. As a result, this unusual and new technology enables the Carolan guitar to share a growing 'digital footprint' throughout its lifetime, but in a way that resonates with both the aesthetic of an acoustic guitar and the craft of traditional luthiery.</gtr:description><gtr:id>16399529-044C-4784-888C-A2D11B247C96</gtr:id><gtr:impact>The FAST Nottingham team have documented over forty encounters with players in homes, studios, gigs, workshops and lessons (see www.carolanguitar.com).</gtr:impact><gtr:outcomeId>56d808595f6e20.16395390</gtr:outcomeId><gtr:title>The Carolan Guitar (Team: Nottingham)</gtr:title><gtr:type>Artefact (including digital)</gtr:type><gtr:url>http://www.carolanguitar.com</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>Working with Patrick Shaw, a very active guitarist and performer, enabled the FAST IMPACt Nottingham team to have some insight into how the Artcodes technology and the concept of Accountable Artefacts are understood by a practicing musician. 

Part of Patrick Shaw's rationale for having scannable Artcodes embedded into his guitar was so that he could promote broader awareness and discussion around this work with the musicians he plays with in the future.</gtr:description><gtr:id>3A31D2ED-EBCC-4EBB-8204-854DED5734BE</gtr:id><gtr:impact>The adoption of the accountable artefacts concept by a performance practioner.</gtr:impact><gtr:outcomeId>58b81aafaceb43.71343708</gtr:outcomeId><gtr:title>Collaboration with guitarist Patrick Shaw: Artcodes</gtr:title><gtr:type>Artefact (including digital)</gtr:type><gtr:url>https://carolanguitar.com/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>Collaboration with BBC R&amp;amp;D with EP/L019981/1</gtr:description><gtr:id>F237DFF6-E7E9-401D-8D74-4E04F4E42D10</gtr:id><gtr:impact>To be completed.</gtr:impact><gtr:outcomeId>56d04571e5a357.72413958-1</gtr:outcomeId><gtr:partnerContribution>BBC R&amp;amp;D is a partner on EP/L019981/1. Lead participant, Dr Frank Melchior attends our 6 monthly major meetings and guides lines of research within the project while we assist in their research around IP in the studio.

Dr Melchior contributed to the writing of the proposal, especially at the early formation stages. Some principles they have been adopting within their technological development have been adopted within the project.</gtr:partnerContribution><gtr:piContribution>BBC R&amp;amp;D is a partner on EP/L019981/1. Lead participant, Dr Frank Melchior attends our 6 monthly major meetings and guides lines of research within the project while we assist in their research around IP in the studio.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Newcastle</gtr:collaboratingOrganisation><gtr:country>Australia, Commonwealth of</gtr:country><gtr:description>Collaboration with Tracy Readhead: prototype of a dynamic song app based on the Semantic Player Framework (in progress)</gtr:description><gtr:id>6A15CBFE-0BAD-4673-8916-022CCF83943D</gtr:id><gtr:impact>-Prototype of the app
-Paper about the composition process submitted to NIME 2017</gtr:impact><gtr:outcomeId>58b82da7052610.08802930-1</gtr:outcomeId><gtr:partnerContribution>-Composition and compilation of the audio material
-design of the sensor mappings
-Testing the app at various stages of iteration</gtr:partnerContribution><gtr:piContribution>-Further development of the Semantic Player Framework to suit the ideas emerged from the collaboration (performance optimization)
-implementation of an algorithm that automatically generates the dynamic music objects and mappings necessary for the experience
-packaging the experience as an app for android</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Birmingham</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Semantic Audio Feature Extraction (SAFE)</gtr:description><gtr:id>E6589AF8-6733-4AD3-93E3-E11638063A48</gtr:id><gtr:impact>-A set of software plug-ins deployed.
-Data collected and published about the application of audio effects
-Papers published about the architecture and the analysis of the collected data.</gtr:impact><gtr:outcomeId>58b8491536a5a9.57781044-1</gtr:outcomeId><gtr:partnerContribution>Built and released SAFE plugins in various formats compatible with several digital audio workstations. Set up data collection architecture.</gtr:partnerContribution><gtr:piContribution>This project is a collaboration between the Digital Media Technologies (DMT) lab at Birmingham City University and QMUL.

Designed SAFE plugin architecture for collecting metadata in music production. Designed SAFE ontology for describing the collected data.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Sustrans</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with SUSTRANS: Musically accompanied walking experiences to motivate behaviour change in commuters</gtr:description><gtr:id>1961547D-7965-45FB-B6FF-FFCFD5D6EB6E</gtr:id><gtr:impact>There are as yet no formalised outcomes. The proposed project is interdisciplinary in nature (HCI, music theory, music production).</gtr:impact><gtr:outcomeId>56d043538a3752.05034684-1</gtr:outcomeId><gtr:partnerContribution>To date Sustains have engaged in a series of preliminary discussions and a design workshop focused on devising a collaborative project around adaptive musical walking experiences to promote behaviour change in commuters who currently use car and public transport. Currently in stasis due to changes in Sustains staff, I.e., the primary contact has left their post. Some Sustains staff participated in the musically accompanied walking study listed above.</gtr:partnerContribution><gtr:piContribution>Following initial discussions with Sustrans the research team at the MRL devised and undertook a mobile musically accompanied walking study. This prelimanary activity intends to inform and drive forward a larger scale research activity with Sustrans.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Internet Archive</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Collaboration with Internet Archive: Exploration Tool for Grateful Dead Live Performances</gtr:description><gtr:id>21F7377C-BC7A-4152-A9B9-B90FF5867845</gtr:id><gtr:impact>A Web application specifically developed for the exploration of Grateful Dead concerts will be the outcome. The project aims to demonstrate how Semantic Audio and Linked Data technologies can produce an improved user experience for browsing and exploring music collections. It is motivated by the ongoing interest in detailed descriptions of Grateful Dead performances, evidenced by the large amount of information available on the Web detailing various aspects of those events. The application links the large number of concert recordings by the Grateful Dead available in the Internet Archive with audio analysis data and retrieves additional information and artefacts (e.g. band lineup, photos, scans of tickets and posters, reviews) from existing Web sources, to explore and visualise the collection.

Conversations with Bob George from the Archive of Contemporary Music (ARC) and Brewster Kahle from the Internet Archive have taken place. Both Bob George and Brewster Kahle are quite interested in this project. Bob George might be able to provide us with additional resources from archives about the Grateful Dead.</gtr:impact><gtr:outcomeId>56d044ac751f34.19881361-1</gtr:outcomeId><gtr:partnerContribution>1. access to the audio material of the archive - this includes non-public lossless audio material, that is otherwise only available in streaming lossy format;
2. a virtual machine located at the Internet Archive for the processing of non-public audio material which should not be copied to outside servers.</gtr:partnerContribution><gtr:piContribution>1. production of low-level and high-level audio feature data of the complete Grateful Dead collection of the Live Music Archive;
2. development of tools for the aggregation and analysis of topic-related information from a variety of sources on the conventional Web. This information can be in text, image, or video form;
3. development of a suitable data model for the representation of the data (including the audio feature data) on the Semantic Web;
4. Development of a Web App using the aforementioned data with a suitable GUI and data visualisation.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>New York University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Center for Data Science</gtr:department><gtr:description>Outgoing visiting research to New York University (K Choi)</gtr:description><gtr:id>15CCCBC8-58E3-4155-BA11-4D3AE1C39730</gtr:id><gtr:impact>The most visible outcome of this collaboration is the conference paper by K. Choi, M. Sandler, G. Fazekas and K. Cho that has been accepted to the 42nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).

The visiting enabled Choui to collaborate closely with researchers outside Queen Mary and gain knowledge of areas other than signal processing side of music information retrieval. He mainly worked with Dr. Brian McFee and Prof. Kyunghyun Cho from the Center for Data Science (CDS), New York University. He discussed many implementation details of his work with Dr. McFee who has deep knowledge and intensive experience of MIR and Python. As a result, Choi reported he improved his programming techniques. Working with Prof. Cho and other students who are doing their research on deep learning and natural language processing deepened his understanding on deep learning in general.</gtr:impact><gtr:outcomeId>58ac3b3f92cb25.71298503-1</gtr:outcomeId><gtr:partnerContribution>Kyunghyun Cho supervised Keunwoo Choi for deep learning knowledge and implementations. In addition, CDS provided K Choi high-performance GPU servers. His work benefited from those facilities as it involves computationally extensive experiments.</gtr:partnerContribution><gtr:piContribution>Mark Sandler, George Fazekas, and Keunwoo Choi collaborated with Kyunghyun Cho for their research. They had regular Skype meetings to discuss research details and co-write the paper submitted to the 42nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Manchester</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>CALMA (Computational Analysis of the Live Music Archive)</gtr:description><gtr:id>E0911937-2208-4F41-ADC3-205E1549BEC4</gtr:id><gtr:impact>Publications (there are reported under the FAST Publications Outcome), published datasets on the Semantic Web, completed data analysis. The project is multi-disciplinary involving, computer science, digital signal processing and musicology.</gtr:impact><gtr:outcomeId>58bd60dd3697a8.94911841-1</gtr:outcomeId><gtr:partnerContribution>Oxford contributed to ontology design and built data analytics framework in R and analysed key typicality over a large collection of recordings. IA provided access to Live Music Archive audio and metadata, Manchester contributed to publishing analysis and provenance data as linked data.</gtr:partnerContribution><gtr:piContribution>This is a collaboration between QMUL, U. of Manchester, Oxford, and the Internet Archive. Built data analysis framework using Vamp audio analysis plugins. Designed ontology to describe provenance of data processing. Built and published CALMA dataset containing audio features extracted from live sound recordings.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Solid State Logic</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with SSL</gtr:description><gtr:id>089D1BA9-5B9A-495B-B6DE-6785D7C39F95</gtr:id><gtr:impact>none to date, other than the innovate uk grant proposal and 2 CASE studentship agreements.</gtr:impact><gtr:outcomeId>56d043e988a656.96009459-1</gtr:outcomeId><gtr:partnerContribution>Innovate UK application, pending. Sponsoring two EPSRC CASE students to be appointed.</gtr:partnerContribution><gtr:piContribution>Innovate UK application, pending. Sponsoring two EPSRC CASE students to be appointed.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Illinois</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Oxford Illinois Digital Libraries Placement Programme</gtr:description><gtr:id>303E351E-F38C-4DBB-BAFE-B43411CB386C</gtr:id><gtr:impact>Not applicable at this time.</gtr:impact><gtr:outcomeId>58c2674be4cd74.16839908-1</gtr:outcomeId><gtr:partnerContribution>The student worked with the Oxford team on the SALT software in June-August 2016. The Semantic Alignment and Linking Tool (SALT) enables the creation of bridging structures supporting unified access to music datasets.</gtr:partnerContribution><gtr:piContribution>The Oxford FAST IMPACt team hosted a student studying for their Masters in Library and Information Science at the University of Illinois.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Rough Trade Records</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Locative Audio Experiences: The Rough Mile &amp;amp; B076</gtr:description><gtr:id>35C1C16D-D7D5-4E68-A475-4EC39A710540</gtr:id><gtr:impact>The collaboration with Rough Trade Records and Jocelyn Spence was multi-disciplinary, bringing together the academic disciplines of Human Computer Interaction and Performance Studies alongside the commercial focus of Rough Trade Records. The main (hard) outcome was the realisation and deployment of the Rough Mile experience. Softer outcomes are found in the establishment of a relationship between Rough Trade Records and the Mixed Reality Lab, University of Nottingham. 

The collaboration with Jocelyn Spence is still active; the collaboration with Rough Trade Records ended in August 2016.

The collaboration with Florian Thalmann, alongside other colleagues at Queen Mary University of London, has pushed forward the ongoing development of the Semantic Player. Parallel to this activity an ontology for Interactive Composition is being developed in collaboration, which will aims to inform and support composers working in this area with tools such as the Semantic Player and DaoPlayer.</gtr:impact><gtr:outcomeId>58b8048c236aa4.10533149-2</gtr:outcomeId><gtr:partnerContribution>Jocelyn Spence led the design of 'The Rough Mile', a two-part locative experience for friends to engage with. The experience found participants undertaking an interactive audio narrative on the streets of Nottingham, which encouraged them to think about their friend and choose music for them to listen to in part two. Part two repeated the walk but re-authored around the music chosen in part one. Jocelyn brough a specific set of skills in designing and deploying interactive, novel performance work. This enabled her to lead the design of the Rough Mile experience. Jocelyn, also contributed to the design and development of the B076 audio experience.

A second collaboration to 'The Rough Mile' project was 'Rough Trade Records', Nottingham. Rough Trade Records is a legendary independent record label that was at the epicentre of the punk explosion in 1976, who continue to champion high quality, imaginative, musically diverse and innovative independent music to this present day. Rough Trade Records hosted The Rough Mile experience (the walk commenced and finished at Rough Trade shop in Nottingham). They support the project with publicity and promotion, thus recruiting participants to undertake the experience.

Florian Thalmann responded to the requirements of the B076 audio experiences as set out by the MRL team and Jocelyn Spence.</gtr:partnerContribution><gtr:piContribution>Main research questions: to explore interactive (locative) audio experiences, to inform the requirements and subsequent developments of bespoke software for such experiences, and capture a deep understanding of the nature of them.

The FAST IMPACT Nottingham team at the Mixed Reality Lab (MRL) supported Jocelyn Spence in designing and realising 'The Rough Mile' and B076 experiences. Specifically, this included the ongoing development and authoring of a bespoke software tool for interactive locative audio experiences entitled the DaoPlayer, which was used for 'The Rough Mile', and the collaboration with Florian Thalmann in extending the Semantic Player used for the B076 experience. The MRL provided complementary skills and experience in staging and studying the deployment of such experiences, alongside other resources and infrastructures required to support the process.

The B076 experience is a collaboration and partnership with FAST IMPACt member, Florian Thalmann, C4DM, Queen Mary University of London and Jocelyn Spence. The Nottingham team worked with Florian Thalmann to extend and hone the existing functionality of the Semantic Player to enable the development, authoring and deployment of the B076 experience, which used iBeacons for indoor positioning.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Internet Archive</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>CALMA (Computational Analysis of the Live Music Archive)</gtr:description><gtr:id>A36CA592-AD92-4DD6-A5F6-C13597D179D0</gtr:id><gtr:impact>Publications (there are reported under the FAST Publications Outcome), published datasets on the Semantic Web, completed data analysis. The project is multi-disciplinary involving, computer science, digital signal processing and musicology.</gtr:impact><gtr:outcomeId>58bd60dd3697a8.94911841-3</gtr:outcomeId><gtr:partnerContribution>Oxford contributed to ontology design and built data analytics framework in R and analysed key typicality over a large collection of recordings. IA provided access to Live Music Archive audio and metadata, Manchester contributed to publishing analysis and provenance data as linked data.</gtr:partnerContribution><gtr:piContribution>This is a collaboration between QMUL, U. of Manchester, Oxford, and the Internet Archive. Built data analysis framework using Vamp audio analysis plugins. Designed ontology to describe provenance of data processing. Built and published CALMA dataset containing audio features extracted from live sound recordings.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>B3 Media</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with the B3 Media TalentLab programme (Mixed Reality Lab, Nottingham)</gtr:description><gtr:id>BEEDD218-007C-4822-AF3E-24484A50DF5B</gtr:id><gtr:impact>Richard Ramchurn's successful Talent Lab residency concluded with him being offered a PhD studentship on the University of Nottingham's Centre for Doctoral Training, where he is placed in the Mixed Reality Lab.

Grant Smith completed his Talent Lab residency and in doing so made new relationships with a range of other practitioners: for example Rachel Jacobs working in the field of streaming and performing data; Anthony Brown, who assisted in the development of solutions for networking remote microphones; and Alan Chamberlain and Patrick Brundell who have engaged with Grants deployable environmental microphones.</gtr:impact><gtr:outcomeId>58b81e06acf269.45901816-1</gtr:outcomeId><gtr:partnerContribution>The B3 Media Talent Lab artists enable the MRL researchers to engage and collaborate with working practitioners. This in turn provides opportunities to situate, test out and extend existing research into the field, thus providing a deeper understanding of the implications of our research.</gtr:partnerContribution><gtr:piContribution>The B3 Media partnership with Mixed Reality Lab, Nottingham, is now in its third year. The Mixed Reality Lab provide residency opportunities for establishing artists and practitioners to develop and realise project ideas. A number of participants have gone through this programme, the three stated here represent those who have worked within the FAST remit of audio/music:
1. Richard Ramchurn (2014/5)
2. Grant Smith (2016)
3. Lula Mebrahtu (2017)

The following contributions are being offered by the MRL team:
1. Mentoring: Talent Lab participants are paired with MRL researchers who can offer advice and collaborative engagement with the participants area of interested.
2. Workshop and Lab space: as part of the residency, the MRL provides space and resources to support realisation of participant's project.
3. Technical support: Many of these participants are engaging with new novel technologies, the MRL provides access and expertise with these.
Post programme support and opportunities: if appropriate for all parties, the MRL is open to further engagement with a Talent Lab artists.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Department of Biochemistry</gtr:department><gtr:description>CALMA (Computational Analysis of the Live Music Archive)</gtr:description><gtr:id>8CC1C78D-F5B6-4EDA-B465-F4D4C8A808A2</gtr:id><gtr:impact>Publications (there are reported under the FAST Publications Outcome), published datasets on the Semantic Web, completed data analysis. The project is multi-disciplinary involving, computer science, digital signal processing and musicology.</gtr:impact><gtr:outcomeId>58bd60dd3697a8.94911841-2</gtr:outcomeId><gtr:partnerContribution>Oxford contributed to ontology design and built data analytics framework in R and analysed key typicality over a large collection of recordings. IA provided access to Live Music Archive audio and metadata, Manchester contributed to publishing analysis and provenance data as linked data.</gtr:partnerContribution><gtr:piContribution>This is a collaboration between QMUL, U. of Manchester, Oxford, and the Internet Archive. Built data analysis framework using Vamp audio analysis plugins. Designed ontology to describe provenance of data processing. Built and published CALMA dataset containing audio features extracted from live sound recordings.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Omnifone</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Omnifone</gtr:description><gtr:id>6E39D9B2-4BF8-41E2-9138-1980D93A7E67</gtr:id><gtr:impact>To be completed.</gtr:impact><gtr:outcomeId>56d045087579e3.81526349-1</gtr:outcomeId><gtr:partnerContribution>see above</gtr:partnerContribution><gtr:piContribution>Omnifone have installed QMUL audio feature extraction software into their audio signal processing workflow. They use our Sonic Annotator with VAMP plugins to extract features from the audio to assist with providing APIs to let third party companies create new music services. This is covered in an agreement. The company will provide QMUL with these features, which we will publish on the semantic web via SPARQL query point. This will enable researchers around the world, but especially at QMUL to do research on Big Music Data and Metadata.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Pompeu Fabra University</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:department>Music Technology Group (MTG)</gtr:department><gtr:description>Audio Feature Ontology (ongoing)</gtr:description><gtr:id>02371D50-647B-4DD9-90FB-DD82FEB70BA5</gtr:id><gtr:impact>Prior work related to this activity has led to European Funding (H2020): Audio Commons (grant no 688382, running between 2016-2019) to build an ecosystem that supports the reuse of Creative Commons licensed audio content in the creative industries. The project includes work that uses the ontology developed by this activity.</gtr:impact><gtr:outcomeId>58b832bb5cc6b2.38645102-1</gtr:outcomeId><gtr:partnerContribution>There is an ongoing collaboration with the Music Technology Group (MTG), UPF, Barcelona, Spain, to enhance the message response format of the AcousticBrainz project. AcousticBrainz hosts crowed-sourced acoustic features for research and retrieval purposes. The collaboration aims to facilitate the inclusion of a Linked-Data mechanism into AcousticBrainz so the data provided may be used in the context of the Semantic Web.</gtr:partnerContribution><gtr:piContribution>There is an ongoing collaboration with the Music Technology Group (MTG), UPF, Barcelona, Spain, to enhance the message response format of the AcousticBrainz project. AcousticBrainz hosts crowed-sourced acoustic features for research and retrieval purposes. The collaboration aims to facilitate the inclusion of a Linked-Data mechanism into AcousticBrainz so the data provided may be used in the context of the Semantic Web.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Application of Semantic Audio Analysis to the Music Production Workflow, 139th Convention of the Audio Eng. Society</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C614199D-BFFB-4933-92CE-9B4CE2FCEF16</gtr:id><gtr:impact>139th Convention of the Audio Eng. Society - Workshop: Application of Semantic Audio Analysis to the Music Production Workflow - Gy&amp;ouml;rgy Fazekas (co-chair), Ryan Stables (co-chair), Jay LeBoeuf and Bryan Pardo (panelists). This was a workshop intended for audio engineers.The semantic player was demonstrated to show the potential of new music formats. Questions were raised regarding how to produce content for the format proposed. This influenced further research direction and collaborations.</gtr:impact><gtr:outcomeId>58b82ec3a56fe7.50469855</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.aes.org/events/139/workshops/?ID=4703</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Ada Lovelace Symposium, Oxford</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>29A3BA87-3444-4B57-B28E-C734E89E5AF7</gtr:id><gtr:impact>The Ada Lovelace Symposium was held on the 10-11 December 2015. There was a display in the coffee area with a slideshow based on the performance and played audio clips over headphones (silent disco!), which gave the participants a chance to sit, talk, and demo with interested delegates (many were musicians). On Wednesday afternoon there was a session in the symposium constructed as a conversation on stage between David de Roure and the composer Emily Howard. This also featured a clip of Emily's work &amp;quot;Mesmerism&amp;quot;, which is another part of her Ada Lovelace Trilogy, and an example from David de Roure of generating a number sequence on a simulator of the Analytical Engine and developing the output as a music theme (i.e. directly addressing the Lovelace quote on which all our discussions have been based). 

There was an additional musical event at the Wednesday evening reception in Blackwell Hall - world premi&amp;egrave;res of &amp;quot;An algorithmic study on ADA&amp;quot; and &amp;quot;ADA&amp;quot;, composed by James Whitbourn, performed by Commotio (mixed-voice contemporary choir), Andrew Bernardi (violin), Anna Lapwood (harp), and conducted by Matthew Berry. 

David de Roure gave a presentation on all the above at QMUL as part of the Digital Music Research network event. The Transforming Musicology and FAST projects were acknowledged at both events.</gtr:impact><gtr:outcomeId>56d71b14515114.87395483</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://dl.dropboxusercontent.com/u/15772302/NumbersIntoNotes/index.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>FACT</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>6D94CB20-35A0-4A45-AF56-AF47120EE2A7</gtr:id><gtr:impact>In July 2015 the Nottingham team hosted a public exhibition and series of associated workshops at the Foundation for Art and Creative technology (FACT) a major UK venue for interactive art in Liverpool. Themed under the general banner of 'Performing Data', this included several pieces of work that were produced by or with support from the Fast project, including works by the artists Albino Mosquito, Di Wilshire and Caroline Locke discussed below as well as a workshop to learn to hack the D-Box musical instruments that have been created by Andrew McPherson and his team from QMUL.</gtr:impact><gtr:outcomeId>56d470862a4dc0.88917589</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshop, &quot;Audio in Place: Media, Mobility &amp; HCI - Creating Meaning in Space&quot;  (by Chamberlain A., B?dker M., Hazzard A. and Benford S.)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D5AB92BB-775B-4FFD-9705-4FB4D7198631</gtr:id><gtr:impact>Workshop based on presentation by Chamberlain A, B&amp;oslash;dker M, Hazzard A and Benford S. (2016), &amp;quot;Audio in Place: Media, Mobility &amp;amp; HCI - Creating Meaning in Space&amp;quot;, In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services. 

Short workshop presentation, followed by discussion and then practical activities based around capturing and using audio to highlight issues related to HCI.

The workshop was a catalyst for further discussion and debate, new collaborations betweens attendees have arisen in the form of submitted research outputs (publications).</gtr:impact><gtr:outcomeId>58b7fc404d1526.41527760</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description> Bela Workshop at STEIM, Amsterdam (F. Morreale, G. Moro, A. McPherson, A. Chamberlain)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6194287E-1F9C-489D-963A-514826896350</gtr:id><gtr:impact>The Bela Workshop took place on the 10th, 11th &amp;amp; 12th of August, 2016, STEIM, Achtergracht 19, 1017WL, Amsterdam. Participants of the workshops (academics and artists) used Bela (http://bela.io), a new embedded audio / sensor platform based on the BeagleBone Black which provides submillisecond latency between action and sound, and which replaces the need for a laptop and external microcontroller boards such as Arduino to create digital musical instruments.

 The programme of the workshop included:
- a comprehensive hands-on introduction to developing with Bela;
- a number of design activities aimed at elaborating ideas and objectives when designing DMIs;
- a prototyping session where members of STEIM and C4DM will help participants port their instruments to Bela
- a test/evaluation session;
- a concluding performance or demo with the newly built instrument.

A major goal of the workshop was to make the new instrument designs from the workshop sustainable by thoroughly documenting the process of building and using the instrument. In addition to the new artefacts created in the workshop, we released documentation that allowed others in the community to replicate and modify the instruments. In addition to technical help, the organising team helped the participants document their efforts as they went along, recording short interviews as part of a research study on digital musical instrument sustainability.</gtr:impact><gtr:outcomeId>58b7faeb4ca6e8.47995455</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://bela.io</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Rough Mile</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>5FFF6117-5EE7-4E91-B361-65E992DD8F34</gtr:id><gtr:impact>The Rough Mile experience engaged with 22 members of the public, who signed up to take part. They were recruited via Rough Trade records, posters and flyers distributed around Nottingham City. 

The Rough Mile experience consisted of two locative audio walks that took place on different days, for pairs of friends to come and experience together. The first walk was a pre-defined narrative performance that in part captured information from the friends about music tracks choices they would like to gift to each other. The second walk then consisted of this gifted experience, where they listened to these music tracks as between pairs of friends. The experience was a research exercise, but also a standalone 'artistic engagement' in its own right. Anecdotal evidence was captured from participants discussing their enjoyment and subsequent desire to seek out and engage with other similar experiences.

The most significant outcome / impact was the successful deployment of a detailed locative audio experience with a universally positive reception.</gtr:impact><gtr:outcomeId>58b807e90e2124.11434295</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshops with BBC, Abbey Road, Rectify, Weav</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>C297EBD5-04A4-4E6D-9042-E3E82F7DB032</gtr:id><gtr:impact>Small discussion meetings exploring dynamic music formats and how it relates to Digital Music Objects and Dynamic Music.</gtr:impact><gtr:outcomeId>56d70a9cb4fb91.93373040</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Music Hack Day and Sonar Festival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>84B0AA2E-7367-43AA-8A60-E6750BA08F72</gtr:id><gtr:impact>Three C4DM members (Matthieu Barthet, Gyorgy Fazekas and Alo Allik) attended the Barcelona Music Hack Day 2015 collocated with the Sonar festival (music, creativity and technology). They won one of the awards from the Rapidmix EU project for the most original hack using biosignal/multimodal wearable technology for music performance. (The award is listed in the Rapidmix section). The prize was a BITalino toolkit to prototype applications using body signals and was awarded by the company biosignalplux.

Description of the hack MoodBox:

MoodBox is a collaborative music jukebox which selects music following user emotional states as characterised by body signals measured with wireless wearable sensors. Correlates of arousal (excitation) and valence (pleasantness) are computed using electrodermal activity (EDA) and electromyography (EMG) with the Biosignalsplux Bitalino wearable sensor as well as electroencephalography (EEG) with the Neuroelectrics Enobio brain helmet. We use our mood recognition technology merging affective computing and semantic web techniques to map a collection of 10,000 tracks to the arousal/valence space. Our visualiser displays the track which is played back in the arousal/valence space according to how users feel on average!

MoodBox cited in thedrum.com article:
http://www.thedrum.com/opinion/2015/06/26/hacking-brain-real-virtual-reality-coolest-creative-tech-s-nard

More news about the Music Hack Day in Barcelona:
http://musichackday.upf.edu/mhd/2015/?p=206</gtr:impact><gtr:outcomeId>56d7054bda21e8.22645518</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.semanticaudio.ac.uk/news/c4dm-team-win-an-award-at-the-music-hack-day-barcelona-17-19-june/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Demonstrations and posters at ISMIR 2016 (Oxford team)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BAAF2078-8798-46C6-9A9A-8C49FB66CFD7</gtr:id><gtr:impact>ISMIR is the premier international conference on music information retrieval. 2016's demonstration session was, for the first time, open to the public, as well as to the academics and industry representatives attending ISMIR. The Oxford FAST IMPACt team presented their three poster/demos side-by-side, sparking interesting discussions about the individual demos as well as on the wider FAST project:
1) &amp;quot;Numbers into Notes: Cast your mind back 200 years&amp;quot;, D. de Roure, P. Willcox, and D. M. Weigl
2) &amp;quot;Dynamic Semantic Notation: Jamming together Music Encoding and Linked Data&amp;quot;, D. M. Weigl and K. R. Page
3) &amp;quot;MIR user studies through the lens of relevance: Promoting the impact of MIR user research&amp;quot;, D. M. Weigl, D. Steele, J. C. Bartlett, &amp;amp; C. Guastavino</gtr:impact><gtr:outcomeId>58c26a7fda0289.55231549</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>https://wp.nyu.edu/ismir2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>D-Box paper presentation and demo at ACM Designing Interactive Systems Conference, Brisbane, Australia, July 2016.</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>ED0B04C3-30A0-4DA5-BCCD-A02D94BB803C</gtr:id><gtr:impact>The D-Box demo provided an opportunity for conference delegates to configure, reconfigure and play the D-Box, while discovering more about an instrument designed to promote exploratory play.

Approximately 15 participants played the D-Box, and discussed the underlying concepts of exploratory play and design of such instruments. This led to discussion around similar and related work and the forming of networks between academics.</gtr:impact><gtr:outcomeId>58b819b84d5d83.06478273</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>MPEG Media Value Chain Ontology meetings</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FEF66ECC-8FFC-4DA2-A572-40F1A1211EFA</gtr:id><gtr:impact>QMUL is working to standardise aspects of the musicontology.com within the context of the wider MPEG Media Value Chain Ontology. They are attending meetings every four months; this is ongoing and continuing.</gtr:impact><gtr:outcomeId>56d7109d5b1196.27429064</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>8C77169C-E955-4B9D-93F9-385B5EB99FB1</gtr:id><gtr:impact>The Nottingham team hosted a one day visit by a team from BBC R&amp;amp;D to discuss new collaborations. They have visited the BBC in February 2016 for a follow-up conversation about possible musical projects in relation to their new 'Homelab' activity.</gtr:impact><gtr:outcomeId>56d6f6c1a7f4b0.55991770</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Hacking Sound workshop (Oxford)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>73949946-AC6A-4BC7-AC5B-F609D87F38DD</gtr:id><gtr:impact>The Centre for Digital Scholarship and the University of Oxford e-Research Centre organised a day of sound hacking for the general public. 

The hack day experimented with ways of representing data with sound or music and exploring the sonic world. The participants were encouraged to bring data and existing projects to share, and to start on fresh ideas. The day encouraged networking and developing ideas and projects, ending with a showcase of what was produced and future directions that might be taken.</gtr:impact><gtr:outcomeId>58c273cb7091f5.81345480</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://blogs.bodleian.ox.ac.uk/digital/2016/09/20/an-invitation-to-hack-sound/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Soundlincs</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D88076D7-1B9E-47C9-8A5E-B808AF8B54F2</gtr:id><gtr:impact>The Nottingham FAST team have established a collaboration with Soundlincs, a company that delivers musical outreach activities to disadvantaged communities around the midlands (http://www.soundlincs.org). Following an initial visit, they hosted a one day workshop to explore collaboration at which they identified various opportunities for future joint-projects.</gtr:impact><gtr:outcomeId>56d6e707a8ae62.92211627</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.soundlincs.org</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Carolan guitar blog (Nottingham)</gtr:description><gtr:form>Engagement focused website, blog or social media channel</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>03BFFD74-A501-41B1-A08F-3F99FB94651C</gtr:id><gtr:impact>Over a period of a fifteen months the Carolan guitar engaged over 50 players, including visiting 6 homes, being played at 3 gigs, taking part in 2 recording sessions, visiting 8 clubs or jam sessions, hosting an 'open mic' event in Liverpool, residing in a shop and undertaking an international road-trip. These activities were documented in in over 50 posts on Carolan's blog www.carolanguitar.com. To date this has been
visited by 7,000 people, receiving 23,000 views. A video about Carolan published on the Computerphile youtube channel (https://www.youtube.com/watch?v=tyjgn5YO1Lk) has received 72,000 views to date.</gtr:impact><gtr:outcomeId>56d46fd9b9a545.29044988</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.carolanguitar.com</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Collaboration with artist Albino Mosquito</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>E6EDCFCA-F219-47EF-A266-04BADD973931</gtr:id><gtr:impact>The Nottingham University team have collaborated with the artists group Albino Mosquito to produce an interactive film (with soundtrack) called #Scanners that is controlled through a braincomputer interface. This was exhibited first at the ACM's Creativity and Cognition conference and then at FACT in July 2015. The following video trailer produced by the artists documents the experience as it appeared at FACT:
https://vimeo.com/135983464</gtr:impact><gtr:outcomeId>56d47163079861.20708695</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://vimeo.com/135983464</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Collaboration with Di Wilshire</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>D6716128-2F6F-4D4F-90B2-AB307BC870C0</gtr:id><gtr:impact>The Nottingham team have collaborated with the artist Di Wilshire to create an interactive sound installation called Sentiment. This features a series of provocative audio interviews with people that are edited to be played over a circular array of speakers, with the mix being controlled by the listener's movement in the space. Di also recorded galvanic skin response data from interviewees (a possible measure of stress) that is also played back to listeners using a wearable tactile device. Sentiment premiered at FACT in July 2015 and is now being extended ready touring with support from an Arts Council grant.</gtr:impact><gtr:outcomeId>56d472188b49d0.66679261</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The &quot;Ada Sketches&quot; event, Oxford</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>B811C6AA-3156-4EBB-9F3C-B4DF5C92D194</gtr:id><gtr:impact>The &amp;quot;Ada sketches&amp;quot; event was held on 30 November, in the Mathematical Institute in Oxford. It opened with a performance of Ada sketches by four musicians from the Royal Northern College of Music, after which the work was explained musically and mathematically by its composer Emily Howard and mathematician Lasse Rempe- Gillen (from Liverpool), then it was performed a second time. After the break the participants went into the &amp;quot;co-creation phase&amp;quot; where audience members turned &amp;quot;numbers into notes&amp;quot; and these were performed by the musicians. The event captured a live annotation of the first performance by Carolin, and the audience response to the work before and after explanation was collected via questionnaires designed in conjunction with Daniel and Iris from Goldsmiths. The audience included several attendees of the IT as a Utility network &amp;quot;Internet of Audio Things event&amp;quot; which was held earlier that day.</gtr:impact><gtr:outcomeId>56d71991e71ac3.29498679</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Moodplay Interactive Experience Digital Shoreditch</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3FDFCF20-628D-43F9-8369-26968C5A04C4</gtr:id><gtr:impact>The C4DM team showcased the Moodplay Interactive experience at the Digital Shoreditch exhibition. Very good interaction with visitors was noted during the event and a number of questionnaires were completed by the public; these provide valuable information on feed back from the user.</gtr:impact><gtr:outcomeId>56d70473cab6e8.29139021</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Returning Ghosts - Performance and New Technologies WG Interim Event 24th May 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>8247E686-84F8-4804-BF4D-D3318C976DD1</gtr:id><gtr:impact>The audience engaged with during this workshop were academic researchers in the Performing Arts discipline.

Returning Ghosts focused on identifying new ways to recall memories using performance and technology and features performances and presentations. Jocelyn Spence gave a talk on the Rough Mile Experience.

One of the outcomes that has arisen was an early discussion about collaborating on a book chapter of performative works. It is hoped to facilitate dissemination of work (primarily situated in an HCI research community) to a Performing Arts research community.</gtr:impact><gtr:outcomeId>58b80b7b83f9c6.50598286</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Organisation of DMRN +10 conference</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D08D3960-859A-49AF-8D71-BEAF22DE81D5</gtr:id><gtr:impact>FAST sponsored the workshop and FAST presentations at the Digital Music Research Network (DMRN+10) 2015 event held at QMUL. This is an annual event organised by C4DM.</gtr:impact><gtr:outcomeId>56d708ba39c9e2.04955104</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshop on Auditory Neuroscience, Cognition and Modelling 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C2AB14F7-DFA0-496A-AE8A-2C81B6EC1F83</gtr:id><gtr:impact>The main aim of the organisers (Marcus Pearce, Emmanouil Benetos, Yvonne Blockland) was to organise and host at QMUL a workshop on Auditory Neuroscience and Computation, which took place on Wednesday 17 February 2016, at the Charterhouse Square campus. The workshop brought together auditory neuroscientists, cognitive scientists, as well as researches in music and audio signal processing and related fields. New insights on the cognitive and neural underpinnings of speech, music and sound processing were presented, with a large focus on EEG and MEG data analysis. The workshop focused on academic impact, however it had industry participation (from Audio Analytic, Red Apple Creative). Following an open call for abstracts and selection process., 32 presentations were accepted: 3 keynote talks, 6 oral presentations and 23 poster presentations. 

The workshop website includes a PDF book of abstracts and video recorded oral presentations (see website below). Post-workshop summaries are being written by both workshop organizers and attendees, to be published to several venues: Psychomusicology, Young Acousticians Network newsletter, Audio Analytics Lab blog. Future plans: discussions are underway on applying for an EPSRC Research Network grant, for funding future workshops and network activities. 

Post-workshop summaries are being written by both workshop organizers and attendees, to be published to several venues: Psychomusicology, Young Acousticians Network newsletter, Audio Analytics Lab blog.

- Psychomusicology report: 
o Workshop on Auditory Neuroscience, Cognition and Modeling. Agres, Kat; Sauv&amp;eacute;, Sarah Anne Psychomusicology: Music, Mind, and Brain, Vol 26(3), Sep 2016, 288-292. http://dx.doi.org/10.1037/pmu0000151
- Audio Analytic Lab blog post:
http://www.audioanalytic.com/aa-labs-impressions-on-the-workshop-on-auditory-neuroscience-cognition-and-modelling-2016/
- Young Acousticians Network newsletter:
https://euracoustics.org/activities/yan/yan-newsletter-folder/2016/march-2016/at_download/file

Preliminary discussions have been made with Iris Mencke and Elvira Brattico on hosting the next workshop.</gtr:impact><gtr:outcomeId>56dd95b943a2f3.06662405</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://c4dm.eecs.qmul.ac.uk/wancm2016/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Collaboration with artist Tracy Redhead</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>91CEFBC8-E266-4B18-8380-5B65470248D0</gtr:id><gtr:impact>Recording artist and academic Tracy Redhead was invited to describe her work on a flexible music format to a QMUL FAST meeting. Her ways of working will inform the project's activities on Digital Music Objects.</gtr:impact><gtr:outcomeId>56d7078cbc45c5.27945525</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Linked Music Hackaton, Goldsmiths</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>FC4EA64A-FDC0-4103-B71D-274EFB4239E7</gtr:id><gtr:impact>Linked Music Hackaton event (Programming with Purcell, Hacking with Handel, Linking with Liszt, 9th October 2015, Goldsmiths University of London) 

Over 25 academics and developers gathered for a Linked Music Hackathon, organised by Kevin Page and David Weigl. Attendees were able to use data produced by FAST and more than 20 other Linked Data music sources to produce new mashups prototyped and presented on the same day. A variety of hacks were shown at the end of the event with the winning project &amp;quot;Geobrowsing using RISM&amp;quot; chosen by popular vote. A Radio 1 team were also in attendance to interview attendees for the BBC's &amp;quot;Make It Digital&amp;quot; campaign.</gtr:impact><gtr:outcomeId>56d71820d8cd25.13289440</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://semanticmedia.org.uk/?q=hackathon</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>D-box workshops</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>66546F3C-6C51-41C5-A938-141C9D2CDA8D</gtr:id><gtr:impact>Performers often find ways of playing musical instruments that differ from the designer's original intent. Two &amp;quot;hackable instruments&amp;quot; sessions to explore creative and unexpected uses of digital instruments were organised as workshops:

1) D-Box, FACT, Liverpool, 14th of July 2015 
Each participant received a D-Box,, a simple electronic instrument that has been purpose-designed to be as hackable as possible. D-Boxes can be opened up and then completely rewired to produce new sonic effects. Participants learned to play, explore and modify their D-Boxes, share these skills with others and swap instruments to compare hacks.
 
2) D-BOX workshop as part of Being Human, A Festival of the Humanities, London 2015 16th November. 
As above.</gtr:impact><gtr:outcomeId>56d6f90b7bfaf7.06655216</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.fact.co.uk/whats-on/current/the-mrl-d-box-instrument-hack-workshop.aspx</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Open Symphony</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>CF31440C-2A1E-4020-9FD4-685783103BEA</gtr:id><gtr:impact>In a traditional music performance situation, the audience has limited means to give feedback (e.g. applause!) and little opportunity to interact with the music's performance. What if this were revisited? 'Open Symphony' is an immersive performance system developed as part of a collaboration between Queen Mary University of London and a Masters graduate of Guildhall School of Music and Drama. Open Symphony explores the creativity and spontaneity of real-time human interaction through an ensemble of performers and an audience using mobile technology. The project aims to extend the nature of live music to enable audience members to meaningfully collaborate in the development of a musical piece. By realigning both music and technology as co-operating tools, audience members actively influence the music being played in a live environment, creating a meaningful mutual experience between multiple audience members and performers. &amp;quot;Open Symphony&amp;quot; creates new musical experiences, new music and new compositional methods. This is made possible by using a wide range of technologies including web, information visualisation, and sensors.</gtr:impact><gtr:outcomeId>56d706455732e2.28362287</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://isophonics.net/content/opensymphony</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Digital Musicology: Applied Computational and Informatics Methods for Enhancing Musicology, Oxford</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>D7EA04E5-8523-4453-A4AD-653074802301</gtr:id><gtr:impact>Digital Musicology: Applied computational and informatics methods for enhancing musicology, Monday 20 - Friday 24 July 2015, Oxford

This workshop at the Digital Humanities at Oxford Summer School was organised by Kevin Page, supported by David Weigl. Several other members of the FAST project team also presented to attendees from academia and industry.</gtr:impact><gtr:outcomeId>56d718acbf31f7.99829971</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The B076 audio experience</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>F2B89BD6-476E-4C1A-BB03-783F2479CF36</gtr:id><gtr:impact>The B076 audio experience engaged with 12 members of the public who signed up to take part. They were recruited via the Primary Arts Space and distributed flyers. It was hosted in the Primary Arts Space, Nottingham. It was an indoor interactive installation where visitors walked around an empty exhibition space. iBeacons were distributed around the room which tracked the proximity of visitors, thus determining their location in the room. Their position triggered playback of different audio streams which culminate in a continuous soundscape narrative.

The most significant outcome/impact of this activity was extending the sensor functionality of the Semantic Player.</gtr:impact><gtr:outcomeId>58b80967d65b19.12824494</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Experience the Carolan Guitar, interactivity demo at ACM CHI 2016, San Jose, USA, May 2016.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3E231FEF-BD5D-4BF5-B96A-7C3BDA42D146</gtr:id><gtr:impact>Experience the Carolan Guitar' was an opportunity for conference delegates to spend some time playing the Carolan Guitar and scan its embedded Artcodes patterns. Alongside this practical activity participants could discuss the concepts of instrumenting instruments, artcodes, accountable artefacts (physical objects with digital records of their use and function). Their performances were captured and uploaded to its blog (www.carolanguitar.com), thus contributing to the history of the instrument's usage. 

Approximately 30 participants played the Carolan guitar, most of which contributed a video recorded performance, discussed the underlying concepts of Artcodes, and accountable artefacts. This led to discussion around similar and related work and the forming of networks between academics.

The most signiticant impact was the dissemination of work to a broad HCI audience alongside their 'performed' contribution to the Carolan Guitar's digital history.</gtr:impact><gtr:outcomeId>58b818ceb99c34.90647123</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://www.carolanguitar.com</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Digital Musicology workshop at Digital Humanities at Oxford Summer School 2016 (organised and run by Kevin Page).</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>CD318A07-1364-48B4-B3B8-CE76B2B87467</gtr:id><gtr:impact>Kevin Page (Oxford) organised and ran the Digital Musicology workshop at Digital Humanities at Oxford Summer School. The workshop was attended by 19 registrants (students, academics, industry). Date: July 2016, Oxford. Reach: International reach. 

More information about the workshop and FAST participation can be found in our FAST news item: 
http://www.semanticaudio.ac.uk/news/fast-partners-participate-in-the-digital-humanities-at-oxford-summer-school/

The workshop website:
http://digital.humanities.ox.ac.uk/dhoxss/2016/workshops/digitalmusicology</gtr:impact><gtr:outcomeId>58c26f622a80e1.55237200</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://digital.humanities.ox.ac.uk/dhoxss/2016/workshops/digitalmusicology</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshop co-chair: Objective Evaluation in Semantic Audio Analysis and Processing (G. Fazekas)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>93A836A7-B046-4B9A-81BC-470499D0F2C6</gtr:id><gtr:impact>George Fazekas, Centre for Digital Music, Queen Mary, acted as workshop co-chair: Objective Evaluation in Semantic Audio Analysis and Processing, 138th International AES Convention, May 7-10, 2015. The workshop was intended for audio engineers and music industry professionals.</gtr:impact><gtr:outcomeId>58bd647ebd31c8.73709495</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Innovation in Music (In'Music15) conference</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>3D892500-CAF3-487B-954D-991F558859E0</gtr:id><gtr:impact>The FAST project organized a panel session at this industry-focused conference. Panelists included Matt White (Director of User Experience, Omnifone), Jon Eades (Project Manager, Abbey Road Red), Gyorgy Fazekas (C4DM, QM), Peter Tolmie (MRL, Nottingham), Gary Bromham (composer, recording / mix engineer, producer) and Adrian Hazzard (MRL, Nottingham). The panel session explored and discussed the FAST project themes as an opportunity to reach out to a wider audience to both promote and foster future engagement with other related parties.</gtr:impact><gtr:outcomeId>56d6e64a40af54.25823996</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshops with Omnifone Engineering teams</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>330D9297-5234-4419-A865-C7C61CEE8148</gtr:id><gtr:impact>The FAST QMUL team was training the company in linked data and its advantages.</gtr:impact><gtr:outcomeId>56d70973ec20f0.93920233</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Collaboration and knowledge transfer discussions with BBC</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>A3198F28-63FC-4E49-A786-7BE99A064A28</gtr:id><gtr:impact>Kevin Page and David Weigl from the FAST Oxford team visited BBC R&amp;amp;D to discuss their work, in conjunction with BBC Radio, on the SALT and SLoBR tools, with a view to further use across BBC data and the Corporation's requirements for DMOs.</gtr:impact><gtr:outcomeId>56d71b79a97bb3.26658747</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presentations at Oxford Digital Humanities Summer School (July 2016)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A71E9917-661D-4D4D-97EE-7C4A761FAD2C</gtr:id><gtr:impact>FAST IMPACt had several presentations at the Oxford Digital Humanities Summer School (July 2016). The Summer School is made out of multiple workshops drawn from the over 150 summer school attendees (international; students, academics, professionals, industry.

1) &amp;quot;Creating linked data with Annalist&amp;quot;, Linked Data workshop, Graham Klyne. This was a hands-on session which gave participants an opportunity to create linked data based on modelling they had performed in previous workshop sessions.
2) &amp;quot;Linked Data&amp;quot;, Introduction to Digital Humanities workshop, Kevin Page
3) &amp;quot;The Physical and the Digital via the Meta&amp;quot;, Humanities Data workshop, Kevin Page

FAST produced a news item on FAST participation: http://www.semanticaudio.ac.uk/news/fast-partners-participate-in-the-digital-humanities-at-oxford-summer-school/</gtr:impact><gtr:outcomeId>58c26e7b7c39a0.03435891</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.oerc.ox.ac.uk/news/digi-humanities-summer-school</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presentation at the Music Encoding Conference in Montreal 2016 (Kevin Page)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>33A1E989-8230-429B-94E9-628884552DAB</gtr:id><gtr:impact>Kevin Page (Oxford FAST IMPACt team member) gave a presentation at the Music Encoding Conference in Montreal, May 2016: &amp;quot;Prototypical Scenarios for Contextual Navigation with MEI and Linked Data&amp;quot;.</gtr:impact><gtr:outcomeId>58c271d22ba000.73839043</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Music Hack Day &amp; Sonar Festival, Barcelona, 17-19 June 2015</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>AC667539-566B-4165-BDB3-28DF38310C79</gtr:id><gtr:impact>Music Hack Day is an international 24-hour event where programmers, designers and artists come together to conceptualize, build and demo the future of music. Three C4DM members (Matthieu Barthet, Gyorgy Fazekas and Alo Allik) attended the Barcelona Music Hack Day 2015 collocated with the Sonar festival (music, creativity and technology). They won one of the awards from the Rapidmix EU project for the most original hack using biosignal/multimodal wearable technology for music performance. (The award is listed in the Rapidmix section). The prize was a BITalino toolkit to prototype applications using body signals and was awarded by the company biosignalplux.

Description of the hack MoodBox:

MoodBox is a collaborative music jukebox which selects music following user emotional states as characterised by body signals measured with wireless wearable sensors. Correlates of arousal (excitation) and valence (pleasantness) are computed using electrodermal activity (EDA) and electromyography (EMG) with the Biosignalsplux Bitalino wearable sensor as well as electroencephalography (EEG) with the Neuroelectrics Enobio brain helmet. We use our mood recognition technology merging affective computing and semantic web techniques to map a collection of 10,000 tracks to the arousal/valence space. Our visualiser displays the track which is played back in the arousal/valence space according to how users feel on average!

MoodBox cited in thedrum.com article:

http://www.thedrum.com/opinion/2015/06/26/hacking-brain-real-virtual-reality-coolest-creative-tech-s-nard

More news about the Music Hack Day in Barcelona:
http://musichackday.upf.edu/mhd/2015/?p=206</gtr:impact><gtr:outcomeId>58b83dc67ea7b3.03800923</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.thedrum.com/opinion/2015/06/26/hacking-brain-real-virtual-reality-coolest-creative-tech-s-nard</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Semantic Digital Humanities workshop, Oxford</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>23C91CFA-67D8-4DFE-B22C-5C4D539B340D</gtr:id><gtr:impact>Kevin Page from the FAST Oxford team co-organised this 3 day workshop which brought together contributors from the humanities and computer science, and academia and the arts and cultural heritage industry.</gtr:impact><gtr:outcomeId>56d71c8dae0d67.05401609</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Talks and posters at DMRN+11 (2016) (Oxford team)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>843C15C0-4855-462A-B1D2-363E2AB38928</gtr:id><gtr:impact>Digital Music Research Network (DMRN) aims to promote research in the area of Digital Music, by bringing together researchers from UK universities and industry in electronic engineering, computer science, and music. The following FAST talks and posters were presented: 
1) &amp;quot;Interacting with robots as performers and producers of music&amp;quot;, Alan Chamberlain (University of Nottingham), Kevin R. Page, David De Roure, Graham Klyne and Pip Willcox(University of Oxford)&amp;quot;
2) &amp;quot;Understanding creativity and autonomy in music performance and composition: A proposed 'toolkit' for research and design&amp;quot;, Alan Chamberlain (University of Nottingham), David De Roure, Pip Willcox (University of Oxford), Steve Benford and Chris Greenhalgh (University of Nottingham)
3) &amp;quot;Conceptualizing relevance for music information retrieval&amp;quot;, David M. Weigl (University of Oxford)</gtr:impact><gtr:outcomeId>58c26b4d7795e0.61891395</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://c4dm.eecs.qmul.ac.uk/dmrn/events/dmrnp11/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Internship with Naver Labs, South Korea</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>CC15AB3F-BE0E-4244-AD8C-9F0640E16323</gtr:id><gtr:impact>QMUL-funded FAST PhD student Keunwoo Choi spent 3 months with Naver, South Korea's leading internet search company. He developed visualisations of Convolutional Neural Networks and investigated learning from playcount and playlist data held by the company.</gtr:impact><gtr:outcomeId>56d70922a6fd52.85061952</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presentation by Mark Sandler, Alan Blumlein event, Abbey Road Studios</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E44D56C6-241B-45FA-BF40-72244B58F322</gtr:id><gtr:impact>FAST was presented by Mark Sandler to an audience of ~100 on at Abbey Road Studios, marking the occasion of Blumlein's centenary with a plaque dedicated by IEEE. FAST is now dedicated to the memory of Alan Dower Blumlein, the inventor of binaural and crossed-pair microphone recording techniques, as well as the stereo groove configuration in vinyl LP records.</gtr:impact><gtr:outcomeId>56d7081a16d026.03500115</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Moodplay installation at BBC Sound Now and Next</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>36491884-CC2E-4E76-80D1-EEFC47BAF671</gtr:id><gtr:impact>BBC R &amp;amp; D organised a two day event at the BBC on innovation in sound production. The Queen Mary team was invited as an exhibitor and they presented the Moodplay Installation to professional practitioners, academic and industry and business. 

Mention in press release by AudioMedia International (pro audio magazine):
http://www.audiomediainternational.com/broadcast/report-bbc-sound-now-and-next/04522</gtr:impact><gtr:outcomeId>58b83bdf57b676.06614747</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/rd/events/sound2015</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Mycelia meeting at Sonos Studios</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BDCB3C7C-89F8-4648-B1E9-568518207DDC</gtr:id><gtr:impact>Mycelia is an initiative by the music performer and artist Imogen Heap to galvanise change in the rights and renumeration in the 21st century recorded music industry. Mark Sandler presented overview of EP/L019981/1 on 9 Feb 2016.</gtr:impact><gtr:outcomeId>56d71173e8de24.83312939</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Collaboration with artist Caroline Locke</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>863EB379-932D-4B5B-BF4E-30EBB8B4F1E6</gtr:id><gtr:impact>The Nottingham team have collaborated with artist Caroline Locke who has developed an artistic interface called sound fountains in which sound in visualised through the activation of water (see: http://www.weareprimary.org/people/caroline-locke/). Caroline exhibited a version of sound fountains connected to a live data stream at FACT and is currently exploring how to Twork with us further extend her approach.</gtr:impact><gtr:outcomeId>56d472b9842e12.05002978</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.weareprimary.org/people/caroline-locke/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Conference Presentation at the Audio Mostly Conference, Norrkoping, Sweden (Benford, Chamberlain, Hazzard, Greenhalgh)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>DE221A89-8D5F-4B5D-80A9-3252DEC0AB40</gtr:id><gtr:impact>75 people attended the presentation. The most notable impact or outcome was the introduction of this work - via a published paper- into the international academic community on audio. This conference presentation acted as a catalyst for discussion around the Muzicodes work with a new community of researchers.</gtr:impact><gtr:outcomeId>58b7fe6c5adbf2.33997375</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Real World Studios</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>FBF16B25-A759-4E85-9F51-2DDBC80F2B1B</gtr:id><gtr:impact>The Nottingham team was hosted by Real World Studios for a day where they introduced the FAST project and explored opportunities for future partnership.</gtr:impact><gtr:outcomeId>56d6f63cdf9121.93697717</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://realworldstudios.com</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presentation at the Sonar+D Festival, June 2016,  Barcelona, Spain</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>740779EB-A42E-4471-9939-C39E5BF7858C</gtr:id><gtr:impact>From 16-18 June 2016, the Centre for Digital Music (QMUL) including members from the FAST project presented a public exhibition of its research at the Sonar+D festival, a high-profile annual event in Barcelona which caters to musicians, the music technology industry, and members of the general public. C4DM was chosen by competitive application for a display booth (roughly 4m x 4m) on the exhibition floor, in a prime location near the entrance to the facility. 12 C4DM researchers, including PhD students, postdocs, early- and mid-career academics, attended to showcase their work.

The event was attended by thousands of people, mainly adults but also occasionally children. The booth had many visitors from both large and small businesses, including several different members of the music company Focusrite. Several musicians who were performing and speaking at Sonar also attended the booth, including the well-known composer Brian Eno who took an interest in several of the research projects. The event raised the public profile of C4DM and the individual projects within it.

MusicWeb was demonstrated to app developers who have taken interest in linked music data.</gtr:impact><gtr:outcomeId>58b83555cbf192.43225106</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>2nd International Workshop on Digital Libraries for Musicology (DLfM 2015), USA</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9A2F9E4A-249E-4EFB-AF4A-587BE4622AA6</gtr:id><gtr:impact>The 2nd International Workshop on Digital Libraries for Musicology (DLfM 2015) was held on the 24th June 2015, Knoxville, TN, USA. Kevin Page from the FAST Oxford team co-chaired this workshop, held in conjunction with ACM/IEEE JCDL 2015.</gtr:impact><gtr:outcomeId>56d71bf90293b9.43316046</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Co-chairing Digital Libraries for Musicology workshop 2016 (Kevin Page, Oxford)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>75EF853D-9E29-4ED3-B863-45E986EA7A46</gtr:id><gtr:impact>The Digital Libraries for Musicology (DLfM) workshop is a satellite event of ISMIR 2016. Kevin Page Co-chaired the workshop. The event had an attendance of 45 people (from academia, industry, galleries/libraries/museums sector). Date: August 2016, New York. 

David Weigl (Oxford FAST IMPACt team member) gave a presentation at DLfM 2016: &amp;quot;In Collaboration with In Concert: Reflecting a Digital Library as Linked Data for Performance Ephemera&amp;quot;. 

For more information about the workshop, see:
http://www.transforming-musicology.org/dlfm2016/</gtr:impact><gtr:outcomeId>58c270a43b72c2.93821573</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.transforming-musicology.org/dlfm2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Muzicodes workshops (C. Greenhalgh, S. Benford, A. Hazzard, A. Chamberlain)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>75F748CA-0B04-42F0-9F41-98F2F588A3B6</gtr:id><gtr:impact>Four workshops with a range of participants who engaged with setting up, authoring and performing the Muzicodes system, using their own musical instruments. These workshops identified groups of potentially interested participants, thus recruitment was by invite. 

These workshops enabled the FAST IMPACt Nottingham team to observe how musicians interacted with the system, thus drawing some broad conclusions, which then inform further technical developments. They disseminated the work to range of practitioners and researchers (some of whom are continuing to engage with it, e.g., Maria Kallionpaa) and enabled its continuing development.</gtr:impact><gtr:outcomeId>58b7fd5410e883.96284525</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Series of talks in the Centre for Digital Scholarship: &quot;Research Uncovered&quot; (Oxford team)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6A9B6974-7757-453E-9789-E21F7FE66485</gtr:id><gtr:impact>Talks in the Centre for Digital Scholarship: &amp;quot;Research Uncovered&amp;quot; is a series of public talks on digital scholarship; many attendees come from around the University of Oxford but others come from the region, and audiences includes visitors from around the country and the world):

1) &amp;quot;Research Uncovered-David De Roure on Ada Lovelace, Numbers, and Notes&amp;quot;, 22 Jan 2016. See: https://blogs.bodleian.ox.ac.uk/digital/2016/01/11/research-uncovered-david-de-roure-on-ada-lovelace-numbers-and-notes/ 
2) &amp;quot;Research Uncovered-The imagination of Ada Lovelace: creative computing and experimental humanities&amp;quot;, part of the Oxford Women's International Festival. 7th March. See: https://blogs.bodleian.ox.ac.uk/digital/2017/01/09/pip-willcox/ 
3) &amp;quot;Research Uncovered-Making Numbers into Notes: the making of Ada Lovelace's generative music&amp;quot;, part of the Oxford Women's International Festival. 7th March. See:
http://blogs.bodleian.ox.ac.uk/digital/2017/02/24/making-numbers-into-notes-the-making-of-ada-lovelaces-generative-music/
4) &amp;quot;Research Uncovered-Finding music to move to: Relevance in Music Information Retrieval&amp;quot;, 31 Jan 2017. See: http://blogs.bodleian.ox.ac.uk/digital/2017/01/06/david-weigl/</gtr:impact><gtr:outcomeId>58c26cf3135e36.81707664</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Media Talentlab project</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>2D09E622-6A93-46A2-9920-39DC20FD7AAE</gtr:id><gtr:impact>TalentLab is an annaul programme of events that supports the creative development of BAME artists and filmmakers who are ready to take their careers to the next level. The engagement activities with artists this project is involved in show how working with external artists is part and parcel of our approach to research, allowing us to explore innovative new ideas while also delivering public impact. In order to broaden the diversity of artists with whom we work the Nottingham team have recently established a relationship with the company B3 Media and their Talentlab programme which provides training, development and support for Black, Asian and Minority Ethnic artists. By running workshops with Talentlab 2015 (see: http://www.b3media.net/talentlab#.Vq9f7OsZ7uU) we have recruited three new artists - all working with sound and music in different ways - to collaborate with us on new projects over the coming year. Their proposals are investigating interfaces for live performance, remote streaming of environmental sounds, and interactive musical interfaces for wellbeing.</gtr:impact><gtr:outcomeId>56d48386463681.06831290</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.b3media.net/talentlab#.Vq9f7OsZ7uU</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Special Session panelist: Fusing  Semantic  and  Audio  Technologies  for  Intelligent  Audio  Production  and  Consumption project (FAST)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>47320431-6A1C-44E8-BCBD-494D129C2181</gtr:id><gtr:impact>George Fazekas, Centre for Digital Music, Queen Mary acted as a special session panelist: Fusing Semantic and Audio Technologies for Intelligent Audio Production and Consumption project (FAST), Innovation in Music 2015, Anglia Ruskin University, Cambridge, UK.

Intended for music industry professionals. Audience: 60-80.</gtr:impact><gtr:outcomeId>58bd6546de7209.97244724</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://innovationinmusic.com/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>676850</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>Horizon 2020 (H2020-ICT-2015)</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>Proposal number 688382</gtr:fundingRef><gtr:id>85F9B865-4739-4308-B4D7-943A237A76C8</gtr:id><gtr:outcomeId>56dff7a55117e4.60000148</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>25000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>The Royal Society Wolfson Research Merit Award</gtr:description><gtr:end>2020-03-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>to be confirmed.</gtr:fundingRef><gtr:id>B80B960F-BA5B-4C74-81BD-15D85582BC75</gtr:id><gtr:outcomeId>56d455a7154a04.34930255</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Preliminary results are becoming of interest to parts of the recorded music industry. Musician Imogen Heap's mycelia project seeks to wrest value back from the major music labels and return it to the musicians. This is coupled with a vision of enhanced recorded music products and services for the music consumer. The programme grant's technologies provide a really good fit to these aims.
At the recent meeting of mycelia, Sandler presented FAST and this led to meetings with two london based companies and one still pending with an artists' representation organisation.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>4E9ED702-B8FF-47BD-85E3-491BA31B3474</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56dc5845382648.33770953</gtr:outcomeId><gtr:sector>Creative Economy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>New ontological model for the representation of aligned recorded fragments of the same performance, based on the music ontology and timeline ontology. Other ontologies fail to describe different recordings of the same event in comparably detailed ways.

-Computer model/ algorithm
-used internally, yet unpublished</gtr:description><gtr:id>F6782088-ABA0-4511-89F6-AC2753A4ACF2</gtr:id><gtr:impact>N/A</gtr:impact><gtr:outcomeId>58b8312e9d4d42.06219262</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Grateful Dead project</gtr:title><gtr:type>Computer model/algorithm</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Musical Mood Dataset: a Semantic Web dataset combining publication metadata of over 10,000 commercial tracks with crowd-sourced mood tags and arousal-valence coordinates.

The musical mood dataset is a linked data verison of the ILM10K dataset that consists of samples from 10,199 commercial tracks from iLikeMusic1 (ILM). It combines audio recordings used in the Moodplay interactive experience with track metadata derived from MusicBrainz music encyclopedia and social tag data collected from Last.fm. The collection includes tracks from over 5,600 unique artists. The database also includes automatically annotated arousal and valence mood ratings for each track using the Affective Circumplex Transformation (ACT). This method ensures that coordinates obtained by applying Multi-dimensional Scaling (MDS) to crowd-sourced tags conform to locations of mood adjectives validated in human Psychological experiments. (See: P. Saari, G. Fazekas, et al.: Genre-Adaptive Semantic Computing and Audio-Based Modelling for Music Mood Annotation. IEEE Trans. Affective Computing 7(2): 122-135, 2016)</gtr:description><gtr:id>6CC89536-11BF-4ABE-BE24-8375FDF618AE</gtr:id><gtr:impact>Not applicable at this time.</gtr:impact><gtr:outcomeId>58b83c54273df2.43773752</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Musical Mood Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The goal of this project is to populate the recently developed Open Multitrack Testbed (multitrack.eecs.qmul.ac.uk), an online platform for storing, searching and browsing multitrack audio and mixtures thereof. It is a resource intended for but not limited to users such as researchers in MIR and intelligent music production, as well as sound engineering students and audio software developers. The researchers' aim is to realise the testbed's potential impact by including the most popular multitrack datasets around, complemented with extensive metadata. The idea of publishing a multi-track database has been raised several times and a few prior attempts have been made within the Queen Mary researchers' group. The project is led by Brecht De Man, who collected audio files for a number of multi-track recordings. Gy&amp;ouml;rgy Fazekas (FAST IMPACt) advised on semantic technologies, having developed the Studio Ontology Framework which is being used in the testbed. Mariano Mora-Mcginity (also a FAST IMPACt team member) finished developing the testbed (barring solving any issues that may arise during the annotation) to a state in which it 
1. shows all content for the user to browse through; 
2. has a search interface to search and filter using any of the metadata fields; 
3. allows an authorised user (password protected with customisable permission) to enter new metadata associated with audio on the testbed server (c4dm.eecs.qmul.ac.uk/ multitrack) or elsewhere on the web. 

The C4DM team has annotated over 600 multitracks have been annotated, comprising over 3000 individual tracks. Full details have been maintained about the instrumentation, genre, recording process, artifacts, attribution.... This is a sufficient number for this data to now be used in machine learning and data mining applications. It is contributing to research on music mixing practices and multitrack audio processing.</gtr:description><gtr:id>AA61E031-24D7-49E4-8AD9-29787D0E03BF</gtr:id><gtr:impact>Several researchers have referenced the testbed in their papers since its inception, and it was well received during a session on multitrack audio datasets of the Unconference at 15th Conference of the International Society for Music Information Retrieval in Taipei, Taiwan, November 2014. Furthermore, within C4DM the local multitrack repository is extensively used for research, but as the licenses are not open - or unclear - disseminating data and results is less than straightforward. This is especially a problem when funding bodies, journals or conferences require open data for sustainability and reproducibility purposes. The research team is positive that this resource, the first of its kind, has the potential of becoming the main resource for any researcher, student or developer who needs multitrack audio and mixtures thereof, provided it has the critical mass to become appealing as a go-to platform. It is evident that there is already a strong support base for this idea, and already extensive usage of the still limited dataset. 

Multitrack audio is a valuable resource for a wide range of audio professionals, educators, students, developers, and researchers. Many of these communities are not that aware of C4DM and its activities. Notably, this project is expected to have significant impact with music technology students and educators, who will use the content in learning and teaching music production skills. R&amp;amp;D areas in which large and diverse sets of high quality, multi-stream audio are essential in music information retrieval (MIR) research, development of improved music production tools, and tasks like audio source separation, audio segmentation and research in auditory perception.</gtr:impact><gtr:outcomeId>56d8048a94bb36.58325250</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Multitrack Testbed (Team: QMUL)</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://multitrack.eecs.qmul.ac.uk</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The Semantic Audio Feature Extraction Dataset (SAFE-DB) is a continually updating database of semantically annotated music production metadata, taken from an international user group of sound engineers. The data is taken from 4 audio effects: a dynamic range compressor, an overdrive distortion, a parametric equaliser and an algorithmic reverb.</gtr:description><gtr:id>E732D7E6-3F44-48FB-891E-8D71238B1BA4</gtr:id><gtr:impact>Not applicable at this time.</gtr:impact><gtr:outcomeId>58b84a624eece7.23995279</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>The Semantic Audio Feature Extraction Dataset SAFE-DB</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.semanticaudio.co.uk/datasets/data/</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>ILM10K dataset is a mood annotated music dataset of 10,000 tracks using the method developed in Pasi Saari, Gy&amp;ouml;rgy Fazekas, Tuomas Eerola, Mathieu Barthet, Olivier Lartillot, Mark B. Sandler: (2016) Genre-Adaptive Semantic Computing and Audio-Based Modelling for Music Mood Annotation. IEEE Trans. Affective Computing 7(2): 122-135 (2016)</gtr:description><gtr:id>2DFA20F9-2BD2-4E9A-863F-43828AA89459</gtr:id><gtr:impact>Not applicable at this time.</gtr:impact><gtr:outcomeId>58bd669e9a7f82.53133313</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>ILM10K dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://www.computer.org/csdl/trans/ta/2016/02/07173419-abs.html</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Semantic Player Framework, 2015-present, ongoing development, used internally within the project, as well as in a collaboration with Birmingham and University of Newcastle, Australia, no public release yet.
 
The Semantic Music Player is a cross-platform web and mobile app built with Ionic and the Web Audio API that explores new ways of playing back music on mobile devices, particularly indeterministic, context-dependent, and interactive ways. It is based on Dynamic Music Objects, a format that represents musical content and structure in an abstract way and makes it modifiable within definable constraints. For each Dynamic Music Object, the Semantic Music Player generates a custom graphical interface and enables appropriate user interface controls and mobile sensors based on its requirements. When the object is played back, the player takes spontaneous decisions based on the given structural information and the analytical data and reacts to sensor and user interface inputs. The Dymo Designer is a prototypical web app that allows people to create and analyze Dynamic Music Objects in a visual, interactive, and computer-assisted way. 

The Semantic Player framework includes a number of npm packages which can be embedded in other web-based software.
https://github.com/florianthalmann/semantic-player
https://github.com/florianthalmann/dymo-core
https://github.com/florianthalmann/dymo-designer
https://github.com/florianthalmann/music-visualization
https://github.com/florianthalmann/dymo-generator
https://github.com/florianthalmann/example-dymos</gtr:description><gtr:id>D8477A1F-E7E5-4B17-B25E-709BD64F4B8D</gtr:id><gtr:impact>Ongoing, no impact yet.</gtr:impact><gtr:outcomeId>58b8304a5a7e67.46086931</gtr:outcomeId><gtr:title>Semantic Player Framework</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>https://github.com/florianthalmann/semantic-player</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Prototype of a website that links data, music, and artifacts from concerts of the Grateful Dead, from various sources, and presents them to the user in a flexible way. Audio feature data and analytical information can be visualized in various ways and the audio from the Live Music Archive can be streamed directly on the site. 2016, not publicly released.</gtr:description><gtr:id>307ABC35-C197-4593-ACA8-E1BB6B22A39C</gtr:id><gtr:impact>Not applicable at this time.</gtr:impact><gtr:outcomeId>58b831da489e11.06157479</gtr:outcomeId><gtr:title>Grateful Dead project</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/florianthalmann/grateful-dead</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>MusicWeb: a web platform that provides users a browsing, searching and linking platform of music artist and group information by integrating open linked semantic metadata from various Semantic Web, music recommendation and social media data sources. Artists are linked by various commonalities such as style, geographical location, instrumentation, record label as well as more obscure categories, for instance, artists who have received the same award, have shared the same fate, or belonged to the same organi-
sation. These connections are further enhanced by thematic analysis of journal articles, blog posts and content-based similarity measures focussing on high level musical cate- gories. Realised in 2015/2016</gtr:description><gtr:id>CEB176C3-7B58-4964-B172-B7BB514D6D0E</gtr:id><gtr:impact>Ongoing.</gtr:impact><gtr:outcomeId>58b8346bc4c444.96005967</gtr:outcomeId><gtr:title>MusicWeb</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>http://musicweb.eecs.qmul.ac.uk</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>&amp;quot;Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent.&amp;quot; Ada Lovelace, 1843

The Numbers into Notes software is available on github; https://github.com/davidderoure/NumbersIntoNotes</gtr:description><gtr:id>7026808E-6172-4F41-BD88-E6F001B37A21</gtr:id><gtr:impact>The application has been used to support a variety of engagement events.</gtr:impact><gtr:outcomeId>58c2d6ba2b6399.48757490</gtr:outcomeId><gtr:title>Numbers into Notes web application</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>http://demeter.oerc.ox.ac.uk/NumbersIntoNotes/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>26BBCC3E-83EF-44F6-B0B1-378AFA4D4EF4</gtr:id><gtr:title>Perceptual Evaluation and Analysis of Reverberation in Multitrack Music Production</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e72cb7b44cf4c6c84c06036531451e5"><gtr:id>2e72cb7b44cf4c6c84c06036531451e5</gtr:id><gtr:otherNames>de Man B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ada864a8fa24.08227530</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>20FFFE42-1CBA-4E59-89F8-74CDE8F3E6DE</gtr:id><gtr:title>Structured Dropout for Weak Label and Multi-Instance Learning and Its Application to Score-Informed Source Separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bdb5cbc255f13d1e0765d8dc08d86d3"><gtr:id>4bdb5cbc255f13d1e0765d8dc08d86d3</gtr:id><gtr:otherNames>Sebastian Ewert</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58acbb7b63caf8.25512439</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>598958E6-432C-483A-8C41-0AA59B880000</gtr:id><gtr:title>Audio in Place: Media, Mobility &amp;amp;38; HCI - Creating Meaning in Space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b2d268b0f646194196247fd2c52d94f"><gtr:id>7b2d268b0f646194196247fd2c52d94f</gtr:id><gtr:otherNames>Chamberlain, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b57f97804a71.54112759</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4226239C-01F5-4E43-B63D-809C8F49FBEE</gtr:id><gtr:title>Navigating Ontological Structures Based on Feature Metadata with the Semantic Music Player</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1eb37718dd257b5a1ccd517783fbbb42"><gtr:id>1eb37718dd257b5a1ccd517783fbbb42</gtr:id><gtr:otherNames>Thalmann, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf1992dcca69.04364809</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6ABE68C8-9D55-4F12-9575-0CFFD4E93DC4</gtr:id><gtr:title>On providing semantic alignment and unified access to music library metadata</gtr:title><gtr:parentPublicationTitle>International Journal on Digital Libraries</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cfae4e8d7eb5b7ff60bb066b7ed435b4"><gtr:id>cfae4e8d7eb5b7ff60bb066b7ed435b4</gtr:id><gtr:otherNames>Weigl D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe5768e3239.31339187</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2FE2D041-E3D9-40FE-9B19-72F9CB184AA0</gtr:id><gtr:title>The Semantic Music Player: A Smart Mobile Player Based on Ontological Structures and Analytical Feature Metadata</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1eb37718dd257b5a1ccd517783fbbb42"><gtr:id>1eb37718dd257b5a1ccd517783fbbb42</gtr:id><gtr:otherNames>Thalmann, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cf1b0b9dd3f7.84104959</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92D3C70B-A215-40BF-80E9-78AF261BACD8</gtr:id><gtr:title>Carolan: a Guitar That Tells its Story</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45866e7a40e830503f185653372546c3"><gtr:id>45866e7a40e830503f185653372546c3</gtr:id><gtr:otherNames>Benford, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cef99a52f5c4.00659134</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5B055AF-4587-4E85-ABFC-7289AF1ABF54</gtr:id><gtr:title>Making Links: Connecting Humanities Resources for Scholarly Investigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/384914f1777b1084d227dad6897c40b3"><gtr:id>384914f1777b1084d227dad6897c40b3</gtr:id><gtr:otherNames>Page, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf122fcaad68.11222872</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A7E27DD-ACD8-4A78-95B4-146231328B1E</gtr:id><gtr:title>Contextual interpretation of digital music notation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/384914f1777b1084d227dad6897c40b3"><gtr:id>384914f1777b1084d227dad6897c40b3</gtr:id><gtr:otherNames>Page, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>58bd6be09a9ea3.73425210</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4D43EC75-F785-4F86-B109-82A73FB2366A</gtr:id><gtr:title>SAFE: A system for the extraction and retrieval of semantic audio descriptor,s</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d6b02f1bcdb46887b58dc2d7bcf188e"><gtr:id>0d6b02f1bcdb46887b58dc2d7bcf188e</gtr:id><gtr:otherNames>Stables, R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58b848297bc6e2.85255697</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95AFAF7C-8A2E-45DF-83F5-3F314A22BE22</gtr:id><gtr:title>Piano Transcription in the Studio Using an Extensible Alternating Directions Framework</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ecbadd3514247d9dd2617ec865425671"><gtr:id>ecbadd3514247d9dd2617ec865425671</gtr:id><gtr:otherNames>Ewert S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d401a4e1d26.35591882</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BFD1CC0-8D48-4DB9-863A-B8BE180041D1</gtr:id><gtr:title>Identifying Missing and Extra Notes in Piano Recordings Using Score-Informed Dictionary Learning</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e28ac287c010756bd48daf4772ba293c"><gtr:id>e28ac287c010756bd48daf4772ba293c</gtr:id><gtr:otherNames>Wang S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe2b6467250.06254109</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4EAB6C7F-458D-4200-A80C-004A982B40D0</gtr:id><gtr:title>Sonifying the Scene: Re-framing and Manipulating Meaning Through Audio Augmentation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b2d268b0f646194196247fd2c52d94f"><gtr:id>7b2d268b0f646194196247fd2c52d94f</gtr:id><gtr:otherNames>Chamberlain, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf00f705cdd3.10074945</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B41EEE54-451D-4DAF-AA23-C8900DC91550</gtr:id><gtr:title>Implementation and Assessment of Joint Source Separation and Dereverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/88a2bf3a49c9153f619135eaace64319"><gtr:id>88a2bf3a49c9153f619135eaace64319</gtr:id><gtr:otherNames>Moffat, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd8bfca43ce9.74015973</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>81E113C5-C3CD-450F-BCF3-DC66DED374F6</gtr:id><gtr:title>Ways of Walking: Understanding Walking's Implications for the Design of Handheld Technology Via a Humanistic Ethnographic Approach</gtr:title><gtr:parentPublicationTitle>Human Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c6a37de942affeadb55040caa1961449"><gtr:id>c6a37de942affeadb55040caa1961449</gtr:id><gtr:otherNames>Eslambolchilar P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e659dadef0.78390471</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5C1919C4-9302-4411-B10D-BDCBF90FD94D</gtr:id><gtr:title>MusicWeb: music discovery with open linked semantic metadata</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d63820089e593e5f0d405224a12aaad"><gtr:id>2d63820089e593e5f0d405224a12aaad</gtr:id><gtr:otherNames>Allik, A.,</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e2ddc80b34.93734174</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>444E8591-1B0E-4D86-A472-7DC8615D0530</gtr:id><gtr:title>Towards Incorporating Derived Features In Dataset Alignment And Linking (accepted)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2bb6a040c8b8d65f22c5fbfdf0276917"><gtr:id>2bb6a040c8b8d65f22c5fbfdf0276917</gtr:id><gtr:otherNames>Blauvelt, C.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>58bd6c52535137.15260558</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6A3A61B-A096-42E9-9883-57F1F05D95FC</gtr:id><gtr:title>Music Encoding in context: MEI and Linked Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/384914f1777b1084d227dad6897c40b3"><gtr:id>384914f1777b1084d227dad6897c40b3</gtr:id><gtr:otherNames>Page, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf12739e03e9.50382983</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DBD6A579-F62A-47FC-BD6D-07B6AF6EB804</gtr:id><gtr:title>Navigating Ontological Structures Based on Feature Metadata with the Semantic Music Player</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1eb37718dd257b5a1ccd517783fbbb42"><gtr:id>1eb37718dd257b5a1ccd517783fbbb42</gtr:id><gtr:otherNames>Thalmann, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf19406cd4a6.05038080</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F4069524-AD6A-4E64-8A87-FE7A1C522C38</gtr:id><gtr:title>Vocal Imitation of Pitch, Spectral Shape and Loudness Envelopes&amp;quot;</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/946a1d83374b2eab922704d0e99fba23"><gtr:id>946a1d83374b2eab922704d0e99fba23</gtr:id><gtr:otherNames>Mehrabi, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf1128261600.05664638</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>578C59AF-8194-44AB-9FDA-454FD93D41B4</gtr:id><gtr:title>Identification of drum overhead-microphone tracks in multi-track recordings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f25e435dbce332942b61ba948152393f"><gtr:id>f25e435dbce332942b61ba948152393f</gtr:id><gtr:otherNames>Arimoto, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adbb0f03da84.70404767</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94B7C749-E310-47A9-B149-1EED614E96D0</gtr:id><gtr:title>Numbers into Notes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/72c4a51e93570b46f577021fbac43ad9"><gtr:id>72c4a51e93570b46f577021fbac43ad9</gtr:id><gtr:otherNames>De Roure, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d544429fd2.97396540</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C13D29B9-34A3-4A28-A35E-5029EF65E5D5</gtr:id><gtr:title>Interactive music applications by MPEG-A support in Sonic Visualiser</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/73c86fb00e8fb9abd5ef011328a82730"><gtr:id>73c86fb00e8fb9abd5ef011328a82730</gtr:id><gtr:otherNames>Garcia, J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58b593bf54cb80.04963929</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C0F749AA-C0A7-401B-A7A0-D5BCA9517405</gtr:id><gtr:title>Audio Technology and Mobile Human Computer Interaction</gtr:title><gtr:parentPublicationTitle>International Journal of Mobile Human Computer Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bde0de2add3c84ba4bb3eaf0d9f119eb"><gtr:id>bde0de2add3c84ba4bb3eaf0d9f119eb</gtr:id><gtr:otherNames>Chamberlain A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a5f6c5f94f0b4.61983020</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9282C509-2E3B-4DB7-9DE9-BFA89B6EF8ED</gtr:id><gtr:title>An ontology for audio features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d63820089e593e5f0d405224a12aaad"><gtr:id>2d63820089e593e5f0d405224a12aaad</gtr:id><gtr:otherNames>Allik, A.,</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e0ce11be83.04717875</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15FDA2D2-9F64-429C-A888-8742F75FFA9E</gtr:id><gtr:title>Geo-location Adaptive Player</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/06112fc8469b2cd4ccf388f2b50854de"><gtr:id>06112fc8469b2cd4ccf388f2b50854de</gtr:id><gtr:otherNames>Perez-Carillo, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cf15c4e25a89.59971659</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B39EDA8D-D6D0-4DD8-A95B-96D561407EA9</gtr:id><gtr:title>Numbers into Notes: Digital Prototyping as Close Reading of Ada Lovelace's 'Note A'.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/72c4a51e93570b46f577021fbac43ad9"><gtr:id>72c4a51e93570b46f577021fbac43ad9</gtr:id><gtr:otherNames>De Roure, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>58bd6ba17892d7.04466219</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2090891D-3046-4284-AC5A-AE9F1A9E9702</gtr:id><gtr:title>Towards Playlist Generation Algorithms Using RNNs Trained on Within-Track Transitions</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef72cdee399b9f44871a1f918bdc9f6d"><gtr:id>ef72cdee399b9f44871a1f918bdc9f6d</gtr:id><gtr:otherNames>Choi Keunwoo</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b580be9d15a6.66609419</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0B62F1E1-7E5C-4A61-9D49-BDB874BA9F15</gtr:id><gtr:title>A Higher-Dimensional Expansion of Affective Norms for English Terms for Music Tagging.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7ee10a155d7fdf18de10c5e703a0ca0a"><gtr:id>7ee10a155d7fdf18de10c5e703a0ca0a</gtr:id><gtr:otherNames>Buccoli,M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b83783e15cc1.85712288</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>865283B6-576E-44C8-8990-38AA093F732B</gtr:id><gtr:title>Grateful Live: Mixing Multiple Recordings of a Dead Performance into an Immersive Experience</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5ae683b9113fc22ec77d6eb812b1c1e6"><gtr:id>5ae683b9113fc22ec77d6eb812b1c1e6</gtr:id><gtr:otherNames>Wilmering, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6df4968acb4.97715488</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F51918C8-331D-4789-9D44-8A536CCF72BC</gtr:id><gtr:title>Action-Sound Latency: Are Our Tools Fast Enough?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/807c035132ebdf435b30222fdcc9f233"><gtr:id>807c035132ebdf435b30222fdcc9f233</gtr:id><gtr:otherNames>Mcpherson, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d0225e6c31.11304805</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5C4774B6-8B40-4ECD-B67A-BA90FBDE91A6</gtr:id><gtr:title>On the description of process in digital scholarship</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/72c4a51e93570b46f577021fbac43ad9"><gtr:id>72c4a51e93570b46f577021fbac43ad9</gtr:id><gtr:otherNames>De Roure, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d5f2b4e549.51704934</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B85826D9-9B5D-4B42-84D8-136F7336491D</gtr:id><gtr:title>Genre-Adaptive Semantic Computing and Audio-Based Modelling for Music Mood Annotation</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7c92816b2a582aaba721e22aa41459e"><gtr:id>d7c92816b2a582aaba721e22aa41459e</gtr:id><gtr:otherNames>Saari P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cefcb97e3a60.88418997</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E8AFDFC9-8BE1-4C77-A2DC-339058A0895B</gtr:id><gtr:title>Making High-Performance Embedded Instruments with Bela and Pure Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fefc83c722323368ccdcc350bac029ac"><gtr:id>fefc83c722323368ccdcc350bac029ac</gtr:id><gtr:otherNames>Moro, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d1b80be8a1.04561203</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>40DDC6D0-CCB2-4167-81FB-5395D880616B</gtr:id><gtr:title>Automating Annotation of Media with Linked Data Workflows</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5ae683b9113fc22ec77d6eb812b1c1e6"><gtr:id>5ae683b9113fc22ec77d6eb812b1c1e6</gtr:id><gtr:otherNames>Wilmering, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf24e7751528.66525242</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8E2913EF-2C0A-4DA0-9FD1-C5F31FC2DCFB</gtr:id><gtr:title>A Dynamic Programming Variant of Non-Negative Matrix Deconvolution for the Transcription of Struck String Documents</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e4162b34f41e36022fc1f3e1a467c1e3"><gtr:id>e4162b34f41e36022fc1f3e1a467c1e3</gtr:id><gtr:otherNames>Ewert, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cedee9daad50.51279534</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0557CB93-BD3F-40A0-AE76-FDF7C0FAF61F</gtr:id><gtr:title>Towards Music Structural Segmentation across Genres</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Intelligent Systems and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d4295036b925d78acf53ac26e33a695b"><gtr:id>d4295036b925d78acf53ac26e33a695b</gtr:id><gtr:otherNames>Tian M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d7256fbe40.69534182</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6541049C-25D1-4A7B-9CB2-55DA72A50EE7</gtr:id><gtr:title>Turning Numbers into Notes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/427e78e43a4d0ded36a58505e5dbb746"><gtr:id>427e78e43a4d0ded36a58505e5dbb746</gtr:id><gtr:otherNames>Howard, E.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58b6cd2e7eb2e3.55332264</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D40B4DC2-8573-4E0E-8762-7E785EF41395</gtr:id><gtr:title>Robust and Efficient Joint Alignment of Multiple Musical Performances</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e28ac287c010756bd48daf4772ba293c"><gtr:id>e28ac287c010756bd48daf4772ba293c</gtr:id><gtr:otherNames>Wang S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d40219ab358.47698634</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>162B12C1-E338-4DB5-9AA6-1680C0034EF5</gtr:id><gtr:title>The impact of subgrouping practices on the perception of multitrack mixes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d0c3b1e19a127525da1b7c3123e28c8"><gtr:id>0d0c3b1e19a127525da1b7c3123e28c8</gtr:id><gtr:otherNames>Ronan, D. M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8adaa1efa4.59749715</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3381AF10-DCC6-43CC-A492-AAB9EAE35938</gtr:id><gtr:title>MPEG-M: A digital media ecosystem for interoperable applications</gtr:title><gtr:parentPublicationTitle>Signal Processing: Image Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/577b4a12b2504af8aded30a6d4a72fb8"><gtr:id>577b4a12b2504af8aded30a6d4a72fb8</gtr:id><gtr:otherNames>Kudumakis P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58b6ce5e97bc58.34206291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B7935436-4973-426C-9500-719E86A8BECC</gtr:id><gtr:title>MIR User Studies Through the Lense of MIR User Studies: Promoting the Impact of MIR User Research</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a72525e075193cc8f08e3e37e20a6b49"><gtr:id>a72525e075193cc8f08e3e37e20a6b49</gtr:id><gtr:otherNames>Weigl, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d8ffef0132.45507839</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CC05B544-54B8-4941-8974-97C506136B6B</gtr:id><gtr:title>Music Structural Segmentation Across Genres with Gammatone Features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/daafdc7f7d32b071a029e836ee354358"><gtr:id>daafdc7f7d32b071a029e836ee354358</gtr:id><gtr:otherNames>Tian, M..</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d6ab3fb140.07166312</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2009D55B-DA8C-4B8E-8AAB-5BFAF5BCC18A</gtr:id><gtr:title>MusicWeb: similarity modelling strategies for artist discovery</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d63820089e593e5f0d405224a12aaad"><gtr:id>2d63820089e593e5f0d405224a12aaad</gtr:id><gtr:otherNames>Allik, A.,</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e1647ba992.26873893</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>67E508BD-E034-4CEC-8CBA-7BDC5FDF58D9</gtr:id><gtr:title>Emotions in context: examining pervasive affective sensing systems, applications, and analyses</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/10357639897018d50a722671592f1946"><gtr:id>10357639897018d50a722671592f1946</gtr:id><gtr:otherNames>Kanjo E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f6638ab83</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D33F8EFF-B4A1-4560-8A9E-0291755D5E7E</gtr:id><gtr:title>Automatic Tagging using Deep Convolutional Neural Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd01de688471425898aee5aedaee41ee"><gtr:id>cd01de688471425898aee5aedaee41ee</gtr:id><gtr:otherNames>Choi, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b5803ce14811.08824857</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E7F7B7E7-70FE-4E35-B952-26CA1CC263A4</gtr:id><gtr:title>Automatic subgrouping of multitrack audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d0c3b1e19a127525da1b7c3123e28c8"><gtr:id>0d0c3b1e19a127525da1b7c3123e28c8</gtr:id><gtr:otherNames>Ronan, D. M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8a832bb2a3.43275796</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BB51153C-A006-43C3-9BA2-DECA2FA3996E</gtr:id><gtr:title>#Scanners: Exploring the Control of Adaptive Films using Brain- Computer Interaction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a42a9a81bf3264b638b0cc3d78204372"><gtr:id>a42a9a81bf3264b638b0cc3d78204372</gtr:id><gtr:otherNames>Pike, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cf16fb3785d8.73361497</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7DCDCAB1-79EB-4505-9C06-D702D2C1C9B7</gtr:id><gtr:title>A Web Application for Audience Participation in Live Music Performance: The Open Symphony Use Case</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/92f013d9f63ef0be9b2d9d45f957f79e"><gtr:id>92f013d9f63ef0be9b2d9d45f957f79e</gtr:id><gtr:otherNames>Zhang, L.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e00cb3d969.41615661</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>77048796-FAC2-4052-9DF1-BE38BDA52C9F</gtr:id><gtr:title>Geo-location Adaptive Music Player</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/06112fc8469b2cd4ccf388f2b50854de"><gtr:id>06112fc8469b2cd4ccf388f2b50854de</gtr:id><gtr:otherNames>Perez-Carillo, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf12b01134f5.15197766</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DE4ADFDF-AD8F-4973-880B-06684AD66034</gtr:id><gtr:title>The Open Multitrack Testbed</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58b582f5095d48.37333942</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FE325B0B-4834-4976-A95D-12240A9D6771</gtr:id><gtr:title>An Efficient Temporally-Constrained Probabilistic Model for Multiple-Instrument Music Transcription</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d9a2ab9e2c4aa3a6b319c700f2aefc95"><gtr:id>d9a2ab9e2c4aa3a6b319c700f2aefc95</gtr:id><gtr:otherNames>Benetos, E.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf00b2ddedc9.46066635</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DA3154B1-FF88-4C2A-A6F3-904813369EAE</gtr:id><gtr:title>Prototypical Scenarios for Contextual Navigation with MEI and Linked Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/384914f1777b1084d227dad6897c40b3"><gtr:id>384914f1777b1084d227dad6897c40b3</gtr:id><gtr:otherNames>Page, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58b6d46355d701.24580140</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>42CDADF8-AD15-4DBD-8981-B6498503F6A8</gtr:id><gtr:title>Score-Informed Source Separation for Musical Audio Recordings: An overview</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ecbadd3514247d9dd2617ec865425671"><gtr:id>ecbadd3514247d9dd2617ec865425671</gtr:id><gtr:otherNames>Ewert S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460fc08aaa3f9.42563801</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0389BAA6-53BE-4E47-A4C7-FFFC5B6C368C</gtr:id><gtr:title>residUUm: user mapping and performance strategies for multilayered live audiovisual generation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/083b1df2718c844f1243b11f2d9e0396"><gtr:id>083b1df2718c844f1243b11f2d9e0396</gtr:id><gtr:otherNames>Olowe, I.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d39fb4bdd0.61874206</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53E5535C-9346-4D97-B5ED-FAD53FD00684</gtr:id><gtr:title>Accountable Artefacts: the Case of the Carolan Guitar</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45866e7a40e830503f185653372546c3"><gtr:id>45866e7a40e830503f185653372546c3</gtr:id><gtr:otherNames>Benford, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cefedab81691.73788395</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2667F63F-BFBA-4878-A2E3-FF0033884042</gtr:id><gtr:title>Moodplay: An Interactive MoodBased Musical Experience</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a871fb78abe479a4111251e571bcfaa"><gtr:id>6a871fb78abe479a4111251e571bcfaa</gtr:id><gtr:otherNames>Barthet, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cef825c800f7.37115754</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C62782B-4845-420C-8EFA-D2AEBB0F45D9</gtr:id><gtr:title>Spatially Rendering Decomposed Recordings - Integrating Score-Informed Source Separation and Semantic Playback Technologies</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7a49cfd157c2e74b375b0b500d811d24"><gtr:id>7a49cfd157c2e74b375b0b500d811d24</gtr:id><gtr:otherNames>Thalmann, Florian</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58acc16f788146.21407981</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6875F16A-AD22-4895-BB4C-2D96B3CA1372</gtr:id><gtr:title>Annalist: A practical tool for creating, managing and sharing evolving linked data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9610bec801d7149a3aacc90ce8cd28d9"><gtr:id>9610bec801d7149a3aacc90ce8cd28d9</gtr:id><gtr:otherNames>Klyne, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6ce2074ab73.04715243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>81BEE13F-4D67-4027-84D9-919B37ACBC81</gtr:id><gtr:title>Interconnected Alchemy: An Apparatus for Alchemical Algorithms</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/72c4a51e93570b46f577021fbac43ad9"><gtr:id>72c4a51e93570b46f577021fbac43ad9</gtr:id><gtr:otherNames>De Roure, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58b6e6d00cfd70.34712312</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F9ED5F0E-71BE-42D6-BB3C-FEC3F8F1BF97</gtr:id><gtr:title>Are the Robots Coming?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bde0de2add3c84ba4bb3eaf0d9f119eb"><gtr:id>bde0de2add3c84ba4bb3eaf0d9f119eb</gtr:id><gtr:otherNames>Chamberlain A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8ed13144ed11.73862565</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>249A40FD-CB51-41F8-9951-60D667A7EEF5</gtr:id><gtr:title>Representation of Musical Structure for a Computationally Feasible Integration with Audio-Based Methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e4162b34f41e36022fc1f3e1a467c1e3"><gtr:id>e4162b34f41e36022fc1f3e1a467c1e3</gtr:id><gtr:otherNames>Ewert, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b591091de0f7.33339659</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BB8D5B94-445F-4AE8-B8F5-662A1ABE9EB8</gtr:id><gtr:title>From Interactive to Adaptive Mood-Based Music Listening Experiences in Social or Personal Contexts</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/14262211abdfa8e8ffb73d446196413a"><gtr:id>14262211abdfa8e8ffb73d446196413a</gtr:id><gtr:otherNames>Bartet M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b57bafcdc220.62254291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>58DAD6DD-E0A6-4417-95B6-BDA6F7A7DF44</gtr:id><gtr:title>The Mobile Audio Ontology: Experiencing Dynamic Music Objects on Mobile Devices</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1eb37718dd257b5a1ccd517783fbbb42"><gtr:id>1eb37718dd257b5a1ccd517783fbbb42</gtr:id><gtr:otherNames>Thalmann, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cf1a28ecd265.63485489</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9EC3D4F3-4D5E-4DEA-B1FF-505A016D5870</gtr:id><gtr:title>New sonorities for jazz recordings: Separation and mixing using deep neural networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5744161248cb24efa38c26348adbbed3"><gtr:id>5744161248cb24efa38c26348adbbed3</gtr:id><gtr:otherNames>Mimilakis, S. I.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adba317a5623.89231894</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5B53305C-0078-4D98-ACC3-08C7E8E65847</gtr:id><gtr:title>Automating Annotation of Media with Linked Data Workflows</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5ae683b9113fc22ec77d6eb812b1c1e6"><gtr:id>5ae683b9113fc22ec77d6eb812b1c1e6</gtr:id><gtr:otherNames>Wilmering, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58b8463e0394b3.57943515</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BD822145-0509-4F49-A112-C74736E6C0EB</gtr:id><gtr:title>Perceptual Evaluation of Music Mixing Practices</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8b71dc7cc6.19966332</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60165BD4-1279-4F3A-9F3F-07CA91F37068</gtr:id><gtr:title>Crossroads: Interactive Music Systems Transforming Performance, Production and Listening</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a871fb78abe479a4111251e571bcfaa"><gtr:id>6a871fb78abe479a4111251e571bcfaa</gtr:id><gtr:otherNames>Barthet, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b57c83315800.85258173</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1B90731D-215B-4F37-888A-002616DA0F58</gtr:id><gtr:title>Accounting for phase cancellations in non-negative matrix factorization using weighted distances</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ecbadd3514247d9dd2617ec865425671"><gtr:id>ecbadd3514247d9dd2617ec865425671</gtr:id><gtr:otherNames>Ewert S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460fcaa27e8b1.44817204</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2D1D53BD-BAB1-4A57-AB48-42B8D69A0C99</gtr:id><gtr:title>FEATUR.UX: An approach to leveraging multitrack information for artistic music visualization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/083b1df2718c844f1243b11f2d9e0396"><gtr:id>083b1df2718c844f1243b11f2d9e0396</gtr:id><gtr:otherNames>Olowe, I.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d2e7555ef3.28132743</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D411FE3E-40D7-4E9E-90A7-17FFFD9AD60A</gtr:id><gtr:title>Semantic Dopes?: An Ethnomethodological Study of Doing Prior-to-Play Domestic Music Consumption</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b2d268b0f646194196247fd2c52d94f"><gtr:id>7b2d268b0f646194196247fd2c52d94f</gtr:id><gtr:otherNames>Chamberlain, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cf07102868a2.60677075</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D3EFD129-9E8F-4ED0-98C9-B8516666FAEE</gtr:id><gtr:title>Automatic transcription of pitched and unpitched sounds from polyphonic music</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460fcc451d3b1.59917247</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C029622E-5422-45A6-B7C2-55E76F27E7F9</gtr:id><gtr:title>Digital Musicology: Through Research and Teaching (paper accepted)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c31e0c635a5498893be362f27a5f3798"><gtr:id>c31e0c635a5498893be362f27a5f3798</gtr:id><gtr:otherNames>Crawford, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>58bd6b640ca0e2.01371420</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D73F3CF6-EF14-48B4-85BA-4F07B2514DF9</gtr:id><gtr:title>Automatic Control of a Digital Reverberation Effect using Hybrid Models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/18a23c5c75e9505791bc6081775e580b"><gtr:id>18a23c5c75e9505791bc6081775e580b</gtr:id><gtr:otherNames>Chourdakis, E.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd8bbe90d5d1.35507490</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7261BD76-20D0-4483-A65A-F11F446D8C94</gtr:id><gtr:title>Robust Joint Alignment of Multiple Versions of a Piece of Music</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a773798606aa4ac0972c7973e0f45812"><gtr:id>a773798606aa4ac0972c7973e0f45812</gtr:id><gtr:otherNames>Wang Siying</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d8299c2629.14927659</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B61DDF00-7D54-4FBD-8038-24516AC666D5</gtr:id><gtr:title>Navigating Ontological Structures Based on Feature Metadata with the Semantic Music Player</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1eb37718dd257b5a1ccd517783fbbb42"><gtr:id>1eb37718dd257b5a1ccd517783fbbb42</gtr:id><gtr:otherNames>Thalmann, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf1897271094.68881049</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BA93541-A5A2-47ED-901B-3F5978B00969</gtr:id><gtr:title>A Network of Noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8e755e87e38b0d56694198f6102462b6"><gtr:id>8e755e87e38b0d56694198f6102462b6</gtr:id><gtr:otherNames>Emsley I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8ed17516fc25.90348158</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5482F7C9-365A-4553-B74C-1B7F1433AE83</gtr:id><gtr:title>Interacting with Robots as Performers and Producers of Music</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b2d268b0f646194196247fd2c52d94f"><gtr:id>7b2d268b0f646194196247fd2c52d94f</gtr:id><gtr:otherNames>Chamberlain, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e5aa1f4bb4.39230768</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>69C75B03-65C4-42A6-ADAD-B100E9560D6E</gtr:id><gtr:title>A score-informed shift-invariant extension of complex matrix factorization for improving the separation of overlapped partials in music recordings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be3b33e20a1649727ad2dd843473c23a"><gtr:id>be3b33e20a1649727ad2dd843473c23a</gtr:id><gtr:otherNames>Rodriguez-Serrano F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15206149</gtr:issn><gtr:outcomeId>58acc124be99e8.31192223</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7AE22522-8F5C-4800-B158-32BDAFF9A120</gtr:id><gtr:title>The Carolan guitar</gtr:title><gtr:parentPublicationTitle>interactions</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/271cca50dde31e51584fd7d588233095"><gtr:id>271cca50dde31e51584fd7d588233095</gtr:id><gtr:otherNames>Benford S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cefcb950f818.49602272</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>83EC915C-5608-460D-BBFB-4BCDF3FA38C4</gtr:id><gtr:title>Expert-Guided Semantic Linking of Music-Library Metadata for Study and Reuse</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a72525e075193cc8f08e3e37e20a6b49"><gtr:id>a72525e075193cc8f08e3e37e20a6b49</gtr:id><gtr:otherNames>Weigl, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf24592d26d0.93992143</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EB4AE1C-CC92-43BF-92B5-D4E8F3098CBB</gtr:id><gtr:title>Improved Template Based Chord Recognition Using The CRP Feature</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a81560e3291db42edcd144eb4ec93650"><gtr:id>a81560e3291db42edcd144eb4ec93650</gtr:id><gtr:otherNames>O'Hanlon, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58bd5818ba5138.79201555</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AD136EB-B74C-49AF-A4C0-8656451B6985</gtr:id><gtr:title>The Rough Mile</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5506c3710988b935a620b09aacb969ec"><gtr:id>5506c3710988b935a620b09aacb969ec</gtr:id><gtr:otherNames>Hazzard A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fed4eb30a90.10973143</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1B13BBD3-020B-4277-B96B-19BBDEDDF1B5</gtr:id><gtr:title>Robust Joint Alignment of Multiple Versions of a Piece of Music</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57fcb015a363298d25f9ca1ab6f9fb5f"><gtr:id>57fcb015a363298d25f9ca1ab6f9fb5f</gtr:id><gtr:otherNames>Wang, S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460fe992a7799.62597103</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CB3275C6-A888-4336-83D7-ACE8B392FF69</gtr:id><gtr:title>Text-based LSTM networks for Automatic Music Composition</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef72cdee399b9f44871a1f918bdc9f6d"><gtr:id>ef72cdee399b9f44871a1f918bdc9f6d</gtr:id><gtr:otherNames>Choi Keunwoo</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b582551c2993.39417633</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>99D46640-B68B-42C6-9F12-3DF0E8C33B64</gtr:id><gtr:title>Intelligent Multitrack Dynamic Range Compression</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/51bf4dfa986916ecad0537b64e1e1c57"><gtr:id>51bf4dfa986916ecad0537b64e1e1c57</gtr:id><gtr:otherNames>Ma Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58b6efbf1d3d03.06254202</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>28757A56-6078-43F7-A536-254129DD7D65</gtr:id><gtr:title>Improving Time-Scale Modification of Music Signals Using Harmonic-Percussive Separation</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c828cd33d2cc826aee11466967750bb6"><gtr:id>c828cd33d2cc826aee11466967750bb6</gtr:id><gtr:otherNames>Driedger J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460fc49c50d94.97224599</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C925663-9310-4C0A-93A1-12B02B08DC3E</gtr:id><gtr:title>The Open Multitrack Testbed: Features, content and use cases</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ca1d0064bba38feb3ca2a009419e973c"><gtr:id>ca1d0064bba38feb3ca2a009419e973c</gtr:id><gtr:otherNames>De Man, B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58aafd2b934cc7.05062250</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87444199-844D-4F96-863D-1BB0073D33EE</gtr:id><gtr:title>Towards High Level Feature Extraction from Large Live Music Recording Archives</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5ae683b9113fc22ec77d6eb812b1c1e6"><gtr:id>5ae683b9113fc22ec77d6eb812b1c1e6</gtr:id><gtr:otherNames>Wilmering, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58b846da4b9379.10647462</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60E4AA77-A65C-4969-94E0-1CD6E7023A68</gtr:id><gtr:title>Augmenting a Guitar with its Digital Footprint</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45866e7a40e830503f185653372546c3"><gtr:id>45866e7a40e830503f185653372546c3</gtr:id><gtr:otherNames>Benford, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cef8dc11a746.27181017</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB3C5A72-768A-4084-AF1C-D16770D0541A</gtr:id><gtr:title>Doing It For Themselves: The Practices of Amateur Musicians and DIY Music Networks in a Digital Age</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/207c5709dfca942d74e4912a305da2ea"><gtr:id>207c5709dfca942d74e4912a305da2ea</gtr:id><gtr:otherNames>Hoare, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56cf10c1cac962.93229881</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CEEC8470-0E8C-415C-BFAA-16FA1A3408CE</gtr:id><gtr:title>Dynamic Semantic Notation: Jamming Together Music Encoding and Linked Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a72525e075193cc8f08e3e37e20a6b49"><gtr:id>a72525e075193cc8f08e3e37e20a6b49</gtr:id><gtr:otherNames>Weigl, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d98e2cd7b5.88593431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>501AA67D-104F-47A9-9E32-E2D8B93E89CC</gtr:id><gtr:title>Audio Commons: Bringing Creative Commons Audio Content to the Creative Industries</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d488e1e88d0ac61b8a7050bff49a909"><gtr:id>0d488e1e88d0ac61b8a7050bff49a909</gtr:id><gtr:otherNames>Font, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adbb864be4e9.19234226</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8171FB77-6FAC-470E-8773-664EFD424FD3</gtr:id><gtr:title>Searching for music: understanding the discovery, acquisition, processing and organization of music in a domestic setting for design</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bde0de2add3c84ba4bb3eaf0d9f119eb"><gtr:id>bde0de2add3c84ba4bb3eaf0d9f119eb</gtr:id><gtr:otherNames>Chamberlain A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d5ec124ed95.13196684</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2F948AAD-9A86-4967-BFB4-1FB182644D01</gtr:id><gtr:title>Considering musical structure in location-based experiences</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0632980fd87f421ad27635a7ce7db706"><gtr:id>0632980fd87f421ad27635a7ce7db706</gtr:id><gtr:otherNames>Hazzard, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf09b7d104d5.39385124</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13DBC884-ED4F-4383-8439-6224BC58FC29</gtr:id><gtr:title>Interference Reduction in Music Recordings Combining Kernel Additive Modelling and Non-Negative Matrix Factorization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e4088b1f923a327d76860411d292a204"><gtr:id>e4088b1f923a327d76860411d292a204</gtr:id><gtr:otherNames>Delia Fano Yela</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58acbbffa57776.71293518</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3C9DB24C-D8FB-4E44-9065-FB25F3EB4178</gtr:id><gtr:title>myMoodplay: an Interactive Mood-Based Music Discovery App.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d63820089e593e5f0d405224a12aaad"><gtr:id>2d63820089e593e5f0d405224a12aaad</gtr:id><gtr:otherNames>Allik, A.,</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cef7bc99a8f1.05017476</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>124C0860-EFB9-43AB-8842-9610992DAEE1</gtr:id><gtr:title>Auralisation of Deep Convolutional Neural Networks: Listening to Learned Features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd01de688471425898aee5aedaee41ee"><gtr:id>cd01de688471425898aee5aedaee41ee</gtr:id><gtr:otherNames>Choi, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf0767890db1.64748866</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A307F15A-4789-49A8-8AFD-69B78897908E</gtr:id><gtr:title>Spatially Rendering Decomposed Recordings Integrating Score-Informed Source Separation and Semantic Playback technologies</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6bafffdb724574f4651e04351bc25e1d"><gtr:id>6bafffdb724574f4651e04351bc25e1d</gtr:id><gtr:otherNames>Thalman, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf183f668344.88567277</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C9C0485F-D9BA-4E0B-8489-41368083695A</gtr:id><gtr:title>Audio Commons: Bringing Creative Commons Audio Content to the Creative Industries</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d488e1e88d0ac61b8a7050bff49a909"><gtr:id>0d488e1e88d0ac61b8a7050bff49a909</gtr:id><gtr:otherNames>Font, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd8c39d86000.39848123</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>74CB09E7-174B-46CA-A07F-F8AE520EDA40</gtr:id><gtr:title>Building a Maker Community Around an Open Hardware Platform</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/84a4dbcfedd5eea84d1c6ddf3a426d67"><gtr:id>84a4dbcfedd5eea84d1c6ddf3a426d67</gtr:id><gtr:otherNames>Moreale, F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58b6e465210d31.45793373</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A442FA9-F97B-47BC-9443-525E2CBA0B3A</gtr:id><gtr:title>A Machine-Learning Approach to Application of Intelligent Artificial Reverberation</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/90da4ed181c4d86807d80eba2f5db50f"><gtr:id>90da4ed181c4d86807d80eba2f5db50f</gtr:id><gtr:otherNames>Chourdakis E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ada7ee2efd74.04268662</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B8CAF3AD-6607-4C42-A1B0-53EAEE8AC152</gtr:id><gtr:title>Template-Based Vibrato Analysis in Music Signals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f9bc97bc7060c8f8d44ebdf54509a1a2"><gtr:id>f9bc97bc7060c8f8d44ebdf54509a1a2</gtr:id><gtr:otherNames>Jonathan Driedger</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58acc090914399.70534742</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DC252448-E7F0-46A9-9A97-DCAD80D68302</gtr:id><gtr:title>The Mobile Audio Ontology: Experiencing Dynamic Music Objects on Mobile Devices</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c51cce21305fa2796d04a77079659eaf"><gtr:id>c51cce21305fa2796d04a77079659eaf</gtr:id><gtr:otherNames>Thalmann F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6d62fb81757.64618683</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>346F148A-5D2A-4BA8-8C7C-75F0C823F523</gtr:id><gtr:title>Mapping Media and Meaning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bde0de2add3c84ba4bb3eaf0d9f119eb"><gtr:id>bde0de2add3c84ba4bb3eaf0d9f119eb</gtr:id><gtr:otherNames>Chamberlain A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8ed107bf51f3.81873149</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>176672F3-B1EC-4BFF-B7BF-60E7532A6445</gtr:id><gtr:title>Understanding Music Playlists</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd01de688471425898aee5aedaee41ee"><gtr:id>cd01de688471425898aee5aedaee41ee</gtr:id><gtr:otherNames>Choi, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b581e2097533.26267299</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B474AE7C-0118-4DA4-9859-E4186EC94E8C</gtr:id><gtr:title>Score-Informed Identification of Missing and Extra Notes in Piano Recordings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bdb5cbc255f13d1e0765d8dc08d86d3"><gtr:id>4bdb5cbc255f13d1e0765d8dc08d86d3</gtr:id><gtr:otherNames>Sebastian Ewert</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58acc04ca7f577.37284538</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E22D726C-F9E8-45D0-A960-B043D172C384</gtr:id><gtr:title>Towards Music Captioning: Generating Music Playlist Descriptions</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef72cdee399b9f44871a1f918bdc9f6d"><gtr:id>ef72cdee399b9f44871a1f918bdc9f6d</gtr:id><gtr:otherNames>Choi Keunwoo</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b58113a6d0c8.45572974</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CEED1A99-9ED9-4CAC-AF0C-6939A4E0A4B0</gtr:id><gtr:title>^ muzicode$: composing and performing musical codes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a340f524af76bc707aa4eaf0ba13f1c9"><gtr:id>a340f524af76bc707aa4eaf0ba13f1c9</gtr:id><gtr:otherNames>Greenhalgh, C.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b594a3e5b3c2.71809241</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A62CB198-1765-43BE-A17E-96697CC60159</gtr:id><gtr:title>Music and Science: Parallels in Production</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/72c4a51e93570b46f577021fbac43ad9"><gtr:id>72c4a51e93570b46f577021fbac43ad9</gtr:id><gtr:otherNames>De Roure, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf080f650301.98796119</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B9CB220C-D526-4A27-B493-744709E62877</gtr:id><gtr:title>Musical Intersections across the Digital and Physical</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0632980fd87f421ad27635a7ce7db706"><gtr:id>0632980fd87f421ad27635a7ce7db706</gtr:id><gtr:otherNames>Hazzard, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56cf093ee67345.14250364</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53403071-622B-4F5E-A7F9-49DAB839D772</gtr:id><gtr:title>Compensating for Asychronies Between Musical Voices in Score-Performance Alignment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d04f3972f5a6fa5e85e015596bce9f30"><gtr:id>d04f3972f5a6fa5e85e015596bce9f30</gtr:id><gtr:otherNames>Wang, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf1c5caab1b2.41299559</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1A42FD90-8D35-4E31-9CEF-7D66C51F9C47</gtr:id><gtr:title>A Unified Access to Media Industry and Academic Datasets: A Case Study in Early Music</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a72525e075193cc8f08e3e37e20a6b49"><gtr:id>a72525e075193cc8f08e3e37e20a6b49</gtr:id><gtr:otherNames>Weigl, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf1d26e3aab2.65257865</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>957ABD7A-CEAD-419B-A253-FD84CD661B26</gtr:id><gtr:title>Experimental Digital Humanities: Creative interventions in algorithmic composition on a hypothetical mechanical computer.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/72c4a51e93570b46f577021fbac43ad9"><gtr:id>72c4a51e93570b46f577021fbac43ad9</gtr:id><gtr:otherNames>De Roure, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e6304e7df9.87519761</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DCA6898A-3016-462D-ADE8-A6A38529D6E2</gtr:id><gtr:title>Understanding Creativity and Autonomy in Music Performance and Composition: A proposed 'toolkit' for research and design</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b2d268b0f646194196247fd2c52d94f"><gtr:id>7b2d268b0f646194196247fd2c52d94f</gtr:id><gtr:otherNames>Chamberlain, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b6e55c5e3c86.99000603</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L019981/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>