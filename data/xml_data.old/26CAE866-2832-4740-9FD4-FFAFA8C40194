<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/86A3E46F-01F0-4CBF-8692-6345AA2CC0D3"><gtr:id>86A3E46F-01F0-4CBF-8692-6345AA2CC0D3</gtr:id><gtr:name>British Library The</gtr:name><gtr:address><gtr:line1>96 Euston Road</gtr:line1><gtr:postCode>NW1 2DB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/BA8EB263-C726-4DDE-A797-C357800EF65D"><gtr:id>BA8EB263-C726-4DDE-A797-C357800EF65D</gtr:id><gtr:name>I Like Music Ltd</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:department>Computing</gtr:department><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/86A3E46F-01F0-4CBF-8692-6345AA2CC0D3"><gtr:id>86A3E46F-01F0-4CBF-8692-6345AA2CC0D3</gtr:id><gtr:name>British Library The</gtr:name><gtr:address><gtr:line1>96 Euston Road</gtr:line1><gtr:postCode>NW1 2DB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BA8EB263-C726-4DDE-A797-C357800EF65D"><gtr:id>BA8EB263-C726-4DDE-A797-C357800EF65D</gtr:id><gtr:name>I Like Music Ltd</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/014F19C3-9A22-45BE-8280-481C1D69DDE9"><gtr:id>014F19C3-9A22-45BE-8280-481C1D69DDE9</gtr:id><gtr:name>I Like Music</gtr:name><gtr:address><gtr:line1>St John's Studios</gtr:line1><gtr:line2>6-8 Church Road</gtr:line2><gtr:postCode>TW9 2QA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/98842265-278F-4528-A476-8AD3E8D988FF"><gtr:id>98842265-278F-4528-A476-8AD3E8D988FF</gtr:id><gtr:firstName>Tillman</gtr:firstName><gtr:otherNames>Erik</gtr:otherNames><gtr:surname>Weyde</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/E0FB05FF-8AA1-4B31-958C-727E475E0EEB"><gtr:id>E0FB05FF-8AA1-4B31-958C-727E475E0EEB</gtr:id><gtr:firstName>Mahendra</gtr:firstName><gtr:surname>Mahey</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A815C209-760F-4D7E-BD19-FCA673896A63"><gtr:id>A815C209-760F-4D7E-BD19-FCA673896A63</gtr:id><gtr:firstName>Emmanouil</gtr:firstName><gtr:surname>Benetos</gtr:surname><gtr:orcidId>0000-0002-6820-6764</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/19C895FA-CA99-466F-A797-F5DE4C80403B"><gtr:id>19C895FA-CA99-466F-A797-F5DE4C80403B</gtr:id><gtr:firstName>Nicolas</gtr:firstName><gtr:surname>Gold</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0B7D181B-7663-4AD6-AD67-F92E38A26AC2"><gtr:id>0B7D181B-7663-4AD6-AD67-F92E38A26AC2</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:surname>Cottrell</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/621ACC54-EEA4-4D7A-BB60-D7BA844B7B07"><gtr:id>621ACC54-EEA4-4D7A-BB60-D7BA844B7B07</gtr:id><gtr:firstName>Jason</gtr:firstName><gtr:otherNames>Antony</gtr:otherNames><gtr:surname>Dykes</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=AH%2FL01016X%2F1"><gtr:id>26CAE866-2832-4740-9FD4-FFAFA8C40194</gtr:id><gtr:title>Digital Music Lab - Analysing Big Music Data</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/L01016X/1</gtr:grantReference><gtr:abstractText>Music research, particularly in fields like systematic musicology, ethnomusicology, or music psychology, has developed as &amp;quot;data oriented empirical research&amp;quot;, which benefits from computing methods. In ethnomusicology particularly, there has been a recent growing interest in computational musicology and its application to audio data collections. Similarly, the empirical study of performance of Western music, such as timing, dynamics and timbre and their relation to musical structure has a long tradition. However, this music research has so far been limited to relatively small datasets, because of technological and legal limitations. 

On the other hand, researchers in Music Information Retrieval (MIR) have started to explore large datasets, particularly in commercial recommendation and playlisting systems (e.g. The Echo Nest, Spotify), but there are differences in the terminologies, methods, and goals between MIR and musicology as well as technological and legal barriers. The proposed Digital Music Lab will support music research by bridging the gap to MIR and enabling access to large music collections and powerful analysis and visualization tools. 

The Digital Music Lab project will develop research methods and software infrastructure for exploring and analysing large-scale music collections. A major output of the project will be a service infrastructure with two prototype installations. One installation will enable researchers, musicians and general users to explore, analyse and extract information from music recordings stored in the British Library (BL). Another installation will be hosted by the Centre for Digital Music at Queen Mary University of London and provide facilities to analyse audio collections such as the I Like Music, CHARM and the Isopohnics datasets, creating a data collection of significant size (over 1m pieces). 

We will provide researchers with the tools to analyse music audio, scores and metadata. The combination of state-of-the-art music analysis on the audio and the symbolic level with intelligent collection-level analysis methods will allow for exploration and quantitative research on music that has not been not possible at this scale so far. 

The results of these analyses will be made available in the form of highly interactive visual interfaces. Musical questions we will explore include: how does performance style change change over time in relation to a particular genre or style, in classical, world, jazz, or popular music; how might performances of a given genre vary by geographical location; how does a performer's individual performance aesthetic develop over their lifetime; how might we identify the influence of one performer on another. Starting points for the analysis will be questions of musical timing and structure in piano music as well as in folk songs. We will also explore more generic musicological questions, such as the role of specific instruments in different cultures using data mining on the collection level, e.g. for relating similarities on the signal and metadata level. The resulting derived data that can be aggregated for research use, and the annotation of audio files with metadata, using all open standards such as the Music Ontology. 

The use of the proposed framework will be demonstrated in musicological research on classical music (building on the AHRC-funded CHARM and CMPCP research centres), as well as in folk, world and popular music. All results will be made available as open data/open source software. We feel that this project has the potential to bring together communities from musicology and MIR to mutual benefit.</gtr:abstractText><gtr:potentialImpactText>The beneficiaries of this project include those directly involved, and those involved through our partner the British Library, and from any likely commercialisation of project outcomes. Those directly involved are:
(1) Users of the British Library, in particular of the Digital Music Collections
(2) The British Library, in particular the British Library Labs
(3) Existing and future users of Sonic Visualiser/Annotator (estimated as at least 10,000 currently) who will benefit from the extended capabilities that will be developed by this project.
Those indirectly involved are:
(4) Potential licensees and adopters of the music analysis tools showcased by this project
(5) Potential licensees and adopters of the big data analysis infrastructure and interfaces developed for this project
(6) Customers of licensees and adopters, specifically musicologists and the music listening public

These different constituents benefit in differing ways. Users of the BL will be able to access large data sources, to analyse large datasets, to download analysis results and to have access to interactive visualisations displaying analysis results. The BL will be able to improve their service and infrastructure, and will be able to exploit the large amounts of data, which already exist in the BL Sound Archive. Users of Sonic Visualiser/Annotator will benefit from tools that offer batch analysis of large corpora and provide interactive visualisations of analysis results. Potential licensees of music analysis tools, primarily industrial and public bodies in the music technology and digital libraries sectors, will be in a position to analyse efficiently and robustly large amounts of recordings, while their customers will enjoy a more informative and compelling experience of music appreciation. Finally, licensees of big data analysis technologies will benefit from an existing infrastructure, tested on one of the largest libraries worldwide, while the provided technology will be transferable to other types of data, for example books, images, videos, or metadata.

We expect beneficiaries (1) to (2) to gain significant benefit during the lifetime of this project, with this increasing as the main outcomes of the project are disseminated. We foresee benefit to beneficiaries (3) during and after the project. Finally, beneficiaries (4), (5) and (6) should see benefit after the end of the project. 

The project includes regular interaction between researchers, partners, and users, with particular emphasis on a training workshop for the developed tools, which will take place towards the end of the project. We will also develop documentation for the project, with the objective of making it available for use by the greater public, and we will create and manage a project website and repository. In order to inform and engage users, press activities will be organised, including presence in university Open Days, blogs and social media. Mechanisms to present this project to the public will be sought in conjunction with Press and Publications Office of City, Queen Mary, and UCL. We expect also to present the work at the London-based Music Tech Festival, which attracts potential beneficiaries from the creative industry.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-04-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2014-01-02</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>451750</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>'Computer Says Show' is a two part television program on the creation of the world's first computer-generated musical 'Beyond the Fence'.</gtr:description><gtr:id>4B517447-44B7-41F0-994B-8CD378452853</gtr:id><gtr:impact>This program was broadcast on Sky Arts on the 25th Feb and 3rd March 2016.</gtr:impact><gtr:outcomeId>56de437cc1a591.67943132</gtr:outcomeId><gtr:title>TV programme: Computer Says Show</gtr:title><gtr:type>Film/Video/Animation</gtr:type><gtr:url>http://www.wingspanproductions.co.uk/news-and-awards/read/48/Beyond-the-Fence-the-world-s-first-computer-generated-musical</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>The musical Beyond the Fence was the first mainly computer generated musical, and the DML infrastructure with ASymMus similarity analysis was used to conduct research that supported the generation.</gtr:description><gtr:id>254A2FA7-EFCA-4306-A352-DC1E76E0386B</gtr:id><gtr:impact>The musical Beyond the Fence was performed in the Arts Theatre in London's West End 22 Feb - 5 March 2016.</gtr:impact><gtr:outcomeId>56de41dc938b79.54843975</gtr:outcomeId><gtr:title>Musical Composition and Production: Beyond the Fence</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:url>http://beyondthefencemusical.com/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>I Like Music Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>I Like Music Ltd</gtr:description><gtr:id>C3F443A1-77A4-46CA-8B28-144A6A60DA3F</gtr:id><gtr:impact>The ILM data is an important contribution to enable music analysis on sizeable collections.</gtr:impact><gtr:outcomeId>56e0e19728e709.37869182-1</gtr:outcomeId><gtr:partnerContribution>I Like Music provides access to over 1 million audio tracks of commercial music and several hundred thousand tracks of production music.</gtr:partnerContribution><gtr:piContribution>The DML and ASymMus project provide the hard- and software infrastructure for music analysis.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>The British Library</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>British Library</gtr:description><gtr:id>288793AD-2527-47E1-A42A-8F27814F5A44</gtr:id><gtr:impact>The collaboration has extended through out the the DML and ASymMus project and is ongoing in the above mentioned projects. We have also applied for new projects with the European Commission (under review) and are planning to apply for a research project with the EPSRC.</gtr:impact><gtr:outcomeId>56dcd4a29e1889.86228444-1</gtr:outcomeId><gtr:partnerContribution>Researchers at City University London and other contributing universities (Queen Mary University of London, University College London, Lancaster University) have created hard and software infrastructure and applications that support the British Library in serving their users with advanced services and remote access.</gtr:partnerContribution><gtr:piContribution>The British Library has been a research partner in the Digital Music Lab and has supported the ASymMus AHRC projects, and the just started Making Sense of Sounds project by Prof Plumbley (EPSRC) and the RAEng fellowship for Dr. Benetos. In these collaborations, the British Library provides access to audio and other media data, the curation and/or digitisation of collections and the creation of metadata.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>DML project at NCSML Workshop on Big Data</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>2C32D46C-9696-4D86-A2F2-05BA175D3C3D</gtr:id><gtr:impact>The Digital Music Lab project was presented at the workshop &amp;quot;Big Data, Big Models, it is a Big Deal&amp;quot; that was organised by the EPSRC network on Computational Statistics and Machine Learning (NCSML). The workshop took place at the University of Warwick between 1-2 September 2014.</gtr:impact><gtr:outcomeId>56dce08245bfe6.31104842</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www2.warwick.ac.uk/fac/sci/statistics/research/csmlnetwork/network-activities/workshops/2014/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at &quot;Quantitative Data and Music Research&quot; symposium</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D64158AA-CE49-45A6-8A4B-46522BE51FD4</gtr:id><gtr:impact>The DML project was presented at the &amp;quot;Numbers, Noises and Notes: Quantitative Data and Music Research&amp;quot; symposium, which took place on Tuesday 16th June at The Sussex Humanities Lab, University of Sussex.

During the symposium, Dr Tillman Weyde (PI for the DML project) gave a talk on &amp;quot;Analysing Big Music Data: Audio Transcription and Pitch Analysis of World and Traditional Music&amp;quot;.</gtr:impact><gtr:outcomeId>56dce8ac65ded1.18475489</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.eventbrite.co.uk/e/numbers-noises-and-notes-quantitative-data-and-music-research-symposium-tickets-17240817750</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML and ASymMus projects at Musical Timbre Workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E3693C62-E386-4071-B2BF-C8F9BC8C2880</gtr:id><gtr:impact>Recent work on instrumentation recognition and on music that was carried out as part of the DML project was presented at the Workshop on Musical Timbre, that took place on 14th November at T&amp;eacute;l&amp;eacute;com ParisTech, in Paris, France.</gtr:impact><gtr:outcomeId>56dce4781072d2.10862731</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://musictimbre.wp.mines-telecom.fr</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Digital Music Lab 1st Workshop on Analysing Big Music Data</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>4234129E-6552-4AB4-AA79-41E754410C51</gtr:id><gtr:impact>We gathered requirements and desiderata for technological support of musicological research. This has been documented on our web site and further influenced our work in the DML project.

The information gathered was very useful for the DML project so far and the workshop has raised awareness of the DML project in the relevant academic communities.</gtr:impact><gtr:outcomeId>5473c20968b902.66476388</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:url>http://dml.city.ac.uk/workshop/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at ISMIR 2014</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3C6879D3-ABD8-4273-A55D-819540AA6EB0</gtr:id><gtr:impact>Current work on the DML project was presented at the 15th International Society for Music Information Retrieval Conference (ISMIR 2014, Taipei, Taiwan):

At the Late-breaking and demo session, we presented the demo/poster &amp;quot;Visualising chord progressions in music collections: a big data approach&amp;quot; 
At the Unconference session, we organised a discussion on &amp;quot;Big Data for Music Analysis and Musicology&amp;quot;</gtr:impact><gtr:outcomeId>56dcdee7cde091.26340185</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>https://docs.google.com/presentation/d/1QCOEin_knKc0LzgyZozpZKUcULCJwa6MDjqKhju3Ob0/edit#slide=id.p</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML presentation at QMUL EECS Research Showcase</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>1F3D76C9-9BEF-4382-BB96-DF74643ACCB1</gtr:id><gtr:impact>DML presentation at QMUL EECS Research Showcase (event open to the public), April 2015.</gtr:impact><gtr:outcomeId>56dceb2bd454a9.11422445</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.eecs.qmul.ac.uk/research-showcase-2015</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Talks at the European Conference on Data Analysis</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5EE83C31-EA42-42E2-AE2B-50A3903783DB</gtr:id><gtr:impact>The two presentations lead to questions and discussions on audio data analysis at scale and its use for music research.

The talks met with interest by the international academic community present.</gtr:impact><gtr:outcomeId>5473c3a6a56761.66683841</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at BL Labs Symposium 2014</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>364636A8-8E6B-4752-AB30-4D973A87C448</gtr:id><gtr:impact>Current progress and future directions of the DML project were presented by Daniel Wolff and Adam Tovell at the British Library Labs Symposium 2014, which took place on Monday 3rd November 2014, at the British Library Conference Centre.</gtr:impact><gtr:outcomeId>56dcde1bacc587.24224192</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.eventbrite.co.uk/e/british-library-labs-symposium-2014-tickets-13385782235</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML at UCL Digifest</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>06A7318A-4085-4C1C-8054-F3B61DEEDB78</gtr:id><gtr:impact>The first UCL Digifest was held last month, from 10th-14th November. Digifest is &amp;quot;a celebration of all things digital at UCL&amp;quot;, which for the UCL Music Systems Engineering Resarch Team (a.k.a MUSERT) , a.k.a Nicolas Gold, Samer Abdallah and Christodoulos Aspromalis) meant a chance to show off some of our recent activities as well as to call together the first meeting and performance of the UCL Laptop Orchestra, or UCLOrk.

At the showcase session on Friday 14th, we demonstrated various music-related applications, including Christodoulos's affective generative music system for computer games, the MiCLUES app (MiCLUES is a Share Academy/Arts Council England-funded project in collaboration with the Royal College of Music (RCM)) to guide museum visitors to the Museum of Instruments at the RCM., a device to help keep to the recommended 4 minute shower time limit, and a prototype of the DML information management system. The DML prototype allowed users to browse an RDF graph containing information about a local collection of audio files and symbolic scores, and then use this as a jumping off point for going out into the Semantic Web to pull in more information (via Linked Open Data and SPARQL endpoints), for example, from MusicBrainz (and LinkedBrainz) or DBpedia.</gtr:impact><gtr:outcomeId>56dce3437140a2.66377176</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Undergraduate students</gtr:primaryAudience><gtr:url>http://ucldigifest.org</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML and ASyMMuS projects at DMRN+9 workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>158B4E84-3A88-40E5-8529-08C50A73DEDA</gtr:id><gtr:impact>Current progress on the DML and ASyMMuS projects were presented at the Digital Music Research Network Workshop 2014 (DMRN+9), taking place on Tuesday 16th December at Queen Mary University of London. The list of project-related presentations is as follows:

&amp;quot;The ASyMMuS project: An integrated audio-symbolic model of music similarity&amp;quot;, Emmanouil Benetos, Daniel Wolff, Tillman Weyde (City University London), Nicolas Gold, Samer Abdallah (University College London) and Alan Marsden (Lancaster University)
&amp;quot;Towards analysing big music data - Progress on the DML research project&amp;quot;, Tillman Weyde, Stephen Cottrell, Jason Dykes, Emmanouil Benetos, Daniel Wolff, Dan Tidhar, Alexander Kachkaev (City University London), Mark Plumbley, Simon Dixon, Mathieu Barthet, Steven Hargreaves (Queen Mary University of London), Nicolas Gold, Samer Abdallah (University College London), Aquiles Alancr-Brayner, Mahendra Mahey and Adam Tovell (The British Library)</gtr:impact><gtr:outcomeId>56dce5acad9527.68985606</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://c4dm.eecs.qmul.ac.uk/dmrn/events/dmrnp9/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at AES 'Cutting Edge Research' event</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>06876482-3DF6-4A83-A3B8-C3B2896868A4</gtr:id><gtr:impact>The DML project was presented at the event sponsored by the Audio Engineering Society, entitled &amp;quot;Cutting Edge Research - from City University and King's College London&amp;quot;, which took place at City University on 14th October.

The event showcased cutting edge research from City University's Music Informatics Research group and King's College London's Centre for Telecommunications Research. As part of the event, Tillman Weyde gave a talk on the group's activities (including the DML project), and Dr Dan Tidhar presented the poster entitled &amp;quot;Big Data for Musicology and Music Retrieval&amp;quot;.</gtr:impact><gtr:outcomeId>56dcdf68e52140.16764716</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.aes-uk.org/forthcoming-meetings/cutting-edge-research/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at the Society for Ethnomusicology Annual Meeting</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EB65CA34-1B8A-483C-989D-E40894949CAF</gtr:id><gtr:impact>Progress on the DML project was presented by Stephen Cottrell at the 59th annual meeting of the Society for Ethnomusicology, taking place on 13-16 November in Pittsburgh, Pennsylvania.</gtr:impact><gtr:outcomeId>56dce3e10aecb6.84510119</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.indiana.edu/~semhome/2014/index.shtml</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at Imperial Big Data workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>861FFAAC-CEE5-4ACA-B69D-F455CCD7306F</gtr:id><gtr:impact>The Digital Music Lab project was presented during the 3-day workshop on &amp;quot;Big Data: Challenges and Applications&amp;quot;, that took place at Imperial College London on 17-19th February.</gtr:impact><gtr:outcomeId>56dcdc630788c2.78389294</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://wwwf.imperial.ac.uk/~bm508/bigdata14.html</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML and ASymMus projects at FMA 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F1EF0E72-8700-4859-BC5E-D81E14C9F4E4</gtr:id><gtr:impact>Work carried out on analysing world and traditional music as part of the DML and ASymMus projects was presented at the 5th International Workshop on Folk Music Analysis (FMA 2015). FMA took place on 10-12 June in Paris, France. Project-related papers are listed below:

S. Abdallah, A. Alencar-Brayner, E. Benetos, S. Cottrell, J. Dykes, N. Gold, A. Kachkaev, M. Mahey, D. Tidhar, A. Tovell, T. Weyde, and D. Wolff, &amp;quot;Automatic transcription and pitch analysis of the British Library World &amp;amp; Traditional Music Collection&amp;quot;
A. Leroi, M. Mauch, P. Savage, E. Benetos, J. P. Bello, M. Panteli, J. Six, and T. Weyde, &amp;quot;The deep history of music project&amp;quot;</gtr:impact><gtr:outcomeId>56dce95ba10fc4.97205693</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://fma2015.sciencesconf.org</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at THATCamp British Library Labs</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>0261FFAC-1D43-48A1-9CA3-88AC5FBBB52D</gtr:id><gtr:impact>DML project members participated at the THATCamp British Library Labs, which took place on 13th February 2015 at the British Library. THATCamp stands for &amp;quot;The Humanities and Technology Camp&amp;quot;, that is an open, inexpensive meeting where humanists and technologists of all skill levels learn and build together in sessions pitched and voted on at the beginning of the day.</gtr:impact><gtr:outcomeId>56dce78e4e0f26.26504796</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://britishlibrary.typepad.co.uk/digital-scholarship/2015/02/thatcamp-british-library-labs.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Digital Music Lab Final Workshop on Analysing Big Music Data</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8E2634A3-81D0-4F8B-90ED-AEFEF6798385</gtr:id><gtr:impact>The final workshop of the DML project took place at the British Library on 13 March 2015. Following short presentations and demos of project outputs and tools, the workshop was dedicated to a hands-on guided session, in which the project's analysis and visualisation tools were applied to relevant large-scale music collections (including the British Library's Sound Archive). Musicological insights obtained by the big data approach to these collections were shared and discussed.</gtr:impact><gtr:outcomeId>56dce6f347bc85.55386874</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://dml.city.ac.uk/final-workshop/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>6000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Automatic segmentation of audio recordings to speech and music</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>City, University of London</gtr:fundingOrg><gtr:id>126344D0-3CDB-41F4-8899-AE5DC37206C1</gtr:id><gtr:outcomeId>56db027dba95f7.92292132</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>404470</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>A Machine Learning Framework for Audio Analysis and Retrieval</gtr:description><gtr:end>2020-03-02</gtr:end><gtr:fundingOrg>Royal Academy of Engineering</gtr:fundingOrg><gtr:fundingRef>RF/128</gtr:fundingRef><gtr:id>2FCF2EEA-DF57-47A5-99B9-4FA322D539A3</gtr:id><gtr:outcomeId>56db259a5882e3.14212469</gtr:outcomeId><gtr:sector>Learned Society</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>6000</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Musical theatre project</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>Wingspan Productions Ltd</gtr:fundingOrg><gtr:id>A7DC99FC-DB49-43A2-9749-9A6385267214</gtr:id><gtr:outcomeId>56dc55570c38f7.28698653</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1275401</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Making Sense of Sounds</gtr:description><gtr:end>2018-12-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N014111/1</gtr:fundingRef><gtr:id>B72C51BF-7FA9-4206-BA1F-4694921F88C4</gtr:id><gtr:outcomeId>568bdd81e07af0.10810814</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The DML system has been used in research underpinning the creation of the first computer-generated musical theatre piece. 
This work was initiated and documented by Sky Arts and led to a documentation broadcast on Sky Arts and performances of the generated musical &amp;quot;Beyond the Fence&amp;quot; in the Arts Theatre in London's West End in Feb/Mar 2016. 

The content from the BL Sound Archive has been disseminated through our user interface and the workshops we held. We also provide access to the analyses of over 200k recordings from I Like Music (BBC primary content provider).</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>06BB2C87-AB93-47B8-9D1C-59AE1E940CE3</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d99d8e426310.99440542</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We developed the Digital Music Lab system that provides an interface for analysing large amounts of data. The system has analysed over 250,000 audio tracks from the British Library and from I Like Music Ltd, and has access to around 1m further tracks, which we are aiming to analyse. 

The technological challenges of data analysis were met by combining Big Data frameworks (Apache Spark) with existing audio analysis technology (VAMP plug-ins and Sonic Annotator). The findings regarding the computational aspects suggest that with modern BigData Hard- and Software, it is possible to support music research with large-scale analysis. To make the work interactive, it is necessary to employ smart methods for caching and background calculations. 


The interaction with existing systems, technologies, and standards, such as MusicBrainz IDs and Semantic Web technologies, specifically the Music Ontology, enables the integration with other data sources on the Semantic Web.

Interactive visual interfaces to support musicological research have been developed with musicologists and evaluated. We also gathered feedback from in two workshops at City University and the British Library, respectively, where music researchers with varying degrees of technological expertise were invited and geva valuable comments and feedback.

We made some progress towards the definition musicological methods for big data, but here significant work still remains to be done. This is for two reasons: some parts of music research need to change their methods fundamentally to address Big Data, and also the tools and interfaces are only beginning to develop, so that we expect interaction an mutual learning in the future research into Big Music Data research and technology.</gtr:description><gtr:exploitationPathways>The results of this research project are available as open source software, and our software is designed for the extension to a networked system. We would hope for more libraries, archives and research institutions to start running their DML systems, so that a networked infrastructure for music research would be established. We are currently in contact with other institutions such as the Smithsonian institution (USA, DC) and Columbia University (USA, NY) to explore such collaborations. 

We are continuing work and hope to engage more collaborators to use our system to develop music research methods. To this end the server at the British Library is still available and we are disseminating the tools and approach among researchers in digital humanities. 

The developed infrastructure can also be useful for other researchers, e.g. in history of recorded speech, or other audio-related tasks, such as animal sound analysis or environmental sound analysis. The DML installation at the BL is actually being put to use by two research projects: Making Sense of Sounds (Prof M. Plumbley EP/N014111/1) and a Royal Academy of Engineering Research Fellowship (Dr E Benetos, RF/128).</gtr:exploitationPathways><gtr:id>D72246D9-5AE6-4BE5-9486-E853A6AE6D30</gtr:id><gtr:outcomeId>56d99b14566065.19733079</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Environment,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://dml.city.ac.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The features of the audio tracks analysed by the DML and ASymMus projects, including low, mid, and high-level features and similarity data, are available for download through our SPARQL endpoint. Also on request in other formats.</gtr:description><gtr:id>2D29AFEC-8099-4C0F-B59A-ACCCC67CBA73</gtr:id><gtr:impact>This data is the basis for musicological work with the DML interface, which continues to take place.</gtr:impact><gtr:outcomeId>56dd916f1c2f47.86588554</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>DML audio features, high and mid level analysis, and similarity data</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://mirg.city.ac.uk/cp/home</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Temperament data for audio analysis.</gtr:description><gtr:id>73B13B04-EDC3-4BD5-92D3-1A52A467EF20</gtr:id><gtr:impact>Publication in Early Music</gtr:impact><gtr:outcomeId>56dcf220a2b3e0.73241517</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Temperament resources</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://dml.city.ac.uk/resources/temperament/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>The DML chord sequence visualisation enables the explorative study of musical chord sequences in large datasets.</gtr:description><gtr:id>A8CA36D7-3F3D-4571-B2B2-748B88694810</gtr:id><gtr:impact>The chord sequence visualisation was used in musicological studies.</gtr:impact><gtr:outcomeId>56dcee91be9fe1.70679087</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>DML chord sequence visualisation</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>http://dml.city.ac.uk/chordseqvis/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchMaterialOutput><gtr:researchMaterialOutput><gtr:description>The DML back-end enables parallelised audio feature extraction.</gtr:description><gtr:id>6B78D0B4-D4E0-42EB-BAE9-AF79E929240A</gtr:id><gtr:impact>The outcomes are available through the feature extractions of audio material by the British Library and other sources.</gtr:impact><gtr:outcomeId>56dcedea3c2588.96531288</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>DML parallel audio processing back-end</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/dml_processing</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchMaterialOutput><gtr:researchMaterialOutput><gtr:description>The DML infrastructure is distributed framework for on-site audio analysis and collection-level analysis, semantic computation management system, with an interactive visualisation interface.</gtr:description><gtr:id>1ADCCA44-A27F-4F82-A0E5-F875554218B2</gtr:id><gtr:impact>A distributed research system for big music data analysis.</gtr:impact><gtr:outcomeId>56dcecf8109c13.04007998</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>DML System framework</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>http://mirg.city.ac.uk/cp/home</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The visualisation layer of the DML, allowing the analysis of music data collections.</gtr:description><gtr:id>E65239D7-109B-420A-A390-A44D5A8311F4</gtr:id><gtr:impact>This software was used in the DML and it's follow-on project ASymMus, as well as in the recent Musical Theatre project.</gtr:impact><gtr:outcomeId>56e0e3e22d4e69.68825292</gtr:outcomeId><gtr:title>DML Visualisation Framework</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/hg/dml-open-vis</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>https://code.soundsoftware.ac.uk/hg/dml-open-cliopatria.</gtr:description><gtr:id>A1FE3B2B-0A36-4E80-A23C-0C820C7A0B20</gtr:id><gtr:impact>This software implements the back-end information management developed in the DML and ASymMus projects that enables data analysis with the DML. It provides the API for the Web interface and access via SPARQL and Prolog.</gtr:impact><gtr:outcomeId>56dd92bdb190e5.25192690</gtr:outcomeId><gtr:title>DML Research Information and Result Management System</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/hg/dml-open-cliopatria</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D330E842-3113-463B-80F1-643CA1841936</gtr:id><gtr:title>The Beyond The Fence Musical and Computer Says Show Documentary</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dfb3539ddc79baa25031032f45c73d7a"><gtr:id>dfb3539ddc79baa25031032f45c73d7a</gtr:id><gtr:otherNames>Colton S</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56de449e0e4c63.59398834</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D68C534-A671-4EA9-B0E6-6A80E0035D9A</gtr:id><gtr:title>The deep history of music project</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d8cfe536e7081e21e1e4f51627644b53"><gtr:id>d8cfe536e7081e21e1e4f51627644b53</gtr:id><gtr:otherNames>Leroi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9c08f05a3c5.20406448</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C13102FF-6312-4195-B6E9-EF0A09525601</gtr:id><gtr:title>Visualising Chord Progressions in Music Collections: A Big Data Approach</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a2d7bf717fa5ef3de27543f4b1307bd2"><gtr:id>a2d7bf717fa5ef3de27543f4b1307bd2</gtr:id><gtr:otherNames>Kachkaev A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dc533bdad774.02366506</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>831C1D03-B367-4F27-B906-0B7B200FC29B</gtr:id><gtr:title>Compression-based Dependencies Among Rhythmic Motifs in a Score</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33c0aaf95fff2477184f45440bcf100"><gtr:id>f33c0aaf95fff2477184f45440bcf100</gtr:id><gtr:otherNames>Donat-Bouillud P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e0e0245deda1.05048353</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C8546433-F28D-43C0-8047-94212FED10B5</gtr:id><gtr:title>Automatic transcription and pitch analysis of the British Library World &amp;amp; Traditional Music Collection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9bfe4a519d1.52848227</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D1A6E6A1-1FE7-4836-AA24-258CB6BC360D</gtr:id><gtr:title>Comparing Models of Symbolic Music using Probabilistic Grammars and Probabilistic Programming</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e0df44a2fad1.76065335</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>76E91690-A036-4FDC-A062-AEA8B72F16EF</gtr:id><gtr:title>Computational Music Analysis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:isbn>978-3-319-25929-1</gtr:isbn><gtr:outcomeId>56e194750ecf89.93968825</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B0174E77-C994-40AC-B604-DF779744CE6A</gtr:id><gtr:title>The temperament police</gtr:title><gtr:parentPublicationTitle>Early Music</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97108c66f07effc5e61dfe5e5b783580"><gtr:id>97108c66f07effc5e61dfe5e5b783580</gtr:id><gtr:otherNames>Tidhar D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d9be42b88e17.14181508</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>64FF25B8-7711-42CB-8260-3437AE731AE3</gtr:id><gtr:title>The Digital Music Lab: A Big Data Infrastructure for Digital Musicology</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Intelligent Systems and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56de45bb7ab374.33960071</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E0E915CE-3DAA-4D77-BE93-5BD960E7847D</gtr:id><gtr:title>The Digital Music Lab</gtr:title><gtr:parentPublicationTitle>Journal on Computing and Cultural Heritage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58cab67b8a7009.58620194</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">AH/L01016X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>55773495-BB0B-43EB-B99D-D5C15272A52F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musical Performance</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>120E89AC-386D-4E25-9417-A3FE4D6FE83A</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musicology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>