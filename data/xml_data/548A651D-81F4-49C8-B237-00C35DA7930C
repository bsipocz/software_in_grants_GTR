<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/EC23DA53-CA73-4104-A3F6-2A9523484E69"><gtr:id>EC23DA53-CA73-4104-A3F6-2A9523484E69</gtr:id><gtr:name>Queen's University of Belfast</gtr:name><gtr:department>Electronics Electrical Eng and Comp Sci</gtr:department><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Belfast</gtr:line4><gtr:line5>County Antrim</gtr:line5><gtr:postCode>BT7 1NN</gtr:postCode><gtr:region>Northern Ireland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EC23DA53-CA73-4104-A3F6-2A9523484E69"><gtr:id>EC23DA53-CA73-4104-A3F6-2A9523484E69</gtr:id><gtr:name>Queen's University of Belfast</gtr:name><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Belfast</gtr:line4><gtr:line5>County Antrim</gtr:line5><gtr:postCode>BT7 1NN</gtr:postCode><gtr:region>Northern Ireland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/4B448898-7F57-4AC5-98BB-386452CDF5B0"><gtr:id>4B448898-7F57-4AC5-98BB-386452CDF5B0</gtr:id><gtr:firstName>Brian Desmond</gtr:firstName><gtr:surname>Green</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/59A38156-54C3-4231-B4EE-F6D5DA0090D4"><gtr:id>59A38156-54C3-4231-B4EE-F6D5DA0090D4</gtr:id><gtr:firstName>HUIYU</gtr:firstName><gtr:surname>ZHOU</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN011074%2F1"><gtr:id>548A651D-81F4-49C8-B237-00C35DA7930C</gtr:id><gtr:title>AUTOMAC: AUTOmated Mouse behAviour reCognition</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N011074/1</gtr:grantReference><gtr:abstractText>Context of the research

Neurodegenerative diseases are characterised by motor deficiencies. For many of them, there are no successful neuroprotective or neuroregenerative therapies clinically available. In order to address this problem, the development of valid animal models for motor disorders has become active growing and vibrant field in preclinical research. Behaviour analysis of laboratory animals has been recognised as a useful tool to assess therapeutic efficacy. The entire process consists of animal tracking and motion categorisation. Despite tremendous efforts made within the research community, there is no system which can perform reliable recognition of complex animal behaviours and interactions. In this project, a fully automated and trainable computer vision system is proposed to monitor and analyse complex mouse behaviours and interactions using video data recorded by calibrated cameras.

Aims and objectives

Scientific: 
(1) To develop a system of combining multi-camera tracking and a Hidden Markov Model.
(2) To improve the multi-camera tracking performance combining covariance descriptors and probabilistic data association.
(3) To accelerate K-means clustering using an approximate nearest neighbour algorithm.
Practical:
(1) To improve the performance and scalability of the existing behaviour analysis systems.
(2) To widen the scope of the applicability of the developed tracker and the behaviour recognition system. 
(3) To associate healthcare applications with the image and vision computing community.

Potential applications and benefits
(1) This project will help researchers in the healthcare/medical community to significantly reduce annotation time/errors and hence improve medical research quality. The research outcomes of the proposed multidisciplinary project can reach both ICT and healthcare communities by our attendance at conferences in different domains. In the meantime, the research communities will benefit from our publications in journals and publicly accessible tools/databases for sharing skills and experiences.
(2) The proposed research may be commercialised in the form of software tools. The UK hosts many companies that offer services related to the treatment of neurodegenerative diseases, e.g. GSK, and Orion pharmaceutical companies. These companies and their clients stand to profit from improvements in disease modelling and diagnosis/treatment techniques that depend on animal modelling using a system like that proposed in this project. Significant scientific improvements in this field will have a transformative effect on these businesses.
(3) The technologies developed in this project can be directly transferred and applied in physical security, human computer interface and virtual reality. The tools developed in the proposed research can be used to monitor moving objects (e.g. humans and animals) in different set-ups (e.g. authentic and virtual environments). The software package produced in the proposed research can easily find its customers in manufacture and design, basic science, communication engineering, media and entertainment. As a result, there is great potential for wealth creation and boosted economic prosperity from the developed software package for a wider range of applications.</gtr:abstractText><gtr:potentialImpactText>Neurodegenerative diseases are characterised by motor deficiencies. For many of them, there are no successful neuroprotective or neuroregenerative therapies clinically available. In order to address this problem, the development of valid animal models for motor disorders has become an active field in preclinical research. Behaviour analysis of laboratory animals has been recognised as a useful tool to assess therapeutic efficacy. The entire process consists of animal tracking and motion categorisation. Despite tremendous efforts made within the research community, there is no system which can perform reliable recognition of complex animal behaviours and interactions. In this project, a fully automated and trainable computer vision system is proposed to monitor and analyse complex mouse behaviours and interactions using video data recorded by calibrated cameras. 

Who will benefit from this research?

The direct beneficiaries of this project are organisations in Information, Communications Technologies and healthcare sectors. Research groups in these areas will benefit from our published research results and tools created from this work which will be made available as open source software in a Matlab format. In addition, pharmaceutical companies will benefit from the showcasing of our research results by contributing to the progress of clinical translational research. Furthermore the proposed research holds the potential to benefit industrial applications such as physical security (e.g. IBM and Tyco) and media production (e.g. Northern Ireland Screen and Wolfhound Media for animation).

How will they benefit from this research?

(1) Currently behaviour analysis of laboratory animals for neurodegenerative disease related research is still undertaken manually. This process is extremely tedious and labour-intensive with inter-annotator biases. This project, if successful, will help researchers in the healthcare/medical community to significantly reduce annotation time/errors and hence improve medical research quality. The research outcomes of the proposed multidisciplinary project can reach both ICT and healthcare communities by our attendance at conferences in different domains, e.g. British Machine Vision Conference in 2015 and World Parkinson Congress in 2016. In the meantime, the research communities will benefit from our publications in journals and publicly accessible tools/databases for sharing skills and experiences.
(2) The proposed research may be commercialised in the form of software tools. The UK hosts many companies that offer services related to the treatment of neurodegenerative diseases, e.g. GSK, and Orion pharmaceutical companies. These companies and their clients stand to profit from improvements in disease modelling and diagnosis/treatment techniques that depend on animal modelling using a system like that proposed in this project. Significant scientific improvements in this field will have a transformative effect on these businesses. As diagnosis and treatment of neurodegenerative diseases is a global challenge, UK products and services offered in this area benefit from the proposed research by commercialising and exploiting research results of this project and contributing to the creation international commercial opportunities.
(3) The technologies developed in this project can be directly transferred and applied in physical security, human computer interface and virtual reality. The tools developed in the proposed research can be used to monitor moving objects (e.g. humans and animals) in different set-ups (e.g. authentic and virtual environments). The software package produced in the proposed research can easily find its customers in manufacture and design, basic science, communication engineering, media and entertainment. As a result, wealth creation and economic prosperity will follow from the developed software package for a wider range of applications.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-12-17</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-04-18</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98860</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Invited talk at University of Exeter, December 2016</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>A61C68FD-0307-4275-B202-008AED299DB9</gtr:id><gtr:impact>Approximately 30 professors and research students attended the research talk given by Dr. Huiyu Zhou at University of Exeter. This talk, entitled &amp;quot;Mouse Behaviour Recognition&amp;quot;, sparked questions and discussion afterwards and the University reported increased interest in image processing and behaviour analysis areas.</gtr:impact><gtr:outcomeId>58bdd3103d8fc9.79610141</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>British Council Teaching Demo, Nov 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>0C6E5A87-8E37-43CA-B905-805F3D0B7CAA</gtr:id><gtr:impact>Dr. Huiyu Zhou was invited by British Council to give teaching demo to several Chinese universities, including Beijing Institute of Technology, Dalian University of Technology (Panjin), and Harbin University of Engineering/Harbin Institute of Technology. More than 200 under-and post-graduate students and professors from different disciplines attended the lectures that sparked questions and discussions afterwards. These universities reported increased interests in computer vision and animal behaviour analysis.</gtr:impact><gtr:outcomeId>58bdd601343fa9.08415768</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Undergraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Research in mouse behaviour analysis starts making impact on several fronts:(i) Collaboration with Harbin Institute of Technology (Weihai) of China, we intend to establish Belfast as an active biomedical research centre. (ii) Our technology has the potential to enhance quality of life, and security assurance in the human society.</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>A409421F-C68D-40EC-BE15-3EC6F2A565BD</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>58bddde9db30f3.33933875</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Manual measurement of mouse behavior is highly labor intensive and prone to error. This investigation aims to efficiently and accurately recognize individual mouse behaviors in action videos and continuous videos. In our system each mouse action video is expressed as the collection of a set of interest points. We extract both appearance and contextual features from the interest points collected from the training datasets, and then obtain two Gaussian Mixture Model (GMM) dictionaries for the visual and contextual features. The two GMM dictionaries are leveraged by our spatial-temporal stacked Fisher Vector (FV) to represent each mouse action video. A neural network is used to classify mouse action and finally applied to annotate continuous video. The novelty of our proposed approach is: (i) our method exploits contextual features from spatiotemporal interest points, leading to enhanced performance, (ii) we encode contextual features and then fuse them with appearance features, and (iii) location information of a mouse is extracted from spatio-temporal interest points to support mouse behavior recognition. We evaluate our method against the database of Jhuang et al. and the results show that our method outperforms several state-of-the-art approaches.</gtr:description><gtr:exploitationPathways>The proposed research will directly benefit computer vision and movement disorder researchers, who make continuous efforts to reducing the inherent high labour cost and inter-investigator variability associated with the manual annotation of mouse behaviours. The established systems strongly rely on the use of specialised sensors and are thus limited to the analysis of relatively simple and pre-programmed behaviours. This project, if successful, will bring the research of mouse behaviour analysis a big step forward in the assessment of therapeutic efficacy of neurodegenerative diseases. In addition, the proposed research can be commercialised in the form of software tools and monitoring devices. These commercialised outcomes will be adopted by medical device manufacturers and therefore the immediate beneficiaries of this project consist of pharmaceutical companies, medical associations and healthcare service agencies which perform translational research based on preclinical animal models such as the well-known 1-Methyl-4-phenyl-1,2,3,6-tetrahydropyridine (MPTP) rodent models.</gtr:exploitationPathways><gtr:id>ADC4BD19-4F87-4F9E-B226-FC054A71F4AD</gtr:id><gtr:outcomeId>58bdcaf04d5f24.06565998</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare,Pharmaceuticals and Medical Biotechnology</gtr:sector></gtr:sectors><gtr:url>http://pure.qub.ac.uk/portal/en/publications/behaviour-recognition-in-mouse-videos-using-contextual-features-encoded-by-spatialtemporal-stacked-fisher-vectors(4cf830ef-51a5-4ead-97e3-26da1316dbd4).html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>BEE40A68-177F-4EA1-A64B-A14D0591622A</gtr:id><gtr:title>Hyperspectral Unmixing via Low-Rank Representation with Space Consistency Constraint and Spectral Library Pruning</gtr:title><gtr:parentPublicationTitle>Remote Sensing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0d10ae0b5fee7f7d7a790e1b961f1e4"><gtr:id>f0d10ae0b5fee7f7d7a790e1b961f1e4</gtr:id><gtr:otherNames>Zhang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5aa6aa454bff40.36649620</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1197ED91-C209-4B39-A305-637F5CA3D381</gtr:id><gtr:title>A Biologically Inspired Appearance Model for Robust Visual Tracking.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on neural networks and learning systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/19c26a474e2dea6fb8bf670765ea183b"><gtr:id>19c26a474e2dea6fb8bf670765ea183b</gtr:id><gtr:otherNames>Zhang S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2162-237X</gtr:issn><gtr:outcomeId>59c84aa14e8e46.59300567</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6A279DB-54F3-4976-A74C-37FA8C8C0C0C</gtr:id><gtr:title>Transferring deep knowledge for object recognition in Low-quality underwater videos</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/472da9529c9f003ed033d63c908c5297"><gtr:id>472da9529c9f003ed033d63c908c5297</gtr:id><gtr:otherNames>Sun X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>0925-2312</gtr:issn><gtr:outcomeId>5aa71d9abb7521.66684165</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>85D91EF1-AD72-4530-A35E-38C4314DD496</gtr:id><gtr:title>A novel target detection method for SAR images based on shadow proposal and saliency analysis</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8e49ffed5fb0d02ad5f6ec5fce26614"><gtr:id>a8e49ffed5fb0d02ad5f6ec5fce26614</gtr:id><gtr:otherNames>Gao F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>59c84aa1c5bf18.06479697</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CD88C423-EE37-4DF3-834F-D37E9674AB3E</gtr:id><gtr:title>Evidential event inference in transport video surveillance</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d05dc1ccd62dcbe0e8a8d7708b5336fb"><gtr:id>d05dc1ccd62dcbe0e8a8d7708b5336fb</gtr:id><gtr:otherNames>Hong X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d64ddc672d0.71618987</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>31E31033-E1DE-48FC-966B-4CE483C044C2</gtr:id><gtr:title>Modeling Information Diffusion over Social Networks for Temporal Dynamic Prediction</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Knowledge and Data Engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70757d4f43d859ae837579a97bdeb00f"><gtr:id>70757d4f43d859ae837579a97bdeb00f</gtr:id><gtr:otherNames>Li D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>59c84aa18a9ca5.98747259</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C71AC4ED-C704-4F32-910B-667FAFBEBB4F</gtr:id><gtr:title>Multifeature Hyperspectral Image Classification With Local and Nonlocal Spatial Information via Markov Random Field in Semantic Space</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Geoscience and Remote Sensing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0d10ae0b5fee7f7d7a790e1b961f1e4"><gtr:id>f0d10ae0b5fee7f7d7a790e1b961f1e4</gtr:id><gtr:otherNames>Zhang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a995b8f939d49.17876029</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C852CF32-D7DE-45E1-BFDF-A601195B7DC1</gtr:id><gtr:title>A Novel Active Semisupervised Convolutional Neural Network Algorithm for SAR Image Recognition.</gtr:title><gtr:parentPublicationTitle>Computational intelligence and neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8e49ffed5fb0d02ad5f6ec5fce26614"><gtr:id>a8e49ffed5fb0d02ad5f6ec5fce26614</gtr:id><gtr:otherNames>Gao F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa6a4d0d94053.31847835</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7726E083-D20F-4825-80E2-164590EFE2EB</gtr:id><gtr:title>Automatic Chinese Postal Address Block Location Using Proximity Descriptors and Cooperative Profit Random Forests</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Industrial Electronics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8529b95f762c00cf1e0a6a341a5c28aa"><gtr:id>8529b95f762c00cf1e0a6a341a5c28aa</gtr:id><gtr:otherNames>Dong X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2fe9c290cd80.53530562</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>469649EC-F04F-435A-AE14-962B53BCC599</gtr:id><gtr:title>Point-to-Set Distance Metric Learning on Deep Representations for Visual Tracking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Intelligent Transportation Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/19c26a474e2dea6fb8bf670765ea183b"><gtr:id>19c26a474e2dea6fb8bf670765ea183b</gtr:id><gtr:otherNames>Zhang S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a66fc53048117.68029957</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EBFA7CAA-D68E-4F1D-AF80-92A28279A8B3</gtr:id><gtr:title>BEHAVIOUR RECOGNITION IN MOUSE VIDEOS USING CONTEXTUAL FEATURES ENCODED BY SPATIAL-TEMPORAL STACKED FISHER VECTORS</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/69a337199c246ef01510107be992d222"><gtr:id>69a337199c246ef01510107be992d222</gtr:id><gtr:otherNames>Z Jiang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58bdcf68df3d81.87659788</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5897A1C-9B18-4FEE-B83D-0EF91B6574A7</gtr:id><gtr:title>Recursive Autoencoders-Based Unsupervised Feature Learning for Hyperspectral Image Classification</gtr:title><gtr:parentPublicationTitle>IEEE Geoscience and Remote Sensing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0d10ae0b5fee7f7d7a790e1b961f1e4"><gtr:id>f0d10ae0b5fee7f7d7a790e1b961f1e4</gtr:id><gtr:otherNames>Zhang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fece95ebf61.69060735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>44C03DEF-B475-4F6B-B79E-74C763C8D1AF</gtr:id><gtr:title>An evidential fusion approach for gender profiling</gtr:title><gtr:parentPublicationTitle>Information Sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/34289d1f3f86c00e4e17601fafeefa31"><gtr:id>34289d1f3f86c00e4e17601fafeefa31</gtr:id><gtr:otherNames>Ma J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d67b5185f66.11034886</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N011074/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>