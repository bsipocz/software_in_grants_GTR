<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7AC2BED0-677B-4CF0-8D99-5E42D5DF1D28"><gtr:id>7AC2BED0-677B-4CF0-8D99-5E42D5DF1D28</gtr:id><gtr:firstName>Patrick</gtr:firstName><gtr:otherNames>George</gtr:otherNames><gtr:surname>Healey</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/058B7945-94E9-450F-A970-E88BA39F33D9"><gtr:id>058B7945-94E9-450F-A970-E88BA39F33D9</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Johnathan</gtr:otherNames><gtr:surname>Bryan-Kinns</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE045235%2F1"><gtr:id>F378F76C-2984-41F7-A525-4AFEA6CCBA0A</gtr:id><gtr:title>Platform Grant: Centre for Digital Music</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E045235/1</gtr:grantReference><gtr:abstractText>Digital Music is a rapidly growing research area. Its research covers a range of disciplines, including computer science, information science, audio engineering, digital signal processing and musicology. The music industry is undergoing radical change from a physical to digital distribution model. Digital music downloads are above $1.1bn worldwide and rising rapidly; in the UK these grew over 350% in 2005. The internet and peer-to-peer (P2P) technologies have opened the possibility for new or small bands to release their music to a wide range of listeners, although the problem remains of how their listeners may find them and their music. Finally, it must be stated that music is not simply a commodity to be delivered to consumers, but is also a medium for creative expression and social interaction: people do not merely consume music but engage with it. The Centre for Digital Music (C4DM) at Queen Mary University of London is a world-leading multidisciplinary research group in this field of Music and Audio Technology. Our current research is focussed in two main areas: signal processing of digital music; and digital music performance and interaction. By early 2007 C4DM will have about 40 full-time members, including academic staff, research staff, research students and visitors.This Platform Grant will provide the Centre for Digital Music with background funding to allow us to further enhance our international research reputation, and to continue to be a major contributor to the UK research base in this area.This will be achieved through:* Retention of key research staff;* Retaining key graduating PhD students as named researchers;* An internship and outreach scheme;* Maximizing research impact &amp;amp; exploitation; and* Adventurous interdisciplinary projects.These will be in addition to the usual type of research projects, funded by EPSRC, EU and others, which we will continue through the duration of the Platform Grant and beyond.</gtr:abstractText><gtr:fund><gtr:end>2012-07-29</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-07-30</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1054460</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>New York University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Research visit by Maria Pantelli, New York University (NYU)</gtr:description><gtr:id>817FD70D-242A-4C03-AD61-2EC6BB142F1A</gtr:id><gtr:impact>For testing the algorithms developed in this research, music/speech annotations were created for a corpus of 360 world music recordings and melody contours were annotated for a set of 30 world music recordings. Other outcomes of this visit included the representation of music metadata using graphs, in particular hypergraphs, and the application of community detection algorithms to identify meaningful partitions of the graph. 

Maria and collaborators developed a software package and an interactive visualisation for characterisation of singing styles in world music. She also developed methods for the representation of music metadata using graphs, in particular hypergraphs, and the application of community detection algorithms to identify meaningful partitions of the graph. 

Maria collaborated with faculty and students in the Music Research Lab at NYU, as evidenced by the co-authored paper listed below:

M. Panteli, R. Bittner, J. P. Bello, and S. Dixon, &amp;quot;Towards the Characterization of Singing Styles in World Music,&amp;quot; IEEE International Conference on Acoustics, Speech and Signal Processing, submitted.</gtr:impact><gtr:outcomeId>58ab0a8e5a79b1.02005569-1</gtr:outcomeId><gtr:partnerContribution>The purpose of this research visit was to increase knowledge and resources from collaboration with the music technology group at New York University (NYU). 

Maria and collaborators developed a software package and an interactive visualisation for characterisation of singing styles in world music. She also developed methods for the representation of music metadata using graphs, in particular hypergraphs, and the application of community detection algorithms to identify meaningful partitions of the graph.

Maria collaborated with faculty and students in the Music Research Lab at NYU, as evidenced by the co-authored paper listed below:

M. Panteli, R. Bittner, J. P. Bello, and S. Dixon, &amp;quot;Towards the Characterization of Singing Styles in World Music,&amp;quot; IEEE International Conference on Acoustics, Speech and Signal Processing, submitted.</gtr:partnerContribution><gtr:piContribution>Pantelli's PhD is part of a bigger collaboration, namely the &amp;quot;Deep History of Music (DHOM)&amp;quot; project, involving researchers from six universities. This research visit was expected to contribute to new data resources collected from NYU and affiliated institutions and is also linked to a grant proposal, written by professor Armand Leroi, Imperial, in collaboration with Queen Mary.

 During Maria's stay at New York University between 22 May and 15 August 2016 she had the opportunity to meet with people from both academia and the industry. In particular she attended a meeting between academics of New York University and NYU Abu Dhabi discussing potential collaborations between their institutions and Queen Mary. Following this meeting she has been invited to attend the 3rd workshop on &amp;quot;Cross-disciplinary and Multicultural Aspects of Musical Rhythms&amp;quot;, Abu Dhabi, 17-20 March 2017, as a way to keep the NYU-QM collaboration going. What is more, she attended local events held at Spotify New York and she presented her work to industry-related audience promoting research of Queen Mary University of London. 

The findings of this work are summarized in a research paper (see next section) that has been submitted to the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing 2017. The work focuses on the characterisation of singing styles in world music and includes the development of a software package and an interactive visualization. 

For testing the algorithms developed in this research, music/speech annotations were created for a corpus of 360 world music recordings and melody contours were annotated for a set of 30 world music recordings.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Networking at Audio Engineering Society Convention</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>0B10EBBC-A014-4E13-972B-615E3464E18B</gtr:id><gtr:impact>Reiss and Perez attended the 123rd AES Convention (New York, 2007) to explore collaborative research and exploitation possibilities of their automatic mixing technology. The discussions led to follow-up meetings and collaborations, described below.

This led to new links with DTS (and a subsequent return visit), Sony Entertainment, Midas Klark Teknik, and Wolfson Electronics. Other discussion with Djurek et al (U of Zagreb) led to an AES convention paper and a journal paper in JASA (Reiss et al, 2008a;b). 
This trip was a major contributor to the creation of the spin-off company MixGenius.</gtr:impact><gtr:outcomeId>54608df883a5c8.16385842</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2007</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at industry convention</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8A02F81F-A304-4247-AACC-632F4CF49F2B</gtr:id><gtr:impact>Raised the profile of C4DM and its research to the audio engineering community of Latin America

Further invitations for talks and leading workshops at AES conventions. Discussion of this new emerging field in industry trade magazines and journals.</gtr:impact><gtr:outcomeId>54608cf4bf1a30.70265957</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2010</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1161334</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Platform Grant: Digital Music</gtr:description><gtr:end>2018-01-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/K009559/1</gtr:fundingRef><gtr:id>465B9ED4-A8DC-4D9B-B768-43D590D216BC</gtr:id><gtr:outcomeId>5edbe7205edbe734</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Impacts include a number of artistic outputs, including experimental piano piece with Sarah Nicolls, a wearable musical instrument (the Serendiptichord) with artist Di Mainstone, and an Augmented Instruments Concert. A Beat and Rhythm Warping software API has been used in a rhythm morphing app by LickWorx.</gtr:description><gtr:firstYearOfImpact>2008</gtr:firstYearOfImpact><gtr:id>F07E6100-9097-4F8A-9440-54C35546E9B9</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54635fd36b8206.30319054</gtr:outcomeId><gtr:sector>Creative Economy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The aim of this &amp;quot;Platform Grant&amp;quot; was to maintain and strengthen the Centre for Digital Music as a world-leading research group, through the provision of background funding to provide stability and flexibility in our recruitment of key research staff. We undertook this through Retention of key research staff, Retention of key graduating PhD students as researchers, Maximizing impact (e.g. through presentations at events), Interdisciplinary projects / feasibility studies, and Internship / outreach / visits.</gtr:description><gtr:exploitationPathways>Potential beneficiaries of the research supported by this Platform Grant outside of the academic research community include anyone who could benefit from latest research in audio and music. Examples from various sectors are given below.

Commercial private sector:

* Commercial companies designing audio equipment, through easier access to new audio research

* Companies wishing to provide new ways for their customers to discover or access music based on music recommendation research

* Musicians and sound artists, through their ability to use new research methods and processes for new creative outputs

* Computer games companies, for improved game music and audio, or for new types of audio or music-based computer games

* Hearing aid and cochlear implant manufacturers, through access to new research applicable to hearing improvement

* Audio archiving companies, through access to the latest algorithms and methods for music information retrieval

* Television and radio companies, through ability to use the latest research in the design of new technology-based programmes or in new audio production processes.

Policy-makers and others in government and government agencies:

* Police and security services, through access to methods for analysing and separating audio signals.

Public sector, third sector and others:

* Healthcare workers who work with music and audio, such as music therapists, through new music and audio visualization tools

* Museums, through tools to measure and visualise acoustic properties of objects

* Libraries, through improved open access to our research results for the benefit of their users

* Standards organizations, through access to new research methods on which to base forthcoming standards

* Science promotion organizations, through availability of high-quality usable research tools attractive to people who may be interested in science.

Wider public:

* People interested in exploring music or other audio recordings at home, school, college or university, using the latest research, either for educational or general interest purposes

* Teachers in schools, colleges or universities who want to use our research-based software for teaching audio or music

* Audiences of creative output involving audio and music, through availability of new creative outputs or technology facilitated by our audio and digital music research. Beneficiaries of our research include: other researchers in the audio and digital music research; researchers in other fields who might use the results of our research, including musicologists, audio engineers, bio-acousticians, and auditory psychologists; and researchers who use techniques related to ours in other fields, such as medical signal processing or general semantic web research.</gtr:exploitationPathways><gtr:id>21A2E0D7-56F7-44B4-85C4-90B692C666A3</gtr:id><gtr:outcomeId>r-4937567249.8136457760b5d2</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://c4dm.eecs.qmul.ac.uk/platform.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>An open-source VAMP plugin version of a real-time music transcription algorithm.</gtr:description><gtr:id>74050D1D-6B24-4301-991A-EE6BE8D57C20</gtr:id><gtr:impact>This was the first of its kind. It has resulted in 1000s of downloads, and has been used in further research at other institutions.</gtr:impact><gtr:outcomeId>54608ef1c0d147.55044420</gtr:outcomeId><gtr:title>real-time music transcription</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2009</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Suite of software usable in live performance which is based on our research methods</gtr:description><gtr:id>7F3AC174-45CF-49DD-A538-798279B3AF31</gtr:id><gtr:impact>Stark co-founded start-up Codasign</gtr:impact><gtr:outcomeId>54624d908d5766.75418880</gtr:outcomeId><gtr:title>QM Live Music Lab</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://livemusiclab.eecs.qmul.ac.uk/</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Matlab toolbox for processing signals using adaptive sparse structured representations</gtr:description><gtr:id>0ADA67B0-675E-4EDC-B9EA-940ECD940675</gtr:id><gtr:impact>Continued by sister FP7 project. Toolbox downloaded over 20,000 times.</gtr:impact><gtr:outcomeId>54624c7d75c270.83106836</gtr:outcomeId><gtr:title>SMALLbox</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/smallbox</gtr:url><gtr:yearFirstProvided>2010</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>01B62374-6F0E-425C-B403-D5F23FD34AFD</gtr:id><gtr:title>Estimation of harpsichord inharmonicity and temperament from musical recordings.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6d76a8ab4323a3f8dd805c24e141802"><gtr:id>e6d76a8ab4323a3f8dd805c24e141802</gtr:id><gtr:otherNames>Dixon S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>54647c8c08f449.71469834</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>72F62D13-8E4E-4DFA-A5A6-1CE718CE491F</gtr:id><gtr:title>MuViSync: Realtime music video alignment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/276e783536aba480901f55fd4a3c1817"><gtr:id>276e783536aba480901f55fd4a3c1817</gtr:id><gtr:otherNames>Macrae R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7491-2</gtr:isbn><gtr:outcomeId>5464ec28d3c830.17664763</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2861BBA7-1F08-4895-A80B-DB67CFD92D49</gtr:id><gtr:title>Music recommendation for music learning: Hotttabs, a multimedia guitar tutor</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a871fb78abe479a4111251e571bcfaa"><gtr:id>6a871fb78abe479a4111251e571bcfaa</gtr:id><gtr:otherNames>Barthet, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5463480e745db8.02381084</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE379A8C-1CBD-4334-994A-45679DF30C12</gtr:id><gtr:title>The Temperament Police: The Truth, the Ground Truth and Nothing but the Truth</gtr:title><gtr:parentPublicationTitle>Proceedings of the 12th International Society for Music Information Retrieval Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5db4d8e2093abd51c3e08adaa58cbfbc"><gtr:id>5db4d8e2093abd51c3e08adaa58cbfbc</gtr:id><gtr:otherNames>S. Dixon</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54647dc44652d9.75597057</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2DF59777-D3F8-470B-B892-D109672952D7</gtr:id><gtr:title>Characterisation of acoustic scenes using a temporally-constrained shift-invariant model</gtr:title><gtr:parentPublicationTitle>Proceedings of the 15th International Conference on Digital Audio Effects</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1bf959dfb5fc156aabffd272f886a652"><gtr:id>1bf959dfb5fc156aabffd272f886a652</gtr:id><gtr:otherNames>E. Benetos</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464ed2e3ddec7.36773319</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CEDFD051-7126-42E4-9F89-34EDA84861EF</gtr:id><gtr:title>The temperament police</gtr:title><gtr:parentPublicationTitle>Early Music</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97108c66f07effc5e61dfe5e5b783580"><gtr:id>97108c66f07effc5e61dfe5e5b783580</gtr:id><gtr:otherNames>Tidhar D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54647fcc020a99.78395396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A621E7D-89A1-4761-A5A8-EA00A156EB10</gtr:id><gtr:title>Microphone interference reduction in live sound</gtr:title><gtr:parentPublicationTitle>Proceedings of the 14th International Conference on Digital Audio Effects, DAFx 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/094bce5a19f18bc0a2b2e7df38fd607d"><gtr:id>094bce5a19f18bc0a2b2e7df38fd607d</gtr:id><gtr:otherNames>Clifford A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545fe925a60941.90005751</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E35EED1-76E7-4F32-A8C8-540E21A34C1C</gtr:id><gtr:title>An investigation of musical timbre: Uncovering salient semantic descriptors and perceptual dimensions</gtr:title><gtr:parentPublicationTitle>Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45e63a0e03a91d0d9d97099b9becae3b"><gtr:id>45e63a0e03a91d0d9d97099b9becae3b</gtr:id><gtr:otherNames>Zacharakis A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545fe926201161.32975137</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>11039CD5-F00D-4C3A-8BEF-3D714452ED66</gtr:id><gtr:title>Machine Audition - Principles, Algorithms and Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fab66f57949956d073b8fc01f017a63f"><gtr:id>fab66f57949956d073b8fc01f017a63f</gtr:id><gtr:otherNames>Zhou R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9781615209194</gtr:isbn><gtr:outcomeId>545ce1d45a2850.31151314</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0B5B65D-7C4E-4026-841F-1C0D37BC2034</gtr:id><gtr:title>Determined source separation for microphone recordings using IIR filters</gtr:title><gtr:parentPublicationTitle>129th Audio Engineering Society Convention 2010</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cc80f5abfd6e074e2ae0c7ed52bd44f7"><gtr:id>cc80f5abfd6e074e2ae0c7ed52bd44f7</gtr:id><gtr:otherNames>Uhle C.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>545fea3fe14209.88676787</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95058F73-14A2-4E88-AF5D-DFA11A3E9DAE</gtr:id><gtr:title>Blind source separation of periodic sources from sequentially recorded instantaneous mixtures</gtr:title><gtr:parentPublicationTitle>ISPA 2011 - 7th International Symposium on Image and Signal Processing and Analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/46a1de75c85406c73c5b9493ca7c705f"><gtr:id>46a1de75c85406c73c5b9493ca7c705f</gtr:id><gtr:otherNames>Jafari M.G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545ce60d537316.78325491</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87BB8EE2-D557-4E0B-9F04-D4444E3AA404</gtr:id><gtr:title>An additive synthesis technique for independent modification of the auditory perceptions of brightness and warmth</gtr:title><gtr:parentPublicationTitle>130th Audio Engineering Society Convention 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45e63a0e03a91d0d9d97099b9becae3b"><gtr:id>45e63a0e03a91d0d9d97099b9becae3b</gtr:id><gtr:otherNames>Zacharakis A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545fe92644db08.37039617</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EE50370D-E84D-4E22-A6E8-10EF6B6498D3</gtr:id><gtr:title>Machine Audition - Principles, Algorithms and Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76932d4d499b7c53fc66a3b063bfd255"><gtr:id>76932d4d499b7c53fc66a3b063bfd255</gtr:id><gtr:otherNames>Vincent E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9781615209194</gtr:isbn><gtr:outcomeId>545ce4e9ed0f83.38576242</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5432607-61A6-4A7C-9EAC-BD6B45746CD9</gtr:id><gtr:title>Intelligent systems for mixing multichannel audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c63871656ebadc0fecb3680badeb9bf7"><gtr:id>c63871656ebadc0fecb3680badeb9bf7</gtr:id><gtr:otherNames>Reiss J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0273-0</gtr:isbn><gtr:outcomeId>545ce0db7f4587.48893481</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96CEE316-E801-4E26-B55B-BF7445EF6AD1</gtr:id><gtr:title>Proximity effect detection for directional microphones</gtr:title><gtr:parentPublicationTitle>131st Audio Engineering Society Convention 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/094bce5a19f18bc0a2b2e7df38fd607d"><gtr:id>094bce5a19f18bc0a2b2e7df38fd607d</gtr:id><gtr:otherNames>Clifford A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545fe9257ce669.32804698</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92685986-23F5-4FFD-B233-9C2B1CD5613F</gtr:id><gtr:title>Efficient Bayesian inference for harmonic models via adaptive posterior factorization</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76932d4d499b7c53fc66a3b063bfd255"><gtr:id>76932d4d499b7c53fc66a3b063bfd255</gtr:id><gtr:otherNames>Vincent E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d0020021fe0b8f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BD5BF5F6-E7D4-4A75-9E29-8142CCFBDD2E</gtr:id><gtr:title>Gradient Polytope Faces Pursuit for large scale sparse recovery problems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/303475a79191325e38a730bc1d39b2f3"><gtr:id>303475a79191325e38a730bc1d39b2f3</gtr:id><gtr:otherNames>Gretsistas A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-4295-9</gtr:isbn><gtr:outcomeId>545ce4efb00af1.63728630</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F720BF89-8E08-412D-8263-025F2B5CE2BC</gtr:id><gtr:title>Implementation and evaluation of autonomous multi-track fader control</gtr:title><gtr:parentPublicationTitle>132nd Audio Engineering Society Convention 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61a77db6470d991bc8a033550353db09"><gtr:id>61a77db6470d991bc8a033550353db09</gtr:id><gtr:otherNames>Mansbridge S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545fe869767852.56226000</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BCAAC95A-4A3A-42A2-9BD9-062E6CADCF03</gtr:id><gtr:title>Verification of chaotic behavior in an experimental loudspeaker.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5b0cf193e0fbe2ad1e664d9d4482fba5"><gtr:id>5b0cf193e0fbe2ad1e664d9d4482fba5</gtr:id><gtr:otherNames>Reiss JD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>545fe4c22bcba0.88222550</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB8DBDCD-951F-44FC-8451-9410D260A6A6</gtr:id><gtr:title>Measuring the Performance of Beat Tracking Algorithms Using a Beat Error Histogram</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/18859fae61e0bc2d32267048f675108f"><gtr:id>18859fae61e0bc2d32267048f675108f</gtr:id><gtr:otherNames>Davies M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545ce4ec7516f3.85754396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1FCAD6C1-341A-4AC2-B281-6F28485B9918</gtr:id><gtr:title>A Wiener Filter Approach to Microphone Leakage Reduction in Close-Microphone Applications</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6b63a60f7a562187a2affb36d684a098"><gtr:id>6b63a60f7a562187a2affb36d684a098</gtr:id><gtr:otherNames>Kokkinis E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546089908fc009.50524862</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A64ECE3-453D-4E60-828D-B043F2769DEC</gtr:id><gtr:title>A guitar tablature score follower</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/276e783536aba480901f55fd4a3c1817"><gtr:id>276e783536aba480901f55fd4a3c1817</gtr:id><gtr:otherNames>Macrae R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7491-2</gtr:isbn><gtr:outcomeId>5464eb8aa9b1a8.59816886</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88BB8541-9EF5-4A4C-AFD6-36A604F57970</gtr:id><gtr:title>Onset Event Decoding Exploiting the Rhythmic Structure of Polyphonic Music</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4e6ff1522b3fe45d9a0ebd4aebdbe638"><gtr:id>4e6ff1522b3fe45d9a0ebd4aebdbe638</gtr:id><gtr:otherNames>Degara N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545ce4eb4029d9.60018294</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55646413-6808-458A-A0AD-5837047A6BD4</gtr:id><gtr:title>Reliability-Informed Beat Tracking of Musical Signals</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4e6ff1522b3fe45d9a0ebd4aebdbe638"><gtr:id>4e6ff1522b3fe45d9a0ebd4aebdbe638</gtr:id><gtr:otherNames>Degara N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545ce4e8de9706.33143916</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43CFE567-2443-4E66-B5BE-7BDCDED4C216</gtr:id><gtr:title>Computer aided music therapy evaluation: Testing the Music Therapy Logbook prototype 1 system</gtr:title><gtr:parentPublicationTitle>The Arts in Psychotherapy</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6c7dc80228a5b2e5fc85ef9f11941cbf"><gtr:id>6c7dc80228a5b2e5fc85ef9f11941cbf</gtr:id><gtr:otherNames>Streeter E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545ce1d6098db4.94834165</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B40C0AB1-5CA2-46AC-9CCB-C012BD890EA2</gtr:id><gtr:title>Post-processing fiddle~: A real-time multi-pitch tracking technique using harmonic partial subtraction for use within live performance systems</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2009 International Computer Music Conference, ICMC 2009</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/faa27d0691e46a159226eb9f6ba26bd1"><gtr:id>faa27d0691e46a159226eb9f6ba26bd1</gtr:id><gtr:otherNames>Robertson A.N.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>545ce4ea54f406.16591347</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>739BFE04-2B02-4C9B-B55C-19F71BF1CB49</gtr:id><gtr:title>Detection of 'solo intervals' in multiple microphone multiple source audio applications</gtr:title><gtr:parentPublicationTitle>130th Audio Engineering Society Convention 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07f9cfbc5b7d340441a00879d3ebb434"><gtr:id>07f9cfbc5b7d340441a00879d3ebb434</gtr:id><gtr:otherNames>Kokkinis E.K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>545fe64758f065.56982441</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>30C4899E-F7D1-44FF-88BA-A8D118DA5246</gtr:id><gtr:title>Intonation in unaccompanied singing: accuracy, drift, and a model of reference pitch memory.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ba0eefbf6e56d17315a1570e42ce7dbe"><gtr:id>ba0eefbf6e56d17315a1570e42ce7dbe</gtr:id><gtr:otherNames>Mauch M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>5464e600d28407.89892268</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2B77453-6A95-41C6-BC54-0BD23804B889</gtr:id><gtr:title>Birdsong and C4DM: A survey of UK birdsong and machine recognition for music researchers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f983abfee5f1c8930980aa3371fd098b"><gtr:id>f983abfee5f1c8930980aa3371fd098b</gtr:id><gtr:otherNames>Stowell, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>545cd8c34ed027.24802406</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>681FE8B6-F626-49BE-A001-AC96F89A52DF</gtr:id><gtr:title>An adaptive stereo basis method for convolutive blind audio source separation</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b9356ae1bbebe4e3099c1a924c5866a"><gtr:id>8b9356ae1bbebe4e3099c1a924c5866a</gtr:id><gtr:otherNames>Jafari M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d0020021d1ac39</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13BB3AE6-5E58-4096-AD3A-815F0806FF08</gtr:id><gtr:title>The Serendiptichord: Reflections on the Collaborative Design Process between Artist and Researcher</gtr:title><gtr:parentPublicationTitle>Leonardo</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/470a8b0324678cd2516daace017af908"><gtr:id>470a8b0324678cd2516daace017af908</gtr:id><gtr:otherNames>Murray-Browne T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545ce4e42f4a78.39955194</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>04463161-A58A-4003-935C-03A8B647ACCD</gtr:id><gtr:title>Separating sources from sequentially acquired mixtures of heart signals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3f629f784eea9e5448ce18a7251ac7d"><gtr:id>e3f629f784eea9e5448ce18a7251ac7d</gtr:id><gtr:otherNames>Hedayioglu F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0538-0</gtr:isbn><gtr:outcomeId>545ce4e7b14f04.66150146</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A6F0F2ED-4BF6-4890-B07F-F7019482B399</gtr:id><gtr:title>Towards a musical beat emphasis function</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/18859fae61e0bc2d32267048f675108f"><gtr:id>18859fae61e0bc2d32267048f675108f</gtr:id><gtr:otherNames>Davies M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-3678-1</gtr:isbn><gtr:outcomeId>545ce4e3ba5560.48396359</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>44A82C36-08EF-4118-9EBE-B49F317E68DC</gtr:id><gtr:title>Confirmation of chaos in a loudspeaker system using time series analysis</gtr:title><gtr:parentPublicationTitle>Audio Engineering Society - 125th Audio Engineering Society Convention 2008</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/13d9066a2b3bfc71f3c550010c6f5a0c"><gtr:id>13d9066a2b3bfc71f3c550010c6f5a0c</gtr:id><gtr:otherNames>Reiss J.D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>545fe4c2067128.57323460</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F99F47AD-1BC9-44CF-8421-7AE568D0D048</gtr:id><gtr:title>A Computationally Efficient Method for Polyphonic Pitch Estimation</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Advances in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fab66f57949956d073b8fc01f017a63f"><gtr:id>fab66f57949956d073b8fc01f017a63f</gtr:id><gtr:otherNames>Zhou R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>545fe71a8a4787.41156977</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B886E8FF-2E7D-4867-A76C-D81AAFDCC6E1</gtr:id><gtr:title>Improving Music Genre Classification Using Automatically Induced Harmony Rules</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e194d7894b0a94988dd28113116f7ec9"><gtr:id>e194d7894b0a94988dd28113116f7ec9</gtr:id><gtr:otherNames>Anglade A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>5464ea807cc572.86420533</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B3C416D-A8FA-4C6F-8D48-C071D3D9E168</gtr:id><gtr:title>Event-based Multitrack Alignment using a Probabilistic Framework</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c5fab8b9a72eda3e95e607775c722446"><gtr:id>c5fab8b9a72eda3e95e607775c722446</gtr:id><gtr:otherNames>Robertson A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568abac38ea933.80583373</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>629EBAA3-D9BF-4329-953F-09E424CF1203</gtr:id><gtr:title>Artificial Neural Networks - ICANN 2008</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/33d09e053d30517e9214a72f8f6c8d31"><gtr:id>33d09e053d30517e9214a72f8f6c8d31</gtr:id><gtr:otherNames>Nishimori Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-3-540-87535-2</gtr:isbn><gtr:outcomeId>545ce4ec29a9d0.63030841</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E4A3814-06B9-4240-BD79-5FC3911D6A8A</gtr:id><gtr:title>Information dynamics: patterns of expectation and surprise in the perception of music</gtr:title><gtr:parentPublicationTitle>Connection Science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03d03d0e43e5c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8FA280F4-0AE0-4218-BCDF-7289C35392F9</gtr:id><gtr:title>The Serendiptichord: A wearable instrument for contemporary dance performance</gtr:title><gtr:parentPublicationTitle>128th Audio Engineering Society Convention 2010</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4f58d2ec71a362cbed4558897ec90b2c"><gtr:id>4f58d2ec71a362cbed4558897ec90b2c</gtr:id><gtr:otherNames>Murray-Browne T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>545ce6077fff33.36810170</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>126648C6-AD78-4738-A098-D198F1E390F0</gtr:id><gtr:title>Analysis of musical timbre semantics through metric and non-metric data reduction techniques</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f28ef150e55728edf9a0f23f5ab108b0"><gtr:id>f28ef150e55728edf9a0f23f5ab108b0</gtr:id><gtr:otherNames>Zacharakis A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54608a78879ed1.17013027</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F7051ECC-DC9F-4A1F-AC4B-CAAC0C295407</gtr:id><gtr:title>Genre classification using harmony rules induced from automatic chord transcriptions</gtr:title><gtr:parentPublicationTitle>Proceedings of the 10th International Society for Music Information Retrieval Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f5b72f8178f87ce3380c487971e912c2"><gtr:id>f5b72f8178f87ce3380c487971e912c2</gtr:id><gtr:otherNames>A. Anglade</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>5464e99d36c003.99247094</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E045235/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>