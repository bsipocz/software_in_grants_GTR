<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/EAAD4D43-BD15-432B-9385-2DBD0C65958D"><gtr:id>EAAD4D43-BD15-432B-9385-2DBD0C65958D</gtr:id><gtr:name>University of Bath</gtr:name><gtr:department>Mathematical Sciences</gtr:department><gtr:address><gtr:line1>University of Bath</gtr:line1><gtr:line2>Claverton Down</gtr:line2><gtr:line4>Bath</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EAAD4D43-BD15-432B-9385-2DBD0C65958D"><gtr:id>EAAD4D43-BD15-432B-9385-2DBD0C65958D</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>University of Bath</gtr:line1><gtr:line2>Claverton Down</gtr:line2><gtr:line4>Bath</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/547598D4-6614-4937-9047-87E11C298BE8"><gtr:id>547598D4-6614-4937-9047-87E11C298BE8</gtr:id><gtr:firstName>Andreas</gtr:firstName><gtr:surname>Dedner</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/BF97E88E-2E68-4EB9-818B-018A36174D5D"><gtr:id>BF97E88E-2E68-4EB9-818B-018A36174D5D</gtr:id><gtr:firstName>Eike</gtr:firstName><gtr:otherNames>Hermann</gtr:otherNames><gtr:surname>Mueller</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6E8A5982-B3A2-4FA1-8E6A-49315FC84DA2"><gtr:id>6E8A5982-B3A2-4FA1-8E6A-49315FC84DA2</gtr:id><gtr:firstName>Robert</gtr:firstName><gtr:surname>Scheichl</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=NE%2FK006754%2F1"><gtr:id>C78D66F4-FC16-4A65-915F-CE91DA029053</gtr:id><gtr:title>A scalable dynamical core for Next Generation Weather and Climate Prediction - Phase 2</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>NE/K006754/1</gtr:grantReference><gtr:abstractText>Historically, major improvements in the accuracy of numerical weather forecasts and climate simulations have come from the increased resolution enabled by the exponential growth in computer power. In order to achieve further gains in accuracy through further increases in resolution, it will be necessary to exploit the massively parallel computer architectures that are becoming available. However, current state-of-the-art operational algorithms are not expected to perform well beyond a few thousand processors: the grid structure of the traditional latitude-longitude grid means that interprocessor communication eventually but inevitably becomes a bottleneck.

The overall aim of the proposed project is to develop a new, three-dimensional, fully compressible dynamical core suitable for operational global and regional weather and climate prediction, as well as for research use, on massively parallel machines, and to demonstrate its accuracy, efficiency, and scalability. The accuracy should be comparable to that of existing state of the art algorithms. The algorithm must be efficient enough to run in the available operational time slots, and it must scale well on 100,000 to 1000,000 processors.

Phase 1 of this project (Feb 2011 - Jan 2013) addressed several of the basic scientific questions that underpin the development, including choice of quasi-uniform horizontal grid, choice of horizontal discretization, choice of transport scheme, time integration scheme, and some of the computer science aspects of the project. Several candidate approaches were tested and evaluated in a simplified two-dimensional fluid system (the Shallow Water Equations), and a small number of promising approaches were identified for further development in Phase 2.

Phase 2 of this project will build on the progress made in Phase 1 in order to develop a three-dimensional, fully compressible dynamical core. The work in Phase 2 falls broadly into three work packages:

* Vertical aspects. The stability and accuracy of the discretization depends crucially on the choice of vertical coordinate, the choice of thermodynamic variables predicted, and the vertical placement of variables relative to each other (`staggering'). It will also depend on the details of how, for example, the pressure gradient term is evaluated, especially near steep mountains, and how the vertical discretization couples with the horizontal discretization. Building on current understanding, candidate schemes will be formulated and tested.

* Code design and development. The code for the three-dimensional dynamical core will be based around a carefully designed software framework. The interface between the numerical discretization and its parallel implementation will be optimized, so that modifications to the former require minimal knowledge of the latter. The software framework will be highly flexible, so that it can easily accommodate future evolution of the dynamical core, such as changes in grid structure.

* Testing. The behaviour of complex numerical algorithms can be difficult to predict theoretically, even when individual components are well understood and tested. It will be vital, therefore, to test comprehensively the proposed formulations at the earliest opportunity, and revise if necessary. Early testing will focus on the shallow water formulation arising out of Phase 1 of the project, and on one-dimensional (column) and two-dimensional (vertical slice) prototypes of the vertical formulation. Testing of the three-dimensional formulation will begin as soon as code is available.</gtr:abstractText><gtr:potentialImpactText>See lead proposal.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-06-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/8A03ED41-E67D-4F4A-B5DD-AAFB272B6471"><gtr:id>8A03ED41-E67D-4F4A-B5DD-AAFB272B6471</gtr:id><gtr:name>NERC</gtr:name></gtr:funder><gtr:start>2013-06-24</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>180532</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The solution of the pressure correction equation in atmospheric models is one of the computational bottlenecks if implicit time stepping methods are used. Hence any improvements to the solver algorithm and its implementation will allow the Met Office to deliver more accurate forecasts in a shorter time. In this project we have demonstrated the benefits of an improved, tensor product multigrid algorithm and showed that it scales well on modern massively parallel supercomputer hardware, which will be used for running the next generation atmospheric models. In addition to increasing the Met Office's reputation as a world- leading weather- and climate prediction centre, those improved forecasting capabilities have an impact on a wide range of industries and all parts of society (e.g. by aiding emergency responders and by informing goverment policy to prepare for changes in the earth's climate).

Impact from this project will be realised on two different timescales: since a multigrid solver has already been implemented in the current ENDGame dynamical core, and we demonstrated that this reduces the solution time for the pressure equation by a factor of at least two, we are working with the Met Office to include this improved solver in their current operational model. The new LFRic forecast model - which is expected to become operational beyond 2020 - is based on mimetic finite element discretisations. One of the research outputs was to develop bespoke solver technology for this discretisation. We currently collaborate directly with Met Office scientists to include bespoke multigrid algorithms into the new Fortran 2003 LFRic code base.

We are still actively collaborating with researchers at the Met Office, at STFC and at Imperial College in particular to implement the methods in the future Met Office dynamical core, LFRic. It has also lead to the adoption and implementation in other packages, such as firedrake (Imperial College). Eike Mueller is currently pursuing a secondment at the Met Office to maximise the impact of our research there. This is planned in conjunction with the Bath Institute for Mathematical Innovation. The more immediate impact will be the adoption of the multigrid methods in the current operational dynamical core, EndGAME.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>BF1960F1-7AAC-4165-A965-7AF97F5289A5</gtr:id><gtr:impactTypes><gtr:impactType>Policy &amp; public services</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5463e4d8a40fe9.39865432</gtr:outcomeId><gtr:sector>Environment</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In Phase 2 of this programme grant, we have extended our results from Phase 1, solved a physically more relevant problem, implemented and tested our methods within two parallel, discretisation platforms, DUNE and firedrake, as well as stand-alone solvers on novel architectures, such as GPUs</gtr:description><gtr:exploitationPathways>We have continued publishing all our results in top journals and presenting at a large range of relevant meetings in meteorology and in HPC.
The papers are well cited and we are receiving invitations from other researchers and centres for collaborations, in particular we had discussions about a joint project with the European Centre for Medium Range Weather Forecasting (ECMWF). The implementation of the results at the Met Office is still ongoing and we are still actively involved in it.</gtr:exploitationPathways><gtr:id>79E9217A-37A6-40B4-8093-AF45A3DCF04C</gtr:id><gtr:outcomeId>5463e590ba3309.42551978</gtr:outcomeId><gtr:sectors><gtr:sector>Environment</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The Met Office will use a mimetic finite element discretisation for its next generation forecast model LFRic. To test the performance of bespoke multigrid solvers in this discretisation and to compare to existing AMG solvers, this code solves the pressure equation which arises in the gravity wave propagation scenario in a shallow atmosphere. All code is implemented in the Firedrake/PyOP2 finite element library.</gtr:description><gtr:id>8E0DD8CD-9DF4-4D71-B531-AD62CC6F63C8</gtr:id><gtr:impact>This software will inform the design of solvers for the next generation Met Office forecast model LFric. Currently the data structures and algorithms implemented in the Firedrake version of the code are translated into the Fortran 2003 code base used by the Met Office. The code was also crucial to obtain results in the following paper: Mitchell, L. and M&amp;uuml;ller, E.H., 2016. &amp;quot;High level implementation of geometric multigrid solvers for finite element problems: applications in atmospheric modelling.&amp;quot; Journal of Computational Physics, 327, pp.1-18.</gtr:impact><gtr:outcomeId>58c96072dcb079.69155698</gtr:outcomeId><gtr:title>Mimetic finite element multigrid solver</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/firedrakeproject/firedrake-helmholtzsolver</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This multi-GPU implementation of a tensor-product multigrid solver was used to solve a simplified pressure correction equation and test the performance of the solver on multi-GPU clusters.</gtr:description><gtr:id>6A94604D-E95F-4409-8409-42CE40930818</gtr:id><gtr:impact>Since the Met Office is considering using chip architectures similar to GPUs for their next generation forecast model, the results obtained with this code will have an impact on the ultimate choice of solver algorithm. The code was used to produce results for the following two publications:
M&amp;uuml;ller, E., Guo, X., Scheichl, R. and Shi, S., 2013. &amp;quot;Matrix-free GPU implementation of a preconditioned conjugate gradient solver for anisotropic elliptic PDEs&amp;quot;. Computing and Visualization in Science, 16(2), pp.41-58.
M&amp;uuml;ller, E.H., Scheichl, R. and Vainikko, E., 2015. &amp;quot;Petascale solvers for anisotropic PDEs in atmospheric modelling on GPU clusters&amp;quot;. Parallel Computing, 50, pp.53-69.</gtr:impact><gtr:outcomeId>58c96343380458.63738899</gtr:outcomeId><gtr:title>Multi-GPU implementation of tensor-product multigrid algorithm</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://bitbucket.org/em459/ellipticsolvergpu</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Implementation of a geometric tensor-product multigrid solver in the DUNE C++ library for grid based applications. The code generalises other implementations of the algorithm since it supports more generals grids and more realistic pressure equations encountered in atmospheric modelling.</gtr:description><gtr:id>041725FD-97D1-463C-BB4B-5B3A214A35E8</gtr:id><gtr:impact>The code was used to obtain results in the following paper:
Dedner, A., M&amp;uuml;ller, E. and Scheichl, R., 2016. Efficient multigrid preconditioners for atmospheric flow simulations at high aspect ratio. International Journal for Numerical Methods in Fluids, 80(1), pp.76-102</gtr:impact><gtr:outcomeId>58c96400781278.25478198</gtr:outcomeId><gtr:title>DUNE implementation of tensor-product multigrid solver</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://bitbucket.org/em459/tensorproductmultigrid</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>59F5614A-E752-47E8-BDFF-E687B86BE326</gtr:id><gtr:title>High level implementation of geometric multigrid solvers for finite element problems: Applications in atmospheric modelling</gtr:title><gtr:parentPublicationTitle>Journal of Computational Physics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c0e9261caa69db81a8ffade4e3be326e"><gtr:id>c0e9261caa69db81a8ffade4e3be326e</gtr:id><gtr:otherNames>Mitchell L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d45d2e87d45.72089941</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E371E27-A37B-4A84-98D9-9E562A86C67D</gtr:id><gtr:title>Efficient multigrid preconditioners for atmospheric flow simulations at high aspect ratio</gtr:title><gtr:parentPublicationTitle>International Journal for Numerical Methods in Fluids</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/001bab1f58d6ca8fcc672c0a794f703c"><gtr:id>001bab1f58d6ca8fcc672c0a794f703c</gtr:id><gtr:otherNames>Dedner A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5675eef7757c8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B14AFE87-BF01-4EEF-8CB6-F65C29B00DA9</gtr:id><gtr:title>Matrix-free GPU implementation of a preconditioned conjugate gradient solver for anisotropic elliptic PDEs</gtr:title><gtr:parentPublicationTitle>Computing and Visualization in Science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba4518441fed59aae8b568164dd6676"><gtr:id>dba4518441fed59aae8b568164dd6676</gtr:id><gtr:otherNames>M?ller E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e91d1a8876.59092316</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D187A4F-B695-4575-9681-174D95CBE853</gtr:id><gtr:title>Massively parallel solvers for elliptic partial differential equations in numerical weather and climate prediction</gtr:title><gtr:parentPublicationTitle>Quarterly Journal of the Royal Meteorological Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba4518441fed59aae8b568164dd6676"><gtr:id>dba4518441fed59aae8b568164dd6676</gtr:id><gtr:otherNames>M?ller E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e765d496c3.04897366</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1FE79A29-C94F-4158-8F85-60107703CCD6</gtr:id><gtr:title>Large Scale Inverse Problems: Computational Methods and Applications in the Earth Sciences</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fa37af926653cc5560d585f96c93b40f"><gtr:id>fa37af926653cc5560d585f96c93b40f</gtr:id><gtr:otherNames>Cullen, Mike, Freitag, Melina A., Kindermann, Stefan, Scheichl, Robert</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978223110282221</gtr:isbn><gtr:outcomeId>5460e94bb674f2.74667111</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E9CF8BDB-21DE-4345-91D0-F21E83C32786</gtr:id><gtr:title>Improving multilevel Monte Carlo for stochastic differential equations with application to the Langevin equation.</gtr:title><gtr:parentPublicationTitle>Proceedings. Mathematical, physical, and engineering sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b2d15a78b2a94033c2d9cab8a2f19efc"><gtr:id>b2d15a78b2a94033c2d9cab8a2f19efc</gtr:id><gtr:otherNames>M?ller EH</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1364-5021</gtr:issn><gtr:outcomeId>doi_55fa9da9d9b2d0e2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>37009606-DDD2-4BEF-BBAE-5098A94E2A56</gtr:id><gtr:title>Petascale solvers for anisotropic PDEs in atmospheric modelling on GPU clusters</gtr:title><gtr:parentPublicationTitle>Parallel Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba4518441fed59aae8b568164dd6676"><gtr:id>dba4518441fed59aae8b568164dd6676</gtr:id><gtr:otherNames>M?ller E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9b80626a102.50351054</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">NE/K006754/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EE4457DB-92A3-44EA-8D5F-77013CC107E0</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Climate &amp; Climate Change</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>90</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EE4457DB-92A3-44EA-8D5F-77013CC107E0</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Climate &amp; Climate Change</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>9EAAD5EA-2E54-4986-942F-2E204958FE29</gtr:id><gtr:percentage>90</gtr:percentage><gtr:text>High Performance Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>396591D1-8226-43A9-991D-8E0D265D99D0</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Regional &amp; Extreme Weather</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>