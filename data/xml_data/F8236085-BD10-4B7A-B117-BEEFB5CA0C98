<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5B82D4A9-0A23-4FBA-8291-69944781ED98"><gtr:id>5B82D4A9-0A23-4FBA-8291-69944781ED98</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>Howard</gtr:otherNames><gtr:surname>Foster</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8381DBB3-35A9-48DA-9B94-0E02E5E29EDE"><gtr:id>8381DBB3-35A9-48DA-9B94-0E02E5E29EDE</gtr:id><gtr:firstName>John</gtr:firstName><gtr:otherNames>Peter</gtr:otherNames><gtr:surname>Oakley</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF023669%2F1"><gtr:id>F8236085-BD10-4B7A-B117-BEEFB5CA0C98</gtr:id><gtr:title>Steering visual attention in natural scenes: efficient image rendition by actively guiding fixation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F023669/1</gtr:grantReference><gtr:abstractText>What features of an object or region of a scene attract attention? Research over the last few decades has identified several basic features, including movement, colour, brightness contrast, and orientation. This information is important in understanding the mechanisms and strategies that seeing organisms use in their interactions with the environment. Curiously, much less research has been carried out on what properties of objects or regions do not attract attention. In addition to improving our understanding of biological vision, this knowledge could be important in applications such as image and video coding where there is limited channel capacity. If a region of an image receives little or no attention by a viewer, then less channel bandwidth need be allocated to its representation. If, moreover, it is possible to actively influence the probability of a particular location attracting attention by applying image-processing methods, then still less bandwidth might be required. The general aim of the present work is to understand better what regions of a natural scene are represented incompletely or not at all under natural viewing and to determine how this information might be used to steer attention towards other areas of an image to achieve optimum image coding for an electronic display system or more effective advertising or signage. Two practical deliverables are a pilot scheme for reducing video bandwidth and a real-time image display that incorporates image processing to guide fixation to important regions.</gtr:abstractText><gtr:fund><gtr:end>2011-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>486257</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Looking for an object in the world around us is a common part of everyday experience. Success or otherwise depends on many factors, including the structure of the environment or scene being searched, the nature of the object being sought, and the task itself. Much of what is known about the physical factors determining visual search performance has come from laboratory experiments with simple abstract displays of geometric elements defining a target in a background of distractors. Less is known about visual search in natural scenes, with most work concentrating on the separate question of eye movements and visual saliency. The general aim of the present work was to understand better what properties of natural scenes are important in attracting or steering attention and in facilitating target detection. Application areas include optimum image coding for electronic display systems and more effective advertising and signage. In a series of laboratory experiments, observers were given a search task with images of natural scenes taken from a set of hyperspectral data, rendered on a colour monitor with a target embedded at a random location in the scene. The target was constructed from a small, shaded, grey sphere, matched in mean luminance to its local surround. Observers' gaze was simultaneously monitored with an infra-red video eye-tracker. The main experimental outcomes were as follows. (1) About 60% of the variation in observers' target-detection performance over each scene was accounted for by the variation in local scene colour properties (luminance, chroma, and hue). A similar level of variation was accounted for by the pattern of observers' eye fixations. These levels are similar to the maximum level reported elsewhere for the effects of scene structure on eye movements in free viewing of black-and-white video images. Thus, despite a general assumption that image factors such as local contrast and edges dominate visual search and gaze, the present results suggest that local colour information can be as important as gaze in determining target-detection performance in natural scenes. (2) In viewing natural scenes, however, observers may exploit the familiarity, meaning, and global organisation of such scenes to facilitate target detection. To provide a control on the effects of global organisation, images were electronically cut into quarters, randomly rearranged, and then reassembled. Target detection was little affected with these scrambled images, reinforcing the importance of local scene properties. (3) To see whether observers' fixations could be guided over the scene, the local contrast in regions of the image was selectively increased or decreased in such a way as to be largely unnoticed by the observer. Although it was difficult to shift attention away from regions by decreasing contrast, dynamically increasing local contrast did attract gaze and improve target detection. Critically, this image manipulation was generally less detectable than the target itself, suggesting that it gaze can indeed be subtly manipulated to enhance performance. (4) In the course of analysing observers' gaze behaviour, a computational method was developed to make more objective the classification of fixations, for which there is no standard method. General distributional properties of eye movements were used to classify fixations without the parametric assumptions or expert judgment required by other methods. The method was verified against independent classifications by experts and by computer simulation. Software for the proposed nonparametric method is to be made freely available to the community.</gtr:description><gtr:exploitationPathways>Machine assisted search and target detection, automatic analysis of eye gaze in natural scenes, improving signage, navigation</gtr:exploitationPathways><gtr:id>ED1F18E4-B169-4D94-B674-C22D1B8E5D80</gtr:id><gtr:outcomeId>56cde1fb134af9.11079467</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://personalpages.manchester.ac.uk/staff/d.h.foster/Fixation_Classification/Classifying_Fixations.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Levels of color constancy from some experimental studies. Eighty average constancy indices CI and Brunswik ratios BR are tabulated against experimental method, stimulus configuration, illuminants, judgment by subject, experimental apparatus, illuminant change, cues other than those defined by the experimental method, constraints on stimulus variation, the number of subjects N, and source of data. Values of CI and BR are averages over subjects and conditions, with some entries estimated from published figures.</gtr:description><gtr:id>BE62D8F1-811A-4B28-93C6-F96265BBF904</gtr:id><gtr:impact>According to Google Analytics, 167 pageviews per month</gtr:impact><gtr:outcomeId>545f946d68ff81.20779749</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Table of Color Constancy Indices and Experimental Methods</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://personalpages.manchester.ac.uk/staff/d.h.foster/Table_colour_constancy_indices.html</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>73511011-B89E-4626-9694-BA398F6A3DA8</gtr:id><gtr:title>Influence of local scene color on fixation position in visual search.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/00ad6cdd71d841e57992f410f5700335"><gtr:id>00ad6cdd71d841e57992f410f5700335</gtr:id><gtr:otherNames>Amano K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>545fa458c07159.87232838</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>64EFB7F5-30B6-4283-BAA3-68D6F1DEA5A9</gtr:id><gtr:title>Number of perceptually distinct surface colors in natural scenes.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75fc96c35d3e889a17a10bb3b14cee5e"><gtr:id>75fc96c35d3e889a17a10bb3b14cee5e</gtr:id><gtr:otherNames>Mar?n-Franch I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>545fa1ec059547.98793079</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>56624EF5-E905-47BF-B3FC-F6F65A8BE519</gtr:id><gtr:title>A simple nonparametric method for classifying eye fixations.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7f81efea3d0eb741755a5c1bab979226"><gtr:id>7f81efea3d0eb741755a5c1bab979226</gtr:id><gtr:otherNames>Mould MS</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00f00f09c8da8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3FE254E6-724B-475D-9D72-52782D6BCC6F</gtr:id><gtr:title>Color constancy.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f664539189e6aaf979a85c2173f093a1"><gtr:id>f664539189e6aaf979a85c2173f093a1</gtr:id><gtr:otherNames>Foster DH</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>545f87c8b89953.39396636</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>425374D8-8A2F-461B-868F-D2B779C15F4F</gtr:id><gtr:title>Colour and gaze as predictors of target detection in natural scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de3cfc1fdafbb46a07d7e68b84bbd957"><gtr:id>de3cfc1fdafbb46a07d7e68b84bbd957</gtr:id><gtr:otherNames>John P Oakley</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_2027304293cac2df6e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9F08177A-3592-4620-A53A-A19A99F3C969</gtr:id><gtr:title>Predicting frequency of metamerism in natural scenes by entropy of colors.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8ed82a801810eb941b10862aff3b1113"><gtr:id>8ed82a801810eb941b10862aff3b1113</gtr:id><gtr:otherNames>Feng G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>56c2376db8ece1.65600068</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9DBC61F1-0C43-4843-B1BA-54424F7AE9D7</gtr:id><gtr:title>Visual search in natural scenes explained by local color properties.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/00ad6cdd71d841e57992f410f5700335"><gtr:id>00ad6cdd71d841e57992f410f5700335</gtr:id><gtr:otherNames>Amano K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>doi_53d07e07e9275d76</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B93A7D47-6C9C-4085-81FF-2D77DEDE019D</gtr:id><gtr:title>Tracking categorical surface colour across illuminant changes in natural scenes</gtr:title><gtr:parentPublicationTitle>5th European Conference on Colour in Graphics, Imaging, and Vision and 12th International Symposium on Multispectral Colour Science 2010, CGIV 2010/MCS'10</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a6d964b6e9fa519b54b12483625cfcdf"><gtr:id>a6d964b6e9fa519b54b12483625cfcdf</gtr:id><gtr:otherNames>Amano K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>545f831629c638.90990653</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55AAB075-4AEB-43B5-9B81-F4EEB3F7F88F</gtr:id><gtr:title>Predicting frequency of metamerism in natural scenes by entropy of colors.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8ed82a801810eb941b10862aff3b1113"><gtr:id>8ed82a801810eb941b10862aff3b1113</gtr:id><gtr:otherNames>Feng G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>545fa3a7ba69c6.64264691</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A3FBBCA4-B70C-4446-98F4-D1EDCC655E43</gtr:id><gtr:title>Influence of local scene colour on target detection tested by global rearrangement of natural scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8648fa5f7cda0a2eeb36ace64ca504d0"><gtr:id>8648fa5f7cda0a2eeb36ace64ca504d0</gtr:id><gtr:otherNames>Amano, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>r_9582095332cac4ed40</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>811BAE67-A3DF-48B7-BB29-BD3771F10A3C</gtr:id><gtr:title>The Verriest Lecture: Color vision in an uncertain world</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America A</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/099835fcd20f95cc7b80c6945657f7a9"><gtr:id>099835fcd20f95cc7b80c6945657f7a9</gtr:id><gtr:otherNames>Foster D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a995b7d22c451.45673721</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F023669/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>