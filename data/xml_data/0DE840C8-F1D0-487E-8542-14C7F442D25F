<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/455203A4-9781-4725-A589-223CA5AD2E10"><gtr:id>455203A4-9781-4725-A589-223CA5AD2E10</gtr:id><gtr:name>Australian National University (ANU)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/5EE6C5CB-99A8-406F-A5C7-153D27D89FFD"><gtr:id>5EE6C5CB-99A8-406F-A5C7-153D27D89FFD</gtr:id><gtr:name>University of Canberra</gtr:name><gtr:address><gtr:line1>University of Canberra</gtr:line1><gtr:postCode>ACT 2601</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A452CF3D-B645-40F5-A755-C00526AA57B9"><gtr:id>A452CF3D-B645-40F5-A755-C00526AA57B9</gtr:id><gtr:name>Crowd Emotion</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3F4EBEBA-86E8-4504-AA31-6AC329437FB6"><gtr:id>3F4EBEBA-86E8-4504-AA31-6AC329437FB6</gtr:id><gtr:name>Delft University of Technology</gtr:name><gtr:address><gtr:line1>The Balboratory for</gtr:line1><gtr:line2>Process Equipment</gtr:line2><gtr:line3>Leeghwaterstraat 44</gtr:line3><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/51D78578-97E8-4B05-AE61-A9DAAFACB40A"><gtr:id>51D78578-97E8-4B05-AE61-A9DAAFACB40A</gtr:id><gtr:name>Istanbul Technical University</gtr:name><gtr:address><gtr:line1>ITU Makina Fakultesi</gtr:line1><gtr:line2>Inonu Caddesi No 87</gtr:line2><gtr:line3>Gumussuyu 34437</gtr:line3><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/49C44558-A75A-448F-9BF7-3F947FA67EB7"><gtr:id>49C44558-A75A-448F-9BF7-3F947FA67EB7</gtr:id><gtr:name>Technical University of Munich</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/455203A4-9781-4725-A589-223CA5AD2E10"><gtr:id>455203A4-9781-4725-A589-223CA5AD2E10</gtr:id><gtr:name>Australian National University (ANU)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5EE6C5CB-99A8-406F-A5C7-153D27D89FFD"><gtr:id>5EE6C5CB-99A8-406F-A5C7-153D27D89FFD</gtr:id><gtr:name>University of Canberra</gtr:name><gtr:address><gtr:line1>University of Canberra</gtr:line1><gtr:postCode>ACT 2601</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A452CF3D-B645-40F5-A755-C00526AA57B9"><gtr:id>A452CF3D-B645-40F5-A755-C00526AA57B9</gtr:id><gtr:name>Crowd Emotion</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3F4EBEBA-86E8-4504-AA31-6AC329437FB6"><gtr:id>3F4EBEBA-86E8-4504-AA31-6AC329437FB6</gtr:id><gtr:name>Delft University of Technology</gtr:name><gtr:address><gtr:line1>The Balboratory for</gtr:line1><gtr:line2>Process Equipment</gtr:line2><gtr:line3>Leeghwaterstraat 44</gtr:line3><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/51D78578-97E8-4B05-AE61-A9DAAFACB40A"><gtr:id>51D78578-97E8-4B05-AE61-A9DAAFACB40A</gtr:id><gtr:name>Istanbul Technical University</gtr:name><gtr:address><gtr:line1>ITU Makina Fakultesi</gtr:line1><gtr:line2>Inonu Caddesi No 87</gtr:line2><gtr:line3>Gumussuyu 34437</gtr:line3><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/49C44558-A75A-448F-9BF7-3F947FA67EB7"><gtr:id>49C44558-A75A-448F-9BF7-3F947FA67EB7</gtr:id><gtr:name>Technical University of Munich</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FDC2B8FB-2F53-42C2-948E-A6EB725590C9"><gtr:id>FDC2B8FB-2F53-42C2-948E-A6EB725590C9</gtr:id><gtr:firstName>Hatice</gtr:firstName><gtr:surname>Gunes</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK017500%2F1"><gtr:id>0DE840C8-F1D0-487E-8542-14C7F442D25F</gtr:id><gtr:title>MAPTRAITS: MACHINE ANALYSIS OF PERSONALITY TRAITS IN HUMAN VIRTUAL AGENT INTERACTIONS</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K017500/1</gtr:grantReference><gtr:abstractText>Research findings suggest that personality traits such as extraversion, agreeableness, and openness to experience, are tightly coupled with human abilities and behaviour encountered in daily lives: emotional expression, linguistic production, success in interpersonal tasks, leadership ability, general job performance, teacher effectiveness, academic ability, as well as interaction with technology. In fact, human users tend to anthropomorphise computers and virtual agents, treating them as social beings, and interpreting their behaviour similarly to daily human-human interactions.

The problem of assessing people's personality is very important for multiple research and business domains such as computer-mediated staff assessment and training, human-computer and human-robot interaction. Despite a growing interest and emphasis on personality traits and their effects on human life in general, and recent advances in machine analysis of human behavioural signals (e.g., vocal expressions, and physiological reactions), pioneering efforts focusing on machine analysis of personality traits have started to emerge only recently: (i) there exist a small number of efforts based on unimodal cues such as written texts/ audio/ speech/ static facial features, (ii) despite tentative efforts on multimodal personality trait analysis, the dynamics (duration, speed, etc.) of multiple cues, which have been shown to be important in human judgments of personalities, have mostly been neglected, (iii) although personality analysis research suggests that a trait exists in all people to a greater or lesser degree (i.e. a person can be anywhere on a continuum ranging from introversion to extraversion), none of the proposed efforts have attempted to assess personality traits continuously in time and space (i.e., how a person can be rated along the multiple trait dimensions at a given interaction time and context), and (iv) how machine (automatic) traits analysis can be utilised for personalised, social and adaptive human - virtual agent interaction has not been investigated.

Overall, both the common everyday technology (e.g., personal PCs, smart phones) and the more sophisticated systems people use nowadays (e.g., computer games, assistive technologies, embodied virtual agents, etc.) lack the capability of understanding their human users' personality and behaviour, and of providing socially intelligent, adaptive and engaging human - computer interaction.

To address these issues and limitations, MAPTRAITS project will bring around a set of audio-visual tools that can analyse and predict human personality traits dynamically from multiple nonverbal cues and channels (i.e., upper body, head, face, voice and their dynamics) in continuous time and trait space. There is no prospect of building a perfect system for automatic analysis of personality traits that can be used in all possible application domains in 12 months' time. Therefore, as a proof-of-concept, the MAPTRAITS technology will be developed for automatic matching of virtual agent and user personalities, to automatically model what type of users would like to engage with what type of virtual agents to the aim of user engagement enhancement. The motivation for choosing this application area lies in its significance: (i) Research has shown that people's attitudes toward machines and conversational agents is based on the perceived personality of the agent, and their own personality, and (ii) humans are social beings, and currently their everyday life revolves around interacting with computers, virtual agents and robots that are getting increasingly popular as companions, coaches, user interfaces to smart homes, or household robots.</gtr:abstractText><gtr:potentialImpactText>The MAPTRAITS project will focus on machine analysis of personality traits. It will bring around a set of audio-visual tools to analyse and predict human personality traits dynamically from multiple nonverbal cues and channels in continuous time and trait space (represented with multiple trait dimensions), to match virtual agents with human users. 

The line of research targeted in this funding application is highly interdisciplinary and the research outcomes can be spun into the outer society in various ways. Relevant beneficiaries are identified based on the expected term of impact. 

In the short term, the output of the research proposed will benefit national and international research groups working in the area of social signal processing, human behaviour analysis and interpretation, and embodied and relational agent community and applications. The most immediate application of the proposed research is in the areas of embodied conversational agent and relational agent design by addressing issues related to personalised, adaptive and engaging user - agent interaction, and by providing such agents with the means of analysing and understanding the nonverbal behaviour and personality of their users, to guide the agent's social interaction and personalisation ability. This in turn will improve their effectiveness in various application domains related to learning and health care, namely virtual tutoring, elderly care, cyber coaching and cyber therapy. From an interdisciplinary point of view, the knowledge gained will impact the understanding of the role of the personality traits in human - agent interactions. 

In the medium term, a system capable of assessing human personality traits displayed in naturalistic and spontaneous settings (non-verbal behaviours expressed through facial expressions, body gestures, and vocal cues) has the potential to alter significantly other existing technologies, and become applicable and expandable to multiple domains. Examples of such domains are: (1) recommending interfaces to people (matching people's personality traits with household robots, or virtual agents); (2) personal wellness technologies (prescription of exercise programs taking the patient's personality into account, prescription of therapies for people suffering from personality disorders, and guided therapy); (3) e-learning (to assess a student's personality and engage the student in studying that is most suited to his or her personality); and (4) pre-employment assessment (matching personality traits with job requirements).</gtr:potentialImpactText><gtr:fund><gtr:end>2014-12-19</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-09-23</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98427</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Australian National University (ANU)</gtr:collaboratingOrganisation><gtr:country>Australia, Commonwealth of</gtr:country><gtr:description>ACM MM'14 Panel</gtr:description><gtr:id>E3423D78-F5D9-4F2A-ACDE-8602F074721D</gtr:id><gtr:impact>We have been invited to launch a new area at ACM Multimedia'14 on 'Emotional and Social Signals in Multimedia', and act as the three Area Chairs to promote this emerging area. The collaboration is multidisciplinary as it brings together the disciplines of multimedia, audio processing, computer vision, affective computing, and social signal processing.</gtr:impact><gtr:outcomeId>5437effd062228.15399425-3</gtr:outcomeId><gtr:partnerContribution>The partners worked on the proposal, contributed in suggesting topics and panellists, and ran the panel.</gtr:partnerContribution><gtr:piContribution>We submitted a panel proposal to ACM Multimedia'14 Conference titled 'Looking for emotional and social signals in multimedia: Where art thou?'. The panel took place on November 4th, 2014 in Orlando (USA), prominent academics and researchers from the industry, and conference participants from around the world, discussed the significance of emotional and social signals for multimedia research. The panel has been a great success in drawing the attention of the researchers working in other fields to analysing personality, social signals and emotions.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Istanbul Technical University</gtr:collaboratingOrganisation><gtr:country>Turkey, Republic of</gtr:country><gtr:description>ITU student internship</gtr:description><gtr:id>79D43B04-A0E0-476C-967B-7EA7AE2D0C53</gtr:id><gtr:impact>The student visit enabled development of international research collaborations between QMUL and ITU. The internships also enabled the visiting students improve their communication and team work skills. 
One publication has been jointly generated as part of this collaboration: 
S. Kalayci, H. K. Ekenel, and H. Gunes, &amp;quot;Automatic Analysis of Facial Attractiveness from Video&amp;quot;, Proc. of IEEE International Conference on Image Processing (IEEE ICIP'14), Oct. 2014, Paris, France, pp. 4191-4195.</gtr:impact><gtr:outcomeId>542e9264330e00.93682073-1</gtr:outcomeId><gtr:partnerContribution>Sacide Kalayci from Istanbul Technical University visited my team at QMUL in the period of July-September 2013. ITU provided support and allowed their students to visit QMUL during the summer of 2013 for conducting research.</gtr:partnerContribution><gtr:piContribution>QMUL provided desk and lab space for visiting students from Istanbul Technical University. I initiated the internship, provided the project idea on automatic analysis of beauty, and provided the dataset together with the relevant annotations. I closely supervised the project during the visit and actively worked on writing the conference paper.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Crowd Emotion</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Crowd Emotion Ltd</gtr:description><gtr:id>9AF858FC-EB95-4C9F-A5C7-39B64129484B</gtr:id><gtr:impact>A Non-Disclosure Agreement</gtr:impact><gtr:outcomeId>542e9924da2343.89978331-1</gtr:outcomeId><gtr:partnerContribution>Signed a Non-Disclosure Agreement with Crowd Emotion Ltd.</gtr:partnerContribution><gtr:piContribution>We (QMUL) signed a Non-Disclosure Agreement with Crowd Emotion Ltd.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Technical University of Munich</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>MAPTRAITS Workshop</gtr:description><gtr:id>2BF2E7DE-EE9C-4EC6-92D9-DD1E56EDFFEB</gtr:id><gtr:impact>The collaboration resulted in one conference publication: 
O. Celiktutan, F. Eyben, E. Sariyanidi, H. Gunes, and B. Schuller, &amp;quot;MAPTRAITS 2014: The First Audio/Visual Mapping Personality Traits Challenge &amp;quot;, Proc. of ACM Int' Conf. on Multimodal Interaction (ACM ICMI'14), Nov. 2014, Istanbul, Turkey.
The collaboration is multidisciplinary as it brings together the disciplines of audio processing, computer vision, affective computing, social signal processing, and multimodal interaction.</gtr:impact><gtr:outcomeId>542e8bd11a58c3.42568306-1</gtr:outcomeId><gtr:partnerContribution>The TUM team is co-organising with us the 1st International Audio/Visual Mapping Personality Traits Challenge and Workshop. They extracted informative audio and vocal features and trained models to provide baseline personality and social state prediction results. These were readily provided to the Challenge participants. The TUM team also contributed to the conference paper we wrote on this topic.</gtr:partnerContribution><gtr:piContribution>In March 2014, we (QMUL) initiated the Mapping Personality Traits Challenge and Workshop (MAPTRAITS) series as a competition event aimed at the comparison of signal processing and machine learning methods for automatic visual, vocal and/or audio-visual analysis of traits and social dimensions. The 1st International Audio/Visual Mapping Personality Traits Challenge and Workshop is organised in conjunction with 16th ACM International Conference on Multimodal Interaction on 12 Nov. 2014 in Istanbul.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Delft University of Technology (TU Delft)</gtr:collaboratingOrganisation><gtr:country>Netherlands, Kingdom of the</gtr:country><gtr:description>ACM MM'14 Panel</gtr:description><gtr:id>251A46EC-EFFE-4763-B92D-5DE2FFC4F104</gtr:id><gtr:impact>We have been invited to launch a new area at ACM Multimedia'14 on 'Emotional and Social Signals in Multimedia', and act as the three Area Chairs to promote this emerging area. The collaboration is multidisciplinary as it brings together the disciplines of multimedia, audio processing, computer vision, affective computing, and social signal processing.</gtr:impact><gtr:outcomeId>5437effd062228.15399425-1</gtr:outcomeId><gtr:partnerContribution>The partners worked on the proposal, contributed in suggesting topics and panellists, and ran the panel.</gtr:partnerContribution><gtr:piContribution>We submitted a panel proposal to ACM Multimedia'14 Conference titled 'Looking for emotional and social signals in multimedia: Where art thou?'. The panel took place on November 4th, 2014 in Orlando (USA), prominent academics and researchers from the industry, and conference participants from around the world, discussed the significance of emotional and social signals for multimedia research. The panel has been a great success in drawing the attention of the researchers working in other fields to analysing personality, social signals and emotions.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Technical University of Munich</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>ACM MM'14 Panel</gtr:description><gtr:id>E1D10A6B-D31D-4920-87F7-9051B7DD8C3F</gtr:id><gtr:impact>We have been invited to launch a new area at ACM Multimedia'14 on 'Emotional and Social Signals in Multimedia', and act as the three Area Chairs to promote this emerging area. The collaboration is multidisciplinary as it brings together the disciplines of multimedia, audio processing, computer vision, affective computing, and social signal processing.</gtr:impact><gtr:outcomeId>5437effd062228.15399425-2</gtr:outcomeId><gtr:partnerContribution>The partners worked on the proposal, contributed in suggesting topics and panellists, and ran the panel.</gtr:partnerContribution><gtr:piContribution>We submitted a panel proposal to ACM Multimedia'14 Conference titled 'Looking for emotional and social signals in multimedia: Where art thou?'. The panel took place on November 4th, 2014 in Orlando (USA), prominent academics and researchers from the industry, and conference participants from around the world, discussed the significance of emotional and social signals for multimedia research. The panel has been a great success in drawing the attention of the researchers working in other fields to analysing personality, social signals and emotions.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Canberra</gtr:collaboratingOrganisation><gtr:country>Australia, Commonwealth of</gtr:country><gtr:description>UoC student visit</gtr:description><gtr:id>3305B0CD-6747-4CAB-929A-08DA5C3759F3</gtr:id><gtr:impact>This enabled development of international research collaborations between QMUL and UoC. The internship also enabled the visiting students to get to know London and QMUL as well as improve their team work skills. 
One publication has been jointly generated as part of this collaboration: 
J. Joshi, H. Gunes and R. Goecke, Automatic Prediction of Perceived Traits using Visual Cues under Varied Situational Context , Proc. of International Conference on Pattern Recognition (ICPR'14), pp. 2855-2860.</gtr:impact><gtr:outcomeId>542e9532156bd0.10637343-1</gtr:outcomeId><gtr:partnerContribution>Jyoti Joshi from University of Canberra (UoC) visited my team at QMUL in the period of May-July 2013. Jyoti worked on extracting visual features and using machine learning techniques for automatic prediction of perceived personality traits using visual cues. Jyoti also worked on the conference paper we wrote on this topic.</gtr:partnerContribution><gtr:piContribution>QMUL provided desk and lab space for the visiting student. I provided the project idea, the dataset and the annotations to be used for creating an automatic personality analyser, closely supervised the project during the visit and actively worked on writing the conference paper.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Live Demo at the IEEE Automaitc Face and Gesture Recognition Conference</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D84E2BBF-DADC-4308-B646-D3667252144F</gtr:id><gtr:impact>We had a live demo with NAO robot at the IEEE Automatic Face and Gesture Recognition Conference in Ljubljana, Slovenia. Participants interacted with the robot, one at a time, answering questions about their thoughts, feelings, and experiences. The system then predicted their personality based on the way they were interacting with the robot. 
This live demo received a lot of attention by the Slovenian media, both national radio and TV, as well as other conference participants.</gtr:impact><gtr:outcomeId>56bb28a7e22e76.46491711</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://4d.rtvslo.si/arhiv/prispevki-in-izjave-slovenska-kronika/174334071</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>One-day lecture on 'Affective and Social Signal Processing' with hands-on group activities at the Visum 2016 Summer School</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5F8B074B-7DE2-4741-BCC9-0E23DDDFDE03</gtr:id><gtr:impact>Dr Hatice Gunes gave an one-day lecture on 'Affective and Social Signal Processing' with hands-on group activities at the Visum 2016 Summer School on 7 July 2016. Visum 2016 was the 4th VISion Understanding and Machine intelligence (visum) Summer School that targeted to gather at Universidade do Porto, Portugal, Ph.D. candidates, Post-Doctoral scholars and researchers from academia and industry with research interests in computer vision and machine intelligence. Considering the existing gap between the most fundamental concepts of computer vision and their application in real world scenarios, the realisation of visum school seeked to bridge these two key domains. By creating an expert multicultural environment, visum school aimed to foster junior researchers' awareness of computer vision topics, as well as to enhance all attendees' knowledge regarding the state of the art, provided by leading international experts on the field. Visum comprised three main tracks: fundamental, industrial and application topics, each one with extensive practical `hands-on' sessions. The goal was to enable questioning and cross-fertilization of ideas through being exposed to multidisciplinary topics and tasks, potentially leading to significant breakthroughs in the development of the theses of the student participants. Participants reported that this was indeed their experience during the summer school.</gtr:impact><gtr:outcomeId>58adbc8d20e855.29534206</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://visum.inesctec.pt/visum-2016-4th-edition/#1479315639507-7900b722-1060</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>QMUL Research Open Day</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>C37E5969-1967-4DC7-9E16-C436334CD29D</gtr:id><gtr:impact>I gave lightning talk at EECS 2015 Research Showcase at Queen Mary University of London on 22 Apr. 2015 titled Machine Understanding of Human Social Signals. The talk explained the methodologies we developed to make humanoid robots understand the expressions and personality of the people they interact with. 
We had a live demo at the foyer, where participants could interact with a NAO robot and see for themselves their personality as predicted by the system.</gtr:impact><gtr:outcomeId>56bb277ee67804.71743710</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.eecs.qmul.ac.uk/research-showcase-2015/demonstrations</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote talk on 'Affective and Social Signal Processing for Human-Computer-Robot Interactions' at the 6th Audio/Visual Emotion Challenge and Workshop (AVEC 2016)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>33DDDAD7-A8BF-46C5-8CE7-B131C2A72A98</gtr:id><gtr:impact>Dr Hatice Gunes gave an invited keynote talk titled 'Affective and Social Signal Processing for Human-Computer-Robot Interactions' at the 6th Audio/Visual Emotion Challenge and Workshop (AVEC 2016) organised in conjunction with ACM Multimedia 2016 in Amsterdam, the Netherlands on 16 October 2016. The purpose of the invitation and the talk was to expose the participants to research topics that would be somewhat outside their area of expertise and the scope of the workshop - personality computing and human-robot interaction. The audience was very engaged, asking questions about HRI and providing suggestions. This keynote talk led to Dr Hatice Gunes being invited to give other seminars and talks on similar topics.</gtr:impact><gtr:outcomeId>58adb9682b9b36.25652730</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://dl.acm.org/citation.cfm?doid=2988257.2988271</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>invited talk on 'Affective and Social Signal Processing for Human-Computer and Human-Robot Interaction' at the Hong Kong Polytechnic University (PolyU)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>1F4C2D41-8D88-454D-B92A-302501827E40</gtr:id><gtr:impact>Dr Hatice Gunes was invited to give a research seminar on 'Affective and Social Signal Processing for Human-Computer and Human-Robot Interaction' at the Hong Kong Polytechnic University (PolyU) on 22nd of June 2016.</gtr:impact><gtr:outcomeId>58adba54e47d14.95801377</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>https://www.comp.polyu.edu.hk/files/research_seminar_20160622.pdf</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>90000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Barts and The London School of Medicine and Dentistry</gtr:department><gtr:description>College PhD studentship</gtr:description><gtr:end>2015-12-02</gtr:end><gtr:fundingOrg>Queen Mary University of London</gtr:fundingOrg><gtr:id>2A8EAFE5-F650-4346-8A4F-45D60F4E1892</gtr:id><gtr:outcomeId>5437f1c782c387.31293520</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>400000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Digital Personhood Sandpit</gtr:description><gtr:end>2016-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>ECSA1N4R/S</gtr:fundingRef><gtr:id>B7BABB4B-D700-4993-BCE9-A3BA389BC8C3</gtr:id><gtr:outcomeId>5437f138cfef78.72732048</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>562000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Enhancing User Experience in Retail</gtr:description><gtr:end>2018-03-02</gtr:end><gtr:fundingOrg>Innovate UK</gtr:fundingOrg><gtr:id>1B3444DA-4715-4A7E-B42A-209980DA270C</gtr:id><gtr:outcomeId>56bb262fe22698.51924774</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The automatic personality analysis system has now been implemented as a real-time analyser. We have been incorporating this analysers and the findings of the MAPTRAITS project into the design of the telepresence and human-robot interactions for the EPSRC project titled 'Being There: Humans and Robots in Public Space'. We demonstrated the automatic personality analyser during the EECS Research Showcase at Queen Mary University of London in April 2015 to the industry and the public. We also ran a live demo at IEEE Int'l Conference on Automatic Face and Gesture Recognition in Slovenia in May 2015 which made in onto the Slovenian national TV and radio.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>5351095A-8D07-4685-99FD-3DC29BF6F771</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5453beb0734ae5.83802249</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The most recent findings of the MAPTRAITS project published online in Jan 2016 show that: (1) varying situational context causes the manifestation of different facets of people's personality; (2) human raters' impressions do not change much for conscientiousness, openness and likeability as compared to those for agreeableness, extroversion and facial attractiveness from one context to another; and (3) time-continuous prediction is better suited for the dimensions of agreeableness, conscientiousness, neuroticism, openness, vocal attractiveness and likeability (Celiktutan &amp;amp; Gunes, IEEE TAC 2016); (4) Assessments provided by each rater vary, but we can measure and model the credibility of each rater (Joshi, Gunes and Goecke, ICPR'14); (5) Facial attractiveness perception and computation is affected not only by the static facial features, but also by the subject's behaviour (Kalayci, Ekenel and Gunes, ICIP'14). 
We extended our research from the MAPTRAITS project to the Being There project, and conducted comparative experiments with the extroverted versus introverted robot condition showing that: (1) perceived enjoyment with the NAO robot is found to be significantly correlated with participants' extroversion trait (which validates the similarity rule); (2) NAO robot's perceived empathy positively correlates with participants' extroversion trait - extroverted people feel more control over their interactions and judge them as more intimate and less incompatible; (3) perceived enjoyment with the robot is highly correlated with the agreeableness trait of the participants; (4) a significant relationship is also established between perceived robot's realism and the neuroticism trait of the participants - people who score high on neuroticism tend to perceive their interactions as being forced and strained, therefore artificial behaviours of the robot might appear to them as realistic (Celiktutan and Gunes, IEEE RO-MAN 2016).</gtr:description><gtr:exploitationPathways>The findings of the MAPTRAITS project are now being incorporated into the design of the EPSRC 'Being There: Humans and Robots in Public Space' project (EP/L00416X/1) for telepresence and human-robot interactions. These findings are currently being used for human-robot interaction for creating socially intelligent humanoid robots that can personalise and adapt to their users' personality traits for appropriate level of engagement and adaptation.</gtr:exploitationPathways><gtr:id>FB3FD04E-785B-4327-8781-E0F6EDB4215E</gtr:id><gtr:outcomeId>5453bc333bd6a3.98702012</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Financial Services, and Management Consultancy,Healthcare,Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors><gtr:url>https://maptraits.wordpress.com/key-findings/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The MAPTRAITS Continuous Dataset has been created for continuous prediction of traits in time and in space. The dataset comprises 30 audio-visual clips of 10 subjects who are taking part in a dyadic interaction. The raters used an annotation tool to view each clip and to continuously provide scores over time by scrolling a bar between 0 and 100. Approximately 32-40 visual-only annotations per clip were obtained for the five dimensions of the Big Five model, as well as the social dimensions of engagement, likability and facial attractiveness, and additionally 25 audio-visual annotations per clip were obtained for the dimensions of agreeableness, conscientiousness, openness, engagement and vocal attractiveness.</gtr:description><gtr:id>A60D8FB5-363D-463D-8044-58ED8B26CD70</gtr:id><gtr:impact>Due to the development of this database, it was possible to send a proposal to organsie the 1st International Audio/Visual Mapping Personality Traits Challenge and Workshop and have it accepted at the 16th ACM International Conference on Multimodal Interaction (ACM ICMI'14). The MAPTRAITS Continuous Dataset forms the basis for the Sub-challenge on Continuous Personality Prediction.</gtr:impact><gtr:outcomeId>543038aa0ecfd4.73776536</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>MAPTRAITS Continuous Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://maptraits.wordpress.com/datasets/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The MAPTRAITS Quantized Dataset comprises audio-visual interaction clips of 11 different subjects. These clips have been assessed by 6 raters along the five dimensions of the Big Five personality model, namely, extraversion, agreeableness, conscientiousness, neuroticism, and openness, and the four additional dimensions of engagement, facial attractiveness, vocal attractiveness, and likability. The dimensions were scored on a Likert scale with ten possible values, from strongly disagree to strongly agree, mapped into the range from [1,10].</gtr:description><gtr:id>0270D3FD-610F-4AEE-B3A9-F2FC978BAB38</gtr:id><gtr:impact>Due to the development of this database, it was possible to send a proposal to organsie the 1st International Audio/Visual Mapping Personality Traits Challenge and Workshop and have it accepted at the 16th ACM International Conference on Multimodal Interaction (ACM ICMI'14). The MAPTRAITS Quantized Dataset forms the basis for the Sub-challenge on Quantised Personality Prediction.</gtr:impact><gtr:outcomeId>543033f725dfa2.70656689</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>MAPTRAITS Quantized Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://maptraits.wordpress.com/datasets/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Probabilistic Subpixel Temporal Registration (PSTR) implements the PSTR framework in C++ as presented in:

E. Sariyanidi, H. Gunes and A. Cavallaro, Probabilistic Subpixel Temporal Registration for Facial Expression Analysis, Proc. of the Asian Computer Vision Conference (ACCV'14), Singapore, Nov. 2014 (accepted for oral presentation).</gtr:description><gtr:id>3C0DDCB2-0DA4-497A-94EC-3B8E6CEA43DF</gtr:id><gtr:impact>This robust and powerful registration technique is expected to improve the recognition accuracy of the techniques we have developed for both automatic personality prediction and automatic affect recognition.</gtr:impact><gtr:outcomeId>545381a2ce4d58.69683947</gtr:outcomeId><gtr:title>PSTR</gtr:title><gtr:type>New/Improved Technique/Technology</gtr:type><gtr:url>https://maptraits.wordpress.com/software/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This is a Java-based annotation tool that can be used to annotate human affective and social signals displayed in audio-visual clips in a continuous manner. Annotation Master was developed at Queen Mary University of London by Bhavisha Motichande as a final year undergraduate project.</gtr:description><gtr:id>C93CC3B8-6A79-49BD-A33D-0C61854D894A</gtr:id><gtr:impact>Annotation Master has been used for obtaining continuous annotations for the MAPTRAITS Continuous Dataset that has been subsequently used for the MAPTRAITS'14 Challenge.</gtr:impact><gtr:outcomeId>5437f31f0c0875.94065767</gtr:outcomeId><gtr:title>Annotation Master</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://maptraits.wordpress.com/software/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>QLZM implements the Quantised Local Zernike Moments (QLZM) image representation in C++ as presented in the following conference paper:

E. Sariyanidi, H. Gunes, M. Gokmen, and A. Cavallaro, Local Zernike Moment Representation for Facial Affect Recognition , Proc. of the British Machine Vision Conference (BMVC'13), Bristol, UK, Sep. 2013. 

QLZM has been used for the MAPTRAITS'14 Challenge visual feature extraction.</gtr:description><gtr:id>6758FD71-F54F-4069-AD18-3378B0A2FA3E</gtr:id><gtr:impact>QLZM has been released to the research community and has already been used for extracting and representing the visual features of the MAPTRAITS'14 Challenge data.</gtr:impact><gtr:outcomeId>542ed4b83c8983.83343174</gtr:outcomeId><gtr:title>QLZM</gtr:title><gtr:type>New/Improved Technique/Technology</gtr:type><gtr:url>https://maptraits.wordpress.com/software/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>20F47765-96DA-4634-9509-9B6E52F9181C</gtr:id><gtr:title>Continuous Prediction of Perceived Traits and Social Dimensions in Space and Time</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d02d7a8f438acfcdf60b53d78943de4"><gtr:id>8d02d7a8f438acfcdf60b53d78943de4</gtr:id><gtr:otherNames>Celiktutan O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>542e811b2923f3.04342681</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A289F33F-47D7-4619-AEB8-3F302104AB38</gtr:id><gtr:title>Automatic Prediction of Impressions in Time and across Varying Context: Personality, Attractiveness and Likeability</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d02d7a8f438acfcdf60b53d78943de4"><gtr:id>8d02d7a8f438acfcdf60b53d78943de4</gtr:id><gtr:otherNames>Celiktutan O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>56bb221f44f9d7.84455901</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2B8946EA-3FE1-49F5-9BF9-F8F7C0AB794E</gtr:id><gtr:title>Automatic Analysis of Facial Affect: A Survey of Registration, Representation, and Recognition.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de06bf2f9ec9fc2b1e88d254ad41d00a"><gtr:id>de06bf2f9ec9fc2b1e88d254ad41d00a</gtr:id><gtr:otherNames>Sariyanidi E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>56bb1aa3e6fed5.04619030</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ABF5C1AC-5F2B-459C-A274-34E662B502D8</gtr:id><gtr:title>Proceedings of the 2014 Mapping Personality Traits Challenge and Workshop</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/593240406f6e454fed69cd8cff83da57"><gtr:id>593240406f6e454fed69cd8cff83da57</gtr:id><gtr:otherNames>Gunes, H;</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:isbn>978-1-4503-0480-1</gtr:isbn><gtr:outcomeId>56c3590c438128.69512060</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D51C2290-B835-4195-A41C-686744FB5D42</gtr:id><gtr:title>Local Zernike Moment Representation for Facial Affect Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de06bf2f9ec9fc2b1e88d254ad41d00a"><gtr:id>de06bf2f9ec9fc2b1e88d254ad41d00a</gtr:id><gtr:otherNames>Sariyanidi E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>542e845b99a2b4.40444620</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DAB652C9-E1D7-410E-82D4-60B19480FC2F</gtr:id><gtr:title>Automatic Prediction of Perceived Traits Using Visual Cues under Varied Situational Context</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/85f7ffe2107759190a3e46d0f44f14a7"><gtr:id>85f7ffe2107759190a3e46d0f44f14a7</gtr:id><gtr:otherNames>Joshi J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>542e8077371009.24980092</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96E31379-1EC5-4A62-98D0-E42D4298E8F6</gtr:id><gtr:title>Social Signal Processing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/386f3fde541c7f7ee4e021f0c1d57a95"><gtr:id>386f3fde541c7f7ee4e021f0c1d57a95</gtr:id><gtr:otherNames>Gunes H</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>542e7fd72f63a6.65866495</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>08B31BB8-203B-4A1B-B5FF-FF0BCAA5CE15</gtr:id><gtr:title>MAPTRAITS 2014: The First Audio/Visual Mapping Personality Traits Challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9e25d97e03275fc35d89d38ab18f5129"><gtr:id>9e25d97e03275fc35d89d38ab18f5129</gtr:id><gtr:otherNames>Celiktutan, O;</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56c35a126bc8f9.51257229</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3A29E945-254F-4325-940A-BA5D232BB4D9</gtr:id><gtr:title>Automatic Analysis of Facial Attractiveness from Video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/982c87036e9892c8e3859e99609ac1f8"><gtr:id>982c87036e9892c8e3859e99609ac1f8</gtr:id><gtr:otherNames>Kalayci S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>542e80de003f14.57308052</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K017500/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>