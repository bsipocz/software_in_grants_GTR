<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C842A34F-18F7-454D-A259-FED802368496"><gtr:id>C842A34F-18F7-454D-A259-FED802368496</gtr:id><gtr:name>University of Leicester</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Leicester</gtr:line4><gtr:line5>Leicestershire</gtr:line5><gtr:postCode>LE1 7RH</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C842A34F-18F7-454D-A259-FED802368496"><gtr:id>C842A34F-18F7-454D-A259-FED802368496</gtr:id><gtr:name>University of Leicester</gtr:name><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Leicester</gtr:line4><gtr:line5>Leicestershire</gtr:line5><gtr:postCode>LE1 7RH</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/3CF46228-FC4E-49BE-B553-9CE279ED7A47"><gtr:id>3CF46228-FC4E-49BE-B553-9CE279ED7A47</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Lansdale</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE006876%2F2"><gtr:id>DC3121C0-5F32-4370-9A9F-55B2B8002A55</gtr:id><gtr:title>Exploiting spatial cognition in picture database design</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E006876/2</gtr:grantReference><gtr:abstractText>The commercial and functional potential of picture databases is very great for a range of applications from medicine to medieval history. However, this potential is proving difficult to deliver and research activity in this area is intense. There are two principal approaches: i) extending existing methods to encode pictures by description and keywords; and ii) computational analysis of images to capture superficial aspects such as colour and texture; aiming to remove the effort of entering pictures into a database and to allow user's crude depictions to access subsets of pictures to be searched for recognition. However, current opinion suggests neither approach is likely in the medium term to deliver cost-effective solutions to the problem except in highly specialised areas.Our research proposes to innovate by considering the problem from a third, psychological, perspective: human spatial cognition is robust, and we are generally good at inspecting pictures and recalling their spatial layout later. Furthermore, the layout of most images can be described in ways that preserve elements of meaning and visual distinctiveness. Ideally therefore, databases that encode location information in pictures, and allow users to use that information in retrieval, represent a match of human skills with a method generally applicable to most task domains. To this end, this proposal links two lines of psychological research. First, we are interested in visual attention: how do people look at pictures? For the purposes of database design, we are interested in the relationship between picture content and attention; as expressed in eye movements. Although eye movements are variable, they do show elements of consistency. We will be concerned with how best to represent and evaluate this consistency as a function of factors such as: the picture content; different observers; task domain; and delay between storage and retrieval.Second, we aim to study how the spatial layout of images is remembered as a consequence of attention. Can we use our understanding of visual attention processes (and eye movements in particular) to predict spatial recall? How precise is this spatial knowledge, how could it be used, and how discriminating is it in the retrieval of images from a database? There are two issues here: (i) We know that some location knowledge is acquired very quickly in the inspection processes. This is also the stage when the viewer's eye movements are more predictable by computer because they are driven by visual analysis of the image and less upon its meaning. It follows that if we can model the relationship between early eye movements and location memory, and if that memory is useful in retrieval, then some indexing of pictures into databases can be automated. This research aims to evaluate this potential; (ii), As inspection continues, eye movements become harder to predict as the viewer's understanding of the content of the image develops. We aim to show how this meaning influences eye movements and the impact of this upon location memory beyond that gained in the early stages of viewing. Overall, these two complementary questions will tell us how much picture coding can be automated and how task- and user- specific factors will influence design.As a study of the feasibility of this innovation to the design of picture databases, this proposal also considers the adaptability and efficiency of the approach in different circumstances. Accordingly, in evaluating the cost benefits to picture databases, the project will seek to measure the contribution of: domain expertise, training, and some interface design issues. This will indicate whether the approach has general applicability in picture databases or whether it is best applied to bespoke, specialist, systems where training and expertise is required.</gtr:abstractText><gtr:fund><gtr:end>2009-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>142101</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>refer to EP/E006876/2 which is effectively the same grant.</gtr:description><gtr:id>71F6D890-F242-4590-983E-45861D5E8AC9</gtr:id><gtr:impactTypes/><gtr:outcomeId>545a38167d7fa4.98627478</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>refer to EP/E006876/2 which is effectively the same grant.</gtr:description><gtr:exploitationPathways>refer to EP/E006876/2 which is effectively the same grant.</gtr:exploitationPathways><gtr:id>B46C3652-C6DE-4A7E-98A5-477CA4C54801</gtr:id><gtr:outcomeId>545a386cad62f6.94806023</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Culture, Heritage, Museums and Collections,Pharmaceuticals and Medical Biotechnology</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/E006876/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>324160BC-9DAC-4FE9-AFED-7F313BEF2AB8</gtr:id><gtr:grantRef>EP/E006876/1</gtr:grantRef><gtr:amount>308537.88</gtr:amount><gtr:start>2006-10-16</gtr:start><gtr:end>2008-04-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>DC3121C0-5F32-4370-9A9F-55B2B8002A55</gtr:id><gtr:grantRef>EP/E006876/2</gtr:grantRef><gtr:amount>142101.97</gtr:amount><gtr:start>2008-05-01</gtr:start><gtr:end>2009-12-31</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>