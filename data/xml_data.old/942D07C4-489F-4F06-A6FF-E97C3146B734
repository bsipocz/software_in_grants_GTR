<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F7075D7F-1E12-4B81-A7F3-2A9DF28D7097"><gtr:id>F7075D7F-1E12-4B81-A7F3-2A9DF28D7097</gtr:id><gtr:firstName>Pascal</gtr:firstName><gtr:surname>Belin</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FE003958%2F1"><gtr:id>942D07C4-489F-4F06-A6FF-E97C3146B734</gtr:id><gtr:title>The perception of voice gender and identity: a combined behavioural electrophysiological and neuroimaging approach.</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/E003958/1</gtr:grantReference><gtr:abstractText>How do we know that a voice belongs to a male or a female individual? How can we recognize a person's voice? Despite the apparent ease with which we solve these problems and the importance of these abilities in our everyday communication, little is known on the cerebral mechanisms involved. The proposed research aims to investigate our perception of voice gender and voice identity by combining state-of-the-art techniques in sound synthesis and brain imaging. Recent 'auditory morphing' techniques will be used to generate series of natural-sounding synthetic voices changing progressively between one voice and another. Two types of morphs will be generated, corresponding to the two parts of the proposed research. One type of morph will be generated between male and female voices ('gender morph') and will be used in the first part of the research to investigate how voice gender is perceived. Another type of morph will be generated between a voice A and a voice B with which participants will be familiarized ('identity morph') and will be used in the second part of the research to investigate how we recognize a person's identity from the voice. The two parts of the research - Part one on gender and Part two on identity- will then follow the exact same plan. In each part, a group of normal adult volunteers -half male and half female- will be recruited. Each participant will first be played synthetic voices from the morphs, and will be asked to decide for each voice if it sounds more like a male or a female voice (Part one) or more like voice A or voice B (Part two). This will allow to determine for each participant the voice on the morph that is the most 'ambiguous', i.e. that is classified on half the trials as male and half the trials as female (Part one), or half the trials as voice A and half the trials as voice B (Part two). Then participants will be played pairs of voices drawn for the morph, either from a same side of the ambiguous voice, or from each side of the ambiguous voice, and will be asked to decide if the two voices are the same or different. We expect that on average, participants will obtain a better performance for two voices taken from each side of the ambiguous voice. This would be a proof that they perceive the morphs in a 'categorical manner', i.e., as two distinct categories rather than as a continuum. This 'categorical perception' has already been demonstrated for morphs between different syllables -or in vision between different faces- but never for the gender or identity of voices. Two complementary techniques will then be used to measure participants' brain response to the synthetic voices: electroencephalography (EEG), and functional magnetic resonance imaging (fMRI). These two techniques will determine what parts of the participants' brain are sensitive to changes in voice gender (Part one) or voice identity (Part two). The combination of EEG and fMRI will allow a very precise characterization of these sensitive brain regions both in space (where exactly in the brain) and time (when exactly do they respond after presentation of the voice). Importantly, using synthetic voices from the morphs will ensure that the brain regions identified are really sensitive to the perceived gender (Part one) or identity (Part two) of the voices, not merely to physical characteristics of the voices. Overall, this research will increase our understanding of the way the auditory part of the brain analyses sounds of voice to allow us to rapidly and accurately extract information on a person's gender and identity simply by hearing his/her voice. This is important because very little is known on this aspect of brain function; knowing more in this area of brain research will have important implications on our understanding of how speech and language evolved in the human brain, and how brain diseases with an adverse impact on audio-visual communication, such as aphasia or autism, could be better diagnosed and treated.</gtr:abstractText><gtr:technicalSummary>Human listeners possess exquisite abilities to extract information on a person's physical characteristics from the voice, such as his/her gender, and identity. Despite their importance in human communication, these paralinguistic voice perception abilities have attracted little scientific attention compared to speech perception. This is surprising as 1) comparable research on face processing currently generates huge scientific interest; 2) speech perception itself may have evolved from a pre-existing neural architecture dedicated to voice perception and recognition also present in other species. The present project aims at investigating the cerebral mechanisms underlying two important aspects of voice perception, gender identification (Exp. 1-3) and speaker recognition (Exp. 4-6) by using a combination of behavioural, electrophysiological and neuroimaging methods. Sets of stimuli will be generated via auditory morphing of two natural voice samples: morphs between unfamiliar male and female voices (Gender) and morphs between two familiar voices (Identity). The morphing procedure will ensure that the physical -but not perceptual- difference between any two adjacent stimuli of a same morph is the same. Behavioural experiments will test the hypothesis of categorical perception of the voice continua, by measuring discrimination of pairs of stimuli drawn from a same morph. Pairs drawn 'within' a same side of the individually determined ambiguous stimulus are expected to be harder to discriminate that pairs drawn 'across' this stimulus. Neuronal populations involved in the representation of voice gender and identity will then be precisely identified in time and space. The 'mismatch negativity' (MMN) EEG protocol and the 'fMR-adaptation' protocol will be used to compare the brain response to 'across' vs. 'within' pairs of same physical distance, thus isolating brain regions sensitive to changes in perceived gender or identity irrespective of physical changes.</gtr:technicalSummary><gtr:fund><gtr:end>2010-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2007-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>331211</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F77EC32E-445B-4B7A-9040-BEEE5A7A5814</gtr:id><gtr:title>It doesn't matter what you say: FMRI correlates of&amp;nbsp;voice learning and recognition independent of&amp;nbsp;speech content.</gtr:title><gtr:parentPublicationTitle>Cortex; a journal devoted to the study of the nervous system and behavior</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c1dac5fc7ceaafae7be5fca77e11adc"><gtr:id>9c1dac5fc7ceaafae7be5fca77e11adc</gtr:id><gtr:otherNames>Z?ske R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0010-9452</gtr:issn><gtr:outcomeId>5a2fcc329bf552.25998317</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>22A4F33A-2866-400E-8384-12302A0FC93E</gtr:id><gtr:title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/824e1403e61b9b9bd56cf5f36e17589f"><gtr:id>824e1403e61b9b9bd56cf5f36e17589f</gtr:id><gtr:otherNames>Pernet CR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>5675e7757808e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39553662-D0D3-49A2-A6AD-4D72A36C2276</gtr:id><gtr:title>&amp;quot;Hearing faces and seeing voices&amp;quot;: Amodal coding of person identity in the human brain.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a798f00cbefb31d708c09d59e67f35f2"><gtr:id>a798f00cbefb31d708c09d59e67f35f2</gtr:id><gtr:otherNames>Awwad Shiekh Hasan B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn><gtr:outcomeId>588b62f5723a38.07522854</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FC45883E-4123-4C21-A0D0-236B4898C01C</gtr:id><gtr:title>Abstract encoding of auditory objects in cortical activity patterns.</gtr:title><gtr:parentPublicationTitle>Cerebral cortex (New York, N.Y. : 1991)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/847137823aa4a5a3be74fd867ce3e64d"><gtr:id>847137823aa4a5a3be74fd867ce3e64d</gtr:id><gtr:otherNames>Giordano BL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1047-3211</gtr:issn><gtr:outcomeId>doi_55f94a94a8c7740b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>84B7DBE0-7C0A-4130-8F11-BE9B9FCA998D</gtr:id><gtr:title>Electrophysiological evidence for an early processing of human voices.</gtr:title><gtr:parentPublicationTitle>BMC neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/969860aa70ec850e6f53ff7132edc468"><gtr:id>969860aa70ec850e6f53ff7132edc468</gtr:id><gtr:otherNames>Charest I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1471-2202</gtr:issn><gtr:outcomeId>pm_5435ab05ab087c588</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0529628B-A2E2-453C-8C2C-C0FC06A98441</gtr:id><gtr:title>Learning-induced changes in the cerebral processing of voice identity.</gtr:title><gtr:parentPublicationTitle>Cerebral cortex (New York, N.Y. : 1991)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61c63c6387b5f46a9dbc77b6a8956757"><gtr:id>61c63c6387b5f46a9dbc77b6a8956757</gtr:id><gtr:otherNames>Latinus M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1047-3211</gtr:issn><gtr:outcomeId>doi_53d0460462d77e8e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B1F24FBB-7649-4C5A-BD69-73F21D66F1B5</gtr:id><gtr:title>Effects of vocoding and intelligibility on the cerebral response to speech.</gtr:title><gtr:parentPublicationTitle>BMC neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/55c8bd5826b08dd5c451338e63c37370"><gtr:id>55c8bd5826b08dd5c451338e63c37370</gtr:id><gtr:otherNames>Strelnikov K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1471-2202</gtr:issn><gtr:outcomeId>doi_55f94a94a8d49433</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2627D3D0-AB45-4597-8832-A86C90FDE530</gtr:id><gtr:title>A unified coding strategy for processing faces and voices.</gtr:title><gtr:parentPublicationTitle>Trends in cognitive sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41b7add1ce3e001284eb83c7d141ee42"><gtr:id>41b7add1ce3e001284eb83c7d141ee42</gtr:id><gtr:otherNames>Yovel G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1364-6613</gtr:issn><gtr:outcomeId>pm_5435ab05ab07ca4d9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5B50DCEB-D30C-4F15-B835-87AB7E430986</gtr:id><gtr:title>The Glasgow Voice Memory Test: Assessing the ability to memorize and recognize unfamiliar voices.</gtr:title><gtr:parentPublicationTitle>Behavior research methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b6ab71e4ee92fe1b1c62e90ceef98fb3"><gtr:id>b6ab71e4ee92fe1b1c62e90ceef98fb3</gtr:id><gtr:otherNames>Aglieri V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1554-351X</gtr:issn><gtr:outcomeId>5a66143400e339.62051560</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C6C2842-E58A-48A0-A0A4-86B2A906F50D</gtr:id><gtr:title>Anti-voice adaptation suggests prototype-based coding of voice identity.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61c63c6387b5f46a9dbc77b6a8956757"><gtr:id>61c63c6387b5f46a9dbc77b6a8956757</gtr:id><gtr:otherNames>Latinus M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>doi_55f94a94a8dccd99</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D525A92B-13FC-4986-8DB9-9E1F4211066A</gtr:id><gtr:title>Understanding voice perception.</gtr:title><gtr:parentPublicationTitle>British journal of psychology (London, England : 1953)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2df4f5db543fcf1922201cc6658294c"><gtr:id>d2df4f5db543fcf1922201cc6658294c</gtr:id><gtr:otherNames>Belin P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0007-1269</gtr:issn><gtr:outcomeId>pm_5435ab05ab0adee02</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0D3E9304-95FB-4E4A-832A-AAC907F78CB5</gtr:id><gtr:title>Cerebral processing of voice gender studied using a continuous carryover FMRI design.</gtr:title><gtr:parentPublicationTitle>Cerebral cortex (New York, N.Y. : 1991)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/969860aa70ec850e6f53ff7132edc468"><gtr:id>969860aa70ec850e6f53ff7132edc468</gtr:id><gtr:otherNames>Charest I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1047-3211</gtr:issn><gtr:outcomeId>doi_55f93b93b34c9bad</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>32A0EED0-35CC-40BC-99F0-B163C90DF86D</gtr:id><gtr:title>Vocal attractiveness increases by averaging.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9792451d76dc7dfdf36f0d29e1c14ade"><gtr:id>9792451d76dc7dfdf36f0d29e1c14ade</gtr:id><gtr:otherNames>Bruckert L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>doi_55f94a94a8b4f58a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9EB02CB-55F3-4AF1-9BB8-0A5F2E683573</gtr:id><gtr:title>Similarities in face and voice cerebral processing</gtr:title><gtr:parentPublicationTitle>Visual Cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2df4f5db543fcf1922201cc6658294c"><gtr:id>d2df4f5db543fcf1922201cc6658294c</gtr:id><gtr:otherNames>Belin P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fcc3e410102.77522551</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55B5E194-2A0E-445E-9C6B-24234EC695CF</gtr:id><gtr:title>Norm-based coding of voice identity in human auditory cortex.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61c63c6387b5f46a9dbc77b6a8956757"><gtr:id>61c63c6387b5f46a9dbc77b6a8956757</gtr:id><gtr:otherNames>Latinus M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>doi_55f93c93c114f847</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0F7DD4A0-6D26-45F2-99D7-9ECC0A5A5EDB</gtr:id><gtr:title>Perceptual scaling of voice identity: common dimensions for different vowels and speakers.</gtr:title><gtr:parentPublicationTitle>Psychological research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b705aa279fca2214d7a92e4cb49444da"><gtr:id>b705aa279fca2214d7a92e4cb49444da</gtr:id><gtr:otherNames>Baumann O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0340-0727</gtr:issn><gtr:outcomeId>pm_5435ab05ab091b1cd</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>05601604-4D22-49B4-BAB0-A035F772458E</gtr:id><gtr:title>The role of pitch and timbre in voice gender categorization.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/824e1403e61b9b9bd56cf5f36e17589f"><gtr:id>824e1403e61b9b9bd56cf5f36e17589f</gtr:id><gtr:otherNames>Pernet CR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>pm_5435ab05ab09c3489</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/E003958/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>