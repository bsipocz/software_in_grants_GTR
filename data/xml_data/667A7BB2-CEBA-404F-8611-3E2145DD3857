<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C873E474-E8BE-4A29-ABFE-B99EA58485C9"><gtr:id>C873E474-E8BE-4A29-ABFE-B99EA58485C9</gtr:id><gtr:firstName>Miles</gtr:firstName><gtr:otherNames>Edward</gtr:otherNames><gtr:surname>Hansard</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM01469X%2F1"><gtr:id>667A7BB2-CEBA-404F-8611-3E2145DD3857</gtr:id><gtr:title>Geometric Evaluation of Stereoscopic Video</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M01469X/1</gtr:grantReference><gtr:abstractText>3D films and games have become a popular part of digital entertainment. There are more and more consumer devices that can show 3D, including the new generation of head-mounted displays, which are reawakening the idea of virtual reality. When it works properly, 3D adds a fun and interesting new dimension to the viewing experience. But sometimes there are problems; for example, the scene may look flat, or distorted. Even worse, there may be too much depth, which can eventually cause eye-strain and headaches.

This project is about evaluating the amount of depth that is being shown, so that video producers and game designers can avoid audience discomfort. There are many rules of thumb for setting the right amount of depth, but it would be safer to take a scientific approach. This project will develop the theory of 3D content evaluation, based on the following principles.

In order to see 3D, the images shown to the left and right eye must be slightly different - just as they are when viewing a real scene. The differences between the two images, which are small horizontal offsets, are called binocular disparities. Roughly speaking, the amount of disparity is proportional to the perceived depth. But in order to fully understand the depth/disparity relationship, it is necessary to know something about the human visual system, as well as the 3D display. In particular, we need to know (or guess) where in the scene the viewer is likely to be looking. The project will develop a system that can predict the point of interest, at any given moment. This information can then be used, in conjunction with the binocular disparities, to quantify how much depth is being shown. The video producer or game designer will then be able to modify the content, so that it includes the right amount of 3D.

The project is at the intersection of several research areas, including geometry, signal processing, and visual perception. A prototype software tool will be developed, in conjunction with a UK visual effects company, for the automatic analysis of 3D content. The final system will be tested by asking ordinary people to evaluate a range of 3D videos; if the system can predict the human responses, then it will be useful to video producers and game designers.</gtr:abstractText><gtr:potentialImpactText>The ultimate impact of this project will be to help expand the range of 3D content, including films, games, visualizations, and virtual reality environments. This will be achieved by developing a new model of stereoscopic discomfort, which will be used by video producers and game designers, to ensure that 3D content is safe to view. This will benefit the general public, who will thereby have access to a wider range of digital entertainment. In particular, the tools developed in this project will help to bring 3D content to the new generation of head-mounted displays, including the Oculus Rift and Sony Morpheus.

Software tools, based on the outcomes of this project, will be developed by engineers and producers in the visual effects industry. This sector is very strong in the UK, and makes an important contribution to the national economy (see Case for support). Hence there will be a medium-term economic benefit to these companies, which are well-placed to engage with the project and its outcomes. Impact will be ensured by working directly with a UK visual effects company (see industrial Letter of support).

More generally, the project will have an impact on 3D cinema standards, and on broadcasting research and development. In addition, there will be an impact on specialist applications, such as stereoscopic medical visualization and security scanning. The impact of the project will be transmitted through presentations and publications of the work, tailored for the engineering, biological and industrial audiences. In addition, a software implementation of the new stereo model will be made freely available on the project website, so that other researchers can adapt and extend it. 

In the long term, the model developed in this project will contribute to the understanding of human binocular vision. This has clinical implications for the treatment of oculomotor disorders. The pathway to clinical impact would be via extensive testing of the new model. This could be performed, in a future project, in conjunction with psychophysics and ophthalmology researchers. The UK has a very strong vision research community, which makes this a promising strategy for long-term clinical impact.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>88495</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have developed research software for analyzing 3DTV images. The system can automatically predict whether or not a particular image will be comfortable to view in 3D. We have also developed a range of software tools, which are used to prepare stereo images for display on 3DTVs.</gtr:description><gtr:exploitationPathways>We may make our software and data publicly available, after the key papers have been published (still pending).</gtr:exploitationPathways><gtr:id>C3487BC8-DFE5-452F-A85C-0C3CAD7E561D</gtr:id><gtr:outcomeId>56dc7068e3d531.86192960</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Collection of stereoscopic content, preprocessed for display on 3DTVs (rectified, cropped, and stacked).</gtr:description><gtr:id>3FC56E7B-CCF7-498F-BE16-B00D3843211B</gtr:id><gtr:impact>This database was used in a psychophysical experiment, to measure visual discomfort.</gtr:impact><gtr:outcomeId>58c15c5a1badd7.60681467</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Preprocessed stereoscopic content</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>This model predicts subjective visual discomfort ratings, for stereoscopic content, based on the statistics of the associated disparity map.</gtr:description><gtr:id>AEC82203-AF53-4C9A-9144-0BE285AE0C69</gtr:id><gtr:impact>Currently being used to analyze experimental data.</gtr:impact><gtr:outcomeId>58c15ce8c08921.55856376</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Statistical model of stereoscopic visual discomfort</gtr:title><gtr:type>Computer model/algorithm</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Software for processing and analyzing stereoscopic 3D content. Designed for statistical analysis of disparity maps, in the context of visual discomfort. Written in C99 and R.</gtr:description><gtr:id>EB9BD328-D59F-467A-829B-84FC2E820AFC</gtr:id><gtr:impact>Currently being used to analyze data, for research publications.</gtr:impact><gtr:outcomeId>58c15bbd6a0700.74635022</gtr:outcomeId><gtr:title>Software for stereo image analysis</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>81B2A56D-D8BD-4A9F-9BEE-9BD85EE0A4DF</gtr:id><gtr:title>Methods for Reducing Visual Discomfort in Stereo 3D: A Review</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2883c424ae2bf60a2c17c2c4ff9659d4"><gtr:id>2883c424ae2bf60a2c17c2c4ff9659d4</gtr:id><gtr:otherNames>Terzic, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56dc740e725246.87613610</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C785C4D-29A4-42D6-A35C-65D12E6EC034</gtr:id><gtr:title>Geometric and algorithmic developments in PatchMatch stereo</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fcfefbb2aa3e07c3d788acee9e2e35ca"><gtr:id>fcfefbb2aa3e07c3d788acee9e2e35ca</gtr:id><gtr:otherNames>Ahmed S A</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>58c16152bbb711.65436256</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>512C844A-26ED-4CDE-A495-DE8A7B5EE971</gtr:id><gtr:title>Causes of discomfort while watching stereo 3D: A Review</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2883c424ae2bf60a2c17c2c4ff9659d4"><gtr:id>2883c424ae2bf60a2c17c2c4ff9659d4</gtr:id><gtr:otherNames>Terzic, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56dc747d48d035.01415519</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C7C4F8F8-0DC2-44E7-A5F2-695980EEF87B</gtr:id><gtr:title>Methods for reducing visual discomfort in stereoscopic 3D: A review</gtr:title><gtr:parentPublicationTitle>Signal Processing: Image Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97ddaee7944990725769b3d0befc3bbd"><gtr:id>97ddaee7944990725769b3d0befc3bbd</gtr:id><gtr:otherNames>Terzic K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d67a07f3001.42552338</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E8B52F7C-3EDE-4433-BA13-9EEF68D5FF73</gtr:id><gtr:title>Causes of discomfort in stereoscopic content: a review</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b46bdb9503dec9797ad24dd45d53ca38"><gtr:id>b46bdb9503dec9797ad24dd45d53ca38</gtr:id><gtr:otherNames>Terzic K T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c15da7d5ef79.64328291</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M01469X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>