<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/6E750E7A-C076-435E-8AB0-484F65D6C7AD"><gtr:id>6E750E7A-C076-435E-8AB0-484F65D6C7AD</gtr:id><gtr:name>Kingston University</gtr:name><gtr:department>Fac of Science Engineering and Computing</gtr:department><gtr:address><gtr:line1>River House</gtr:line1><gtr:line2>53-57 High Street</gtr:line2><gtr:line4>Kingston Upon Thames</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>KT1 1LQ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6E750E7A-C076-435E-8AB0-484F65D6C7AD"><gtr:id>6E750E7A-C076-435E-8AB0-484F65D6C7AD</gtr:id><gtr:name>Kingston University</gtr:name><gtr:address><gtr:line1>River House</gtr:line1><gtr:line2>53-57 High Street</gtr:line2><gtr:line4>Kingston Upon Thames</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>KT1 1LQ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E7435785-6960-4EC6-9D4A-BF2F42F120A9"><gtr:id>E7435785-6960-4EC6-9D4A-BF2F42F120A9</gtr:id><gtr:firstName>Dimitrios</gtr:firstName><gtr:surname>Makris</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE033288%2F1"><gtr:id>53CE9284-4C06-44C3-8FDB-684AEEDD74F6</gtr:id><gtr:title>PRoCeSS: Pose Recovery in Context Specific Scenarios</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E033288/1</gtr:grantReference><gtr:abstractText>The aim of PRoCeSS is to advance the state-of-the-art in Pose Recovery (i.e. determining the position and orientation of articulated parts of the human body) to the level that can operate reliably outside specialised labs, in real-life scenarios.The PRoCeSS system will be useful in applications such as the coverage and the analysis of sporting events. Athletes' movements will be captured in a 3D representation and analysed. TV will be able to virtually replay sporting events, which will be constructed automatically. Human-computer interfaces based on 3D body motion will provide people with a physical way to interact with machines. Also, visual surveillance will acquire another tool for analysing human behaviour.PRoCeSS will have to challenge real-life restrictions such as cluttered scenes, single cameras or multiple cameras that are well separated from each other. It will attempt to address these issues by exploiting the context of specific scenarios (e.g walking, chatting, opening the door in surveillance, running, kicking the ball in football, serving the ball in tennis) to increase the feasibility of real life Pose Recovery. It will also exploit the link between certain activities and areas of the scene (e.g. sitting down on seat , opening door at the exit, typing keyboard at computer desk ), or the specific sequencing of actions in particular scenarios (e.g. the scenario penalty , consists of the following activities of the player: placing the ball , walking backwards , running forwards and kicking the ball )</gtr:abstractText><gtr:fund><gtr:end>2010-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>107948</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>110528</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Knowledge Transfer Partnership (KTP)</gtr:description><gtr:end>2010-05-02</gtr:end><gtr:fundingOrg>Technology Strategy Board (TSB)</gtr:fundingOrg><gtr:fundingRef>KTP006563</gtr:fundingRef><gtr:id>AD4B8820-A793-4EEC-93C4-F9A4311DA95C</gtr:id><gtr:outcomeId>5ee10da45ee10db8</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2008-05-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>66838</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC CASE PhD Studentship</gtr:description><gtr:end>2014-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/I501479/1</gtr:fundingRef><gtr:id>4AA20C11-22AE-4D09-82E8-65FF73986312</gtr:id><gtr:outcomeId>5edaa3605edaa36a</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>500D55ED-B0F6-4972-BD6A-9F81D13E8E82</gtr:id><gtr:title>Human pose tracking in low dimensional space enhanced by limb correction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1848af4eddafe162aa9d85710a64527"><gtr:id>b1848af4eddafe162aa9d85710a64527</gtr:id><gtr:otherNames>Moutzouris A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1304-0</gtr:isbn><gtr:outcomeId>5463d7d6b6f237.82326243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>77690C1A-3936-4737-B847-132AE97E68E5</gtr:id><gtr:title>Temporal Extension of Laplacian Eigenmaps for Unsupervised Dimensionality Reduction of Time Series</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4a3fbc6c2ebe758292a48219a26daee4"><gtr:id>4a3fbc6c2ebe758292a48219a26daee4</gtr:id><gtr:otherNames>Lewandowski M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7542-1</gtr:isbn><gtr:outcomeId>5463d659a364e8.64090301</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F085C4D6-D659-443B-8E18-658683D55222</gtr:id><gtr:title>Automatic configuration of spectral dimensionality reduction methods</gtr:title><gtr:parentPublicationTitle>Pattern Recognition Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4a3fbc6c2ebe758292a48219a26daee4"><gtr:id>4a3fbc6c2ebe758292a48219a26daee4</gtr:id><gtr:otherNames>Lewandowski M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d0040045b32b41</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FADF1C2F-D322-49DE-B72F-1CE9534B251F</gtr:id><gtr:title>Integration of bottom-up/top-down approaches for 2D pose estimation using probabilistic Gaussian modelling</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a64a796cdefe0a929a40f837252d391f"><gtr:id>a64a796cdefe0a929a40f837252d391f</gtr:id><gtr:otherNames>Kuo P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53cfecfec47b0204</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>64D16244-9F8B-4A78-B507-7C98F0E20918</gtr:id><gtr:title>Tracking human position and lower body parts using Kalman and particle filters constrained by human biomechanics.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1f9262d9c23ef26e509269770dd5ac8b"><gtr:id>1f9262d9c23ef26e509269770dd5ac8b</gtr:id><gtr:otherNames>Martinez del Rincon J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1083-4419</gtr:issn><gtr:outcomeId>doi_53d05f05f7a44bfc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88713D07-AEB9-4229-A2E6-7008F2B4206E</gtr:id><gtr:title>Human pose tracking by Hierarchical Manifold Searching</gtr:title><gtr:parentPublicationTitle>Proceedings - International Conference on Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4df816792ab1ccc49c842d4ae7228a80"><gtr:id>4df816792ab1ccc49c842d4ae7228a80</gtr:id><gtr:otherNames>Moutzouris A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>10514651</gtr:issn><gtr:outcomeId>5463d8ba242352.68134881</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E033288/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>