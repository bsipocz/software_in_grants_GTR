<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/2CB97252-2D07-4F6F-8691-0E72B465568A"><gtr:id>2CB97252-2D07-4F6F-8691-0E72B465568A</gtr:id><gtr:name>Medieval Settlement Research Group</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/862155C6-0D75-449F-9D76-49B054E0316B"><gtr:id>862155C6-0D75-449F-9D76-49B054E0316B</gtr:id><gtr:name>NICT National Institute of Information and Communications Technology</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/EF0A0061-F60F-4692-947B-E406296939F5"><gtr:id>EF0A0061-F60F-4692-947B-E406296939F5</gtr:id><gtr:name>Chinese Academy of Sciences</gtr:name><gtr:address><gtr:line1>52 Sanlihe Road</gtr:line1><gtr:postCode>100864</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/42869665-273F-42E0-A545-0015A837C063"><gtr:id>42869665-273F-42E0-A545-0015A837C063</gtr:id><gtr:name>National Institute of  Informatics (NII)</gtr:name><gtr:address><gtr:line1>2-1-2 Hitotsubashi</gtr:line1><gtr:line2>Chiyoda-ku</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1085A47A-566B-423D-B2BB-41ECD918125A"><gtr:id>1085A47A-566B-423D-B2BB-41ECD918125A</gtr:id><gtr:name>South China University of Technology</gtr:name><gtr:address><gtr:line1>Guangzhou H.E. Mega Centre</gtr:line1><gtr:line2>Panyu District</gtr:line2><gtr:line4>Guangzhou</gtr:line4><gtr:line5>510006</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>China</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/4B22329D-EC8A-4F4B-942E-5C4FFCBE733C"><gtr:id>4B22329D-EC8A-4F4B-942E-5C4FFCBE733C</gtr:id><gtr:name>Nagoya Institute of Technology</gtr:name><gtr:address><gtr:line1>Gokisko-Cho</gtr:line1><gtr:line2>Showa-Ku</gtr:line2><gtr:line3>Nagoya</gtr:line3><gtr:line4>Aichi 466-8555</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/E89B9BEA-BECB-40CD-ACDD-92C34A8CDF1C"><gtr:id>E89B9BEA-BECB-40CD-ACDD-92C34A8CDF1C</gtr:id><gtr:name>Ericsson</gtr:name><gtr:address><gtr:line1>TORSHAMNSGATAN 21-23</gtr:line1><gtr:line4>STOCKHOLM</gtr:line4><gtr:postCode>16480</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Sweden</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/08EA0163-295D-4563-B0A2-14C7F7AB6C68"><gtr:id>08EA0163-295D-4563-B0A2-14C7F7AB6C68</gtr:id><gtr:name>English Heritage</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D5382568-3EB8-401D-9279-BBE34BB27CA6"><gtr:id>D5382568-3EB8-401D-9279-BBE34BB27CA6</gtr:id><gtr:name>Barnsley Hospital NHS Foundation Trust</gtr:name><gtr:address><gtr:line1>NHS Trust</gtr:line1><gtr:line2>Gawber Road</gtr:line2><gtr:line4>Barnsley</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S75 2EP</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9D95AFAF-0F95-473D-8114-00B6A832CE18"><gtr:id>9D95AFAF-0F95-473D-8114-00B6A832CE18</gtr:id><gtr:name>University of Zaragoza</gtr:name><gtr:address><gtr:line1>Departamento de Quimica Organica</gtr:line1><gtr:line2>Facultad de Ciencias</gtr:line2><gtr:line3>Universidad de Zaragoza</gtr:line3><gtr:postCode>50009 Zara</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/652A40B5-A421-4A42-A892-7F963726C0B9"><gtr:id>652A40B5-A421-4A42-A892-7F963726C0B9</gtr:id><gtr:name>Cereproc Ltd.</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/82F274CB-C3AA-445B-9714-3CF56785F38E"><gtr:id>82F274CB-C3AA-445B-9714-3CF56785F38E</gtr:id><gtr:name>Johns Hopkins University</gtr:name><gtr:address><gtr:line1>3400 North Charles Street</gtr:line1><gtr:line4>Baltimore</gtr:line4><gtr:line5>MD 21218</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Informatics</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2CB97252-2D07-4F6F-8691-0E72B465568A"><gtr:id>2CB97252-2D07-4F6F-8691-0E72B465568A</gtr:id><gtr:name>Medieval Settlement Research Group</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF0A0061-F60F-4692-947B-E406296939F5"><gtr:id>EF0A0061-F60F-4692-947B-E406296939F5</gtr:id><gtr:name>Chinese Academy of Sciences</gtr:name><gtr:address><gtr:line1>52 Sanlihe Road</gtr:line1><gtr:postCode>100864</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1085A47A-566B-423D-B2BB-41ECD918125A"><gtr:id>1085A47A-566B-423D-B2BB-41ECD918125A</gtr:id><gtr:name>South China University of Technology</gtr:name><gtr:address><gtr:line1>Guangzhou H.E. Mega Centre</gtr:line1><gtr:line2>Panyu District</gtr:line2><gtr:line4>Guangzhou</gtr:line4><gtr:line5>510006</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>China</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4B22329D-EC8A-4F4B-942E-5C4FFCBE733C"><gtr:id>4B22329D-EC8A-4F4B-942E-5C4FFCBE733C</gtr:id><gtr:name>Nagoya Institute of Technology</gtr:name><gtr:address><gtr:line1>Gokisko-Cho</gtr:line1><gtr:line2>Showa-Ku</gtr:line2><gtr:line3>Nagoya</gtr:line3><gtr:line4>Aichi 466-8555</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/08EA0163-295D-4563-B0A2-14C7F7AB6C68"><gtr:id>08EA0163-295D-4563-B0A2-14C7F7AB6C68</gtr:id><gtr:name>English Heritage</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5382568-3EB8-401D-9279-BBE34BB27CA6"><gtr:id>D5382568-3EB8-401D-9279-BBE34BB27CA6</gtr:id><gtr:name>Barnsley Hospital NHS Foundation Trust</gtr:name><gtr:address><gtr:line1>NHS Trust</gtr:line1><gtr:line2>Gawber Road</gtr:line2><gtr:line4>Barnsley</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S75 2EP</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9D95AFAF-0F95-473D-8114-00B6A832CE18"><gtr:id>9D95AFAF-0F95-473D-8114-00B6A832CE18</gtr:id><gtr:name>University of Zaragoza</gtr:name><gtr:address><gtr:line1>Departamento de Quimica Organica</gtr:line1><gtr:line2>Facultad de Ciencias</gtr:line2><gtr:line3>Universidad de Zaragoza</gtr:line3><gtr:postCode>50009 Zara</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/652A40B5-A421-4A42-A892-7F963726C0B9"><gtr:id>652A40B5-A421-4A42-A892-7F963726C0B9</gtr:id><gtr:name>Cereproc Ltd.</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/862155C6-0D75-449F-9D76-49B054E0316B"><gtr:id>862155C6-0D75-449F-9D76-49B054E0316B</gtr:id><gtr:name>NICT National Institute of Information and Communications Technology</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/42869665-273F-42E0-A545-0015A837C063"><gtr:id>42869665-273F-42E0-A545-0015A837C063</gtr:id><gtr:name>National Institute of  Informatics (NII)</gtr:name><gtr:address><gtr:line1>2-1-2 Hitotsubashi</gtr:line1><gtr:line2>Chiyoda-ku</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E89B9BEA-BECB-40CD-ACDD-92C34A8CDF1C"><gtr:id>E89B9BEA-BECB-40CD-ACDD-92C34A8CDF1C</gtr:id><gtr:name>Ericsson</gtr:name><gtr:address><gtr:line1>TORSHAMNSGATAN 21-23</gtr:line1><gtr:line4>STOCKHOLM</gtr:line4><gtr:postCode>16480</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Sweden</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/82F274CB-C3AA-445B-9714-3CF56785F38E"><gtr:id>82F274CB-C3AA-445B-9714-3CF56785F38E</gtr:id><gtr:name>Johns Hopkins University</gtr:name><gtr:address><gtr:line1>3400 North Charles Street</gtr:line1><gtr:line4>Baltimore</gtr:line4><gtr:line5>MD 21218</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F1E4BF82-1D43-47A7-AFAE-2D1235418983"><gtr:id>F1E4BF82-1D43-47A7-AFAE-2D1235418983</gtr:id><gtr:name>Toshiba Research Europe Ltd</gtr:name><gtr:address><gtr:line1>Unit 208</gtr:line1><gtr:line2>Cambridge Science Park</gtr:line2><gtr:line3>Milton Road</gtr:line3><gtr:postCode>CB4 0GZ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BA1251D9-C463-48A8-AF87-8B85D4ED952D"><gtr:id>BA1251D9-C463-48A8-AF87-8B85D4ED952D</gtr:id><gtr:name>GCHQ</gtr:name><gtr:address><gtr:line1>F2B Room B2A</gtr:line1><gtr:line2>PO Box 128</gtr:line2><gtr:line4>Cheltenham</gtr:line4><gtr:line5>Gloucestershire</gtr:line5><gtr:postCode>GL52 5UA</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1CB9FE80-EF51-4421-9B8D-59987D96865A"><gtr:id>1CB9FE80-EF51-4421-9B8D-59987D96865A</gtr:id><gtr:name>Toby Churchill Ltd</gtr:name><gtr:address><gtr:line1>Toby Churchill House</gtr:line1><gtr:line2>Norman Way Industrial Estate</gtr:line2><gtr:line3>Over</gtr:line3><gtr:postCode>CB24 5QE</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/37417E05-FE30-4E51-A87F-5C9C08CAD2A5"><gtr:id>37417E05-FE30-4E51-A87F-5C9C08CAD2A5</gtr:id><gtr:name>EADS UK Ltd</gtr:name><gtr:address><gtr:line1>Wellington House</gtr:line1><gtr:line2>125 Strand</gtr:line2><gtr:postCode>WC2R 0AP</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FDB77482-15C3-47CF-92D5-BE6C8624D6B0"><gtr:id>FDB77482-15C3-47CF-92D5-BE6C8624D6B0</gtr:id><gtr:name>CereProc Limited</gtr:name><gtr:address><gtr:line1>Appleton Tower</gtr:line1><gtr:line2>11 Crichton Street</gtr:line2><gtr:line3>Edinburgh</gtr:line3><gtr:postCode>EH8 9LE</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/942A319B-00B9-41F1-8781-83CD9B8BE856"><gtr:id>942A319B-00B9-41F1-8781-83CD9B8BE856</gtr:id><gtr:name>NIHR CLAHRC for South Yorkshire</gtr:name><gtr:address><gtr:line1>Sheffield Teaching Hospitals NHS FT</gtr:line1><gtr:line2>11 Broomfield Road</gtr:line2><gtr:postCode>S10 2SE</gtr:postCode><gtr:region>Yorkshire and the Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0D046D2F-4838-4FC0-B6C7-682F6AAC2A51"><gtr:id>0D046D2F-4838-4FC0-B6C7-682F6AAC2A51</gtr:id><gtr:name>Cisco Systems Inc</gtr:name><gtr:address><gtr:line1>170 Tasman Drive</gtr:line1><gtr:line4>San Jose</gtr:line4><gtr:line5>CA 95134-1708</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C1B87DE7-E72F-40BC-AF3F-8D586C9FE779"><gtr:id>C1B87DE7-E72F-40BC-AF3F-8D586C9FE779</gtr:id><gtr:name>Devices for Dignity</gtr:name><gtr:address><gtr:line1>I Floor, Suite I100</gtr:line1><gtr:line2>Royal Hallamshire Hospital</gtr:line2><gtr:line3>Glossop Road</gtr:line3><gtr:postCode>S10 2JF</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/38B7E232-9903-45B7-B5D3-DFBABC5E8B7C"><gtr:id>38B7E232-9903-45B7-B5D3-DFBABC5E8B7C</gtr:id><gtr:name>Nuance Communications Inc</gtr:name><gtr:address><gtr:line1>1 Wayside Road</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/17C73C17-B084-44B6-B454-7416B4792EF3"><gtr:id>17C73C17-B084-44B6-B454-7416B4792EF3</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Gales</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2ADE8077-D275-4FCF-9D3B-9B08C2CFDBBD"><gtr:id>2ADE8077-D275-4FCF-9D3B-9B08C2CFDBBD</gtr:id><gtr:firstName>Phil</gtr:firstName><gtr:surname>Green</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/67DCA04C-40B7-45BC-B5BD-E5ED376AA1AE"><gtr:id>67DCA04C-40B7-45BC-B5BD-E5ED376AA1AE</gtr:id><gtr:firstName>Junichi</gtr:firstName><gtr:surname>Yamagishi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B1815F85-CAE9-4287-B478-41EE9D4C1924"><gtr:id>B1815F85-CAE9-4287-B478-41EE9D4C1924</gtr:id><gtr:firstName>William</gtr:firstName><gtr:otherNames>J</gtr:otherNames><gtr:surname>Byrne</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6E9EAE62-9ADC-4E84-B950-56966D646A8F"><gtr:id>6E9EAE62-9ADC-4E84-B950-56966D646A8F</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>King</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/AE291CAC-3378-4587-82CB-8DCE144A1CFC"><gtr:id>AE291CAC-3378-4587-82CB-8DCE144A1CFC</gtr:id><gtr:firstName>Thomas</gtr:firstName><gtr:surname>Hain</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/5D7F708A-5218-40FC-BD20-E55CBA1B14F6"><gtr:id>5D7F708A-5218-40FC-BD20-E55CBA1B14F6</gtr:id><gtr:firstName>Steve</gtr:firstName><gtr:surname>Renals</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/77D660F2-0608-4198-82D3-D1E4017D0B70"><gtr:id>77D660F2-0608-4198-82D3-D1E4017D0B70</gtr:id><gtr:firstName>Philip</gtr:firstName><gtr:otherNames>Charles</gtr:otherNames><gtr:surname>Woodland</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI031022%2F1"><gtr:id>F22FEBE2-7582-4761-8E01-6E637870F615</gtr:id><gtr:title>Natural Speech Technology</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I031022/1</gtr:grantReference><gtr:abstractText>Humans are highly adaptable, and speech is our natural medium for informal communication. When communicating, we continuously adjust to other people, to the situation, and to the environment, using previously acquired knowledge to make this adaptation seem almost instantaneous. Humans generalise, enabling efficient communication in unfamiliar situations and rapid adaptation to new speakers or listeners. Current speech technology works well for certain controlled tasks and domains, but is far from natural, a consequence of its limited ability to acquire knowledge about people or situations, to adapt, and to generalise. This accounts for the uneasy public reaction to speech-driven systems. For example, text-to-speech synthesis can be as intelligible as human speech, but lacks expression and is not perceived as natural. Similarly, the accuracy of speech recognition systems can collapse if the acoustic environment or task domain changes, conditions which a human listener would handle easily. Research approaches to these problems have hitherto been piecemeal and as a result progress has been patchy. In contrast NST will focus on the integrated theoretical development of new joint models for speech recognition and synthesis. These models will allow us to incorporate knowledge about the speakers, the environment, the communication context and awareness of the task, and will learn and adapt from real world data in an online, unsupervised manner. This theoretical unification is already underway within the NST labs and, combined with our record of turning theory into practical state-of-the-art applications, will enable us to bring a naturalness to speech technology that is not currently attainable.The NST programme will yield technology which (1) approaches human adaptability to new communication situations, (2) is capable of personalised communication, and (3) takes account of speaker intention and expressiveness in speech recognition and synthesis. This is an ambitious vision. Its success will be measured in terms of how the theoretical development reshapes the field over the next decade, the takeup of the software systems that we shall develop, and through the impact of our exemplar interactive applications.We shall establish a strong User Group to maximise the impact of the project, with a members concerned with clinical applications, as well as more general speech technology. Members of the User Group include Toshiba, EADS Innovation Works, Cisco, Barnsley Hospital NHS Foundation Trust, and the Euan MacDonald Centre for MND Research. An important interaction with the User Group will be validating our systems on their data and tasks, discussed at an annual user workshop.</gtr:abstractText><gtr:potentialImpactText>Leading market analysts predict that revenues from speech technology in North America alone will reach $1 billion by 2011. The reality has lagged behind such predictions in the past since the technology is not refined enough even at this time, but paradigms are shifting. The revolutionary change in connectivity and mobile computing in recent years gives rise to a number of compelling application drivers for the proposed research programme: (1) Rapid developments in mobile computing - decreasing power consumption, high network bandwidth and cloud computing - are stimulating demand for new interfaces. (2) Demographic and economic pressures mean that home care and support systems will become commonplace; such systems will benefit from personalised spoken interaction. (3) Remote meetings are becoming standard, stimulated by the economic conditions and climate change; natural speech technology will enable much richer interactions. (4) As data access becomes more open, the volume of available audio data will increase exponentially; natural speech transcription will result in such data oceans become searchable and structured. (5) There is a potentially huge market (entertainment, consumer apps, robotics) that would be opened up by the availability of adaptive, controllable, expressive speech synthesis. (6) Clinical applications of speech technology will be substantially enriched by the personalised systems proposed in NST. As these drivers have reached a critical level, the NST team has made a number of crucial breakthroughs in adaptive speech synthesis, in conversational speech transcription and in new algorithms to robustly handle changing environments. The research potential is thus poised to meet the application drivers. Beneficiaries of the research can be found in the commercial sector (e.g., remote meeting technology; speech synthesis for computer games; speech archive search; etc), the public sector (e.g., voice reconstruction services for the National Health Service), the third sector (e.g., charities providing support for sufferers of neurodegenerative diseases), art and design, policy makers (e.g., investment in the use of spoken language technology can reduce travel and therefore carbon emissions; it can also enable people to live longer in their own homes, thus reducing the need for residential care services), and the general public (e.g., prospective voice banking and donation could become as commonplace and as widely known as blood donation). The programme's direct training and development impact will be large, through the PhD students and researchers who will work on the project and through researchers on projects that are drawn in on the side of NST; indirectly the training impact will be even larger through other students, researchers and visitors at the three universities, as well as programme workshops.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>6236104</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Ericsson</gtr:collaboratingOrganisation><gtr:country>Sweden, Kingdom of</gtr:country><gtr:description>Ericsson Robust Speech Recognition Studentship</gtr:description><gtr:id>A955E606-AF43-4855-9C0F-6AAE666F08E7</gtr:id><gtr:impact>PhD student recruited to start in 2016</gtr:impact><gtr:outcomeId>56d9a00624e1d3.71095228-1</gtr:outcomeId><gtr:partnerContribution>50% funding of PhD studentship</gtr:partnerContribution><gtr:piContribution>Research, systems, evaluation of robust speech recognition</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Sheffield</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Dysarthric speech organisation in Sheffield</gtr:description><gtr:id>BDD3FB3B-8489-48EB-B7AE-581583F99EDB</gtr:id><gtr:impact>recruitment of hS users</gtr:impact><gtr:outcomeId>56e022d79e83c4.46173121-1</gtr:outcomeId><gtr:partnerContribution>recruitment of hS users</gtr:partnerContribution><gtr:piContribution>recruitment of hS users</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>National Institute of  Informatics (NII)</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>NII</gtr:description><gtr:id>FE66EC8A-60B1-4490-B9F5-6843200D76B4</gtr:id><gtr:impact>many joint publications; collaboration on open source software (HTS); joint work on voice banking; joint position for Dr Junichi Yamagishi</gtr:impact><gtr:outcomeId>56d9b8b200fb32.96724917-1</gtr:outcomeId><gtr:partnerContribution>Joint research in speech synthesis</gtr:partnerContribution><gtr:piContribution>Joint research in speech synthesis</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>English Heritage</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>English Heritage</gtr:description><gtr:id>6D71D1EF-3B67-4C07-82E3-DA52F5F746C2</gtr:id><gtr:impact>A demonstrator for the use of the technology on a set of oral history interviews was developed (http://brodsworthhall.azurewebsites.net/)</gtr:impact><gtr:outcomeId>56e025d9cdd0a2.69233110-1</gtr:outcomeId><gtr:partnerContribution>Provided audio data and summaries from interviews nad gave feedback on their system requirements</gtr:partnerContribution><gtr:piContribution>Development of a system and a platform for information retrieval and content linking in oral archives</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Chinese Academy of Sciences</gtr:collaboratingOrganisation><gtr:country>China, People's Republic of</gtr:country><gtr:description>Pengyuang Zhang - visitor</gtr:description><gtr:id>0CF1E0C2-F386-45D2-82C1-82AF0BF4F445</gtr:id><gtr:impact>The research visit resulted in the output of two research papers:
http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7078564
http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6854663</gtr:impact><gtr:outcomeId>56e18d3acae3f0.61799867-1</gtr:outcomeId><gtr:partnerContribution>Research collaboration with the group</gtr:partnerContribution><gtr:piContribution>Hosting the visitor, and providing access to research facilities at the University of Sheffield</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>BBC Data Science Partnership</gtr:description><gtr:id>9C34816E-19C7-4E7E-8C37-120F847AFEF2</gtr:id><gtr:impact>MGB Challenge
iCASE studentships
EPSRC SCRIPT Project</gtr:impact><gtr:outcomeId>58ca55366c2b53.19946605-1</gtr:outcomeId><gtr:partnerContribution>R&amp;amp;D work from BBC researchers; data sharing.</gtr:partnerContribution><gtr:piContribution>Development of speech and language technology applied to broadcasting and media production</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Johns Hopkins University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Johns Hopkins Bloomberg School of Public Health</gtr:department><gtr:description>Bloomberg PhD Studentship</gtr:description><gtr:id>B89A95E6-F9ED-4908-A038-452564F15A7D</gtr:id><gtr:impact>PhD student commenced work in Sept 2015</gtr:impact><gtr:outcomeId>56d9a0ab770778.17053790-1</gtr:outcomeId><gtr:partnerContribution>Full funding for year 1 of a PhD studentship</gtr:partnerContribution><gtr:piContribution>Research, systems, evaluation of multi-domain speech recognition</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Medieval Settlement Research Group</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Mediaeval</gtr:description><gtr:id>EAD632FE-6E8A-42AD-B52F-56E6A57BED3E</gtr:id><gtr:impact>The transcribed data was used by participants in the evaluation campaign and features in their several publications related to the evaluation. The Mediaeval is an evaluation campaign that aims to improve the information retrieval task in media data. It's organised by Maria Eskevich at Eurecom (France).</gtr:impact><gtr:outcomeId>56e0292f34de18.43845897-1</gtr:outcomeId><gtr:partnerContribution>Provided the data</gtr:partnerContribution><gtr:piContribution>Provided automatic transcription of media data for their evaluation campaigns</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Nagoya Institute of Technology</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>NITech</gtr:description><gtr:id>7D5CBD05-6122-4F17-82A9-3BA93846EAB6</gtr:id><gtr:impact>multiple joint publications</gtr:impact><gtr:outcomeId>56d9b7c72282e0.97497256-1</gtr:outcomeId><gtr:partnerContribution>Joint research in particular focussed on HTS speech synthesis and user generated spoken dialogue systems.</gtr:partnerContribution><gtr:piContribution>Joint research in particular focussed on HTS speech synthesis and user generated spoken dialogue systems.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>MGB Challenge</gtr:description><gtr:id>E85457EE-D2A3-4C93-BA13-98A27E1CF7D2</gtr:id><gtr:impact>P Bell, MJF Gales, T Hain, J Kilgour, P Lanchantin, X Liu, A McParland, S Renals, O Saz, M Wester, and P Woodland, The MGB Challenge: Evaluating multi-genre broadcast media recognition, IEEE ASRU-2015</gtr:impact><gtr:outcomeId>56d99f50b99df8.71658749-1</gtr:outcomeId><gtr:partnerContribution>BBC provided 2000 hrs of multi-genre TV recordings, and 634M words of subtitle transcriptions</gtr:partnerContribution><gtr:piContribution>Organised international speech recognition challenge using BBC data: the MGB Challenge at ASRU-2015. We provided baseline and state-of-the-art systems, defined the challenge procedures</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Cereproc Ltd.</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Cereproc</gtr:description><gtr:id>FE9AF29E-FDD5-4229-AF68-CD01D3F6977E</gtr:id><gtr:impact>Cereproc had developed into one of the leading companies in the speech synthesis area</gtr:impact><gtr:outcomeId>56dd89e40380f7.84990337-1</gtr:outcomeId><gtr:partnerContribution>Brought a deeper understanding of commercial exploitation of speech technology</gtr:partnerContribution><gtr:piContribution>Steve Renals is non-executive director of Cereproc Ltd.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>NICT National Institute of Information and Communications Technology</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>uSTAR collaborative R&amp;amp;D project</gtr:description><gtr:id>2C49F07C-6870-4198-A7E1-0AE29DCA5057</gtr:id><gtr:impact>Building online speech recognition for an ipphone platform for speech to speech translation</gtr:impact><gtr:outcomeId>56e02734d93f40.59985730-1</gtr:outcomeId><gtr:partnerContribution>Building online speech recognition for an ipphone platform for speech to speech translation</gtr:partnerContribution><gtr:piContribution>Building online speech recognition for an ipphone platform for speech to speech translation</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>BBC</gtr:description><gtr:id>DC0AAE49-23C7-49D9-8E39-CCA06F2A33CC</gtr:id><gtr:impact>Several systems for media transcription are available in webASR now (www.webasr.org), a showcase for transcription of Youtube clips is also available (http://staffwww.dcs.shef.ac.uk/people/O.Saztorralba/youtube/)</gtr:impact><gtr:outcomeId>56e025083d6437.06456012-1</gtr:outcomeId><gtr:partnerContribution>Provided audio and video broadcast data and gave feedback on their requirements for future systems</gtr:partnerContribution><gtr:piContribution>Development of systems and showcases of the use of automatic speech processing of media archives</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Sheffield</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>The Centre for Assistive Technology and Connected Healthcare (CATCH)</gtr:description><gtr:id>C59D3F29-3BB8-4B79-8793-80C4F8C41538</gtr:id><gtr:impact>unknown</gtr:impact><gtr:outcomeId>56e02453209504.67509332-1</gtr:outcomeId><gtr:partnerContribution>sharing of knowledge and resources</gtr:partnerContribution><gtr:piContribution>sharing of knowledge and resources</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Barnsley Hospital NHS Foundation Trust</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Barnsley Hospital NHS Foundation Trust</gtr:description><gtr:id>A872AB82-54E5-4A39-8B95-1987888608E8</gtr:id><gtr:impact>recruitment of homeService users</gtr:impact><gtr:outcomeId>56e0226e80faf6.63113314-1</gtr:outcomeId><gtr:partnerContribution>recruitment of homeService users</gtr:partnerContribution><gtr:piContribution>recruitment of homeService users</gtr:piContribution><gtr:sector>Hospitals</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>South China University of Technology</gtr:collaboratingOrganisation><gtr:country>China, People's Republic of</gtr:country><gtr:department>School of Environment and Energy</gtr:department><gtr:description>Visitor Dr Yan-Xiong Li</gtr:description><gtr:id>C498E27D-414B-4FD4-97A3-8C65677361D6</gtr:id><gtr:impact>Annotation of BBC and RT'07 data.</gtr:impact><gtr:outcomeId>58c9d851bec267.97966800-1</gtr:outcomeId><gtr:partnerContribution>Joint annotation and testing experiments on BBC data and AMI meeting data. Public Release of new data.</gtr:partnerContribution><gtr:piContribution>Collaboration on diarisation work on meeting and broadcast media data.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Zaragoza</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:description>Julia Olcoz, visiting researcher</gtr:description><gtr:id>C39FF6B2-5CC9-49D2-997D-A83686B6391B</gtr:id><gtr:impact>The enhanced system for lightly supervised alignment is available in webASR. A paper detailing the system was submitted to Interspeech 2016.</gtr:impact><gtr:outcomeId>56e0287b5d5e15.39142099-1</gtr:outcomeId><gtr:partnerContribution>Developed novel techniques for improving the lightly supervised alignment task</gtr:partnerContribution><gtr:piContribution>Provided a baseline system for the task of lightly supervised alignment in media broadcasts</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>COST APPELE meeting</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5C57FD22-1C71-4CB7-8DF0-636730EE4921</gtr:id><gtr:impact>Engaging academics and industry across Europe</gtr:impact><gtr:outcomeId>56e03a6a68e1a4.81387529</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://aapele.eu/%7Clink</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Multi-Genre Broadcast (MGB) Challenge</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5C7A128E-BDE8-4711-A2B9-0F5A3217BCE8</gtr:id><gtr:impact>Organised and participated the Multi-Genre Broadcast (MGB) challenge. The challenge took place at ASRU 2015 and it served as a meeting for the participants. 20+ research groups participated in the challenge.</gtr:impact><gtr:outcomeId>56e03cdd3695e2.40422943</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.mgb-challenge.org/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Talk at AIST Tsukuba</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>AC1B614D-0468-4168-925C-2BA978B39DC3</gtr:id><gtr:impact>Talk at AIST, Tsukuba, Japan - &amp;quot;Improving speech transcription using out-of-domain data&amp;quot;</gtr:impact><gtr:outcomeId>56dfd177480819.68459903</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The future of Languages - more than just words</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>0440D04E-4A43-4AAE-AE38-48C61B56C7DB</gtr:id><gtr:impact>A public lecture at the Public Library in Amsterdam, followed by a debate with an audience.

Interactions with the audience.</gtr:impact><gtr:outcomeId>545f6dbf8a2f28.04664057</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.clubofamsterdam.com/event.asp?contentid=854</gtr:url><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Using speech synthesis to give everyone their own voice</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>B290B24B-7076-4EB0-98CD-983323BAA061</gtr:id><gtr:impact>Discussions with the audience afterwards.

Follow up emails from members of the public.</gtr:impact><gtr:outcomeId>545f6e1f1599e3.58754132</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>A talk about the homeService experience/project</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>622FEE40-DEFF-411A-871E-66B6E0F34150</gtr:id><gtr:impact>A talk about the homeService experience/project to Birmingham University (Feb 2016</gtr:impact><gtr:outcomeId>56e0391a1e6318.08190766</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Data Science for Media Summit</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>390FE86B-A990-420D-996A-DFBB7FCA47CC</gtr:id><gtr:impact>The Summit was organised by the Alan Turing Institute to bring together researchers and media specialists to discuss in the future directions of research in data science for media</gtr:impact><gtr:outcomeId>56e0383d835295.42015058</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Mobile University outreach event 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>016D0073-7903-46D0-AB41-3A085B6B8BAF</gtr:id><gtr:impact>Our research on speech recognition and machine translation was presented as part of a 'Mobile University' outreach to general public in Sheffield City Centre.</gtr:impact><gtr:outcomeId>56e03bc1aa1e20.76173043</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://mini.dcs.shef.ac.uk/mobileuni2015/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>seminar at INESC 2012</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D8BA4887-AED9-433D-B1E5-F497B715511A</gtr:id><gtr:impact>Talk entitled &amp;quot;Assistive Speech Technology&amp;quot; at INESC-ID, Lisbon</gtr:impact><gtr:outcomeId>56dfd0f3d79263.13136500</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>A talk about the homeService experience/project</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>B1D0F717-1D40-49FF-8A8A-BD3D6A38765C</gtr:id><gtr:impact>A talk about the homeService experience/project to Birmingham University (Feb 2016</gtr:impact><gtr:outcomeId>56e0391cb27684.75167671</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>How technology is changing speech and language therapy (Guardian on-line)</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F1C7C8B1-08DD-4715-BEB6-5CF605779037</gtr:id><gtr:impact>Article in Guardian which includes information about progress in speech recognition technology and specific mention of the EPSRC Natural Speech Technology project.</gtr:impact><gtr:outcomeId>56df52f8dbbb09.93043019</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.theguardian.com/higher-education-network/2015/apr/15/how-technology-is-changing-speech-and-language-therapy</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>CATCH kickoff event</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>3908AF3E-A488-4698-B039-064EBF84BF69</gtr:id><gtr:impact>Introduction of the homeService project to interest audience</gtr:impact><gtr:outcomeId>56e03b014dadb0.45515507</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>A talk about the homeService experience/project to Medical Humanity Sheffield</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>DD6F69E9-E84F-4DF3-856F-F0C318D985CD</gtr:id><gtr:impact>A talk about the homeService experience/project to Medical Humanity Sheffield</gtr:impact><gtr:outcomeId>56e039523926a7.79599216</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>73726</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Response to Tender (1)</gtr:description><gtr:end>2013-04-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>A2E2DC83-906E-4979-B11D-E11710352A42</gtr:id><gtr:outcomeId>56e02ec4ddf320.91697307</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2012-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>5000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Innovation Seed Funding</gtr:description><gtr:end>2015-08-02</gtr:end><gtr:fundingOrg>University of Sheffield</gtr:fundingOrg><gtr:id>E2BF3ACF-A794-40D8-B09F-E675E7A5D8A5</gtr:id><gtr:outcomeId>56e02b580eeb79.52106741</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>15000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>IIKE Early Career Research Scheme</gtr:description><gtr:end>2016-02-02</gtr:end><gtr:fundingOrg>University of Sheffield</gtr:fundingOrg><gtr:id>8D40F2B0-DD31-4ABF-A74F-6FC48D628B6F</gtr:id><gtr:outcomeId>56e030f5d09e84.46627029</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>37716</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Impact Acceleration Award</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>B08F0699-4C43-4048-B7F7-2E9DD7960437</gtr:id><gtr:outcomeId>56d98ecaaf9011.87461902</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1100000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>European Union Seventh Framework Programme</gtr:description><gtr:end>2015-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>287658</gtr:fundingRef><gtr:id>9B4F6389-96AF-4AE5-8FF3-EB38E6886A4C</gtr:id><gtr:outcomeId>56decac2af13b2.06434401</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2012-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>533268</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Responsive Mode</gtr:description><gtr:end>2019-11-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/P011586/1</gtr:fundingRef><gtr:id>FF425BF4-1BD7-42E0-8118-180E2FAA1D29</gtr:id><gtr:outcomeId>58c9451ac4bd48.52787584</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>540000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>EU FP7-ICT-2011-1.5</gtr:description><gtr:end>2014-10-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>287872</gtr:fundingRef><gtr:id>BF9E7757-8FAF-4F6F-BB6B-CD406FF9E0F4</gtr:id><gtr:outcomeId>56decb40375367.56086453</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>98982</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Response to Tender (2)</gtr:description><gtr:end>2014-04-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>4F1FC985-6D6A-4AE4-AEE5-542902F1264F</gtr:id><gtr:outcomeId>56e02efb219f92.82756306</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>125000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Leverhulme International Network</gtr:description><gtr:end>2018-12-02</gtr:end><gtr:fundingOrg>The Leverhulme Trust</gtr:fundingOrg><gtr:id>81952E41-D1B9-492E-A48F-12BF7B1A6BB6</gtr:id><gtr:outcomeId>56e0319c6ee210.46034502</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>42677</gtr:amountPounds><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Bloomberg PhD Studenship</gtr:description><gtr:end>2015-12-02</gtr:end><gtr:fundingOrg>Bloomberg</gtr:fundingOrg><gtr:id>76D37880-1D88-4D96-BCF9-CDAE841545FF</gtr:id><gtr:outcomeId>56deab2faddd70.88480586</gtr:outcomeId><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1100000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>European Community's Seventh Framework Programme (FP7/2007-2013)</gtr:description><gtr:end>2014-10-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>287678</gtr:fundingRef><gtr:id>863A83CF-FF4F-4E85-BF63-8FAFCA2BEA25</gtr:id><gtr:outcomeId>56decbb15e3d44.61245636</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>78684</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Response to Tender (3)</gtr:description><gtr:end>2016-08-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>47AD162C-F3E6-4DD2-B281-D1B43ABF0445</gtr:id><gtr:outcomeId>56e02f2cc14965.90525305</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1999113</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:department>Horizon 2020</gtr:department><gtr:description>EU H2020 ICT Programme</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>688139</gtr:fundingRef><gtr:id>570AA023-2AE6-455C-AB8C-4B6A3C3EC224</gtr:id><gtr:outcomeId>56d98f8b49f001.28181739</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>520000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>EU FP7-ICT-2013-10</gtr:description><gtr:end>2015-11-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>611092</gtr:fundingRef><gtr:id>3096959F-D5E2-4B20-B093-04D01D9D154B</gtr:id><gtr:outcomeId>56deaacab12723.25052316</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-12-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>1. Contributions to widely-used open source software including HTK, Kaldi, HTS, CUED RNN Toolkit, and Merlin
2. Personalised speech synthesis used for voice banking and reconstruction and deployed in assistive technology communication aids
3. Transcription of multi-genre broadcast speech, used commercially at BBC and Ericsson
4. Formation of a spinout company Quorate Techniology
5. Deployment of an application with English Heritage for Browsing Oral Histories</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>F14D6A1A-6E18-4244-9C1B-3615A0F2662E</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5458b83a5d1028.73988048</gtr:outcomeId><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare,Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>High-quality speech synthesis software based on speech technologies developed during my fellowship.</gtr:description><gtr:grantRef>EP/I031022/1</gtr:grantRef><gtr:id>0A80BD78-E802-4170-A34B-7E17BE255BD9</gtr:id><gtr:impact>I have formally licensed the high-quality speech synthesizer to two companies for a commercial basis.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56a1b7e529fb14.92564874</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>High-quality speech synthesizer, HTS voice</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Combilex-ASR is a large scale lexicon for speech recognition in British English. It is licensed under a Creative Commons BY-NC license</gtr:description><gtr:grantRef>EP/I031022/1</gtr:grantRef><gtr:id>D05EAE16-82E9-4522-8F4E-FB0692A07AC5</gtr:id><gtr:impact>This lexicon underpinned the ASRU-2015 MGB Challenge, and is planned to be used in the US IARPA Babel Programme</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56ded0e16d9bc4.29834208</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Combilex-ASR</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>The aim of the project is to significantly advance the state-of-the-art in speech technology by the recognition and synthesis of natural speech, approaching human levels of flexibility, reliability, and fluency.

We have made advances in several areas.

1. Learning and adaptation. We have developed new approaches to learning representations for speech and language based on deep neural networks and recurrent neural networks. In contrast to previous approaches, these new approaches require less feature engineering and human design. These approaches have been applied to both speech recognition and speech synthesis. We have also developed new approaches for the adaptation of systems to a new voice, given just a few seconds of speech. We have also developed new factorised modelling approaches which, for example, enable us to separately model the effects of the talker as distinct to the effects of the recording channel.

2. Speech transcription. We have developed several new acoustic modelling techniques: for example, new techniques can model phonetic context in a more efficient way, and a new approach to recognising speech captured using multiple microphones. We have also developed more accurate language models, based on recurrent neural networks, and have introduced a new algorithm to automatically learn a pronunciation lexicon.

3. Speech synthesis. We have introduced new models for synthesising speech based on multiple average voices, and using prior information automatically extracted from talker characteristics. We have developed a new approach ro characterise the perceptual effects of modelling assumptions in speech synthesis through perceptual experiments using stimuli constructed from repeated natural speech. We have developed new techniques for synthesis of conversational speech, for example through automatic pause insertion.

4. Applications. This work has been applied in a number of areas.
 a) transcription of broadcast speech for subtitling, metadata extraction, and archive search. This is in collaboration with user group partners BBC and Red Bee Media
 b) adaptive speech recognition and dialogue management for users with speech disorders, which is currently undergoing trials in users' homes
 c) voice banking and cloning, to create personalised voice output communication aids for people with diseases such as Motor Neurone Disease and Parkinson's Disease. This is also undergoing trials with users.</gtr:description><gtr:exploitationPathways>Already our findings are having considerable impact. In particular we have released many of the findings made in the project through open source toolkits (Kaldi, HTK, HTS and Festival) which has resulted in significant take-up. Several of our techniques for speech recognition and speech synthesis are being further developed by other groups. 

Our techniques rebelling put to use by several members of the project user group including the BBC and Ericsson (broadcast speech transcription); the Euan MacDonald Centre for Motor Neurone Disease Research, and the Motor Neurone Disease Association (voice banking); Quorate Technology (audio search and browsing); Toshiba (speech synthesis); Airbus (speech recognition).</gtr:exploitationPathways><gtr:id>B276246A-A4F6-4D77-AA07-805F92820D5A</gtr:id><gtr:outcomeId>5458b8f7939801.57781662</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://www.natural-speech-technology.org</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>ROCKIT/CITIA Roadmap</gtr:description><gtr:geographicReach>Europe</gtr:geographicReach><gtr:id>9EE048B8-1E3F-4ED0-A5CF-BD4AFCC8143C</gtr:id><gtr:impact>The ROCKIT/CITIA strategic roadmap for conversational interaction technologies forms the basis of a research and innovation agenda in the area of conversational interaction technologies. 2014 and 2015 we constructed this technology roadmap to enable the conversational interaction technologies vision to be realised. The roadmapping process was carried out at the European level, connecting the strong R&amp;amp;D base, with commercial and industrial activity, and policy makers, at the EU and national levels.</gtr:impact><gtr:outcomeId>56dece2b08b935.35315108</gtr:outcomeId><gtr:type>Citation in other policy documents</gtr:type><gtr:url>http://www.sharpcloud.com/ROCKIT</gtr:url></gtr:policyInfluenceOutput><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>CITIA</gtr:description><gtr:geographicReach>Europe</gtr:geographicReach><gtr:id>96E5AE11-A0B6-4FD0-AA65-3218A227574A</gtr:id><gtr:impact>Steve Renals is founding chairperson of the EU Conversational Interaction Technologies Innovation Alliance, a group which has advised the EU on policy relating speech technology to the multilingual digital single market</gtr:impact><gtr:outcomeId>56d9bedd03e8c5.04903341</gtr:outcomeId><gtr:type>Participation in a advisory committee</gtr:type><gtr:url>http://citia.eu</gtr:url></gtr:policyInfluenceOutput></gtr:policyInfluenceOutputs><gtr:productOutputs><gtr:productOutput><gtr:description>Adaptive speech synthesis may be be used to develop personalised synthetic voices for people who have a vocal pathology. In 2009 Dr. Sarah Creer from University of Sheffield and I have successfully applied it to clinical voice banking for laryngectomees (individuals who have had their vocal cords removed due to a developing cancer) to reconstruct their voices. In 2010, I have &amp;quot;implanted&amp;quot; the personalised synthetic voice of a patient who has motor neurone disease into their assistive communication device. Such a personalised voice can lead to far more natural communication for patients, particularly with family. A &amp;quot;voice reconstruction&amp;quot; trial has been tested with about 100 patients in total at the Euan MacDonald Centre for MND Research and the Anne Rowling Regenerative Neurology Clinic in Edinburgh.</gtr:description><gtr:id>C2F4C20D-F9AE-45DC-892D-414BA655D676</gtr:id><gtr:impact>We have recorded about 100 MND patients at the Euan MacDonald Centre for MND Research and the Anne Rowling Regenerative Neurology Clinic in Edinburgh and have constructed personalized speech synthesizers based on their disordered voices. We have received and analyzed feedback from the patients and we have confirmed that this new speech synthesis technology can improve their quality-of-life.</gtr:impact><gtr:outcomeId>5464c03ec5ad59.48662773</gtr:outcomeId><gtr:stage>Initial development</gtr:stage><gtr:status>Actively seeking support</gtr:status><gtr:title>Clinical trial of personalized speech synthesis voices for MND patients</gtr:title><gtr:type>Health and Social Care Services</gtr:type><gtr:yearDevCompleted>2015</gtr:yearDevCompleted></gtr:productOutput></gtr:productOutputs><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset is associated with the paper &amp;quot;'SAS: A speaker verification spoofing database containing diverse attacks': presents the first version of a speaker verification spoofing and anti-spoofing database, named SAS corpus. The corpus includes nine spoofing techniques, two of which are speech synthesis, and seven are voice conversion. We design two protocols, one for standard speaker verification evaluation, and the other for producing spoofing materials. Hence, they allow the speech synthesis community to produce spoofing materials incrementally without knowledge of speaker verification spoofing and anti-spoofing. To provide a set of preliminary results, we conducted speaker verification experiments using two state-of-the-art systems. Without any anti-spoofing techniques, the two systems are extremely vulnerable to the spoofing attacks implemented in our SAS corpus&amp;quot;.</gtr:description><gtr:id>09E013CE-CB34-47C7-8201-E586013FAC41</gtr:id><gtr:impact>This SAS database is the first version of a standard dataset for spoofing and anti-spoofing research. Currently, the SAS corpus includes speech generated using nine spoofing methods, each of which comprises around 300000 spoofed trials. To the best of our knowledge, this is the first attempt to include such a diverse range of spoofing attacks in a single database. The SAS corpus is publicly available at no cost.</gtr:impact><gtr:outcomeId>56a1a35e1164e6.36580517</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Spoofing and Anti-Spoofing (SAS) corpus v1.0</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>This CSTR VCTK Corpus includes speech data uttered by 109 native speakers of English with various accents. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald &amp;amp; Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximise the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf

All speech data was recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, were downsampled to 48 kHz based on STPK, and were manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies.</gtr:description><gtr:id>43D18EBF-7942-4AC7-96A2-57BD83ECCAC2</gtr:id><gtr:impact>This is the first free corpus that is designed and appropriate for speaker-adaptive speech synthesis. This starts to become a standard database to build and compare speaker-adaptive speech synthesis systems and voice conversion systems. This was also used even for speaker verification systems.</gtr:impact><gtr:outcomeId>5464c1d0125495.31351301</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>CSTR VCTK Corpus -- Multi-speaker English Corpus for CSTR Voice Cloning Toolkit</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>an audio corpus of spontaneous dysarthric speech</gtr:description><gtr:id>8EBE604C-B024-4216-AFE7-6E451EF7E7F4</gtr:id><gtr:impact>first example of semi-spontaneuos dysarthric speech corpus</gtr:impact><gtr:outcomeId>56e0464314b1c0.08364045</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>the homeService corpus</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://mini.dsc.shef.ac.uk/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The database has been used in the first Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2015). Genuine speech is collected from 106 speakers (45 male, 61 female) and with no signi?cant channel or background noise effects. Spoofed speech is generated from the genuine data using a number of different spoo?ng algorithms. The full dataset is partitioned into three subsets, the ?rst for training, the second for development and the third for evaluation. More details can be found in the evaluation plan in the summary paper.</gtr:description><gtr:id>3243492B-21DD-4751-ACEF-51DF82BBE48C</gtr:id><gtr:impact>Automatic speaker verification (ASV) offers a low-cost and flexible biometric solution to person authentication. While the reliability of ASV systems is now considered sufficient to support mass-market adoption, there are concerns that the technology is vulnerable to spoofing, also referred to as presentation attacks. Spoofing refers to an attack whereby a fraudster attempts to manipulate a biometric system by masquerading as another, enrolled person. Acknowledged vulnerabilities include attacks through impersonation, replay, speech synthesis and voice conversion.

This database has been used for the 2015 ASVspoof challenge, which aims to encourage further progress through (i) the collection and distribution of a standard dataset with varying spoofing attacks implemented with multiple, diverse algorithms and (ii) a series of competitive evaluations. The first ASVspoof challenge was held during the 2015 edition of INTERSPEECH in Dresden, Germany. The challenge has been designed to support, for the first time, independent assessments of vulnerabilities to spoofing and of countermeasure performance and to facilitate the comparison of different spoofing countermeasures on a common dataset, with standard protocols and metrics.</gtr:impact><gtr:outcomeId>56a1a4cb263644.04033581</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2015) Database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The Voice Conversion Challenge (VCC) 2016, one of the special sessions at Interspeech 2016, deals with speaker identity conversion, referred as Voice Conversion (VC). The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. The objective of the VCC was to compare various VC techniques on identical training and evaluation speech data. The samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This dataset consists of the participants' VC submissions and the listening test results for naturalness and similarity.</gtr:description><gtr:id>1F847DE2-722D-4B47-B152-9BE34628DB47</gtr:id><gtr:impact>17 groups working in VC around the world have used this database and have developed their own VC systems.</gtr:impact><gtr:outcomeId>58c9580a48b9c3.44032406</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>The Voice Conversion Challenge 2016 database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://datashare.is.ed.ac.uk/handle/10283/2211</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Further recordings of two days, playing the game Warhammer, yielding a total of 20 hours of transcribed speech.</gtr:description><gtr:id>EA0ED31A-456A-4F2F-8418-F8F614802B02</gtr:id><gtr:impact>No direct impact has been recorded, however the Kaldi team has proposed to refine the system scripts included in the corpus.</gtr:impact><gtr:outcomeId>58c9e2901165c9.56345236</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Wargames Day 2 and 3</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://mini.dcs.shef.ac.uk/resources/sheffield-wargames-corpus/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The MGB database is the official database of the MGB challenge. It contains 2,000 hours of audio, 700 million words of transcripts plus other metadata.</gtr:description><gtr:id>B489F2E8-B478-45B8-B3C6-751F4CB14F35</gtr:id><gtr:impact>Features in the MGB challenge and the accompanying workshop at ASRU 2015.</gtr:impact><gtr:outcomeId>56e046de2f4611.73080192</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>MGB database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The MGB Challenge data was released to support the MGB Challenge evaluation of multi-genre broadcast speech recognition systems. It consists of approximately 1,600 hours of broadcast audio taken from seven weeks of BBC output across all TV channels, captions as originally broadcast on TV, accompanied by baseline lightly-supervised alignments using an ASR system, with confidence measures, several hundred million words of subtitle text from BBC TV output collected over a 15 year period, and a hand-compiled British English lexicon derived from Complex.</gtr:description><gtr:id>D1CED0DC-CFE1-4613-B2F7-E7FC10063049</gtr:id><gtr:impact>This research database supported the MGB Challenge at the IEEE ASRU-2015 workshop</gtr:impact><gtr:outcomeId>56ded01316a6b4.00637320</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>MGB Challenge</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://mgb-challenge.org</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Studio recording of female native British English talker producing three sets of Harvard sentences (thirty prompts), each prompt repeated forty times. Available both as unprocessed 96 kHz recordings and standardised 16 kHz files.</gtr:description><gtr:id>98880DD1-BA59-4728-9DF0-9AE42FC7506B</gtr:id><gtr:impact>The following paper has been published: G. E. Henter, T. Merritt, M. Shannon, C. Mayo, and S. King, &amp;quot;Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech,&amp;quot; in Proc. Interspeech, 2014</gtr:impact><gtr:outcomeId>56debecebdb2b2.64528084</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>REHASP</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Recordings of groups of people playing the Warhammer hgame, recorded in 96 audio channels and 3 media streams, fully transcribed.</gtr:description><gtr:id>3B4B1B20-E055-4FA6-874C-B464A5E9FA13</gtr:id><gtr:impact>It was discussed at ASRU 2016 as a potential candidate for future tasks. University of Sheffield are recording part II and part III which will be made available shortly, for this objective.</gtr:impact><gtr:outcomeId>56e17708cab916.13517925</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Wargames I</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>Speech recognition software, based the open source Kaldi toolkit, was released to enable the construction of lightly supervised multi-genre broadcast speech recognition systems.</gtr:description><gtr:id>1B1B16DF-ECAD-438F-886C-34FBFAF5F8F6</gtr:id><gtr:impact>These systems provided the baselines for the 2015 MGB Challenge</gtr:impact><gtr:outcomeId>56decf1f0db5e4.94278339</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>MGB Challenge Speech Recognition Systems</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>http://mgb-challenge.org</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>HTS is an open source toolkit for statistical speech synthesis. I am a member of a team developing the the free open-source research software packages for speech synthesis.</gtr:description><gtr:id>4B6B3E06-5E58-4656-847C-25707134794E</gtr:id><gtr:impact>The HTS toolkit is used worldwide by both academic and commercial organisations, such as Microsoft, Nuance, Toshiba, Pentax, and Google. The number of downloads of HTS exceeds 10,000 and various commercial products using HTS are on the market. Therefore, this toolkit is a very influential platform for me to disseminate outcomes and form an immediate pathway to impact.</gtr:impact><gtr:outcomeId>56a1babc0dd3a5.64447489</gtr:outcomeId><gtr:title>HTS ver 2.3</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://hts.sp.nitech.ac.jp</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Software to train recurrent neural network language models (RNNLMs) for speech recognition and other applications. The software features efficient training (on a GPU) and efficient evaluation (on a CPU). It includes a patch to HTK 3.4.1 that enables the application of RNNLMs to HTK-based speech recognition lattices. RNNLMs can be trained with additional features in the input layer for better performance (e.g. topic adaptation using an LDA-based topic vector).</gtr:description><gtr:id>13B04219-003D-47D1-913D-2E51D894A4FB</gtr:id><gtr:impact>This software was used to train RNNLMs that were used in the Cambridge University transcription systems used in the 2015 ASRU multi-genre broadcast challenge (international challenge involving the automatic transcription of BBC broadcast audio). The Cambridge system gave the lowest error rates in the challenge and the use of RNNLMs efficiently trained on a large corpus of subtitle material was a key component, as well as the use of topic adaptation.</gtr:impact><gtr:outcomeId>56df4c4dd94b36.63873534</gtr:outcomeId><gtr:title>CUED-RNNLM Toolkit</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://mi.eng.cam.ac.uk/projects/cued-rnnlm/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The homeService protocol to develop and human-machine interaction for users with speech and mobility impairment. Example of virtuous cycle.</gtr:description><gtr:id>6620F81F-6ED5-45FC-ADBA-682B69E9FF5B</gtr:id><gtr:impact>Not at this point.</gtr:impact><gtr:outcomeId>56e1759678d892.31657985</gtr:outcomeId><gtr:title>homeService protocol</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Merlin is the Neural Network (NN) based Speech Synthesis System developed at the Centre for Speech Technology Research (CSTR), University of Edinburgh</gtr:description><gtr:id>59F46233-A074-41B7-8DDF-814F4F0FE874</gtr:id><gtr:impact>Since its release at the end of the Natural Speech Technology project, Merlin has established a significant base of users and developers.</gtr:impact><gtr:outcomeId>58c9443253b145.10037334</gtr:outcomeId><gtr:title>Merlin</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/CSTR-Edinburgh/merlin</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Publicly available webtool (www.webasr.org) and two showcases on media transcription (http://staffwww.dcs.shef.ac.uk/people/O.Saztorralba/youtube/) and alignment of lecture subtitles (http://staffwww.dcs.shef.ac.uk/people/O.Saztorralba/ted/)</gtr:description><gtr:id>2310F9BF-FC61-434F-91DE-7A563EB7716D</gtr:id><gtr:impact>New version of webASR (www.webasr.org) with new systems and demonstrators</gtr:impact><gtr:outcomeId>56e0477e6908f6.93810950</gtr:outcomeId><gtr:title>webASR</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>http://www.webasr.org/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Implementation of new diarisation methods as published in ICASSP 2016</gtr:description><gtr:id>C04DAC27-302E-4688-B4D6-54F6900CB0F8</gtr:id><gtr:impact>Not yet.</gtr:impact><gtr:outcomeId>56e1765c2d5a82.55070915</gtr:outcomeId><gtr:title>Diarisation scoring tools</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Implementation of the alignment scoring rules tha the University of Sheffield has defined as part of organising the MGB Challenge</gtr:description><gtr:id>B66C15C1-A224-42E4-AD85-E355C0B6A35A</gtr:id><gtr:impact>Used in the MGB Challenge and used already for several publications by other research groups.</gtr:impact><gtr:outcomeId>56e1760bdd1b59.68871430</gtr:outcomeId><gtr:title>Alignment task and scoring software</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>HTK is a portable toolkit for building and manipulating hidden Markov models which has been developed over many years primarily at Cambridge University Engineering Dept.. HTK is primarily used for speech recognition research although it is also widely used for speech synthesis and other applications. 
HTK 3.5 adds built-in support for artificial neural network (ANN) models while maintaining compatibility with most existing functions (including hybrid and tandem models, sequence training and CPU/GPU math kernels), as well as support for decoding RNN language models
HTK is supplied in source form with a specific licence that allows any use of the models produced but does not allow software re-distribution.
HTK has over 100,000 registered users.</gtr:description><gtr:id>88D199DD-4129-483F-9F31-A3880251EC26</gtr:id><gtr:impact>HTK 3.5 has been used as a platform to develop various types of speech technology research at Cambridge, building on developments over many years to focus on the use of deep neural network acoustic models and recurrent neural network language models.

A particular outcome has been the development of the Cambridge University systems for the 2015 ARSU multi-genre broadcast (MGB) challenge. This required the processing of more than 1600 hours of BBC TV audio data and developing systems for transcription, subtitle alignment and diarisation. This was embodied in 4 tasks in the MGB challenge, and Cambridge University systems based on HTK 3.5 had the best performance for all these tasks.

Many HTK users have downloaded HTK 3.5 and are actively using it to develop both research and commercial systems.</gtr:impact><gtr:outcomeId>56dec3436406a3.21270169</gtr:outcomeId><gtr:title>HTK 3.5</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://htk.eng.cam.ac.uk/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Festival offers a general framework for building speech synthesis systems as well as including examples of various modules. As a whole it offers full text to speech through a number APIs: from shell level, though a Scheme command interpreter, as a C++ library, from Java, and an Emacs interface. Festival is multi-lingual (currently English (British and American), and Spanish) though English is the most advanced. Other groups release new languages for the system. And full tools and documentation for build new voices are available through Carnegie Mellon's FestVox project (http://festvox.org).

The software was first released in the 1990s, but has been under continuous development, improvement, and maintenance since then. v2.1 q
was released in November 2010.</gtr:description><gtr:id>C260EA11-6CE5-4938-B434-EC49C7777F71</gtr:id><gtr:impact>Festival is distributed as default in a number of standard Linux distributions including Arch Linux, Fedora, CentOS, RHEL, Scientific Linux, Debian, Ubuntu, openSUSE, Mandriva, Mageia and Slackware, and can easily be installed on any Linux distribution that supports apt-get. More recently our work on statistical parametric speech synthesis and the algorithms for adaptation have been incorporated in the HTS toolkit (one of the coordinators (Yamagishi) is from Edinburgh), which integrates with Festival. These toolkits are the most used open-source speech synthesis systems and have also formed the high performing baseline systems for the international Blizzard evaluation of (commercial and research) speech synthesis also organised by Edinburgh.</gtr:impact><gtr:outcomeId>5463d734a11660.77968175</gtr:outcomeId><gtr:title>The Festival Speech Synthesis System</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.cstr.ed.ac.uk/projects/festival/</gtr:url></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The de facto industry standard toolkit for developing text-to-speech systems.</gtr:description><gtr:id>1EFC2A98-F02C-46C6-BCFE-4E43C4103025</gtr:id><gtr:impact>Commercial products from AT&amp;amp;T.
Spinout company Rhetorical Systems.</gtr:impact><gtr:outcomeId>56dd8742b28040.68655141</gtr:outcomeId><gtr:title>The Festival Speech Synthesis system - version 2.4</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.cstr.ed.ac.uk/projects/festival/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>Quorate Technology</gtr:companyName><gtr:description>Quorate Technology provides Quorate, a Speech Recognition and Analysis Suite that unlocks information in recordings. The company's technology uses speech recognition for 

 - Search: Recorded speech is made searchable and transcripts can be generated automatically. Quorate enables search terms to be located in both the recording and in a computer-generated transcript.

 - Browsing: Quorate generates keyword summaries of recordings. These keywords represent the terms that best characterise a single recording in the context of a group of recordings.

 - Analysis: Recorded speech can be examined from many perspectives, via the extraction of rich metadata in order to enable recordings to be segmented, filtered and connected to related materials.</gtr:description><gtr:id>45E56212-4F4A-44FD-9538-DE2F329D9939</gtr:id><gtr:impact>The company has a variety of commercial contracts.There are at least 10 full-time scientific positions.</gtr:impact><gtr:outcomeId>5463c80f13f8c6.58320584</gtr:outcomeId><gtr:url>http://quoratetechnology.com</gtr:url><gtr:yearCompanyFormed>2012</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>AA884B3A-F343-46A1-A64B-1BF82524915B</gtr:id><gtr:title>Spoofing and countermeasures for speaker verification: A survey</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b909c7c9009.29285108</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C09A0C9-39A4-40B0-A819-60FE44AE0CA2</gtr:id><gtr:title>Human vs Machine Spoofing Detection on Wideband and Narrowband Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb9c6953df517c636c9268be995b5039"><gtr:id>cb9c6953df517c636c9268be995b5039</gtr:id><gtr:otherNames>Wester, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de011f8686d1.19568996</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6207E07B-0E08-41CA-AAAD-B33372B05896</gtr:id><gtr:title>Punctuated transcription of multi-genre broadcasts using acoustic and lexical approaches</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c0420bc4c20d1ba65e0dc4356a46361a"><gtr:id>c0420bc4c20d1ba65e0dc4356a46361a</gtr:id><gtr:otherNames>Klejch O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c92b9f681c17.84773126</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3CF3CD83-42BC-48A4-BF2F-D2619CEB454B</gtr:id><gtr:title>Building personalised synthetic voices for individuals with severe speech impairment</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd3b1d1091b2b0dd95c42a5e9b15a9c7"><gtr:id>dd3b1d1091b2b0dd95c42a5e9b15a9c7</gtr:id><gtr:otherNames>Creer S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7cc5b1766.09655731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF76C4F7-5249-4446-818C-9D19FC3A0C9A</gtr:id><gtr:title>Where are the challenges in speaker diarization?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/501e3f69b93ed09e222e638f41daa313"><gtr:id>501e3f69b93ed09e222e638f41daa313</gtr:id><gtr:otherNames>Sinclair M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dd75f0608894.08191986</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D99C65F0-728F-4705-AF21-D99226ABF2F2</gtr:id><gtr:title>The MGB-2 challenge: Arabic multi-dialect broadcast media recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6f392e2e926f042fbe3cfa981e4fed71"><gtr:id>6f392e2e926f042fbe3cfa981e4fed71</gtr:id><gtr:otherNames>Ali A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c92b9f954b69.61623826</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>28B31409-33FB-4827-B993-29E7B96BF60E</gtr:id><gtr:title>Speech-Enabled Environmental Control in an AAL setting for people with Speech Disorders: a Case Study</gtr:title><gtr:parentPublicationTitle>IET International Conference on Technologies for Active and Assisted Living, TechAAL 2015</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b846cb109.19707375</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DCCE437C-0217-4A88-97E2-F4394C63C247</gtr:id><gtr:title>The effect of filled pauses and speaking rate on speech comprehension in natural, vocoded and synthetic speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c350efb7793c2fef1f2211d9f1fef0a"><gtr:id>8c350efb7793c2fef1f2211d9f1fef0a</gtr:id><gtr:otherNames>Dall, R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56ddf8a283b728.72763267</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E7AD5463-E1CB-41B7-9EBD-5DA19D0C25FE</gtr:id><gtr:title>A system for automatic alignment of broadcast media captions using weighted finite-state transducers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d457dc00d57.86003513</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCBD5A47-1CD0-4C95-9D77-69C835A390AF</gtr:id><gtr:title>Fast, low-artifact speech synthesis considering global variance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/34713d0179abf3b064fb50fa3794c7c5"><gtr:id>34713d0179abf3b064fb50fa3794c7c5</gtr:id><gtr:otherNames>Shannon M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521aee4d3080.55687910</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4FD4436A-9289-4C58-908F-15249834251C</gtr:id><gtr:title>Small-Footprint Highway Deep Neural Networks for Speech Recognition</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fec23e92686.94629702</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19E75F7D-9EDD-4E27-9577-2DC1FCC99ECB</gtr:id><gtr:title>A comparative study of adaptive, automatic recognition of disordered speech</gtr:title><gtr:parentPublicationTitle>Proc Interspeech 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56e14b84ad7280.10370342</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ABCFC941-3E4F-4A64-8D6E-A996C3AB613C</gtr:id><gtr:title>A General Artificial Neural Network Extension for HTK</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26279d0dd6e455efef4955d84e6f9437"><gtr:id>26279d0dd6e455efef4955d84e6f9437</gtr:id><gtr:otherNames>Zhang, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de017bc57125.91790426</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E218BB9C-8BDF-4E93-9F43-188AE850E661</gtr:id><gtr:title>Improved DNN-based segmentation for multi-genre broadcast audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4fb28c7b7f759ed5da54d4f587d639a"><gtr:id>c4fb28c7b7f759ed5da54d4f587d639a</gtr:id><gtr:otherNames>Wang L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15206149</gtr:issn><gtr:outcomeId>58bc00f9ca08f1.79160561</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D28D6B91-5CDF-462F-A5BD-04B46887D592</gtr:id><gtr:title>A Lattice-based Approach to Automatic Filled Pause Insertion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4a03d750e8f13bedc8a1f37e1b9f192c"><gtr:id>4a03d750e8f13bedc8a1f37e1b9f192c</gtr:id><gtr:otherNames>Tomalin, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de0059c5b883.48297561</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EB45DCA0-F1FD-4886-95C9-2BDFA4E053D1</gtr:id><gtr:title>A framework for collecting realistic recordings of dysarthric speech - the homeService corpus</gtr:title><gtr:parentPublicationTitle>The International Conference on Language Resources and Evaluation - LREC 2016</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/30b71fb32f143da5f0014d0ef27dc886"><gtr:id>30b71fb32f143da5f0014d0ef27dc886</gtr:id><gtr:otherNames>Nicolao M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e14b829533c7.49348228</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98727FCA-B1CE-433A-9602-2C239BEA1297</gtr:id><gtr:title>Multiple-average-voice-based speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6afb3b8f4605c0aaef1c394307ee1181"><gtr:id>6afb3b8f4605c0aaef1c394307ee1181</gtr:id><gtr:otherNames>Lanchantin P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460df1d39a377.45333729</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19A6F7CF-20A1-4039-B194-7F64C4DC27C5</gtr:id><gtr:title>Speech-Enabled Environmental Control in an AAL setting for people with Speech Disorders: a Case Study</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f639d33eb8393676b3e93970523f9f6"><gtr:id>9f639d33eb8393676b3e93970523f9f6</gtr:id><gtr:otherNames>Christensen, H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfe8b83e589.88940245</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EDF4A5C3-81B1-4C45-9C06-FA533624E4E4</gtr:id><gtr:title>Standalone training of context-dependent deep neural network acoustic models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f2d31a05ff149de838f7af04b5a46315"><gtr:id>f2d31a05ff149de838f7af04b5a46315</gtr:id><gtr:otherNames>Zhang C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e10888b854.07734286</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F259AD1-9DB6-40F6-80CD-AF6D5B6A7D2F</gtr:id><gtr:title>The 2015 Sheffield System for Longitudinal Diarisation of Broadcast Media</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/259975ab4407bac1ea513db0c2164f97"><gtr:id>259975ab4407bac1ea513db0c2164f97</gtr:id><gtr:otherNames>Milner R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b850183c1.42880373</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA509D94-99FB-49BF-8960-AC3C245BAA4E</gtr:id><gtr:title>Cross-Lingual Automatic Speech Recognition Using Tandem Features</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/20bfa1cd748439a1fec236e721fd03bf"><gtr:id>20bfa1cd748439a1fec236e721fd03bf</gtr:id><gtr:otherNames>Lal P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dd75da9aafa1.19242274</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2649DF79-4A6D-4489-BF9F-A4668183E7FE</gtr:id><gtr:title>Investigating automatic &amp;amp; human filled pause insertion for speech synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/245d46ebe80e0ec94d7ad5ed4cdb0cee"><gtr:id>245d46ebe80e0ec94d7ad5ed4cdb0cee</gtr:id><gtr:otherNames>Dall R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75e06843e6.98579891</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>47FF9BCB-5BCD-4EFF-B362-BC955F9A23B4</gtr:id><gtr:title>Combining i-vector representation and structured neural networks for rapid adaptation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1a86fb804dd161bd520ef0e0c1b13874"><gtr:id>1a86fb804dd161bd520ef0e0c1b13874</gtr:id><gtr:otherNames>Wu C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15206149</gtr:issn><gtr:outcomeId>58bc00fad56557.22710186</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>51D1E334-FE40-43D2-B8E1-752D721535FB</gtr:id><gtr:title>Artificial Personality and Disfluency</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb9c6953df517c636c9268be995b5039"><gtr:id>cb9c6953df517c636c9268be995b5039</gtr:id><gtr:otherNames>Wester, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de0577053e74.26856260</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6038AA67-4BFA-4F5C-8D62-BB0F11B12769</gtr:id><gtr:title>Paraphrastic neural network language models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42f7c7c6107a6cd7198876022cb295cf"><gtr:id>42f7c7c6107a6cd7198876022cb295cf</gtr:id><gtr:otherNames>Liu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e05f516135.15517179</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>754E465B-AD64-4318-9442-F2EA1F4CA076</gtr:id><gtr:title>Adaptive speech recognition and dialogue management for users with speech disorders</gtr:title><gtr:parentPublicationTitle>Proceedings of Interspeech\textquoteright14</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd326b47bb73265a538a9963cbbef731"><gtr:id>bd326b47bb73265a538a9963cbbef731</gtr:id><gtr:otherNames>Casanueva I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e14b8544a324.20250210</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0B600300-5392-42EE-8147-70F5905B0743</gtr:id><gtr:title>Analysis of Speaker Clustering Strategies for HMM-Based Speech Synthesis</gtr:title><gtr:parentPublicationTitle>Proc. Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/469f245a27e7589f1fcf74ccf97a21b5"><gtr:id>469f245a27e7589f1fcf74ccf97a21b5</gtr:id><gtr:otherNames>Dall, R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5433b1acd57201.75988142</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>02D33753-4B12-4365-BA7E-48DEF8EA777A</gtr:id><gtr:title>Studio report: Linux audio for multi-speaker natural speech technology.</gtr:title><gtr:parentPublicationTitle>Proc. Linux Audio Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5cf06d39af5e60cd7619f648bf365985"><gtr:id>5cf06d39af5e60cd7619f648bf365985</gtr:id><gtr:otherNames>Fox C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56e14b82dda282.93436217</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C3545D3-5EDE-4A9B-9C11-A3A6E852CA6F</gtr:id><gtr:title>The Sheffield Wargames Corpus</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2013 14thAnnual Conference of the International Speech Communication Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0169cce3e5844db6b70f22954121df48"><gtr:id>0169cce3e5844db6b70f22954121df48</gtr:id><gtr:otherNames>Fox, c</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545219b94ae630.34728101</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A3DA617B-86A3-4F18-B15E-969D049A5A7B</gtr:id><gtr:title>Data-selective Transfer Learning for Multi-Domain Speech Recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of the 16th Annual Conference of the International Speech Communication Association (Interspeech)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e6e2dbf7ea6284d8c683a75956e1eb4"><gtr:id>2e6e2dbf7ea6284d8c683a75956e1eb4</gtr:id><gtr:otherNames>Doulaty M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b861482d2.88328311</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>318B5BDC-C61C-4E9D-A5DB-C505693A6635</gtr:id><gtr:title>Towards minimum perceptual error training for DNN-based speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfd62b62c73.61847250</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A1D426F7-E32F-47A6-AF00-A2EA5BF618A6</gtr:id><gtr:title>A Lattice-based Approach to Automatic Filled Pause Insertion</gtr:title><gtr:parentPublicationTitle>Proc. DiSS 2015</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/785f496adb4bd5c0c2de62131e503611"><gtr:id>785f496adb4bd5c0c2de62131e503611</gtr:id><gtr:otherNames>Tomalin M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd75d5424e68.74633629</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D8F1F0E-3422-4587-90A0-6D188F214FCA</gtr:id><gtr:title>Speech synthesis technologies for individuals with vocal disabilities: Voice banking and reconstruction</gtr:title><gtr:parentPublicationTitle>Acoustical Science and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/183dc4a8fc3549b73816452269943b6c"><gtr:id>183dc4a8fc3549b73816452269943b6c</gtr:id><gtr:otherNames>Yamagishi J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f93d93de23e603</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98F90BAB-63FB-42D6-B152-592C4E631CE3</gtr:id><gtr:title>Combining a Vector Space Representation of Linguistic Context with a Deep Neural Network for Text-To-Speech Synthesis</gtr:title><gtr:parentPublicationTitle>-</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/beca939df8c5c79a71194d2ca0bd3067"><gtr:id>beca939df8c5c79a71194d2ca0bd3067</gtr:id><gtr:otherNames>Lu, H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433b31ac93376.44228257</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2445A26C-A563-4DE3-B2DC-45A5EE5377CC</gtr:id><gtr:title>Improving intelligibility in noise of HMM-generated speech via noise-dependent and -independent methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c9ddd7f6.96142116</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6F647FFD-94C2-46C4-8A37-8EEB2FE5EE3C</gtr:id><gtr:title>Revisiting semi-continuous hidden Markov models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c96ef49dec1a0d2c50aff8e0838de53c"><gtr:id>c96ef49dec1a0d2c50aff8e0838de53c</gtr:id><gtr:otherNames>Riedhammer K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5432c39c840e76.36711731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>36B7C41D-F0F6-4BBA-8832-8B49D346285C</gtr:id><gtr:title>Lightly supervised discriminative training of grapheme models for improved sentence-level alignment of speech and text data</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/705dac958004c99e3d563f633ca3d557"><gtr:id>705dac958004c99e3d563f633ca3d557</gtr:id><gtr:otherNames>Stan A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>5464b7c99204f4.18601557</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A680D00E-E1C1-4707-A7BC-92202DB2E4F7</gtr:id><gtr:title>Combining in-domain and out-of-domain speech data for automatic recognition of disordered speech</gtr:title><gtr:parentPublicationTitle>Proc. Interspeech 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8a2547bd30ecf71647e6850002f3c647"><gtr:id>8a2547bd30ecf71647e6850002f3c647</gtr:id><gtr:otherNames>Christensen, H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433aafe5c7486.00625445</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1122B473-E53D-4C2D-94FC-2128EC083399</gtr:id><gtr:title>Asynchronous Factorisation of Speaker and Background with Feature Transforms in Speech Recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of the 14th Annual Conference of the International Speech Communication Association (Interspeech)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b82579111.35670850</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4383D484-3DA8-42A9-A3A5-C51A4B9B9101</gtr:id><gtr:title>Probabilistic Linear Discriminant Analysis with Bottleneck Features for Speech Recognition</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d98885331934.60697762</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>79410354-1D62-4C16-976D-32089D793421</gtr:id><gtr:title>Evaluation of objective measures for intelligibility prediction of HMM-based synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0538-0</gtr:isbn><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7cc35ba29.12840075</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8FF3B94E-D566-4648-9FFC-F82431BF2DFB</gtr:id><gtr:title>Combining Vocal Tract Length Normalization With Hierarchical Linear Transformations</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3e397272cbec8689ecfec75bccb6ef6"><gtr:id>e3e397272cbec8689ecfec75bccb6ef6</gtr:id><gtr:otherNames>Saheer L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464b6b0e41a14.26155321</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7B2DE551-F015-44B1-ADC6-50D0513FC466</gtr:id><gtr:title>Studio report: Linux audio for multi-speaker natural speech technology.</gtr:title><gtr:parentPublicationTitle>Proc. Linux Audio Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5cf06d39af5e60cd7619f648bf365985"><gtr:id>5cf06d39af5e60cd7619f648bf365985</gtr:id><gtr:otherNames>Fox C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56e14b842a4bd9.05929815</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E4B79E98-B2B6-40F5-8FFE-F6941DAB7A82</gtr:id><gtr:title>homeService: Voice-enabled assistive technology in the home using cloud-based automatic speech recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4152d5ef61608f3ef9a7bff35382f698"><gtr:id>4152d5ef61608f3ef9a7bff35382f698</gtr:id><gtr:otherNames>Christensen, Heidi</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545214071710e0.16620435</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>330E98AE-9C7A-4D6B-B677-119DC024572C</gtr:id><gtr:title>Synthetic speech discrimination using pitch pattern statistics derived from image analysis</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b5dc02c54a5a954ea2a4f8e2a92c8f50"><gtr:id>b5dc02c54a5a954ea2a4f8e2a92c8f50</gtr:id><gtr:otherNames>De Leon P.L.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7c72505b1.31461981</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F5B88447-A70A-4CCA-B2AA-A6B53DA4EEB6</gtr:id><gtr:title>Articulatory Control of HMM-Based Parametric Speech Synthesis Using Feature-Space-Switched Multiple Regression</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4d35805892fea2bd41853d9893397711"><gtr:id>4d35805892fea2bd41853d9893397711</gtr:id><gtr:otherNames>Zhen-Hua Ling</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7cc841c95.90092937</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5B01ABE-9463-490E-BA3E-2DB4D2DD97D7</gtr:id><gtr:title>A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c954772c71650d9033c7fcc9d1eb3cd6"><gtr:id>c954772c71650d9033c7fcc9d1eb3cd6</gtr:id><gtr:otherNames>Takaki S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4586724733.03085005</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>36775AEF-F400-497F-BA39-A67F4DD5E66F</gtr:id><gtr:title>Towards glottal source controllability in expressive speech synthesis</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b81b4b94988f9858b8d33c947e428577"><gtr:id>b81b4b94988f9858b8d33c947e428577</gtr:id><gtr:otherNames>Lorenzo-Trueba J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7c67ec163.19146382</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E4068313-2AE4-4F4F-9FE3-EC747001D14F</gtr:id><gtr:title>Structured Output Layer with Auxiliary Targets for Context-Dependent Acoustic Modelling</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98883adcb94.26252374</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D84CF1FC-60B9-4595-AF7F-9FCFF9BBA136</gtr:id><gtr:title>Intelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the glimpse proportion</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f973973fcdb184</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB92FE40-BA65-4B36-8202-C0F22D0F6E5E</gtr:id><gtr:title>Multilingual training of deep neural networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aeae1a330fa5fd6027c90e2092ebd72c"><gtr:id>aeae1a330fa5fd6027c90e2092ebd72c</gtr:id><gtr:otherNames>Ghoshal A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39c082906.27148870</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>625822BF-F6F3-466E-815C-577789B2C15D</gtr:id><gtr:title>Background-Tracking Acoustic Features for Genre Identification of Broadcast Shows</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2014 Spoken Language Technology (SLT) Workshop</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e14b8213f559.57223568</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B096A8FE-5BBD-44AD-B6E8-F051AF6BD423</gtr:id><gtr:title>Neural networks for distant speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2b77685afb59131cef21f12162b6fb96"><gtr:id>2b77685afb59131cef21f12162b6fb96</gtr:id><gtr:otherNames>Renals S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5a748c3dc3ccd0.08444433</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B938F185-4725-4DDD-B1F2-B129E1E99E1C</gtr:id><gtr:title>End-to-End Neural Segmental Models for Speech Recognition</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cf3b9ff4977ea091de42d5947ac1f124"><gtr:id>cf3b9ff4977ea091de42d5947ac1f124</gtr:id><gtr:otherNames>Tang H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a748d405cd053.53852830</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A522713-520D-462A-BBBE-2B83DEDE9C6D</gtr:id><gtr:title>Using Contextual Information in Joint Factor Eigenspace MLLR for Speech Recognition in Diverse Scenarios</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2014 International Conference on Acoustic, Speech and Signal Processing (ICASSP)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e14b82308db4.47698735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2AD34B1-52C4-4269-A6E7-672FD8CE160F</gtr:id><gtr:title>Noise-matched training of CRF based sentence end detection models</gtr:title><gtr:parentPublicationTitle>Interspeech 2015</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f9beb056e827346ba733f931e9727176"><gtr:id>f9beb056e827346ba733f931e9727176</gtr:id><gtr:otherNames>Hasan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b85cb4929.33355498</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6BF48AB-39AB-4E5D-BAA4-C6023B19E4BE</gtr:id><gtr:title>Efficient Training and Evaluation of Recurrent Neural Network Language Models for Automatic Speech Recognition</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7771ec39b9bd63d704f50fabca5f24ae"><gtr:id>7771ec39b9bd63d704f50fabca5f24ae</gtr:id><gtr:otherNames>Chen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>23299290</gtr:issn><gtr:outcomeId>58bc00fb64ca19.62499760</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CB0DE53E-C19B-430B-AD8D-9A09EF396246</gtr:id><gtr:title>Deep neural network-guided unit selection synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd983924b6a8f9f4e82a419ca228d8af"><gtr:id>dd983924b6a8f9f4e82a419ca228d8af</gtr:id><gtr:otherNames>Merritt T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4587832182.15482439</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C9115BB-7D78-4709-9B25-449CC1A2F7EA</gtr:id><gtr:title>Efficient GPU-based training of recurrent neural network language models using spliced sentence bunch</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a00c0dd6c0a176e32a737014e762a1de"><gtr:id>a00c0dd6c0a176e32a737014e762a1de</gtr:id><gtr:otherNames>Chen, X.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dde1718341b8.67857320</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E74B899-CD33-4ECB-AA44-7EFCF0845F5C</gtr:id><gtr:title>Analysis of speaker clustering strategies for HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e49f2bafd907f5c670d3407eae6733e"><gtr:id>3e49f2bafd907f5c670d3407eae6733e</gtr:id><gtr:otherNames>Dall R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7ccd01431.36116826</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D25CC43F-24FC-4BA7-8886-B143562DAA82</gtr:id><gtr:title>Speaker diarisation and longitudinal linking in multi-genre broadcast data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70934333059a54e373a24b5e539ba009"><gtr:id>70934333059a54e373a24b5e539ba009</gtr:id><gtr:otherNames>Karanasou P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58bc00fa5d3d68.35803827</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52825C3D-FB9D-4DBF-B832-3FB1A1D1761B</gtr:id><gtr:title>Using neighbourhood density and selective SNR boosting to increase the intelligibility of synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>568b9f107a6b17.69332253</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8CE9540E-EE66-4088-9457-EBFB95E3D44C</gtr:id><gtr:title>SAS: A speaker verification spoofing database containing diverse attacks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfba8de6916.73704370</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B7186404-BEA7-4F92-922D-C63A1F8A6009</gtr:id><gtr:title>Investigating gated recurrent networks for speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4582b9f668.69822633</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EC40F3C9-D220-484D-AA53-E500A53EEFF6</gtr:id><gtr:title>I-Vector Estimation Using Informative Priors for Adaptation of Deep Neural Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ec1b9fcd08a79c17da2373d99469510"><gtr:id>4ec1b9fcd08a79c17da2373d99469510</gtr:id><gtr:otherNames>Karanasou, P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de00bf15dee0.04958610</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1CD8AC42-43BB-4E56-89CD-563B598D32E8</gtr:id><gtr:title>Revisiting hybrid and GMM-HMM system combination techniques</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39d76cde7.42873061</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A1958537-1F4B-4DB9-B9FA-FE68A79632BE</gtr:id><gtr:title>Transcription of multi-genre media archives using out-of-domain data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-5125-6</gtr:isbn><gtr:outcomeId>5432c39c45bb35.84454726</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2F8F1BCB-EFA6-449B-A262-57A0535E588A</gtr:id><gtr:title>Combining perceptually-motivated spectral shaping with loudness and duration modification for intelligibility enhancement of HMM-based synthetic speech in noise</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ee279d6b0887d0abf52ac6218b3af7a"><gtr:id>4ee279d6b0887d0abf52ac6218b3af7a</gtr:id><gtr:otherNames>Valentini-Botinhao C.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>5464b7cbdcff06.58014252</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D8B096F-4C3A-4594-9D2B-02FBA08DEDE0</gtr:id><gtr:title>homeService: Voice-enabled assistive technology in the home using cloud-based automatic speech recognition</gtr:title><gtr:parentPublicationTitle>4th Workshop on Speech and Language Processing (SLPAT)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b844d4c03.44851330</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BBF2531-A382-446A-A5B5-F4623D7E9488</gtr:id><gtr:title>Learning Speaker-Specific Pronunciations of Disordered Speech</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2013 14thAnnual Conference of the International Speech Communication Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8a2547bd30ecf71647e6850002f3c647"><gtr:id>8a2547bd30ecf71647e6850002f3c647</gtr:id><gtr:otherNames>Christensen, H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521c1b41d8b7.50551508</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>979BB2F5-2F58-4A37-A2DB-7ADCF14B8BD8</gtr:id><gtr:title>Improving the training and evaluation efficiency of recurrent neural network language models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a00c0dd6c0a176e32a737014e762a1de"><gtr:id>a00c0dd6c0a176e32a737014e762a1de</gtr:id><gtr:otherNames>Chen, X.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dde0831f5f91.49563444</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1CE61C5D-16B0-4736-A2DB-FD23D8983AFD</gtr:id><gtr:title>Generating exact lattices in the WFST framework</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/322e41386d79b15bb11a87686a15278a"><gtr:id>322e41386d79b15bb11a87686a15278a</gtr:id><gtr:otherNames>Povey D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5433af033a5856.40491723</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0513DE87-B67C-4EFB-9042-9A05CB9053E7</gtr:id><gtr:title>Speech and Audio Signal Processing - Processing and Perception of Speech and Music</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f16485f74a22f7473f68a18aaa808f46"><gtr:id>f16485f74a22f7473f68a18aaa808f46</gtr:id><gtr:otherNames>Gold B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9780470195369</gtr:isbn><gtr:outcomeId>54520c87c0ea63.37078157</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5EE68E40-C7CD-4BB1-9A07-2A9217B630B8</gtr:id><gtr:title>Using Bayesian Networks to find relevant context features for HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication Association, Portland, Oregon, USA, September 9-13, 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95b8687dee7e37ed876c00e270da3178"><gtr:id>95b8687dee7e37ed876c00e270da3178</gtr:id><gtr:otherNames>Lu, L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5432c5661a17c5.05876765</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9006399C-F8D6-4C75-89CC-53EF23E77038</gtr:id><gtr:title>SPECS - an embedded platform, speech-driven environmental control system evaluated in a virtuous circle framework</gtr:title><gtr:parentPublicationTitle>Proc. Workshop on Innovation and Applications in Speech Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8a2547bd30ecf71647e6850002f3c647"><gtr:id>8a2547bd30ecf71647e6850002f3c647</gtr:id><gtr:otherNames>Christensen, H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521e5c223932.91527898</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F76A28FF-E375-413F-A3A8-449C7BE924CE</gtr:id><gtr:title>Symbolic Modeling of Prosody: From Linguistics to Statistics</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4b4cee7199e409d9b4916b0a96e673b"><gtr:id>c4b4cee7199e409d9b4916b0a96e673b</gtr:id><gtr:otherNames>Obin N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfae0ce8ba2.13139398</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9B3EE79-798A-4FAB-AFD4-FC0C7F1DCB75</gtr:id><gtr:title>Deep neural network context embeddings for model selection in rich-context HMM synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/74515d13ab6d63d6ed201508f8d83810"><gtr:id>74515d13ab6d63d6ed201508f8d83810</gtr:id><gtr:otherNames>Merritt, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de041a21d1d1.43634952</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3E28C70-4238-447D-8D5F-215B8AEAD09E</gtr:id><gtr:title>Regularization of context-dependent deep neural networks with context-independent multi-task training</gtr:title><gtr:parentPublicationTitle>Proc IEEE ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d988818eed55.86429733</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AED31EE0-5F11-4E1E-AD81-46407CA86ABE</gtr:id><gtr:title>Soft context clustering for F0 modeling in HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Advances in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4628b39c4e80c5ab10ce51c826bbe0c3"><gtr:id>4628b39c4e80c5ab10ce51c826bbe0c3</gtr:id><gtr:otherNames>Khorram S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f125761db</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>31A881C4-72DA-4C13-BBBB-CFAB0BC454BD</gtr:id><gtr:title>Dysarthria Intelligibility Assessment in a Factor Analysis Total Variability Space</gtr:title><gtr:parentPublicationTitle>Interspeech\textquoteright13</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57a54cc2dd9dea0144426fe0074963fb"><gtr:id>57a54cc2dd9dea0144426fe0074963fb</gtr:id><gtr:otherNames>Gonz{\'a}lez D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b83a6e813.34978795</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>581457F7-1AD7-4F51-BDE4-BF7AC3C6CC1E</gtr:id><gtr:title>Lightly supervised learning from a damaged natural speech corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5cf06d39af5e60cd7619f648bf365985"><gtr:id>5cf06d39af5e60cd7619f648bf365985</gtr:id><gtr:otherNames>Fox C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545215e22ef4d1.87022229</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AB8812E0-C4FD-416D-AD0E-1E684955958A</gtr:id><gtr:title>An Investigation Into Speaker Informed DNN Front-end for LVCSR</gtr:title><gtr:parentPublicationTitle>Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7c92a675b6502a97a707b04cca69bc1"><gtr:id>d7c92a675b6502a97a707b04cca69bc1</gtr:id><gtr:otherNames>Liu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b83d5a820.14622072</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A262DEE9-8A0F-4F75-A6EE-373878FA287B</gtr:id><gtr:title>A Log Domain Pulse Model for Parametric Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da5972f22b22f57237729b429aaacfa3"><gtr:id>da5972f22b22f57237729b429aaacfa3</gtr:id><gtr:otherNames>Degottex G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a352f7de97516.54666700</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>22E97C1F-1943-4B23-ADE2-28E1D0D57D23</gtr:id><gtr:title>The development of the cambridge university alignment systems for the multi-genre broadcast challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6afb3b8f4605c0aaef1c394307ee1181"><gtr:id>6afb3b8f4605c0aaef1c394307ee1181</gtr:id><gtr:otherNames>Lanchantin P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58bc00f9911945.12736094</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF51EF1D-4FDC-4191-ACF6-17EDED33E915</gtr:id><gtr:title>On the evaluation of inversion mapping performance in the acoustic domain</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e45bf486b896a69a7ae0bb1941313ec"><gtr:id>2e45bf486b896a69a7ae0bb1941313ec</gtr:id><gtr:otherNames>Richmond K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>5464b7c89c96d6.50670609</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AEDFAFBF-03D8-4130-9E2E-53327A918267</gtr:id><gtr:title>Extending Limabeam with discrimination and coarse gradients</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2014, 15th Annual Conference of the International Speech
Communication Association, Singapore, September 14-18, 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5cf06d39af5e60cd7619f648bf365985"><gtr:id>5cf06d39af5e60cd7619f648bf365985</gtr:id><gtr:otherNames>Fox C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e14b838871f4.98049503</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AD977320-ACA2-4FA7-8188-8FBED33116D6</gtr:id><gtr:title>Direct posterior confidence for out-of-vocabulary spoken term detection</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Information Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a0385ddf822030de0fee889762bbc89e"><gtr:id>a0385ddf822030de0fee889762bbc89e</gtr:id><gtr:otherNames>Wang D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56dd75dbe109a0.97733366</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25BC7481-FC4A-4219-98BD-4BADD983CBB5</gtr:id><gtr:title>Background-tracking acoustic features for genre identification of broadcast shows</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56ddfa5530fd18.44853712</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>229D963B-685C-4581-9636-D7BDBBB82EE8</gtr:id><gtr:title>The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52ee38903760a55eef4d00cf5dd63dfa"><gtr:id>52ee38903760a55eef4d00cf5dd63dfa</gtr:id><gtr:otherNames>Veaux C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dd75ec721469.53703980</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CF3552D4-BC04-4E5B-A19F-30A31CAFA5E0</gtr:id><gtr:title>Two Efficient Lattice Rescoring Methods Using Recurrent Neural Network Language Models</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42f7c7c6107a6cd7198876022cb295cf"><gtr:id>42f7c7c6107a6cd7198876022cb295cf</gtr:id><gtr:otherNames>Liu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>23299290</gtr:issn><gtr:outcomeId>58bc00faac5bd3.83416935</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7067AF38-23C2-43A6-A3D5-9CC1E7F525A5</gtr:id><gtr:title>Just-in-time prepared captioning for live transmissions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/32afddb9dc5a184032aa466f0ad03f7e"><gtr:id>32afddb9dc5a184032aa466f0ad03f7e</gtr:id><gtr:otherNames>Simpson M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:isbn>978-1-78561-343-2</gtr:isbn><gtr:outcomeId>585d458b4917d8.95536387</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B8154F8-432D-4C2D-9A67-EFACAF0E0B15</gtr:id><gtr:title>A perceptually-motivated low-complexity instantaneous linear channel normalization technique applied to speaker verification</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/233c3270fe46dcd8fa1119eec666edfb"><gtr:id>233c3270fe46dcd8fa1119eec666edfb</gtr:id><gtr:otherNames>Poblete V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>doi_55f973973fe0e9d8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3367E8B3-7DAD-4363-ADEF-EC050178B9D2</gtr:id><gtr:title>Grapheme and multilingual posterior features for under-resourced speech recognition: A study on Scottish Gaelic</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1bc75d6195d2952ebce553ee133e0021"><gtr:id>1bc75d6195d2952ebce553ee133e0021</gtr:id><gtr:otherNames>Rasipuram R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39e7e36b4.32182333</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60B20777-2569-4B71-A889-FD9F12A28C81</gtr:id><gtr:title>Term-Dependent Confidence Normalisation for Out-of-Vocabulary Spoken Term Detection</gtr:title><gtr:parentPublicationTitle>Journal of Computer Science and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a0385ddf822030de0fee889762bbc89e"><gtr:id>a0385ddf822030de0fee889762bbc89e</gtr:id><gtr:otherNames>Wang D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56dd75eb38f107.49219467</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60BB014C-1AF2-457A-ACAE-CB6F51731AE7</gtr:id><gtr:title>Using Adaptation to Improve Speech Transcription Alignment in Noisy and Reverberant Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bbbb69cd6372e90627cc5992199f7fb4"><gtr:id>bbbb69cd6372e90627cc5992199f7fb4</gtr:id><gtr:otherNames>Mamiya, Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>568ba07fab5da5.91167032</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AB7F466D-1D9A-4ED4-8028-DC26767F11D0</gtr:id><gtr:title>Synthesis and evaluation of conversational characteristics in HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f92a30bde5a5d98e4692a604a0fc36a7"><gtr:id>f92a30bde5a5d98e4692a604a0fc36a7</gtr:id><gtr:otherNames>Andersson S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7c74f6234.00147986</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EB828C51-F289-462D-8D20-76BFF6BBC14D</gtr:id><gtr:title>The MGB Challenge: Evaluating Multi-genre Broadcast Media Recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b827969b3.54721558</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>868D2CB8-954D-4D90-9CC7-066EFED775B5</gtr:id><gtr:title>Regularized subspace Gaussian mixture models for cross-lingual speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4673-0365-1</gtr:isbn><gtr:outcomeId>5432c39b1763c6.72450493</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7340F0E5-04AD-4E1A-9F1B-CCF3C0140467</gtr:id><gtr:title>Statistical parametric speech synthesis for Ibibio</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c042418a3abfeb2c177f01b85261705c"><gtr:id>c042418a3abfeb2c177f01b85261705c</gtr:id><gtr:otherNames>Ekpenyong M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75e99d1084.09483758</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E33A0D19-CA2D-4E07-BDCD-265E41A50720</gtr:id><gtr:title>Minimum trajectory error training for deep neural networks, combined with stacked bottleneck features</gtr:title><gtr:parentPublicationTitle>Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd75e2ee9dd1.15876621</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>230F7B30-F98F-49D3-B571-8FD6E67C013D</gtr:id><gtr:title>Simplified learning with binary orthogonal constraints</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5df6878d7c6b2adb436dfac36c2d2735"><gtr:id>5df6878d7c6b2adb436dfac36c2d2735</gtr:id><gtr:otherNames>Huang Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d45840a1134.68932342</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1EC58B04-398D-4951-9345-AB6BB7DC4D8F</gtr:id><gtr:title>Multi-reference WER for evaluating ASR for languages with no orthographic rule</gtr:title><gtr:parentPublicationTitle>Proc IEEE ASRU</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6f392e2e926f042fbe3cfa981e4fed71"><gtr:id>6f392e2e926f042fbe3cfa981e4fed71</gtr:id><gtr:otherNames>Ali A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9888388d4c2.51855092</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0E8FC7B-0B2D-4334-B7F2-A01638A2D9B7</gtr:id><gtr:title>Sheffield LRE 2015 System Description</gtr:title><gtr:parentPublicationTitle>Odyssey: The Speaker and Language Recognition Workshop (Submitted)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38fb02d73316da34a672dead25c8e8f"><gtr:id>b38fb02d73316da34a672dead25c8e8f</gtr:id><gtr:otherNames>Ng R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e14b8130e6f4.16861278</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1BA66ABD-3929-44C8-9C43-DA9DEB621084</gtr:id><gtr:title>Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d400c06e597.20730255</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>66ADCA57-73DE-42A8-8BF5-9D63B5EE17EA</gtr:id><gtr:title>Dysarthria Intelligibility Assessment in a Factor Analysis Total Variability Space</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2013 14thAnnual Conference of the International Speech Communication Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/37cc04f991b532d3afe96bb4bb94e367"><gtr:id>37cc04f991b532d3afe96bb4bb94e367</gtr:id><gtr:otherNames>Martinez, David</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521b91b35686.14127535</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3AC7DF2E-F839-4067-A74B-7C82E9AE4D66</gtr:id><gtr:title>Asynchronous Factorisation of Speaker and Background with Feature Transforms in Speech Recognition</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2013 14thAnnual Conference of the International Speech Communication Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da081988cc070af06bcfcbeb4317dc6d"><gtr:id>da081988cc070af06bcfcbeb4317dc6d</gtr:id><gtr:otherNames>Saz, Oscar</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521db82d42f3.77651891</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A12EFFC8-97FE-482B-AE9C-851156FC1090</gtr:id><gtr:title>Advanced speech synthesis technologies for vocal disabilities</gtr:title><gtr:parentPublicationTitle>Journal of Information Processing and Management</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd45d575d507c22ea3facdcde57b3066"><gtr:id>cd45d575d507c22ea3facdcde57b3066</gtr:id><gtr:otherNames>YAMAGISHI J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f13c85254</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26952DA8-5120-453A-890C-F87D50EB4217</gtr:id><gtr:title>Lightly supervised GMM VAD to use audiobook for speech synthesiser</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9724130ba82aa3a7bfba4d8bb066ff6b"><gtr:id>9724130ba82aa3a7bfba4d8bb066ff6b</gtr:id><gtr:otherNames>Mamiya Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c96d2ee2.82489300</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8123A70E-D475-4ADA-90DC-B485F210703D</gtr:id><gtr:title>A Deep Generative Architecture for Postfiltering in Statistical Parametric Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fbfd9fb6e692d07596464ca94848bf67"><gtr:id>fbfd9fb6e692d07596464ca94848bf67</gtr:id><gtr:otherNames>Chen L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f12707fee</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5595CB3A-7E73-4F8B-BBDA-0BA355DE9F46</gtr:id><gtr:title>Using HMM-based Speech Synthesis to Reconstruct the Voice of Individuals with Degenerative Speech Disorders</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e95ade432385b6636659af37ed310795"><gtr:id>e95ade432385b6636659af37ed310795</gtr:id><gtr:otherNames>Veaux, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5699075fd6ac92.29965181</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E258AC23-CEE7-4271-9653-C270EEF9C2AD</gtr:id><gtr:title>Using Eigenvoices and Nearest-Neighbors in HMM-Based Cross-Lingual Speaker Adaptation With Limited Data</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1badb2a6df454e0c170bdbec8a770ec2"><gtr:id>1badb2a6df454e0c170bdbec8a770ec2</gtr:id><gtr:otherNames>Sarfjoo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a748d02e65a10.62270485</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B6E6ECA4-98FB-4BC2-B3A5-37E7D630A815</gtr:id><gtr:title>Using Bayesian networks to find relevant context features for HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e96e84f01a13b8c22d773b0b45d3878"><gtr:id>6e96e84f01a13b8c22d773b0b45d3878</gtr:id><gtr:otherNames>Lu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56dd75ef57d635.93146181</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B8B5CE11-D74F-4075-A2E5-FDD4B363F911</gtr:id><gtr:title>A Study of the Recurrent Neural Network Encoder-Decoder for Large Vocabulary Speech Recognition</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98882e1e9c6.89033746</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5CA900C9-58D6-44DB-A450-646CB8567C45</gtr:id><gtr:title>Reactive accent interpolation through an interactive map application</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/54a64c0cfdd9a3b2ef7c9ea39a74ba39"><gtr:id>54a64c0cfdd9a3b2ef7c9ea39a74ba39</gtr:id><gtr:otherNames>Astrinaki M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>5464b7c82b4ba2.49202888</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94456B0F-28E1-4F61-9373-E54924E02520</gtr:id><gtr:title>SPECS - an embedded platform, speech-driven environmental control system evaluated in a virtuous circle framework</gtr:title><gtr:parentPublicationTitle>Proc. Workshop on Innovation and Applications in Speech Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56e14b84d00405.05914351</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA841F9E-33B4-445D-973A-9482293D4A79</gtr:id><gtr:title>Using contextual information in joint factor eigenspace MLLR for speech recognition in diverse scenarios</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e14a4a32f8.25211787</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D41A254D-4257-403F-890B-2F5AE311769D</gtr:id><gtr:title>Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE</gtr:title><gtr:parentPublicationTitle>Proc IEEE ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4a4875add61e3bbec05c3d65c7d45b7b"><gtr:id>4a4875add61e3bbec05c3d65c7d45b7b</gtr:id><gtr:otherNames>Ur\'ia B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98881b09963.54420834</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CF7F72AE-AEB7-4445-A9E0-8DF6E51DC271</gtr:id><gtr:title>Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d43bb1d947be3c941da4dafe0c7504cd"><gtr:id>d43bb1d947be3c941da4dafe0c7504cd</gtr:id><gtr:otherNames>Kamper H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75eeebcbc3.08004792</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>336C723A-F772-46CB-8C73-50BEE55BBF73</gtr:id><gtr:title>Attributing modelling errors in HMM synthesis by stepping gradually from natural to modelled speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd983924b6a8f9f4e82a419ca228d8af"><gtr:id>dd983924b6a8f9f4e82a419ca228d8af</gtr:id><gtr:otherNames>Merritt T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddf8e55825b9.92656924</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E4E3C8FA-78E1-401E-A588-F1FBC50C0099</gtr:id><gtr:title>Multi-level adaptive networks in tandem and hybrid ASR systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39b50c413.25583744</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>14920434-67B5-4496-8DDA-D2147FE6FA5C</gtr:id><gtr:title>Formant-controlled HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a4fed7bcb649bcbb30e92f767023ce10"><gtr:id>a4fed7bcb649bcbb30e92f767023ce10</gtr:id><gtr:otherNames>Lei M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7cabb9282.50099431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CCF97EAB-7B86-4967-A376-41578E17B65C</gtr:id><gtr:title>A comparison of open-source segmentation architectures for dealing with imperfect data from the media in speech synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/82a0f573acc71d9d929afd10cf999e2b"><gtr:id>82a0f573acc71d9d929afd10cf999e2b</gtr:id><gtr:otherNames>Gallardo-Antol?n A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75d4c50b50.91397686</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DDCF61AF-07F5-4ADD-89C8-CE5EF46A5C53</gtr:id><gtr:title>Testing the consistency assumption: Pronunciation variant forced alignment in read and spontaneous speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/245d46ebe80e0ec94d7ad5ed4cdb0cee"><gtr:id>245d46ebe80e0ec94d7ad5ed4cdb0cee</gtr:id><gtr:otherNames>Dall R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d45817d58f0.42359417</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>187E5EA7-65D4-4F36-BAF8-4F0808472600</gtr:id><gtr:title>Learning Hidden Unit Contributions for Unsupervised Speaker Adaptation of Neural Network Acoustic Models</gtr:title><gtr:parentPublicationTitle>Proc IEEE SLT</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d988857865d9.15390508</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D4553D65-F768-4740-80AB-D1CF2EC47468</gtr:id><gtr:title>A Comparative Study of Adaptive, Automatic recognition of Disordered Speech</gtr:title><gtr:parentPublicationTitle>INTERSPEECH 2012 13th Annual Conference of the International Speech Communication Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4152d5ef61608f3ef9a7bff35382f698"><gtr:id>4152d5ef61608f3ef9a7bff35382f698</gtr:id><gtr:otherNames>Christensen, Heidi</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5452130a48abc6.72069740</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ACA71A19-F52E-4951-9CC1-231EF969BAD2</gtr:id><gtr:title>Impacts of machine translation and speech synthesis on speech-to-speech translation</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b002fa9bfa95f9bd6456417932f71bae"><gtr:id>b002fa9bfa95f9bd6456417932f71bae</gtr:id><gtr:otherNames>Hashimoto K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7ca21a498.93840360</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F717E341-AB41-48CF-8FE4-6AFA0E6B4DFA</gtr:id><gtr:title>Studio report: Linux audio for multi-speaker natural speech technology</gtr:title><gtr:parentPublicationTitle>Linux Audio Conference 2012 Proceedings</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0de6683c7f1dde919b2204e32c660f46"><gtr:id>0de6683c7f1dde919b2204e32c660f46</gtr:id><gtr:otherNames>Fox, Charles</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521cd540e3a3.18739948</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B86E8277-CE44-49F4-84F4-EA41E10048FF</gtr:id><gtr:title>Determining the number of speakers in a meeting using microphone array features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97dd30c4f543cb9306a0c87c2d52e9a6"><gtr:id>97dd30c4f543cb9306a0c87c2d52e9a6</gtr:id><gtr:otherNames>Zwyssig E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5432c39a96cca9.51763878</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88D2E5A1-032B-4EA8-AA05-0A0D32EB2EFF</gtr:id><gtr:title>Cross-Lingual Subspace Gaussian Mixture Models for Low-Resource Speech Recognition</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5432c39cb855d5.12681023</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>936775D0-41D4-426B-BF01-8E0710FACE6D</gtr:id><gtr:title>Deep neural networks employing Multi-Task Learning and stacked bottleneck features for speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dde230a70a21.51328155</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15681A15-223D-4C99-BBEB-88C426C9AE15</gtr:id><gtr:title>Paraphrastic language models</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42f7c7c6107a6cd7198876022cb295cf"><gtr:id>42f7c7c6107a6cd7198876022cb295cf</gtr:id><gtr:otherNames>Liu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460df7485d5e4.88752472</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6218EED5-560B-4B84-B381-B5D0871270C9</gtr:id><gtr:title>Smooth talking: Articulatory join costs for unit selection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1863a1af108e484cfbba1abf3daa6033"><gtr:id>1863a1af108e484cfbba1abf3daa6033</gtr:id><gtr:otherNames>Richmond K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4585205855.02239440</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6FBD9D38-9721-4B88-8523-AE811EC33D2A</gtr:id><gtr:title>Investigating the shortcomings of HMM synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of 8th ISCA Speech Synthesis Workshop</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/20cfb96ed279f6909cf786d6aff5d838"><gtr:id>20cfb96ed279f6909cf786d6aff5d838</gtr:id><gtr:otherNames>Merritt, T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433ac2f671999.13539727</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>89A25CAB-B948-4C41-BE5B-B5D66B00A16C</gtr:id><gtr:title>Towards Personalized Synthesized Voices for Individuals with Vocal Disabilities: Voice Banking and Reconstruction</gtr:title><gtr:parentPublicationTitle>SLPAT 2013, 4th Workshop on Speech and Language Processing for Assistive Technologies</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e95ade432385b6636659af37ed310795"><gtr:id>e95ade432385b6636659af37ed310795</gtr:id><gtr:otherNames>Veaux, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433b0e0b3ca45.50854129</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B942D0B2-7E5C-4C50-A4AE-180EAFE7D7B9</gtr:id><gtr:title>A fixed dimension and perceptually based dynamic sinusoidal model of speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4270d19e312228c6755fbae911c04249"><gtr:id>4270d19e312228c6755fbae911c04249</gtr:id><gtr:otherNames>Hu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464b7cd393fd4.31903875</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4463A26B-7899-43EF-B793-3CF372F54967</gtr:id><gtr:title>Neural net word representations for phrase-break prediction without a part of speech tagger</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a844267e69103535963a5bde1e3e2cb"><gtr:id>5a844267e69103535963a5bde1e3e2cb</gtr:id><gtr:otherNames>Watts O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75e48d0fe9.86954949</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>433D8E92-183D-408D-ABA2-AB4F861C0CF8</gtr:id><gtr:title>Adaptation of deep neural network acoustic models using factorised i-vectors.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ec1b9fcd08a79c17da2373d99469510"><gtr:id>4ec1b9fcd08a79c17da2373d99469510</gtr:id><gtr:otherNames>Karanasou, P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dda4e570ed88.72368274</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5255F13-7796-4C3C-970E-13B9229862D9</gtr:id><gtr:title>Personalising speech-to-speech translation: Unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/184560ab12ab876cdf3fd122226edefa"><gtr:id>184560ab12ab876cdf3fd122226edefa</gtr:id><gtr:otherNames>Dines J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c8753cf6.16570604</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0F472E7F-629E-4AC4-B2C0-9C3296F70EF0</gtr:id><gtr:title>Median-based generation of synthetic speech durations using a non-parametric approach</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/33d5018ed5f402176e9e0706004ae146"><gtr:id>33d5018ed5f402176e9e0706004ae146</gtr:id><gtr:otherNames>Ronanki S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a748d03b002e0.86579435</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A64D0C1-12B0-447D-A372-F8F95E063C9D</gtr:id><gtr:title>Automatic Transcription of Multi-genre Media Archives</gtr:title><gtr:parentPublicationTitle>SLAM 2013 Speech, Language and Audio in Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c6285f39e1a1816b62b06f0161ba58b6"><gtr:id>c6285f39e1a1816b62b06f0161ba58b6</gtr:id><gtr:otherNames>Lanchantin, P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433afc20d6a57.18670948</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0138C8E-6DD9-418C-8EF4-9FE633F7F8B6</gtr:id><gtr:title>Combining vocal tract length normalization with hierarchial linear transformations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3e397272cbec8689ecfec75bccb6ef6"><gtr:id>e3e397272cbec8689ecfec75bccb6ef6</gtr:id><gtr:otherNames>Saheer L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5464b7cbb46805.74861415</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E1FA5088-A60D-4F51-A038-7DF2170C795C</gtr:id><gtr:title>Latent Dirichlet Allocation Based Organisation of Broadcast Media Archives for Deep Neural Network Adaptation</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2015 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e6e2dbf7ea6284d8c683a75956e1eb4"><gtr:id>2e6e2dbf7ea6284d8c683a75956e1eb4</gtr:id><gtr:otherNames>Doulaty M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b855f8724.72796959</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0963782F-229B-4B37-8182-C8D5054095C3</gtr:id><gtr:title>Learning speaker-specific pronunciations of disordered speech</gtr:title><gtr:parentPublicationTitle>Interspeech\textquoteright13</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b848c4352.44163605</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BEEDE6B4-B231-405F-A059-4807A507896A</gtr:id><gtr:title>A study of speaker adaptation for DNN-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d988829b3e13.01053723</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46922652-1823-4AEB-89C8-7F1A1551558A</gtr:id><gtr:title>Data-selective Transfer Learning for Multi-Domain Speech Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/22d1f91b72dc6b5dc37a83d366b476cc"><gtr:id>22d1f91b72dc6b5dc37a83d366b476cc</gtr:id><gtr:otherNames>Doulaty, M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de05142bfa77.02020326</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4FD827AF-81B0-4592-9F03-BC355FDEA032</gtr:id><gtr:title>A lecture transcription system combining neural network acoustic and language models</gtr:title><gtr:parentPublicationTitle>In Proc. Interspeech 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a422f7efbd2a63270bcbd1711da98ba5"><gtr:id>a422f7efbd2a63270bcbd1711da98ba5</gtr:id><gtr:otherNames>Bell, P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433ad2be02f09.25356564</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5C11934F-B600-46A3-976D-3316BCB37497</gtr:id><gtr:title>Speech intelligibility enhancement for HMM-based synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>568bb095bc25c2.84712252</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61BD8B3D-C1C5-402F-B0A4-ECBD9B19EDC2</gtr:id><gtr:title>From HMMS to DNNS: Where do the improvements come from?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a844267e69103535963a5bde1e3e2cb"><gtr:id>5a844267e69103535963a5bde1e3e2cb</gtr:id><gtr:otherNames>Watts O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d45804eb8c1.82668102</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CDB51BB8-5C5F-46D3-AAAB-6984DE6540DC</gtr:id><gtr:title>Structured discriminative models using deep neural-network features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f6d5bcc016e2109ca86a62ee0daa691b"><gtr:id>f6d5bcc016e2109ca86a62ee0daa691b</gtr:id><gtr:otherNames>van Dalen R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58bc00fa81e804.93901432</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D6E24E48-4338-4618-9B00-814AEB4E6393</gtr:id><gtr:title>Towards an unsupervised speaking style voice building framework: Multi-style speaker diarization</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b81b4b94988f9858b8d33c947e428577"><gtr:id>b81b4b94988f9858b8d33c947e428577</gtr:id><gtr:otherNames>Lorenzo-Trueba J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7c6a405d2.13378152</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4104A130-EDB5-4556-B4A9-D7A2D171B8BA</gtr:id><gtr:title>Cross-lingual adaptation with multi-task adaptive networks</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d98885107e12.63160331</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7653E690-591B-4C9E-B665-A68C2E0F8891</gtr:id><gtr:title>Investigation of Using Continuous Representation of Various Linguistic Units in Neural Network Based Text-to-Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEICE Transactions on Information and Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e98fe1348ed65d25b2e4aa2b02cb3723"><gtr:id>e98fe1348ed65d25b2e4aa2b02cb3723</gtr:id><gtr:otherNames>WANG X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4597d374b5.85689775</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3F0AC31D-EA54-44B8-85FF-004B05C61729</gtr:id><gtr:title>Sequence training of DNN acoustic models with natural gradient</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/684f022b061ac6913ca22ad8d51d8e7d"><gtr:id>684f022b061ac6913ca22ad8d51d8e7d</gtr:id><gtr:otherNames>Haider A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa2700cc4e3b6.48876735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1B27277F-F7F3-48A8-941C-FF178E5CAB30</gtr:id><gtr:title>Introduction to the Issue on Statistical Parametric Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f4d2c2b7b42ff133ef6283e51d24973a"><gtr:id>f4d2c2b7b42ff133ef6283e51d24973a</gtr:id><gtr:otherNames>Tao J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75e025e539.13396017</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FD3BC1A-9AB6-4673-A621-1EF78EE5046A</gtr:id><gtr:title>Combining in-domain and out-of-domain speech data for automatic recognition of disordered speech</gtr:title><gtr:parentPublicationTitle>Interspeech\textquoteright13</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b85252da1.74406356</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>088DEA7B-E8E2-4B2B-AB6B-E5E3E1129DB4</gtr:id><gtr:title>Multi-frame factorisation for long-span acoustic modelling</gtr:title><gtr:parentPublicationTitle>Proc IEEE ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98881d469a6.54753140</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2549E0F5-E3F5-435B-B89A-CC8392397736</gtr:id><gtr:title>Feature-space transform tying in unified acoustic-articulatory modelling for articulatory control of HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bc720e26998eaf8e1e7606b8e2b6bd1c"><gtr:id>bc720e26998eaf8e1e7606b8e2b6bd1c</gtr:id><gtr:otherNames>Ling Z.-H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7cae2a097.86697731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8654CF72-66BC-4508-B532-E61C023FFC12</gtr:id><gtr:title>Parameterised Sigmoid and ReLU Hidden Activation Functions for DNN Acoustic Modelling</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26279d0dd6e455efef4955d84e6f9437"><gtr:id>26279d0dd6e455efef4955d84e6f9437</gtr:id><gtr:otherNames>Zhang, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddff8898e820.11208791</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AD01EE2-FAC7-4F69-A136-8EAE96FEEEC2</gtr:id><gtr:title>I-Vectors and Structured Neural Networks for Rapid Adaptation of Acoustic Models</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70934333059a54e373a24b5e539ba009"><gtr:id>70934333059a54e373a24b5e539ba009</gtr:id><gtr:otherNames>Karanasou P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c964f35b79b0.95124320</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C522E6F7-779F-4B26-AABF-963C9834AF6E</gtr:id><gtr:title>Complementary tasks for context-dependent deep neural network acoustic models</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98882bd8a70.76224411</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A0A5A4CA-9912-45A1-9B10-ABBB378966BB</gtr:id><gtr:title>Factorized context modelling for Text-to-Speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e96e84f01a13b8c22d773b0b45d3878"><gtr:id>6e96e84f01a13b8c22d773b0b45d3878</gtr:id><gtr:otherNames>Lu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39ad2eb75.76778585</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D348C31-722B-4D4D-9378-71F73106CA80</gtr:id><gtr:title>Disfluencies in change detection in natural, vocoded and synthetic speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c350efb7793c2fef1f2211d9f1fef0a"><gtr:id>8c350efb7793c2fef1f2211d9f1fef0a</gtr:id><gtr:otherNames>Dall, R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de038ce45ca6.63613283</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0903CB75-B80C-416E-B6A3-0E9108465165</gtr:id><gtr:title>Robust TTS duration modelling using DNNS</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd9cf62d7cc83f2a612f7016ccb43ea9"><gtr:id>fd9cf62d7cc83f2a612f7016ccb43ea9</gtr:id><gtr:otherNames>Henter G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d457ecb3629.49353723</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D5A9E1D-13D4-433F-823F-EC38F756343E</gtr:id><gtr:title>Measuring a decade of progress in Text-to-Speech</gtr:title><gtr:parentPublicationTitle>Loquens</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dcb726dfdbcedd1aee72ef7dd407906f"><gtr:id>dcb726dfdbcedd1aee72ef7dd407906f</gtr:id><gtr:otherNames>King S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675f126446e9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>119CF21D-4601-4CAC-B740-CBB7D00FD317</gtr:id><gtr:title>An investigation into speaker informed DNN frontend for LVCSR</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9293644cf4bef6b9087ebdc0acc03b42"><gtr:id>9293644cf4bef6b9087ebdc0acc03b42</gtr:id><gtr:otherNames>Liu, Y.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dda4a22c06d1.63858993</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0CB22821-D997-4DD8-8B13-116BD76BA1DE</gtr:id><gtr:title>Are we using enough listeners? No! An empirically-supported critique of Interspeech 2014 TTS evaluations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb9c6953df517c636c9268be995b5039"><gtr:id>cb9c6953df517c636c9268be995b5039</gtr:id><gtr:otherNames>Wester, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de05b8751655.68038236</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D8AFF3A-F9F5-41F8-B7A9-7005592ABCBD</gtr:id><gtr:title>Combining Perceptually-Motivated Spectral Shaping with Loudness and Duration Modification for Intelligibility Enhancement of HMM-Based Synthetic Speech in Noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>568bd3ecbc11b9.76051394</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1ED9DC67-C2F0-4C35-8B7A-2D679979C2F2</gtr:id><gtr:title>The MGB challenge: Evaluating multi-genre broadcast media recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d457b405876.47687090</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FDC9CE9-7666-4B3D-97F2-27F929D88581</gtr:id><gtr:title>Hybrid acoustic models for distant and multichannel large vocabulary speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39db1f733.31999219</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E714B7DF-9466-4D72-8DE5-7D2B937CF394</gtr:id><gtr:title>An introduction to statistical parametric speech synthesis</gtr:title><gtr:parentPublicationTitle>Sadhana</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dcb726dfdbcedd1aee72ef7dd407906f"><gtr:id>dcb726dfdbcedd1aee72ef7dd407906f</gtr:id><gtr:otherNames>King S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5432c39d3d9b92.95159430</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C9D0F835-F394-4B93-9E44-DBADB18A1C4A</gtr:id><gtr:title>Automatic Transcription of Multi-Genre Media Archives</gtr:title><gtr:parentPublicationTitle>Proceedings of the First Workshop on Speech, Language and Audio in Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6afb3b8f4605c0aaef1c394307ee1181"><gtr:id>6afb3b8f4605c0aaef1c394307ee1181</gtr:id><gtr:otherNames>Lanchantin P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b8173fbf8.87017020</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>45E3D55B-0D85-46D4-8FB0-67D69ED75AE5</gtr:id><gtr:title>Neural networks for distant speech recognition</gtr:title><gtr:parentPublicationTitle>Proc HSCMA</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2b77685afb59131cef21f12162b6fb96"><gtr:id>2b77685afb59131cef21f12162b6fb96</gtr:id><gtr:otherNames>Renals S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d988844918c2.59792309</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26F1BB4D-4793-4E38-8053-FD784184BDD0</gtr:id><gtr:title>Groupwise learning for ASR k-best list reranking in spoken langauge translation</gtr:title><gtr:parentPublicationTitle>Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38fb02d73316da34a672dead25c8e8f"><gtr:id>b38fb02d73316da34a672dead25c8e8f</gtr:id><gtr:otherNames>Ng R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e14b81af87a9.25356778</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1D02F1BB-3B3D-482A-BAC6-C351ADD980E6</gtr:id><gtr:title>Lightly supervised learning from a damaged natural speech corpus</gtr:title><gtr:parentPublicationTitle>Proc. IEEE ICASSP 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5cf06d39af5e60cd7619f648bf365985"><gtr:id>5cf06d39af5e60cd7619f648bf365985</gtr:id><gtr:otherNames>Fox C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e14b82b6c2d5.93146288</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6101FE5C-B2CD-4E51-BC6C-7E1BF515BB2E</gtr:id><gtr:title>Noise-robust whispered speech recognition using a non-audible-murmur microphone with VTS compensation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/851a472230d2cd69a9cab22daa89e275"><gtr:id>851a472230d2cd69a9cab22daa89e275</gtr:id><gtr:otherNames>Yang C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-2506-6</gtr:isbn><gtr:outcomeId>56dd75e4e920b6.96656361</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>818C05D3-7820-43EA-A37C-879928088EFC</gtr:id><gtr:title>Analysis of unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis using KLD-based transform mapping</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6737e652eab0bdf1e92854855ba8f137"><gtr:id>6737e652eab0bdf1e92854855ba8f137</gtr:id><gtr:otherNames>Oura K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7cca9d6b4.41311791</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E95C47F-60D2-4461-B44A-0CB3DED32692</gtr:id><gtr:title>Noise Compensation for Subspace Gaussian Mixture Models.</gtr:title><gtr:parentPublicationTitle>PROC INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e12c27cb98f85724def1dc098a5c9a0c"><gtr:id>e12c27cb98f85724def1dc098a5c9a0c</gtr:id><gtr:otherNames>Liang, L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5433a741b0f9c3.45258018</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A6334E1D-7921-4B06-999C-347931E5395E</gtr:id><gtr:title>Evaluation of Speaker Verification Security and Detection of HMM-Based Synthetic Speech</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd07305f88d78c03e0f1f557a9d76139"><gtr:id>fd07305f88d78c03e0f1f557a9d76139</gtr:id><gtr:otherNames>De Leon P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7cb13ad02.53542351</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A46D50A5-15E5-4A4A-8C98-A2E710F97CAE</gtr:id><gtr:title>SAT-LHUC: Speaker adaptive training for learning hidden unit contributions</gtr:title><gtr:parentPublicationTitle>Proc IEEE ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d98881620aa3.52226294</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>174886F7-2581-456B-9D9F-A8E57CD1DC7F</gtr:id><gtr:title>A flexible front-end for HTS</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/77a5669c318a1beb93cd1862ff4fa155"><gtr:id>77a5669c318a1beb93cd1862ff4fa155</gtr:id><gtr:otherNames>Aylett, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56ddfb44a66631.69922973</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3F6DD9A-FF27-4A37-98D8-8E7C81AD1169</gtr:id><gtr:title>Noise adaptive training for subspace Gaussian mixture models</gtr:title><gtr:parentPublicationTitle>Proceedings of Interspeech 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95b8687dee7e37ed876c00e270da3178"><gtr:id>95b8687dee7e37ed876c00e270da3178</gtr:id><gtr:otherNames>Lu, L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c4caf1c5c1.71333409</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>99813D68-8E94-4F8B-9FEE-FB64C5EEBDF3</gtr:id><gtr:title>The 2015 Sheffield System for Transcription of Multi--Genre Broadcast Media</gtr:title><gtr:parentPublicationTitle>Proceedings of the 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b8193e730.53647359</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>878AF865-E6AE-4344-978E-27EBB3868307</gtr:id><gtr:title>Multi-basis adaptive neural network for rapid adaptation in speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1a86fb804dd161bd520ef0e0c1b13874"><gtr:id>1a86fb804dd161bd520ef0e0c1b13874</gtr:id><gtr:otherNames>Wu C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddffe6dbf0b5.09759958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BDBBD5A7-CABF-433B-B793-1FA9AA1D9BD9</gtr:id><gtr:title>Reactive Control of Expressive Speech Synthesis Using Kinect Skeleton Tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0eabedd8bcfa55302aaf70ed894bfaca"><gtr:id>0eabedd8bcfa55302aaf70ed894bfaca</gtr:id><gtr:otherNames>Clark, Robert A.J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>568bd2529e7672.29430003</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>50656FA8-6378-4386-AADD-F0BF7626148D</gtr:id><gtr:title>Autoregressive Models for Statistical Parametric Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/34713d0179abf3b064fb50fa3794c7c5"><gtr:id>34713d0179abf3b064fb50fa3794c7c5</gtr:id><gtr:otherNames>Shannon M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54521aa1a07da9.83816136</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DC854C1F-112D-45D2-94BA-9CC08F43C610</gtr:id><gtr:title>Joint Uncertainty Decoding for Noise Robust Subspace Gaussian Mixture Models</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3062c93413868b8bbb942d6252c6e9d1"><gtr:id>3062c93413868b8bbb942d6252c6e9d1</gtr:id><gtr:otherNames>Liang Lu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39ebdd1a2.56006807</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F60F44A3-E374-4915-AF4A-0CC7FA9798BA</gtr:id><gtr:title>On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition</gtr:title><gtr:parentPublicationTitle>Proc IEEE ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d988813a8437.70684723</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0EC3C62B-897A-4912-9AEF-7AAB0DB53072</gtr:id><gtr:title>Differentiable pooling for unsupervised speaker adaptation</gtr:title><gtr:parentPublicationTitle>Proc IEEE ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98882778fc7.31707394</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>28EC76D7-28E5-4276-85C8-F8EC04775048</gtr:id><gtr:title>Speech and Audio Signal Processing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d956f1ea257ffaa7890de3965b35b6f"><gtr:id>5d956f1ea257ffaa7890de3965b35b6f</gtr:id><gtr:otherNames>Gold, B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9780470195369</gtr:isbn><gtr:outcomeId>5433a79fcd42a0.50898470</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0CCD615B-D10A-4769-8872-0824E13730E7</gtr:id><gtr:title>Multi-Reference Evaluation for Dialectal Speech Recognition System: A Study for Egyptian ASR</gtr:title><gtr:parentPublicationTitle>Proc WANLP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6f392e2e926f042fbe3cfa981e4fed71"><gtr:id>6f392e2e926f042fbe3cfa981e4fed71</gtr:id><gtr:otherNames>Ali A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98882551f83.85220082</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>58B0B5E0-212F-41A8-B096-1CB018A433E1</gtr:id><gtr:title>Vowel creation by articulatory control in HMM-based parametric speech synthesis</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bc720e26998eaf8e1e7606b8e2b6bd1c"><gtr:id>bc720e26998eaf8e1e7606b8e2b6bd1c</gtr:id><gtr:otherNames>Ling Z.-H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7c582dd50.12566461</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D2C3953-B143-4B62-B106-6D8509C4017D</gtr:id><gtr:title>Feature analysis for discriminative confidence estimation in spoken term detection</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0135aef6f062ae61a61b568c577d7f97"><gtr:id>0135aef6f062ae61a61b568c577d7f97</gtr:id><gtr:otherNames>Tejedor J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75ddafebc7.02592634</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1B586801-49DF-4B4A-AE3B-3CF9EDAB35C6</gtr:id><gtr:title>A study on the stability and effectiveness of features in quality estimation for spoken langauge translation</gtr:title><gtr:parentPublicationTitle>Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38fb02d73316da34a672dead25c8e8f"><gtr:id>b38fb02d73316da34a672dead25c8e8f</gtr:id><gtr:otherNames>Ng R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b81557f24.78090629</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>126A1081-B484-4A02-A83A-F26422F5CDDC</gtr:id><gtr:title>Semi-supervised DNN training in meeting recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3ebd6bdb2f296d2afe5cc783a781edfc"><gtr:id>3ebd6bdb2f296d2afe5cc783a781edfc</gtr:id><gtr:otherNames>Zhang P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dde4a7514527.96092219</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D65202AC-87C0-4F9D-B1A7-D89C46FC0C53</gtr:id><gtr:title>Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5a748c3f70de44.16190787</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D2DE6B8F-84C0-4C8B-9F42-BC9A7EDC6213</gtr:id><gtr:title>CUED-RNNLM - An open-source toolkit for efficient training and evaluation of recurrent neural network language models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7771ec39b9bd63d704f50fabca5f24ae"><gtr:id>7771ec39b9bd63d704f50fabca5f24ae</gtr:id><gtr:otherNames>Chen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15206149</gtr:issn><gtr:outcomeId>58bc00fa371020.09433789</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F75828CD-A33E-43FA-BD5E-0317CD7B901C</gtr:id><gtr:title>Sequence-discriminative training of deep neural networks</gtr:title><gtr:parentPublicationTitle>Proceedings of Interspeech 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3df62dea32fa50c31d147e82b91090dd"><gtr:id>3df62dea32fa50c31d147e82b91090dd</gtr:id><gtr:otherNames>Vesel?, K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433a900a6d8f5.50848976</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>49647F2E-7A86-4C6E-B60C-7D2D363D8D1B</gtr:id><gtr:title>Cross-domain Paraphrasing For Improving Language Modelling Using Out-of-domain Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b99d257c3ab2ed8aa7061e44e0e6edf"><gtr:id>7b99d257c3ab2ed8aa7061e44e0e6edf</gtr:id><gtr:otherNames>Liu, X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460de0141af19.73302835</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>792A4A8E-6D21-46D7-9C7B-8AC52D18FA4D</gtr:id><gtr:title>Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-5125-6</gtr:isbn><gtr:outcomeId>5432c39b89d1c8.74288717</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F67E81B7-1BAB-4460-A6A3-193056CF6CB2</gtr:id><gtr:title>Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/22d1f91b72dc6b5dc37a83d366b476cc"><gtr:id>22d1f91b72dc6b5dc37a83d366b476cc</gtr:id><gtr:otherNames>Doulaty, M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfccd4f4525.87962158</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D7E40F2F-5077-4542-8F23-08E720A3CCB8</gtr:id><gtr:title>The use of articulatory movement data in speech synthesis applications: An overview - Application of articulatory movements using machine learning algorithms -</gtr:title><gtr:parentPublicationTitle>Acoustical Science and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1863a1af108e484cfbba1abf3daa6033"><gtr:id>1863a1af108e484cfbba1abf3daa6033</gtr:id><gtr:otherNames>Richmond K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>567930f8e5efc5.99748892</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92C11C01-513A-4595-8070-A837E1514EA7</gtr:id><gtr:title>A system for automatic alignment of broadcast media captions using weighted finite-state transducers</gtr:title><gtr:parentPublicationTitle>Proc IEEE ASRU</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9888362b891.22955126</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0C429E43-D53A-460D-8BE8-C985BBF3C078</gtr:id><gtr:title>Automatic Selection of Speakers for Improved Acoustic Modelling : Recognition of Disordered Speech with Sparse Data</gtr:title><gtr:parentPublicationTitle>Spoken Language Technology Workshop, SLT\textquoteright14</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7309ab5562d758467290071a4701ce1d"><gtr:id>7309ab5562d758467290071a4701ce1d</gtr:id><gtr:otherNames>Christensen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e14b857af0c6.60349881</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BF5CCCA5-91BF-43FA-B534-BED51C7F7E74</gtr:id><gtr:title>Glottal Spectral Separation for Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0354e6b9c5de838fe3e29ef2e8d29910"><gtr:id>0354e6b9c5de838fe3e29ef2e8d29910</gtr:id><gtr:otherNames>Cabral J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464b7ca94e2d4.37071543</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F041382D-5D22-4953-A584-E696F31A6E8F</gtr:id><gtr:title>Quality estimation for ASR k-best list rescoring in spoken language translation</gtr:title><gtr:parentPublicationTitle>Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38fb02d73316da34a672dead25c8e8f"><gtr:id>b38fb02d73316da34a672dead25c8e8f</gtr:id><gtr:otherNames>Ng R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b81d2b049.29641505</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BC1E0A67-D08A-45FD-B896-C6841CA77435</gtr:id><gtr:title>Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of the 16th Annual Conference of the International Speech Communication Association (Interspeech)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e6e2dbf7ea6284d8c683a75956e1eb4"><gtr:id>2e6e2dbf7ea6284d8c683a75956e1eb4</gtr:id><gtr:otherNames>Doulaty M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14b85ec6169.41111771</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19BB2DBE-BF3D-4235-8E96-616F6791FBC7</gtr:id><gtr:title>Reconstructing Voices Within the Multiple-Average-Voice-Model Framework</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0383ad8dc0892de4add602de54cbb734"><gtr:id>0383ad8dc0892de4add602de54cbb734</gtr:id><gtr:otherNames>Lanchantin, P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfef19c17a1.98532176</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E804CAD7-EFD9-4116-8F09-46226A2DCB30</gtr:id><gtr:title>Maximum a posteriori adaptation of subspace Gaussian mixture models for cross-lingual speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5432c39bc9d219.88876136</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A047A44F-859C-4490-9FC4-CF5E4ABFA3D4</gtr:id><gtr:title>Cambridge university transcription systems for the multi-genre broadcast challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70b025769e3f3065fadda40d46ef890b"><gtr:id>70b025769e3f3065fadda40d46ef890b</gtr:id><gtr:otherNames>Woodland P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58bc00fb3be1b5.34127564</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7A261BC1-AE72-401D-BEA7-4F4A2BD06228</gtr:id><gtr:title>Unsupervised and lightly-supervised learning for rapid construction of TTS systems in multiple languages from 'found' data: evaluation and analysis</gtr:title><gtr:parentPublicationTitle>8th ISCA Workshop on Speech Synthesis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a844267e69103535963a5bde1e3e2cb"><gtr:id>5a844267e69103535963a5bde1e3e2cb</gtr:id><gtr:otherNames>Watts O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dd75ed77d356.77327719</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B185FC0-71A5-4C64-BD0D-F8A6FCF31225</gtr:id><gtr:title>Mel Cepstral Coefficient Modification Based on the Glimpse Proportion Measure for Improving the Intelligibility of HMM-Generated Synthetic Speech in Noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>568baff64d8666.53309707</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>57C7E9E0-C5A8-4AC7-8008-04BDCFA8598E</gtr:id><gtr:title>Acoustic data-driven pronunciation lexicon for large vocabulary speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5432c39e373869.98165006</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55D98C9C-9B39-4064-A3E4-52F4DDFC982D</gtr:id><gtr:title>ALISA: An automatic lightly supervised speech segmentation and alignment tool</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/318072fc41ab4f4f937acb7487c265e6"><gtr:id>318072fc41ab4f4f937acb7487c265e6</gtr:id><gtr:otherNames>Stan A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56792bf761d2f1.35430342</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5C4F88C-38EC-42E7-ABCC-9BE2F3914092</gtr:id><gtr:title>Unsupervised continuous-valued word features for phrase-break prediction without a part-of-speech tagger</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7fcdb5319cec12658fa8271b2073c487"><gtr:id>7fcdb5319cec12658fa8271b2073c487</gtr:id><gtr:otherNames>Watts O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7c63204e7.05395999</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3224DDA3-017C-4B18-85BF-5D79596E366F</gtr:id><gtr:title>ASVspoof: The Automatic Speaker Verification Spoofing and Countermeasures Challenge</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-4553</gtr:issn><gtr:outcomeId>5a3625ff6d6818.83889119</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4CDB5349-D462-47F1-9097-BADD921981AD</gtr:id><gtr:title>Segment-oriented evaluation of speaker diarisation performance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/259975ab4407bac1ea513db0c2164f97"><gtr:id>259975ab4407bac1ea513db0c2164f97</gtr:id><gtr:otherNames>Milner R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e14b85a61707.66838328</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D7A27D2-26BF-433D-B422-F559E055C364</gtr:id><gtr:title>The UEDIN system for the IWSLT 2012 evaluation</gtr:title><gtr:parentPublicationTitle>Proc. International Workshop on Spoken Language Translation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26e570912565e3600d5773a2f78d8980"><gtr:id>26e570912565e3600d5773a2f78d8980</gtr:id><gtr:otherNames>Hasler, E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5433b267195080.88457218</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>75395EDF-1DE5-4491-A53C-069BBAE0DCD2</gtr:id><gtr:title>Context-dependent acoustic modeling based on hidden maximum entropy model for statistical parametric speech synthesis</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Audio, Speech, and Music Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4628b39c4e80c5ab10ce51c826bbe0c3"><gtr:id>4628b39c4e80c5ab10ce51c826bbe0c3</gtr:id><gtr:otherNames>Khorram S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd75d9ec24e8.13880897</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5643BFA9-2093-4416-AEA3-70F7153FD062</gtr:id><gtr:title>Investigating source and filter contributions, and their interaction, to statistical parametric speech synthesis.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/74515d13ab6d63d6ed201508f8d83810"><gtr:id>74515d13ab6d63d6ed201508f8d83810</gtr:id><gtr:otherNames>Merritt, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56ddf939b45598.94556926</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>74798C5A-5FD3-40A2-8D89-5E47F2EBD2AB</gtr:id><gtr:title>Cepstral analysis based on the glimpse proportion measure for improving the intelligibility of HMM-based synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5464b7cc108bd1.22136271</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BAE35AA2-BFF8-4622-A7ED-91763DEF3872</gtr:id><gtr:title>Probabilistic Linear Discriminant Analysis for Acoustic Modeling</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f97497400bd4c6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>944EE59F-5C92-49AE-A743-2DCD5587333F</gtr:id><gtr:title>On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d457c932850.54053699</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A7877C3-D98B-40A2-AB3E-E09CE17C1473</gtr:id><gtr:title>Feed Forward Pre-training for Recurrent Neural Network Language Models</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/755dc3f05865aea033262a8efd75bee4"><gtr:id>755dc3f05865aea033262a8efd75bee4</gtr:id><gtr:otherNames>Gangireddy S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d98884c39212.07321816</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3A7F3F33-C606-432B-99D7-EFC17E95E891</gtr:id><gtr:title>The UEDIN ASR Systems for the IWSLT 2014 Evaluation</gtr:title><gtr:parentPublicationTitle>Proc IWSLT</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d98885d05106.60934433</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E3DD4F5D-25F7-451D-BB4F-87E4182712CE</gtr:id><gtr:title>Using neural network front-ends on far field multiple microphones based speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7c92a675b6502a97a707b04cca69bc1"><gtr:id>d7c92a675b6502a97a707b04cca69bc1</gtr:id><gtr:otherNames>Liu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e17016a105.65000754</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D03B570-11C3-4ECE-A832-B70678C8F95E</gtr:id><gtr:title>Prosodically-enhanced Recurrent Neural Network Language Models</gtr:title><gtr:parentPublicationTitle>Proc Interspeech2015</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/755dc3f05865aea033262a8efd75bee4"><gtr:id>755dc3f05865aea033262a8efd75bee4</gtr:id><gtr:otherNames>Gangireddy S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98882344249.39443348</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E79D76E2-854C-4014-B031-96E08F2CD76D</gtr:id><gtr:title>Speech Synthesis Based on Hidden Markov Models</gtr:title><gtr:parentPublicationTitle>Proceedings of the IEEE</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/837f196e3b692bddce90b7dea72e8d0b"><gtr:id>837f196e3b692bddce90b7dea72e8d0b</gtr:id><gtr:otherNames>Tokuda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c7eef173.74403544</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C405DFD6-8159-4EFB-9FC4-48D1ED89C7BC</gtr:id><gtr:title>TUNDRA: A multilingual corpus of found data for TTS research created with light supervision</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/705dac958004c99e3d563f633ca3d557"><gtr:id>705dac958004c99e3d563f633ca3d557</gtr:id><gtr:otherNames>Stan A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>5464b7c658d265.27940234</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EE1119EB-CF28-4A7E-BBDD-D00BB1661876</gtr:id><gtr:title>Acoustic adaptation to dynamic background conditions with asynchronous transformations</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/955ef4e33ec684e14b1ee01e852f0c5a"><gtr:id>955ef4e33ec684e14b1ee01e852f0c5a</gtr:id><gtr:otherNames>Saz O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c9ddfe7e7df4.19379042</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CB2745BE-D82B-48D3-984A-C019E74CA3B4</gtr:id><gtr:title>Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/58a229b60fcf5304f5555d827daeb1c5"><gtr:id>58a229b60fcf5304f5555d827daeb1c5</gtr:id><gtr:otherNames>Henter ,G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56ddf98ce6e030.87986079</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ECBB796A-EE42-4FBD-8FE0-C16F359609F0</gtr:id><gtr:title>Improving Trajectory Modelling for DNN-Based Speech Synthesis by Using Stacked Bottleneck Features and Minimum Generation Error Training</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4006399647.22841373</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B29F1CD-DFAC-420D-843D-89B02F338C9F</gtr:id><gtr:title>Knowledge distillation for small-footprint highway networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a748d0182f246.99666379</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>236C8973-2162-441F-80E8-827286EFC4C6</gtr:id><gtr:title>The NST-GlottHMM entry to the Blizzard Challenge 2015</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a844267e69103535963a5bde1e3e2cb"><gtr:id>5a844267e69103535963a5bde1e3e2cb</gtr:id><gtr:otherNames>Watts O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e14aa53414c4.70451885</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>68C7D962-6DE9-40AA-88C1-E4F67AA39319</gtr:id><gtr:title>The MGB Challenge: Evaluating multi-genre broadcast media recognition</gtr:title><gtr:parentPublicationTitle>Proc IEEE ASRU</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d98883147336.08661034</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BCBB1F9A-FAE1-4D9D-A23D-23D2C1FA01C0</gtr:id><gtr:title>Investigation of back-off based interpolation between recurrent neural network and n-gram language models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7771ec39b9bd63d704f50fabca5f24ae"><gtr:id>7771ec39b9bd63d704f50fabca5f24ae</gtr:id><gtr:otherNames>Chen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58bc00fb945ae1.14674301</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4CA95643-E7B2-4B3A-9347-00FD3AA8096D</gtr:id><gtr:title>Improving Lightly Supervised Training for Broadcast Transcription</gtr:title><gtr:parentPublicationTitle>Proc Interspeech 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f645c2454cc1aa52381be5336ddeff1"><gtr:id>5f645c2454cc1aa52381be5336ddeff1</gtr:id><gtr:otherNames>Long, Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545217cbbcabb6.58578158</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1DB42BA6-824A-48AC-9E68-D32601A5C28E</gtr:id><gtr:title>Differentiable Pooling for Unsupervised Acoustic Model Adaptation</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a9303d178dfedac2415b624d767c4aa"><gtr:id>5a9303d178dfedac2415b624d767c4aa</gtr:id><gtr:otherNames>Swietojanski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4011b67a02.53758608</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>458C9196-19B2-4B40-9CC4-F018954DC503</gtr:id><gtr:title>Blind speech segmentation using spectrogram image-based features and Mel cepstral coefficients</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/318072fc41ab4f4f937acb7487c265e6"><gtr:id>318072fc41ab4f4f937acb7487c265e6</gtr:id><gtr:otherNames>Stan A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a748cfbe303f5.53346622</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB5C4B30-8349-48B0-92E5-DF20290E8074</gtr:id><gtr:title>Paraphrastic language models and combination with neural network language models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42f7c7c6107a6cd7198876022cb295cf"><gtr:id>42f7c7c6107a6cd7198876022cb295cf</gtr:id><gtr:otherNames>Liu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460e0062a06c2.46981679</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ACB3B144-C3EE-4D24-8C78-B30B3A9EDF96</gtr:id><gtr:title>Efficient lattice rescoring using recurrent neural network language models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42f7c7c6107a6cd7198876022cb295cf"><gtr:id>42f7c7c6107a6cd7198876022cb295cf</gtr:id><gtr:otherNames>Liu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460de9fa645d6.21512049</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5AAA3E6D-5D13-4678-A66E-CEF70D4170F5</gtr:id><gtr:title>Recurrent neural network language model training with noise contrastive estimation for speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/92239183067682adaa1b2a1671dc4406"><gtr:id>92239183067682adaa1b2a1671dc4406</gtr:id><gtr:otherNames> Liu,X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dddffca8ff55.93414884</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B1F96A02-C33A-4780-A77D-590D979FA254</gtr:id><gtr:title>Rating naturalness in speech Synthesis: The effect of style and expectation</gtr:title><gtr:parentPublicationTitle>Proceedings of the International Conference on Speech Prosody</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e49f2bafd907f5c670d3407eae6733e"><gtr:id>3e49f2bafd907f5c670d3407eae6733e</gtr:id><gtr:otherNames>Dall R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>23332042</gtr:issn><gtr:outcomeId>5464b7c8514027.48445854</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B090019E-D4BA-48D9-9FB2-3D5D7D245ABC</gtr:id><gtr:title>Feature-space Speaker Adaptation for Probabilistic Linear Discriminant Analysis Acoustic Models</gtr:title><gtr:parentPublicationTitle>Proc Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a52ae52cf115d5f18b9025c28dd0b6b"><gtr:id>5a52ae52cf115d5f18b9025c28dd0b6b</gtr:id><gtr:otherNames>Lu L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d988833b2fe6.91565590</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8421D1ED-403B-4063-995A-52F53F31DEC2</gtr:id><gtr:title>On the effect of snr and superdirective beamforming in speaker diarisation in meetings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97dd30c4f543cb9306a0c87c2d52e9a6"><gtr:id>97dd30c4f543cb9306a0c87c2d52e9a6</gtr:id><gtr:otherNames>Zwyssig E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5432c39cf26aa1.47153564</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FABB7250-4D2C-451F-91EE-9B0C120EE905</gtr:id><gtr:title>Description of the UEDIN System for German ASR</gtr:title><gtr:parentPublicationTitle>Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT 2013)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e8af05fa515d1ffcb0788a1154eee964"><gtr:id>e8af05fa515d1ffcb0788a1154eee964</gtr:id><gtr:otherNames>Driesen, J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433b13b98c595.78765396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BED3CE19-0E4A-4D8D-A88E-E0FB5E198B09</gtr:id><gtr:title>The UEDIN English ASR System for the IWSLT 2013 Evaluation</gtr:title><gtr:parentPublicationTitle>Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT 2013)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a422f7efbd2a63270bcbd1711da98ba5"><gtr:id>a422f7efbd2a63270bcbd1711da98ba5</gtr:id><gtr:otherNames>Bell, P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433b357e3b039.94454109</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F9043C07-6A86-4B5F-803D-BD1385CD0516</gtr:id><gtr:title>Multitask Learning of Context-Dependent Targets in Deep Neural Network Acoustic Models</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7be181e2274cd538e69eddbe5aec3d7"><gtr:id>a7be181e2274cd538e69eddbe5aec3d7</gtr:id><gtr:otherNames>Bell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>585d402a6509f6.96937709</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>680D834A-16EF-431F-A7C3-752500FB6AE8</gtr:id><gtr:title>Anti-Spoofing for Text-Independent Speaker Verification: An Initial Database, Comparison of Countermeasures, and Human Performance</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4000d3db93.23657873</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>51FCBC0D-1817-49D9-941F-8FBDE9283117</gtr:id><gtr:title>The Temporal Delay Hypothesis: Natural, Vocoded and Synthetic Speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb9c6953df517c636c9268be995b5039"><gtr:id>cb9c6953df517c636c9268be995b5039</gtr:id><gtr:otherNames>Wester, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfdb1b7ed92.61102155</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A98BBE0-7725-4BBF-A423-AD0FC8DF1A54</gtr:id><gtr:title>Joint optimisation of tandem systems using Gaussian mixture density neural network discriminative sequence training</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f2d31a05ff149de838f7af04b5a46315"><gtr:id>f2d31a05ff149de838f7af04b5a46315</gtr:id><gtr:otherNames>Zhang C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa270aa159c98.17992544</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I031022/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>