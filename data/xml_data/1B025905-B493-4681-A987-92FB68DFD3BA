<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/9A22541F-DA58-481E-8D0C-C4BCF4EA030E"><gtr:id>9A22541F-DA58-481E-8D0C-C4BCF4EA030E</gtr:id><gtr:name>EMBL - European Bioinformatics Institute</gtr:name><gtr:department>Ensembl Group</gtr:department><gtr:address><gtr:line1>Wellcome Trust Genome Campus</gtr:line1><gtr:line2>Hinxton</gtr:line2><gtr:line4>Cambridge</gtr:line4><gtr:line5>Cambridgeshire</gtr:line5><gtr:postCode>CB10 1SD</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9A22541F-DA58-481E-8D0C-C4BCF4EA030E"><gtr:id>9A22541F-DA58-481E-8D0C-C4BCF4EA030E</gtr:id><gtr:name>EMBL - European Bioinformatics Institute</gtr:name><gtr:address><gtr:line1>Wellcome Trust Genome Campus</gtr:line1><gtr:line2>Hinxton</gtr:line2><gtr:line4>Cambridge</gtr:line4><gtr:line5>Cambridgeshire</gtr:line5><gtr:postCode>CB10 1SD</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A3773215-6476-4414-9BB0-4AAD931E2A5A"><gtr:id>A3773215-6476-4414-9BB0-4AAD931E2A5A</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:surname>Flicek</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F7B7ADEC-030A-49F0-B76E-382EA3D73D2B"><gtr:id>F7B7ADEC-030A-49F0-B76E-382EA3D73D2B</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:otherNames>David</gtr:otherNames><gtr:surname>Yates</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FM020398%2F1"><gtr:id>1B025905-B493-4681-A987-92FB68DFD3BA</gtr:id><gtr:title>eHive-RPC: A Remote Procedure Call Public Interface for eHive</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/M020398/1</gtr:grantReference><gtr:abstractText>Ensembl developed 'eHive' as a production system that manages and optimizes the running of tasks (called 'jobs'), on a compute cluster that may have thousands of Central Processing Units (CPUs). A CPU is the hardware within a computer that carries out the instructions of a computer program by performing the basic input and output operations of the system. Each computer has one or more CPUs.

Some compute clusters comprise many thousands of CPUs distributed amongst many computers. With so many computers and CPUs, it is important that jobs are sent to these CPUs in a fair and efficient manner, especially when many users are competing to use the same resources. Clusters usually rely on a central queuing system that holds a list of all the jobs that need to be run and can give individual computers in the cluster explicit instructions about which job to execute. This type of queuing system works well if the jobs each take an hour or more to complete. However, when jobs complete faster than they can be scheduled it creates a processing bottleneck e.g. if a job executes in minutes or less. The usual way to solve the bottleneck is to implement another system on top of the scheduler that 'batches' similar jobs together to make operations more efficient.

eHive's novel solution to the issue of job queuing is to move away from this central job scheduling: eHive is a 'distributed' processing system based on 'autonomous agents' with the behavioural structure of honeybees, hence the term 'eHive'. eHive maintains the ability to monitor and track jobs via a central 'blackboard'. Workers are efficiently created on a compute cluster, known as a meadow, with no specific task assigned to them. Once running, each worker contacts the blackboard, is able to find the most suitable kind of job, specializes to claim work and runs multiple jobs of this type in a row. Workers are able to re-specialize to claim other types of jobs once they exhaust their original designation. Each worker regularly updates its status in the blackboard to allow other workers to optimize the overall job distribution.

The benefits of eHive are (a) a reduction in the overhead of individual job processing, (b) an increase in the maximum number of tasks that can be running at any one time, (c) an increase in the tolerance to faults in the compute cluster, and (d) the allowance of complicated processes running in parallel.

Although eHive was originally designed for the purpose of Ensembl, its functionality is applicable to all data types that have large compute requirements. In this project we aim to transform the possibilities of eHive further, by developing a 'Remote Procedure Call system (RPC) for eHive. This will allow jobs to run on remote clusters as well as local clusters, thereby expanding the use of eHive to multiple compute clusters and cloud computing services. This will enable wider use of eHive within data-intensive fields in the life sciences and beyond.</gtr:abstractText><gtr:technicalSummary>eHive-RPC attempts to bring a system for creating efficient Remote Procedure Calls (RPC) for concurrent pipelines. eHive is a distributed processing system built by the Ensembl Project and used to control single pipelines on large-scale compute farms.

RPC allows inter-process communication, meaning that local code can cause the execution of code in another address space. There is no limit on how long the remote task should take, which makes waiting for the response inefficient. Instead, developers use asynchronous solutions that allow the local work to be queued. Some solutions will immediately hand back a token that identifies a unit of work and requires periodic client polling asking if the work is finished. Polling is usually considered inefficient too, as clients cannot proceed the moment the remote call has finished.

Our system attempts to solve this problem by extending the eHive workflow system. eHive is essentially event-driven, and is able to control concurrent work units using semaphores. In this context, a blocking work unit can represent a remote call, and another, blocked, work unit can be local. The latter can be unblocked using an inter-pipeline semaphore system when the RPC call has completed. The remote server will communicate task completion, including appropriate error reporting.

eHive-RPC also requires efficient two-way data transfer between local and remote servers. Here we aim to extend an existing architecture called accumulators and move towards the transparent transfer of data between caller and remote server.

Our implementation will be integrated into the existing eHive project both as a client and a server, making any eHive pipeline an RPC target. We expect the system to be generic and useable by any client or workflow engines e.g. Galaxy or Taverna. We also aim to disseminate eHive knowledge by hosting two training courses alongside extensive documentation of the protocols and message formats developed by this project.</gtr:technicalSummary><gtr:potentialImpactText>The last decade has seen the advancement of laboratory techniques that enable research data to be produced at a cheaper and faster rate, e.g. 'Next-Generation' DNA sequencing. In addition new laboratory techniques now make it possible to probe new areas of biology, such as gene regulation, through epigenetic mechanisms. These rapid improvements in methodology impact many different disciplines in the life sciences, from basic research to applied areas such as plant and animal breeding.

The primary beneficiaries from this proposed development for eHive will be Bioinformaticians in academia and industry, both in the UK and beyond, including those supporting research by analyzing data, and those producing and maintaining archives and data resources for the research community. Bioinformaticians are actively working on developing new algorithms and more efficient means of handling and processing these data, so that they can be interpreted quickly and accurately. In order to process these data, large-scale compute is required and the management of these data on compute clusters becomes an increasing challenge.

World-leading pharmaceutical companies, bioinformatics service companies, and animal breeding companies have in-house Bioinformaticians to produce customized data analysis on private data. These companies therefore have the expertise to use software such as eHive. Evidence that EMBL-EBI supports these areas includes our long-standing Industry Programme and the more recent announcement for the Centre for Therapeutic Target Validation (CTTV). 

Suppliers of open source and commercial 'omics tools (e.g. Taverna and Galaxy) will also benefit from access to compute farms and software that our eHive development will provide.

Enabling research in these areas impacts socio-economic outcomes, contributing both in areas of basic research that promote understanding, as well as benefitting the wider public with improved health care for humans and animals, and productivity increases in agriculture.

How will these users benefit?
In this grant we propose to undertake some major enhancements to eHive. These enhancements will set the stage for the use of eHive in a wider context than is currently possible, and we believe that this will make it a more appealing and more accessible tool for Bioinformaticians and Bioscientists who wish to run large-scale compute.

Enabling eHive to communicate across more than one compute cluster will bring about novel use-cases for eHive:

- For time-critical work, being able to redirect certain job types to a second cluster (eg. Compute cloud) will allow the required workload to be achievable in the limited time frame. Running a pipeline within only one compute cluster can be a disadvantage when that cluster is being heavily used by other users, or when the capacity of a cluster is small. 

- Being able to run a pipeline that spans more than one compute cluster will enable the user to run sections of their pipeline on the most appropriate cluster for that type of job. Some compute clusters are optimized for a particular type of job, and may work well for one type of pipeline but not another: compute clusters may be optimized for the number of jobs running simultaneously, the number and size of files stored on disk (several small files versus few large files), multi-threaded jobs, etc. 

Support for eHive is growing and we now have users in both academia (the Roslin Institute, EMBL-EBI, Gramene in Cold Spring Harbor, USA) as well as industry (Eagle Genomics). Having a common job scheduling tool promotes efficiency, as one group can focus on the development for the tool and all groups benefit from the improvements.

We support outreach activities, and there is demand for training workshops (Eagle genomics, pers. comm. with Kathryn Beal). Our mailing list is open and developers from any background are free to ask questions, help, and share opinions on the use of eHive.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2015-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>132014</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Perl workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>93D821F5-4816-4CC3-9F08-83FE7FBBCE16</gtr:id><gtr:impact>Brandon Walts gave a talk at a Perl workshop at the University of Westminster on '?Perl meets big data and high performance computing with the eHive framework.'.</gtr:impact><gtr:outcomeId>58ad6f194388f2.94356841</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://act.yapc.eu/lpw2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Participation in an activity, workshop or similar - YAPC::EU Perl Conference</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>66D13FE5-A29D-41E4-A0F6-1689BEF17E8A</gtr:id><gtr:impact>Andrew Yates presented the eHive software toolkit as a lightning talk at YAPC::EU; a European wide conference for Perl programmers. The talk was well received and triggered extensive discussion with other programmers. This was as part of a much larger engagement at YAPC::EU concerning bioinformatics.</gtr:impact><gtr:outcomeId>58ad7573d121b5.45220828</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://act.yapc.eu/ye2016/talk/6929</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Ensembl workshop in Taiwan</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>63F11BF5-D651-49EF-8A7F-CF2A46279564</gtr:id><gtr:impact>Ensembl organised a comprehensive workshop in Taiwan. Part of the content was on eHive, presented by Brandon Walts, at Hsinchu and Taipei, Taiwan.</gtr:impact><gtr:outcomeId>58ad7094f41c16.94229872</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://training.ensembl.org/events/2017/2017-02-15-ehivetaiwan</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>During the course of this award we have been able to repurpose a number of existing eHive concepts into a remote procedure calling framework and have been able to make an efficient and scalable solution. We have expanded our outreach and documentation of the eHive system to increase its usefulness to researchers but also to target those who run large scale processing workflows in research and industry.

One of the biggest barriers to using remote procedures efficiently is the time it takes to tell a calling process when a remote task has completed. Normally when working with remote procedure frameworks this is done in two way. Through periodic checking of a ticket number when computation will take an unknown amount of time or by responding in a timely manner. eHive no longer relies on this as it has expanded its key control framework called semaphores to work in this setting. 

Semaphores are counters that start tasks when they count down to zero when work completes. The semaphores are sent back the the caller meaning dependent jobs can start the moment a remote task finishes and provides the best of both previously mentioned solutions. In addition eHive can now understand local and remote data structures through the use of URLs (universal resource locators) similar to those used on the web. Our APIs hide this complexity meaning pipelines do not need to be significantly adapted to work with remote calls.

We have also reached out to two major communities to promote the usage of eHive and of the products of this research. The first is communities that already use eHive pipelines or work in the same domain as us. The second is the Perl programming community to target those who do not work in research and expand the user base of eHive.</gtr:description><gtr:exploitationPathways>All documentation and training materials have been made publicly available. The developments made on this project will continue to be supported and enhanced by the Ensembl team for the foreseeable future. Through our outreach to industry via the Perl conference and workshop circuit we aim to attract a new audience of eHive users. We will also continue this advertising process outside of the bioinformatics community. In addition we will continue advertising, training and outreach within bioinformatics as this is where the majority of our user community comes from.</gtr:exploitationPathways><gtr:id>4B71FF77-30A1-406C-9AC4-A9BA52E78947</gtr:id><gtr:outcomeId>56cd8c041631d6.83010899</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Pharmaceuticals and Medical Biotechnology</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The eHive system has been extended to improve the control of flow between local and remote pipelines. Firstly the system is capable of accessing objects held in local and remote eHive systems either via numerical identifier or URI. Secondly the system now implements semaphores across the entire system and uses these extensively to both halt and restart pipelines. Semaphores are handled locally but transmit their state to a remote calling system once certain conditions have been met i.e. all jobs have been run. In both cases the eHive internal APIs have been developed to hide this complexity away from the pipeline designer.</gtr:description><gtr:id>545416BE-2183-4CF3-8C13-E36315B0B0A1</gtr:id><gtr:impact>Both developments enable to transfer of a pipeline's control/flow to a remote system for further execution and done so in a manner that reduces network traffic between local and remote resources. This ensures remote pipeline execution is both achievable and scalable.</gtr:impact><gtr:outcomeId>58c2b0d9ae7f78.58584030</gtr:outcomeId><gtr:title>Task 1: Efficient control flow transfer between pipelines</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Data transfer between remote pipeline jobs is essential to the smooth execution of tasks in the eHive RPC framework. We have extended our current framework to support write operations between pipeline databases. In addition we attach data output from jobs to semaphores and use these to send results to downstream jobs. This ensures a single transaction of work is sent back to the calling pipeline with all data required for further downstream processing.</gtr:description><gtr:id>7E765A2F-B53A-4B59-B0EC-D5654A76E80B</gtr:id><gtr:impact>The software is capable of robust data transfer between a local and remote pipeline ensuring pipeline stability. This will aid in future developments around the RPC framework.</gtr:impact><gtr:outcomeId>58c2bca7f30519.10775840</gtr:outcomeId><gtr:title>Task 2: Efficient data transfer between eHive pipelines</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Work is progressing on introducing improvements to the eHive object system to enable the RPC work flow. These improvements will be made available in the upcoming eHive version 2.4 software release in Q2 2016. Our eHive GUI management system has also been improved to understand multiple types of pipelines specifically local and remote.</gtr:description><gtr:id>F0033B34-5850-449A-8A0E-562CE2775B80</gtr:id><gtr:impact>There are no impacts as yet.</gtr:impact><gtr:outcomeId>56cd8cf1bf4ab7.83226642</gtr:outcomeId><gtr:title>Task 1: Efficient control flow transfer between eHive pipelines</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>We have built extensive documentation into the eHive system available from our public GitHub repositories. In addition we have built a number of bioinformatic slanted example pipelines. Specifically a GC count and kmer pipeline. Both are common bioinformatics workflows and can be used to explain high-level eHive concepts through a system that is predictable and can be inspected in a running system</gtr:description><gtr:id>94C297B2-5CB5-44B4-94CF-088CC2B0700F</gtr:id><gtr:impact>The documentation helps to explain the eHive ecosystem. The addition of bioinformatic example workflows will help new users to understand how eHive pipelines are formed and how to build their own pipelines.</gtr:impact><gtr:outcomeId>58c2bf3a349011.48583014</gtr:outcomeId><gtr:title>Task 4: Outreach</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://rawgit.com/Ensembl/ensembl-hive/version/2.4/docs/index.html</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This work is currently in a prototype phase. We expect to reuse the existing flow frameworks mentioned in tasks 1 and 2 to build a system of efficiently detailing events back to the calling pipeline.</gtr:description><gtr:id>16531F94-5591-4F13-BDDC-7D26F05228F3</gtr:id><gtr:impact>We have discovered that communicating in-depth reports to remote pipeline errors e.g. a remote task ran out of memory is counterintuitive. We now believe that it is the remote pipeline's responsibility to only communicate failure back to a calling pipeline when tasks cannot be completed. User errors e.g. those caused by incorrect formatted data must also be inspected by the remote calling pipeline. This increases the cost of building pipelines to be used in a remote mode as they must be capable of handling multiple error conditions in order to make calling pipeline logic easier.</gtr:impact><gtr:outcomeId>58c2bdb209ee56.92803187</gtr:outcomeId><gtr:title>Task 3: Efficient Transfer of Events Back to the Calling Pipeline</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>200CB9C1-7275-4202-8509-416876F4305E</gtr:id><gtr:title>Ensembl 2018.</gtr:title><gtr:parentPublicationTitle>Nucleic acids research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f2d438575d300d51c13ce2d6307115e"><gtr:id>8f2d438575d300d51c13ce2d6307115e</gtr:id><gtr:otherNames>Zerbino DR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>0305-1048</gtr:issn><gtr:outcomeId>5a6f1380f3db05.43706603</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C209BF2-436B-4FA6-A96D-87143E040424</gtr:id><gtr:title>Ensembl 2016.</gtr:title><gtr:parentPublicationTitle>Nucleic acids research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9674d21dd8ce5ce19546dfd4d89e286d"><gtr:id>9674d21dd8ce5ce19546dfd4d89e286d</gtr:id><gtr:otherNames>Yates A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0305-1048</gtr:issn><gtr:outcomeId>585d72f9ea4306.77225879</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D20A6801-266B-4705-B59A-490D682C8B15</gtr:id><gtr:title>Ensembl 2017.</gtr:title><gtr:parentPublicationTitle>Nucleic acids research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/82791bf6a9aef72b5f4cd01d4fe2200d"><gtr:id>82791bf6a9aef72b5f4cd01d4fe2200d</gtr:id><gtr:otherNames>Aken BL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0305-1048</gtr:issn><gtr:outcomeId>58ac6a5d250370.46545190</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/M020398/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>1F190F7B-B800-40B2-B250-62AD4842598F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Bioinformatics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>9EAAD5EA-2E54-4986-942F-2E204958FE29</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>High Performance Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>3493671D-A4CF-4197-90A7-3CCFBE1C0627</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Tools for the biosciences</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>FA4A8455-3074-48A4-B0CD-5B85D94B79F5</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>eScience</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>