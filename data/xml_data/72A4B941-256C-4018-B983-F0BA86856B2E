<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9C10D78F-6430-4CA7-9528-B96B0762A4C6"><gtr:id>9C10D78F-6430-4CA7-9528-B96B0762A4C6</gtr:id><gtr:name>Cardiff University</gtr:name><gtr:address><gtr:line1>Research &amp; Consultancy</gtr:line1><gtr:line2>PO Box 923</gtr:line2><gtr:line4>Cardiff</gtr:line4><gtr:line5>South Glamorgan</gtr:line5><gtr:postCode>CF10 3TE</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9C10D78F-6430-4CA7-9528-B96B0762A4C6"><gtr:id>9C10D78F-6430-4CA7-9528-B96B0762A4C6</gtr:id><gtr:name>Cardiff University</gtr:name><gtr:address><gtr:line1>Research &amp; Consultancy</gtr:line1><gtr:line2>PO Box 923</gtr:line2><gtr:line4>Cardiff</gtr:line4><gtr:line5>South Glamorgan</gtr:line5><gtr:postCode>CF10 3TE</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/14736142-9044-43D1-9FE0-F47F954528F4"><gtr:id>14736142-9044-43D1-9FE0-F47F954528F4</gtr:id><gtr:name>Oticon Eriksholm Research Centre</gtr:name><gtr:address><gtr:line1>Kongebakken 9</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/011C50E7-97D3-455C-8361-BC5E7BE774D6"><gtr:id>011C50E7-97D3-455C-8361-BC5E7BE774D6</gtr:id><gtr:firstName>Patrick</gtr:firstName><gtr:otherNames>A</gtr:otherNames><gtr:surname>Naylor</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7DA69D49-CADC-472C-9848-A45CBF1A754A"><gtr:id>7DA69D49-CADC-472C-9848-A45CBF1A754A</gtr:id><gtr:firstName>Stuart</gtr:firstName><gtr:surname>Rosen</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B7928C11-A2B0-4E8C-A49D-541FD6C69C8E"><gtr:id>B7928C11-A2B0-4E8C-A49D-541FD6C69C8E</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:otherNames>Andrew</gtr:otherNames><gtr:surname>Huckvale</gtr:surname><gtr:orcidId>0000-0002-4503-5552</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7D52C98A-D844-4A88-B446-20AB4F329F95"><gtr:id>7D52C98A-D844-4A88-B446-20AB4F329F95</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>Michael</gtr:otherNames><gtr:surname>Brookes</gtr:surname><gtr:orcidId>0000-0001-7105-4936</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM026698%2F1"><gtr:id>72A4B941-256C-4018-B983-F0BA86856B2E</gtr:id><gtr:title>Environment-aware Listener-Optimized Binaural Enhancement of Speech (E-LOBES)</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M026698/1</gtr:grantReference><gtr:abstractText>Age-related hearing loss affects over half the UK population aged over 60. Hearing loss makes communication difficult and so has severe negative consequences for quality of life. The most common treatment for mild-to-moderate hearing loss is the use of hearing aids. However even with aids, hearing impaired listeners are worse at understanding speech in noisy environments because their auditory system is less good at separating wanted speech from unwanted noise. One solution for this is to use speech enhancement algorithms to amplify the desired speech signals selectively while attenuating the unwanted background noise.

It is well known that normal hearing listeners can better understand speech in noise when listening with two ears rather than with only one. Differences between the signals at the two ears allow the speech and noise to be separated based on their spatial locations resulting in improved intelligibility. Technological advances now make feasible the use of two hearing aids that are able to share information via a wireless link. By sharing information in this way, it becomes possible for the speech enhancement algorithms within the hearing aids to localize sound sources more accurately and, by jointly processing the signals for both ears, to ensure that the spatial cues that are present in the acoustic signals are retained. It is the goal of this project to exploit these binaural advantages by developing speech enhancement algorithms that jointly enhance the speech received by the two ears.

Most current speech enhancement techniques have evolved from the telecommunications industry and are designed to act only on monaural signals. Many of the techniques can improve the perceived quality of already intelligible speech but binary masking is one of the few techniques that has been shown to improve the intelligibility of noisy speech for both normal and hearing impaired listeners. In the binary masking approach regions of the time-frequency domain that contain significant speech energy are left unchanged while regions that contain little speech energy are muted. In this project we will extend existing monaural binary masking techniques to provide binaural speech enhancement while preserving the inter-aural time and level differences that are critical for the spatial separation of sound sources.

To train and tune our binaural speech enhancement algorithm we will also develop within the project an intelligibility metric that is able to predict the intelligibility of a speech signal for a binaural listener with normal or impaired hearing in the presence of competing noise sources. This metric is the key to finding automatically the optimum settings an individual listener's hearing aids in a particular environment.

The final evaluation and development of the binaural enhancement algorithm assess speech perception in noise in a panel of hearing-impaired listeners who will also be asked to assess the quality of the enhanced speech signals.</gtr:abstractText><gtr:potentialImpactText>The immediate beneficiaries from this research will be hearing aid manufacturers and, through them, hearing aid users. We expect that the techniques and algorithms developed in this research will make it substantially easier for hearing aid users to distinguish the individual speakers that are present especially when in a noisy listening environment.

However there are other circumstances in which listeners must use headphones or earphones when operating in an environment with high ambient noise. The techniques developed here can also benefit such users by allowing them to communicate intelligibly with others in a noisy environment.

The project will develop methods of characterizing the listening context and the acoustic properties of the listening environment from the received microphone signals and, from this information, identifying the listening context automatically. This is potentially of considerable benefit to designers of mobile phones or other mobile devices who will be able to adapt the operation of their systems to the context in which it is being used.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>983623</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Cardiff University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>School of Psychology</gtr:department><gtr:description>MRC Microphone Network</gtr:description><gtr:id>456F49E9-F5D1-4911-9A62-149E32C01505</gtr:id><gtr:impact>The network has provided an opportunity to meet others working in the field and it is expected that this will lead to further bilateral collaboration in the future.</gtr:impact><gtr:outcomeId>58c689bba68209.44585635-1</gtr:outcomeId><gtr:partnerContribution>Other members of the network have contributed intellectual input and expertise from a wide range of backgrounds.</gtr:partnerContribution><gtr:piContribution>Three members of the research team have participated in meetings of the MRC &amp;quot;Microphone&amp;quot; Network (MR/M025616/1) contributing intellectual input and expertise.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University College London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>MRC Over-Hear Network</gtr:description><gtr:id>1F5BAD5E-C88B-4CA0-8447-8E34B0B2C53D</gtr:id><gtr:impact>The network has provided an opportunity to meet others working in the field.</gtr:impact><gtr:outcomeId>58c68c0db4dec0.39262908-1</gtr:outcomeId><gtr:partnerContribution>Other members of the network have contributed intellectual input and expertise from a wide range of backgrounds.</gtr:partnerContribution><gtr:piContribution>Two members of the research team have participated in meetings of the MRC &amp;quot;Over-Hear&amp;quot; Network (MR/M025659/1 ) contributing intellectual input and expertise.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Invited talk (Oticon A/S, Copenhagen, Denmark)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6F38C361-9C73-48F7-94C9-3A2E52C01E0B</gtr:id><gtr:impact>I presented some research research to an audience at a seminar given at Oticon's premises in Copenhagen. The audience was primarily research and development engineers working on hearing aids in the company. The secondary audience was audiology practitioners who work directly with the hearing impaired. The talk was simultaneously broadcast to researchers at the company's research facility in Eriksholm and to collaborators in Sennheiser. The talk gave rise to several follow-up discussions and an invitation to visit the research centre at Eriksholm. The slides from the talk were requested and have been posted within the company's intranet.</gtr:impact><gtr:outcomeId>58c019b4c58bc7.72287457</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Institute of Acoustics/Audio Eng Soc - invited talk by Alastair Moore</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>C8EB8F82-33ED-4986-9049-090A79EFA3F5</gtr:id><gtr:impact>This invited talk was presented to professional practitioners in audio and acoustics. Its purpose was to broaden knowledge and to raise awareness of techniques for combating unwanted reverberation.</gtr:impact><gtr:outcomeId>58c684a2caadb5.51014237</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://ioa.org.uk/civicrm/event/info?id=210&amp;reset=1</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Research Presentation at Uni. Aalborg, Denmark</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>88E1F7F0-BFBA-4CB9-AD70-F0EFF3F6E3CE</gtr:id><gtr:impact>I presented a research seminar within the Centre for Acoustic Signal Processing Research (CASPR) at University of Aalborg. This talk covered recent research in acoustic signal processing related to hearing aids and was attended by a mixture of university professors and their PhD/UG students as well as representatives from industry. The talk made the front-page news item of their website: http://caspr.es.aau.dk at the time. Collaboration links and potential for student exchanges were established subsequently.</gtr:impact><gtr:outcomeId>58c01b05210cc3.32650119</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://caspr.es.aau.dk</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Audio3D is a program to simulate simple virtual audio environments. Multiple monophonic sound sources can be placed in a virtual room and are combined using room response and head-related transfer functions into a real-time binaural audio signal that can be listened to over headphones. Audio3D also supports a head-tracker such that the sound sources appear at a constant room position even if the header orientation changes. A server version can be used to run listening experiments in virtual rooms. The server runs the real time binaural rendering and head tracking, while the experiment can be controlled by external clients written in Javascript or MATLAB.</gtr:description><gtr:id>07F47179-C9B5-4564-85A1-A30AD944BC11</gtr:id><gtr:impact>The Audio3D system has been used to simulate listening environments for speech-in-noise intelligibility experiments. We have shown that intelligibility scores in a virtual environment match scores measured in a physical environment. In future we will use the system with hearing-impaired listeners to investigate the effects of novel spatial signal processing algorithms on the intelligibility of speech in noise.</gtr:impact><gtr:outcomeId>56d5cc85337c67.50167303</gtr:outcomeId><gtr:title>Audio3D - Spatial Audio Simulation System</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.phon.ucl.ac.uk/resource/audio3d/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The software efficiently estimates one or more psychometric functions such as, for example, the intelligibility of speech as a function of signal-to-noise ratio</gtr:description><gtr:id>E9AAE05C-0E61-4A08-A79A-A3508E4FFBC6</gtr:id><gtr:impact>The software is open source but no usage or download statistics are available at present.</gtr:impact><gtr:outcomeId>58c68e1552aba0.19210025</gtr:outcomeId><gtr:title>psycest - estimate psychometric functions</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/doc/voicebox/psycest.html</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>AF56A07A-54B2-4519-B473-CE848485E8AD</gtr:id><gtr:title>Speech enhancement for robust automatic speech recognition: Evaluation using a baseline system and instrumental measures</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e77f248010c1b434120ad44ce4ec6fd"><gtr:id>6e77f248010c1b434120ad44ce4ec6fd</gtr:id><gtr:otherNames>Moore A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>588b586f676880.67453435</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D9C0C76F-CA75-4A1F-88EE-9FD516F26525</gtr:id><gtr:title>Improving the perceptual quality of ideal binary masked speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/467ef1da04e40b51424bfec1489a7a59"><gtr:id>467ef1da04e40b51424bfec1489a7a59</gtr:id><gtr:otherNames>Lightburn L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a578aea2a69e8.21742209</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A6CF694C-15D9-47F5-B295-B2840BB202DF</gtr:id><gtr:title>Single-Channel Online Enhancement of Speech Corrupted by Reverberation and Noise</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/21f0e6bb47c57e3ecb069870750a0d40"><gtr:id>21f0e6bb47c57e3ecb069870750a0d40</gtr:id><gtr:otherNames>Doire C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>588b5bdabd0da0.71684624</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF06C4AA-B32F-42ED-86B7-C53B57253EC6</gtr:id><gtr:title>Cross-correlation based under-modelled multichannel blind acoustic system identification with sparsity regularization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f2ef100d5bb2b3a3380b4cb38acfff04"><gtr:id>f2ef100d5bb2b3a3380b4cb38acfff04</gtr:id><gtr:otherNames>Xue W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>588b5b93544567.65867592</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8932B5F0-5DE2-430E-87BD-B0AD6174A103</gtr:id><gtr:title>Direction of Arrival Estimation in the Spherical Harmonic Domain Using Subspace Pseudointensity Vectors</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e77f248010c1b434120ad44ce4ec6fd"><gtr:id>6e77f248010c1b434120ad44ce4ec6fd</gtr:id><gtr:otherNames>Moore A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>585d40273e37b4.77735464</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CCD61ABC-D5B2-4BF1-AE07-9E9DD7A2227F</gtr:id><gtr:title>Robust and efficient Bayesian adaptive psychometric function estimation.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62e675b90fff62395b511d50812b48d8"><gtr:id>62e675b90fff62395b511d50812b48d8</gtr:id><gtr:otherNames>Doire CSJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>5a351b5796bbc8.37444295</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B635F4DB-F87C-4AEB-A4AB-9A466182A594</gtr:id><gtr:title>Frequency-domain under-modelled blind system identification based on cross power spectrum and sparsity regularization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f2ef100d5bb2b3a3380b4cb38acfff04"><gtr:id>f2ef100d5bb2b3a3380b4cb38acfff04</gtr:id><gtr:otherNames>Xue W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a578ae1448337.55574237</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B37FDA57-E445-4382-B41C-79302A1FDA0F</gtr:id><gtr:title>Active speech level estimation in noisy signals with quadrature noise suppression</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/091815852122293b935dbcab979345d3"><gtr:id>091815852122293b935dbcab979345d3</gtr:id><gtr:otherNames>Dionelis N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd3a03404c96.15419457</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1DB04B53-BAE1-4622-85FD-3BD2B4F6C2FC</gtr:id><gtr:title>Robust spherical harmonic domain interpolation of spatially sampled array manifolds</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e77f248010c1b434120ad44ce4ec6fd"><gtr:id>6e77f248010c1b434120ad44ce4ec6fd</gtr:id><gtr:otherNames>Moore A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a578ae8904bd4.99037069</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E804893-7A79-4E13-8DAE-43617AD598AF</gtr:id><gtr:title>Multiple DOA estimation based on estimation consistency and spherical harmonic multiple signal classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fb154a3d55090879c97a507ae2824b30"><gtr:id>fb154a3d55090879c97a507ae2824b30</gtr:id><gtr:otherNames>Hafezi S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9d8687dd5235.42800718</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E656EE1C-A37A-4145-AB20-1746D928246A</gtr:id><gtr:title>A weighted STOI intelligibility metric based on mutual information</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/467ef1da04e40b51424bfec1489a7a59"><gtr:id>467ef1da04e40b51424bfec1489a7a59</gtr:id><gtr:otherNames>Lightburn L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d6b851ba7f2.45842212</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>808F1937-B7D9-4C65-9FAC-A6A41A3EC887</gtr:id><gtr:title>Prediction of speech-in-noise intelligibility by hearing-impaired listeners: a re-analysis of Summers et al 2013 auditory processing data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/83e084f3eac057def689d6f91db90f08"><gtr:id>83e084f3eac057def689d6f91db90f08</gtr:id><gtr:otherNames>Huckvale, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58bd60a23b9e71.40839845</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M026698/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>