<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/F758E573-143B-4511-B166-0D33F7F4042F"><gtr:id>F758E573-143B-4511-B166-0D33F7F4042F</gtr:id><gtr:name>Cochlear Ltd.</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:department>Faculty of Engineering &amp; the Environment</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F758E573-143B-4511-B166-0D33F7F4042F"><gtr:id>F758E573-143B-4511-B166-0D33F7F4042F</gtr:id><gtr:name>Cochlear Ltd.</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7D1FF4C7-DD25-4073-920E-B040B150B711"><gtr:id>7D1FF4C7-DD25-4073-920E-B040B150B711</gtr:id><gtr:name>Google Inc</gtr:name><gtr:address><gtr:line1>1600 Amphitheatre Parkway</gtr:line1><gtr:line2>Building 42</gtr:line2><gtr:postCode>94043</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/623195FD-CF8C-4F60-8229-8836431347C6"><gtr:id>623195FD-CF8C-4F60-8229-8836431347C6</gtr:id><gtr:name>Siemens AG</gtr:name><gtr:address><gtr:line1>Corporate Headquarters</gtr:line1><gtr:line2>Siemens Aktiengesellschaft</gtr:line2><gtr:line3>Wittelsbacherplatz 2</gtr:line3><gtr:line4>Munich</gtr:line4><gtr:line5>80333</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/67BD6B34-03B6-43FE-9201-2DBD6B48CA03"><gtr:id>67BD6B34-03B6-43FE-9201-2DBD6B48CA03</gtr:id><gtr:firstName>Matthew</gtr:firstName><gtr:otherNames>Christian Martin</gtr:otherNames><gtr:surname>Wright</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/78330133-F572-4D1F-B1AA-0AD09F7E1090"><gtr:id>78330133-F572-4D1F-B1AA-0AD09F7E1090</gtr:id><gtr:firstName>Arkadiusz</gtr:firstName><gtr:surname>Stasiak</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D15067AF-CF52-4AF5-8432-8FC9036D7E78"><gtr:id>D15067AF-CF52-4AF5-8432-8FC9036D7E78</gtr:id><gtr:firstName>Thomas</gtr:firstName><gtr:surname>Blumensath</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F9A17E4E-748A-43E3-A02B-EE4FF8EAFAE0"><gtr:id>F9A17E4E-748A-43E3-A02B-EE4FF8EAFAE0</gtr:id><gtr:firstName>Jessica</gtr:firstName><gtr:otherNames>Josephine</gtr:otherNames><gtr:surname>Monaghan</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A40BCCE9-40B9-47DA-B585-C38AA64A9A02"><gtr:id>A40BCCE9-40B9-47DA-B585-C38AA64A9A02</gtr:id><gtr:firstName>Stefan</gtr:firstName><gtr:surname>Bleeck</gtr:surname><gtr:orcidId>0000-0003-4378-3394</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/E5239B3D-8C4E-44A3-AFFA-739534AC850F"><gtr:id>E5239B3D-8C4E-44A3-AFFA-739534AC850F</gtr:id><gtr:firstName>Ian</gtr:firstName><gtr:surname>Winter</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK020501%2F1"><gtr:id>74E2CA86-B74A-4033-88EA-8BBFD71F3C73</gtr:id><gtr:title>Designing better hearing aids using physiologically inspired speech enhancement</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K020501/1</gtr:grantReference><gtr:abstractText>Hearing aids can profoundly transform the lives of people with hearing impairments but in the UK alone about 6 million such people do not use them. An important reason for this is that conventional hearing aids don't make it easy to distinguish meaningful signals such as speech from background noise. The combination of a fully functioning ear and the brain are fantastically good at this job and easily outperform the best computerized speech-recognition apps when even just a small amount of noise is present. A conventional hearing aid amplifies both the speech and noise indiscriminately, so even though the neural pathways of the brain may be unimpaired the task of distinguishing speech from noise becomes much harder. Various approaches to automatic speech enhancement have been tried but none comes close to what nature can do.
Despite the demand for better solutions, recent progress in research and development has been slow and no breakthrough technology has yet emerged. Speech enhancement strategies have been generally developed on the basis of mathematical, but not physiological principles. These methods, although based on fundamentally different strategies, have two things in common: first, they operate on signal features that rely primarily on the signals' local energy, and second, they have not improved speech intelligibility. Here, we propose overcoming this conceptual barrier by developing engineering solutions to the speech-in-noise problem that are based on physiological principles. The same technology will also be of benefit for automatic speech recognition systems, since the problems of both are similar.
We expect to see direct applications of our work to be implemented in hearing aids within the next 5 years. Two of the biggest companies in the world in their field (Siemens for hearing aids and Google for signal processing) demonstrate the interest, support and the confidence toward this approach. We have substantial experience with the whole development cycle: we have invented, designed, evaluated and implemented a noise reduction scheme that will be part of the next generation of Cochlea Ltd. cochlear implants. 
Our central hypothesis in this proposal is that the brain uses sparse coding when distinguishing meaningful signals from noise and it uses a dynamic dictionary for sound representation. We are going to investigate this coding mechanism in individual neurons in the auditory brainstem, and based on the results, will develop novel signal-processing strategies. We expect that these algorithms will be better than conventional algorithms and consequently can help hearing impaired. An animal model is essential to this project, because it is impossible to study responses of individual neurons from the auditory brainstem in humans. Sparse 
Neuron adapt their response because they have a limited dynamic rate which they constantly optimize in response to the environment in order to reduce redundancy and to maximise the information flow. In this project, we are going to extend the description of static neural response patterns to include a time varying and context sensitive components and we will measure these dynamic responses in single neurons in the brain stem. Knowing how neuronal responses change in noise will enable us to create a dynamic dictionary that can be used for sparsification. We expect that in this representation speech and noise is separable. 
We will use these dynamic response pattern as the basis of a novel transformation and sparsification in order to enhance the components of speech that are relevant for understanding, thus improving speech intelligibility without reducing the quality. 
We will evaluate the algorithm in substantial clinical trials.</gtr:abstractText><gtr:potentialImpactText>The total annual cost of hearing impairment in the EU has been estimated to be Euro224 Billion (EuroQol: Euro16K/QALY). In the UK alone 6 million more people could benefit from a hearing aid (HA) (&amp;pound;18 Billion p.a.). These costs are mainly a consequence of economical inactivity due to hearing loss.
We have designed a programme that will lead to better hearing aids. We present a programme that integrates physiological recordings, signal processing and evaluation in a coherent work plan. Letters from two of the biggest companies in the world in their field (Siemens for hearing aids and Google for signal processing) demonstrate the interest and support that the world leading industry has toward this approach. We therefore expect to see direct applications of our work to be implemented in hearing aids within the next 5 years. 
Results of this research are designed to break this deadlock and create the impulse for a step change in the signal processing research. Application of the results of this research can therefore lead to improved signal processing algorithms that will enable hearing aid users to better communicate. The results of this project can directly improve the quality of life for 250 million people around the world.
Hearing aids are a market worth around billions world-wide and progress in signal processing is closely watched by the hearing aid industry. Siemens, one of the biggest hearing aid company in the world has expressed the confidence in our leadership and their interest in this work. Siemens, as well as other hearing aid companies have a direct interest to improve hearing aids to remain competitive in the market and they have a track record of quickly implementing novel signal processing strategies. We are aware of IP-issues, and we have established good communication channels to avoid conflicts in future. It is planned to patent results as soon as possible. IP will remain with the University of Southampton and the University of Cambridge.
The proviion of hearing aids is a big part of the NHS budget and attempts are made in various ways to reduce costs. The policy of central tendering by the NHS supply chain enables the cost effective provision of huge numbers of hearing aids, however, a big burden on the cost in this package is that large numbers of aids are not used by wearers after disappointing results soon after consultation and allocation. There is therefore a huge potential for higher efficiency by improving the quality of aids with the side effect of increased hearer satisfaction and time of wearing.
The programme is aligning technical developments with matching experimentation. These experiments will result in useful data for the development for novel signal processing algorithms and can be used in many labs all over the world. The following research streams are also interesting for a variety of researchers: 
- The sparse coding algorithm is interesting for several interest groups. It can be used in hearing aids, but also in cochlear implants, signal compression schemes and communication devices like phones. Google, one of the biggest companies in the world describes in their letter of support how important new research approaches in this area are.
- The neural data collected will be extremely interesting to neuroscientists interested in sensory neural coding and these experiments will be closely scrutinized by the neuroscience community worldwide. 
This programme is of an essential interdisciplinary nature. The success depends crucially on the demonstrated quality and variety of the background of its members. The success of the programme will be measurable by the number of publications in peer reviewed journals and conference presentations of results in national and international conferences.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-02-06</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>613105</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Cochlear Ltd.</gtr:collaboratingOrganisation><gtr:country>Global</gtr:country><gtr:description>Industrial collaboration with Cochlear</gtr:description><gtr:id>1F07503A-4167-4B79-9FEA-A7AD117E82CA</gtr:id><gtr:impact>several reviewed conference papers (see publication list).</gtr:impact><gtr:outcomeId>56c1f358638d56.05053059-1</gtr:outcomeId><gtr:partnerContribution>we gave Cochlea a novel algorithm
Cochlea is undertaking the Psychophysical measurements</gtr:partnerContribution><gtr:piContribution>we have established a collaboration with the Cochelar implant company &amp;quot;cochlear&amp;quot;. We are developing signal processing algorithms that can reduce noise and improve speech. They are interested in finding new ways to improve speech intelligibility for CI users. Under the leadership of me and our PostDoc in Southampton (Dr Jessica Monaghan) we have developed a neural network based algorithm that is now tested in CI patients. This might lead to a new system and patents.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>University of Southampton Science and Engineering day</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>5A785FE5-3F3F-4AE2-A84C-4EE76E79E9A3</gtr:id><gtr:impact>this is an outreach activity where all of our postdoctoral staff participates. We reach &amp;gt;100 people, mostly children and inform about the dangers of hearing loss.
Children and adults alike are often very interested and report a change in behaviour in that they will be more careful with noise exposure.</gtr:impact><gtr:outcomeId>56c1f4b58cbac1.90423850</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2014,2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>British Science Festival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>22BDE6ED-F139-4EA2-9BA8-3A62320BBC27</gtr:id><gtr:impact>more than 100 school children attended at our stall at the festival. 
We organized three interactive hands on workshops where people could experience hearing loss, hearing aids or cochlear implants.

many kids were astonished to hear about the impact of hearing loss on their lives or lives of relatives.</gtr:impact><gtr:outcomeId>54185236af9550.37650462</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Deafness Research UK ARO TRAVEL FELLOWSHIPS</gtr:description><gtr:end>2013-02-02</gtr:end><gtr:fundingOrg>Action on Hearing Loss</gtr:fundingOrg><gtr:id>A044EAEA-F222-46FC-BFC5-9513EC1D9727</gtr:id><gtr:outcomeId>5464bde6cd2ab0.73450528</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2013-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>AVRITH TRAVEL GRANT SCHEME</gtr:description><gtr:end>2014-02-02</gtr:end><gtr:fundingOrg>University of Cambridge</gtr:fundingOrg><gtr:id>DC114096-7A20-4A66-9A7A-C51212E5FCE3</gtr:id><gtr:outcomeId>5464be1950b580.60529027</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our development of a deep neural network that can enhance speech in noisy environments has been shown to help cochlear implant users. This method has been recognised as the most promising signal processing method to help hearing impaired people to communicate better in noise. Hearing aid companies, cochlear implant companies and producers of 'audibles' are investigating this method now, and we helped improving and publicizing this.</gtr:description><gtr:id>6ABA496F-8D18-47F7-9A62-17ECC9E29521</gtr:id><gtr:impactTypes/><gtr:outcomeId>56c496e5234364.19272302</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The key findings in short are: 
1) We have developed a deep neural network, based on our understanding of the human auditory periphery, that can enhance speech for hearing impaired people in noisy situations. We have tested the network it in CI users and hearing impaired. Both experiments were successful and published in peer reviewed papers. Our industrial partner is investigating using the network in their products, potentially helping millions of CI users in the future.
2) we have gained a better understanding of the neural responses in the auditory brainstem where we gained not only insights into single neural responses to complex sounds, but we also suggested a novel classification scheme based on these insights. The findings were submitted to a peer reviewed journal.
3) we have investigated a novel method of speech enhancement based on sparse coding that was directly informed by responses of auditory brain stem neurons. This algorithm was tested and proved not as successful as we hoped.</gtr:description><gtr:exploitationPathways>We have developed a much more systematic understanding of the responses in the auditory brain stem to complex sounds. Our methods of analysing the responses in form of spatio-temporal receptive fields has been published and is already influencing the way other labs are describing neuronal responses. Furthermore, we have established a novel way of characterising neuronal response classes in the auditory brainstem based on a new technique using intervals rather than averaged spike times. We expect that this method will gain track in future and will become the (or at least part of the) standard way of classifying neuronal responses.</gtr:exploitationPathways><gtr:id>9DC570CB-56F2-455B-A6C7-CE14DCCC91B0</gtr:id><gtr:outcomeId>56c496816740f5.23041068</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>In auditory signal processing research, we are often interested in algorithms that modify audible sounds, for example to increase speech intelligibility. Oftentimes, it is interesting and sufficient in an experiment to process such sounds offline and play them back to participants at a later stage. This approach has the advantage of full control over all parameters including background noise. However, in many situations it would be beneficial to have a more direct approach, by being able to manipulate sounds in real time. Real time processing of environmental sounds means giving up on controlling all environmental parameters, but gaining highest possible ecological validity, that is to test the participant in unpredictable situations as they would face in real life. This approach is important for example for hearing aid noise reduction algorithms, where the wearers outside of the lab encounter unpredictable situations. 
In a typical development cycle, an algorithms' benefit must first be demonstrated in laboratory situations, before going real time. However, only when knowing the interaction between algorithm and participant in real live situations, the algorithm can be fully evaluated for it's usefulness. In fact, most algorithms that have shown benefit in laboratory situations were either never evaluated in real time environments or failed to live up to their promise. However, in order to develop algorithms that have potential to be implemented in the next generation of auditory devices, they must be tested in all situations, but testing real time algorithms in real live situations is difficult for many reasons. Firstly, programming them is difficult, as the necessary programming skills are not the same as for offline programming. Secondly, it often requires specialised hardware that is suited and fast enough to do the processing.
For signal processing research it would be beneficial to have a platform that helps to bridge the development from existing offline algorithms to real time. 
proposed model:
We present here a comprehensive solution: a platform that enables current users of matlab to quickly port their algorithms to real time. We provide an open library that can be used freely and that will grow in future. The algorithms can run on standard PC hardware or on special hardware when required. The platform is the result of a European Training network (www.icanhear.eu), and all seven network partners have contributed to the library. The software is distributed freely via soundsoftware.ac.uk, where the library as well as supporting documentation can be downloaded for free under a fair use licence.
The library is open, that is, everybody (who is registered on the web page) can add algorithms.</gtr:description><gtr:id>4646BBBE-8D06-4E2D-B569-01174221D85A</gtr:id><gtr:impact>The platform is publicly available on the internet (www.soundsoftware.co.uk)
Other groups have tested our platform and investigate at the moment if they are going to use it.</gtr:impact><gtr:outcomeId>56c1f5f2c2d766.24015490</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Open Development Platform</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/odp-open-developer-platform-for-auditory-research</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9AEE11C2-6F3E-4571-A251-09951175DE65</gtr:id><gtr:title>Speech enhancement based on neural networks applied to cochlear implant coding strategies</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ddda2c73e6c5e1560cc0afda07a944e0"><gtr:id>ddda2c73e6c5e1560cc0afda07a944e0</gtr:id><gtr:otherNames>Bolner F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca625e6da302.03416850</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A09C9CA3-7D9F-4F97-855C-03FFAD690485</gtr:id><gtr:title>Response to best-frequency tone bursts in the ventral cochlear nucleus is governed by ordered inter-spike interval statistics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/20143e203813f3ea4ad5246c7e8f6400"><gtr:id>20143e203813f3ea4ad5246c7e8f6400</gtr:id><gtr:otherNames>Wright, MCM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54184ec97a8a56.71353768</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D76FC77-B482-45A2-92BD-2D20F9F5E49A</gtr:id><gtr:title>Improving speech intelligibility in perceptual wavelet packet-based speech coding for cochlear implants</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6febe8858ce41df2ed198c01556dd215"><gtr:id>6febe8858ce41df2ed198c01556dd215</gtr:id><gtr:otherNames>Dachasilaruk S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58ca5f3eab5ad7.65352059</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AC8EAC3F-104B-4818-B2E8-18FCA9471B0A</gtr:id><gtr:title>Do Hearing Impaired Individuals tolerate different output delay lengths than normal hearing individuals?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01a1676d156fd79ac8773487df19c742"><gtr:id>01a1676d156fd79ac8773487df19c742</gtr:id><gtr:otherNames>Goehring T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54184f6eee0b20.46882755</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2470ED96-8197-4C29-979F-C535E776C33D</gtr:id><gtr:title>Evaluation of the sparse coding shrinkage noise reduction algorithm in normal hearing and hearing impaired listeners.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1ab18c7ec309d36ef916f8f3dc52745e"><gtr:id>1ab18c7ec309d36ef916f8f3dc52745e</gtr:id><gtr:otherNames>Sang J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>54184e8bb2e3a5.55262179</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7F9EE6E7-245F-4E47-9910-02BAA304C914</gtr:id><gtr:title>Enhancement of forward suppression begins in the ventral cochlear nucleus.</gtr:title><gtr:parentPublicationTitle>Brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45b7452f781a3710974f8edca0be07d6"><gtr:id>45b7452f781a3710974f8edca0be07d6</gtr:id><gtr:otherNames>Ingham NJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0006-8993</gtr:issn><gtr:outcomeId>58ca61e1bb1835.34918425</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A39A51A-7C1F-45CF-B4CF-27E4B71107E9</gtr:id><gtr:title>Tolerable delay for speech production and perception: effects of hearing ability and experience with hearing aids.</gtr:title><gtr:parentPublicationTitle>International journal of audiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01a1676d156fd79ac8773487df19c742"><gtr:id>01a1676d156fd79ac8773487df19c742</gtr:id><gtr:otherNames>Goehring T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>1499-2027</gtr:issn><gtr:outcomeId>5a2fe6cf0a7b44.06109431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E70F3164-8652-4341-BFA1-0348855BA2FB</gtr:id><gtr:title>Non-negative matrix factorization on the envelope matrix in cochlear implant</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f18113a2d750140919bbc345f2f67feb"><gtr:id>f18113a2d750140919bbc345f2f67feb</gtr:id><gtr:otherNames>Hu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>58ca5f3f12c7c1.87522507</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A3298E8-AFE0-48AB-BA95-E88AE32C7BF7</gtr:id><gtr:title>The temporal responses of single units in the ventral cochlear nucleus to conspecific vocalisations in normal hearing or following a mild sensorineural hearing loss</gtr:title><gtr:parentPublicationTitle>ARO Conference, Baltimore, Maryland, USA, 16-20 February 2013.</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c7d6cab83e11be292434cdfd651da3b2"><gtr:id>c7d6cab83e11be292434cdfd651da3b2</gtr:id><gtr:otherNames>Stasiak, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464bc5f7716c8.65366431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D726DE5D-A99A-48A4-9D1C-5BB9EB92B713</gtr:id><gtr:title>Sensitivity to Envelope Interaural Time Differences at High Modulation Rates.</gtr:title><gtr:parentPublicationTitle>Trends in hearing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/84de54adcf00d20700aea0f1160af83d"><gtr:id>84de54adcf00d20700aea0f1160af83d</gtr:id><gtr:otherNames>Monaghan JJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>2331-2165</gtr:issn><gtr:outcomeId>56be0fd8b91c87.32559104</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>24262181-9E07-4925-9F3F-3C2A402A60D1</gtr:id><gtr:title>Real time processing of deep neural network based speech enhancement algorithm with Raspberry Pi</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2defdcf76c50001e261d02a7b536a6b9"><gtr:id>2defdcf76c50001e261d02a7b536a6b9</gtr:id><gtr:otherNames>Wisenschinda, Varut</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca5fa8276402.19051972</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26CFF321-D4CC-430D-B9C0-574B76781361</gtr:id><gtr:title>Response to best-frequency tone bursts in the ventral cochlear nucleus is governed by ordered inter-spike interval statistics.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7da11578445c6ab99391e74f3e895c23"><gtr:id>7da11578445c6ab99391e74f3e895c23</gtr:id><gtr:otherNames>Wright MC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>56be0fd8e2a025.17188742</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F22533C5-6633-4E9E-9521-8C537B6965F5</gtr:id><gtr:title>Evaluating Noise Reduction Algorithms for Hearing Impaired listeners</gtr:title><gtr:parentPublicationTitle>ARO Midwinter Meeting</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/17ab038f110220d20f03d5ea3dfc3b84"><gtr:id>17ab038f110220d20f03d5ea3dfc3b84</gtr:id><gtr:otherNames> Jessica Monaghan (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_912302995713e53810</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>471579A0-FC8E-4D10-87FE-AE68E8C25C7A</gtr:id><gtr:title>Modelling the performance of listeners in a digits-in-noise-speech recognition task</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/587547cf777361be5652c7eb5dfebbf7"><gtr:id>587547cf777361be5652c7eb5dfebbf7</gtr:id><gtr:otherNames>Monaghan JJM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54184fcbdb3e24.01121204</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C62F8EF1-CE4F-46F1-B1AE-78550D4AC9B6</gtr:id><gtr:title>Android application of hearing aids on google glass</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6e7f5402dea7d3fc36fa42eb9fbac5c"><gtr:id>e6e7f5402dea7d3fc36fa42eb9fbac5c</gtr:id><gtr:otherNames>Gautam A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54184f8d93fff7.61087016</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D1177F1D-B973-4CD0-9F8B-07BEECFF8636</gtr:id><gtr:title>A Real-time Hearing Aid Simulator on the Open Development Platform</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01b9bc8b57a0ba34caccb89dd0da9878"><gtr:id>01b9bc8b57a0ba34caccb89dd0da9878</gtr:id><gtr:otherNames>Yang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54184facc1d917.14731951</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1D192539-46CD-4725-A2EF-4268DC5A02F3</gtr:id><gtr:title>A novel method for predicting speech intelligibility based counting time-frequency bins</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/83cc5b16e58f9f35edc52d3fa81ccd7d"><gtr:id>83cc5b16e58f9f35edc52d3fa81ccd7d</gtr:id><gtr:otherNames>Kim SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54184ff6caa3c5.82602919</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5F6238B0-DEB2-4F4C-B516-4B5B7696B016</gtr:id><gtr:title>Sparse Nonnegative Matrix Factorization Strategy for Cochlear Implants.</gtr:title><gtr:parentPublicationTitle>Trends in hearing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f18113a2d750140919bbc345f2f67feb"><gtr:id>f18113a2d750140919bbc345f2f67feb</gtr:id><gtr:otherNames>Hu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>2331-2165</gtr:issn><gtr:outcomeId>56be0fd837d762.00728515</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E0F25017-0ECA-4216-A7AF-E45F69791D46</gtr:id><gtr:title>Speech enhancement based on neural networks improves speech intelligibility in noise for cochlear implant users.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01a1676d156fd79ac8773487df19c742"><gtr:id>01a1676d156fd79ac8773487df19c742</gtr:id><gtr:otherNames>Goehring T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>58ca5f3e7886b7.46368478</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B68712C2-A259-4AA4-AD7D-5C90429B707D</gtr:id><gtr:title>Sensitivity to envelope ITDs at high modulation rates envelope domain</gtr:title><gtr:parentPublicationTitle>ARO Midwinter Meeting</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/36999f0b116494e74d3826eade5cece3"><gtr:id>36999f0b116494e74d3826eade5cece3</gtr:id><gtr:otherNames>McAlpine D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54523faeb0c964.06013917</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2DFC8D33-BFCD-4C9B-98C5-AE5890E423C5</gtr:id><gtr:title>Auditory inspired machine learning techniques can improve speech intelligibility and quality for hearing-impaired listeners.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/84de54adcf00d20700aea0f1160af83d"><gtr:id>84de54adcf00d20700aea0f1160af83d</gtr:id><gtr:otherNames>Monaghan JJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>58ca619b46d3b5.14701215</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FC96CA88-CB54-47BE-8E55-52FF45EB23EE</gtr:id><gtr:title>Neural network based speech enhancement applied to cochlear implant coding strategies</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01a1676d156fd79ac8773487df19c742"><gtr:id>01a1676d156fd79ac8773487df19c742</gtr:id><gtr:otherNames>Goehring T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56be0fd804de70.18662129</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E81A7E25-91D9-447D-975D-2B79C87CD34F</gtr:id><gtr:title>Estimating Spectral-Temporal Receptive Fields in the Cochlear Nucleus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c2d995939506e713506444d7e0ca7637"><gtr:id>c2d995939506e713506444d7e0ca7637</gtr:id><gtr:otherNames>ARek Stasiak  (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_178441619113d89484</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>219E1689-1432-4023-BB9C-4EB24B0B0353</gtr:id><gtr:title>Enhancement of forward suppression begins in the ventral cochlear nucleus</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eba0bf182145da699ef991122a25866f"><gtr:id>eba0bf182145da699ef991122a25866f</gtr:id><gtr:otherNames>Winter I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd54d15d1190.49799832</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E62A0047-7D4C-439C-A17F-CFDC7DC51062</gtr:id><gtr:title>Speech quality evaluation of a sparse coding shrinkage noise reduction algorithm with normal hearing and hearing impaired listeners.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1ab18c7ec309d36ef916f8f3dc52745e"><gtr:id>1ab18c7ec309d36ef916f8f3dc52745e</gtr:id><gtr:otherNames>Sang J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>56be0fd862a2a5.62180622</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D5BCCF5E-6F2E-4C14-AE25-3250EA41AC3A</gtr:id><gtr:title>Development of a real time sparse non-negative matrix factorization module for cochlear implants by using xPC target.</gtr:title><gtr:parentPublicationTitle>Sensors (Basel, Switzerland)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f18113a2d750140919bbc345f2f67feb"><gtr:id>f18113a2d750140919bbc345f2f67feb</gtr:id><gtr:otherNames>Hu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1424-8220</gtr:issn><gtr:outcomeId>58ca5f3eda16d5.78148100</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K020501/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>