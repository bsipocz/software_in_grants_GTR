<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/44160F04-5CBF-4E8E-A6C6-C0EF61A5865C"><gtr:id>44160F04-5CBF-4E8E-A6C6-C0EF61A5865C</gtr:id><gtr:name>Lancaster University</gtr:name><gtr:address><gtr:line1>University House</gtr:line1><gtr:line4>Lancaster</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>LA1 4YW</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7A0397DD-E0C6-4EA3-8031-B841D2503C4D"><gtr:id>7A0397DD-E0C6-4EA3-8031-B841D2503C4D</gtr:id><gtr:name>Royal Holloway, University of London</gtr:name><gtr:address><gtr:line1>Egham Hill</gtr:line1><gtr:line4>Egham</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>TW20 0EX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6440003E-604E-4453-8049-CAF28E2FE8EE"><gtr:id>6440003E-604E-4453-8049-CAF28E2FE8EE</gtr:id><gtr:name>Matrix-Data Ltd</gtr:name><gtr:address><gtr:line1>Matrix-Data Ltd</gtr:line1><gtr:line2>New London House</gtr:line2><gtr:line3>172 Drury Lane</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>WC2B 5QR</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C29F5E21-9A4A-4904-B47D-A257EB274AD8"><gtr:id>C29F5E21-9A4A-4904-B47D-A257EB274AD8</gtr:id><gtr:firstName>Samer</gtr:firstName><gtr:surname>Abdallah</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/01B67481-AD50-4885-A91E-5975AD7B62C3"><gtr:id>01B67481-AD50-4885-A91E-5975AD7B62C3</gtr:id><gtr:firstName>Juan</gtr:firstName><gtr:otherNames>Pablo</gtr:otherNames><gtr:surname>Bello</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE017614%2F1"><gtr:id>1C028C12-B2C5-4149-9D5D-5FD6F0AE848F</gtr:id><gtr:title>OMRAS2: A Distributed Research Environment for Music Informatics and Computational Musicology</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E017614/1</gtr:grantReference><gtr:abstractText>Imagine you have just bought a new iPod, you rip loads of your dad's CDs into it (his music's cool) as well as your own, and pretty soon you have 10,000 tracks and the iPod is full. Now there's a problem. You've never listened to your dad's CDs (not that many of them anyway) and you're really not sure what The Human League sounds like, and there's another 500 CDs of his music in there. Where are the good songs? How can you ever build those really cool playlists to impress your friends with your vast musical knowledge?Online Music Recognition and Searching II A Distributed Framework for Music Informatics and Computational Musicology.Imagine you've just been given a gist subscription to a 2 million song online music store. You can choose 10,000 songs to download onto your music player, but there's a problem. You have never heard a vast majority of these songs so you're not sure which are the one's you like. How can you put together those playlists to impress your friends with your vast musical knowledge?The problem is simular for the radio DJ looking for a new playlist to keep their show on the cutting edge, or the professional violinist doing research into different performances of Vivaldi'd Four Seasons to find a new twist for an expectant audience, or the recors producer trying to find a mathimatical formula for number one singles (yes, they really do this).The answer to the above question and other interesting problems concerning large collections of digital music are exactly what the OMRAS2 project will address. When OMRAS2 is completed, you'll be able to get software that helps you build playlists with songs that you'll love even though you never heard them before; and there will be tools to help the violinist and record producer achive their goals too. Using tools from OMRAS2, your ipod will be able to predict the best sounds to use for the best chart topping number one. If you study music at University, you'll probably use OMRAS2 for analysing and comparing music.OMRAS2 aims to help technology researchers build and investigate the software that is needed to construct these super-tools. But that's not all. It will help musci researchers investigate interesting aspect of music, such as what variations of that riff in Purple Haze did Jimi Hendrix play and how did the differ, and how did different pianists interpret Bach's Goldberg Variations. OMRAS2 will also look deeply at how music and information about music (like CD Insert booklets, but more and online) will be enjoyed at home, not just downloading, but also searching, recomending, browsing and so on. And it wont be hard to use:OMRAS2 will use interfaces that look and react like familiar music software like Adobe Audition or RealAudio player.</gtr:abstractText><gtr:fund><gtr:end>2010-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1447586</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Goldsmiths, University of London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Goldsmiths College</gtr:description><gtr:id>F5532708-E6B5-490E-A051-DE94A21DADCD</gtr:id><gtr:impact>The Transforming Musicology grant from AHRC is one outcome.</gtr:impact><gtr:outcomeId>b99b2bd4b99b2be8-1</gtr:outcomeId><gtr:partnerContribution>Research knowhow, co-writing papers, access to software, joint grant proposals.</gtr:partnerContribution><gtr:piContribution>Various academics from Goldsmiths have collaborated with QM over the years, including Prof T Crawford, Prof A Tanaka, Prof M D'Inverno</gtr:piContribution><gtr:sector>Academic/University</gtr:sector></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>94894</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Jisc</gtr:description><gtr:fundingOrg>Jisc</gtr:fundingOrg><gtr:id>0BD3FDC2-BC87-4C6D-BB24-C6A3CCCFB7D3</gtr:id><gtr:outcomeId>5ec94dd65ec94dea</gtr:outcomeId><gtr:sector>Public</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>83127</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Follow on fund</gtr:description><gtr:end>2011-02-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/H008160/1</gtr:fundingRef><gtr:id>9F1AB6EF-24E9-4745-ABF5-70F31BA7EA2D</gtr:id><gtr:outcomeId>5ed6263c5ed62650</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>947047</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Software sustainability</gtr:description><gtr:end>2014-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/H043101/1</gtr:fundingRef><gtr:id>2628D82D-64D2-4425-A709-C87710D3034F</gtr:id><gtr:outcomeId>5ed9b9005ed9b91e</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>250102</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Digital Economy Research in the Wild</gtr:description><gtr:end>2012-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/I001832/1</gtr:fundingRef><gtr:id>119E2964-E977-4BEC-B69F-1330AA961BE5</gtr:id><gtr:outcomeId>5ed6df325ed6df46</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>25000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>The Royal Society Wolfson Research Merit Award</gtr:description><gtr:end>2020-03-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>to be confirmed.</gtr:fundingRef><gtr:id>185A30B2-FD1F-4C3D-91ED-CA4967310399</gtr:id><gtr:outcomeId>56d455a7154a04.34930255</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>This software analyses musical audio files and divides them into musically meaningful segments, typically 4-10 per song.</gtr:description><gtr:grantRef>EP/E017614/1</gtr:grantRef><gtr:id>0CA1AD72-1BC6-4E7B-83EB-520CEDD60041</gtr:id><gtr:impact>It is often downloaded with Sonic Visualiser - see other IP entry</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>m-7794994560.810732766576a2</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Audio Segmentation VAMP plugin</gtr:title><gtr:yearProtectionGranted>2008</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Soundbite is an iTunes plugin developed under this project. It is currently available from www.isophonics.net as freeware. Commercial licences are under negotiation. Under the Platform Grant it was adapted for an Android platform and reconstructed as a client-server architecture.

2017: A commercial licence for the core technology that was signed with Music XRay, a company based in the USA, several years ago is starting to yield royalties.</gtr:description><gtr:grantRef>EP/E017614/1</gtr:grantRef><gtr:id>58C43486-7A9B-43B9-8861-0751FA7DB78B</gtr:id><gtr:impact>It has been downloaded to hundreds of (unregistered) users. It was deployed in various grants to analyse collections of music for recommendation, navigation and play listing. It has been deployed in several commercial music aggregators workflows, enabling trials of large scale music recommendation systems.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>r-2240247775.0929523d6870ae</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Soundbite</gtr:title><gtr:yearProtectionGranted>2008</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Sonic Visualiser is an application for viewing and analysing the contents of music audio files. This is an open source software framework that works with its own plugin architecture called VAMP. Sonic Visualiser is Free Software, distributed under the GNU General Public License (v2 or later) and available for Linux, OS/X, and Windows.

Sonic Visualiser is intended for use by people in various academic disciplines, but also by non-academics, professional audio users, and interested hobbyists. It supports loading additional analysis plugins that other developers or institutions can publish, and at least 10 institutions (besides QM) have chosen to publish plugins in this format. Sonic Visualiser was first published by the Centre for Digital Music in 2007 and has been continuously developed at the Centre and maintained as free, cross-platform open-source software ever since.</gtr:description><gtr:grantRef>EP/E017614/1</gtr:grantRef><gtr:id>7448DF6D-0541-4A0F-AA21-705633407F73</gtr:id><gtr:impact>According to our ongoing user survey that has been carried out since the start of 2014 (having 1053 responses to date, all from people who were actually using Sonic Visualiser at the time):

 * 49% of respondents use Sonic Visualiser primarily in a personal capacity, 10% for professional work, 33% in academia at some level (the rest &amp;quot;other&amp;quot;);
 * 46% use it primarily in the field of musicology or music analysis, 19% in music composition or production, 16% in audio engineering or signal processing, 5% in software development, 2% speech processing (the rest &amp;quot;other&amp;quot;);
 * 55% said they found it &amp;quot;very enjoyable&amp;quot; to use and 41% &amp;quot;moderately enjoyable&amp;quot;;
 * 27% said they found it &amp;quot;very easy&amp;quot; to use and 62% &amp;quot;moderately easy&amp;quot;;
 * 59% of respondents had some additional analysis plugins installed on top of the default configuration; among these respondents, the mean number of additional plugin sets installed was 3.4.

Sonic Visualiser's version-update-checker logs keep a count of distinct IP addresses from which SV has been used (for users who have agreed to this). They show over 138,000 distinct addresses during the past year, with around 4,000 distinct addresses per week on average.

An impression of the extent of usage can be gained by viewing illustrative Youtube videos about sound analysis made by third parties:
at a quick scan I count at least 100 that show Sonic Visualiser, of varying quality. Sonic Visualiser has also been used in online course material, such as the popular Coursera &amp;quot;Audio Signal Processing for Music Applications&amp;quot;, as well as in courses for academics in non-technical disciplines, such as the Digital Humanities at Oxford Summer School.

This is a free, open source project; it was used as an Impact Case study for REF 2014.

See the Sonic Visualiser website for details: http://sonicvisualiser.org 

Link to a page about the new release: http://sonicvisualiser.org/new-in-v3.html

Sonic Visualiser has been available for about a decade now, and this is one of the most substantial updates it's ever had (hence the updated major release number).</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>m-5840728334.93419776656fcc</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Sonic Visualiser</gtr:title><gtr:yearProtectionGranted>2006</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>The project demonstrated:

(1) the utilities of high-level semantic features of musical audio (including musicological terms and free-form labels such as social tags) in multimedia content management,

(2) the use of low-level audio features and probabilistic statistical models to derive high-level semantic descriptors automatically, facilitating navigation in large online audio collections,

(3) the utilities of the Semantic Web, and Semantic Web technologies in online audio content navigation and delivery,

(4) the use of digital signal processing and machine learning for the manipulation of digital audio content on the semantic level, allowing interaction with notes, chords or performance characteristics such as vibrato by re-synthesising audio from parametrised descriptors.



Several computational algorithms were developed for automatic annotation of musical audio, including novel methods for audio transcription, chord recognition, key recognition, tempo and beat detection, structural segmentation and music similarity. 



Several Semantic Web ontologies were created for describing and publishing music related metadata on the Semantic Web. The Music Ontology, a core framework connecting the OMRAS2 ontologies became a de-facto standard in music-related data publishing.</gtr:description><gtr:exploitationPathways>The are numerous potential uses of OMRAS 2 technologies in a non-academic context. This includes music search engines that are based on audio similarity characterised by acoustic features of sound, search engines and Semantic Web user agents that access audio archives by high level semantic concepts. These include musicological terms such as keys, chords or rhythm, and social-contextual similarity like artist collaborations. These tools can be used in online services, content management platforms, libraries and archives, educational institutions, etc.. Lower level music signal processing tools developed by the project may be utilised in music production and delivery.
 The project developed several easy to use tools allowing the exploitation of project outcomes by academic communities and industry partners. These tools include Sonic Visualiser, and application for examining high or low-level features of sound in the context the audio waveform. Sonic Visualiser is equally useful for researchers working on audio signal processing and machine learning algorithms, as well as musicologists analysing audio collections. The program is supported by a C++/Python Application Programming Interface, that allows distributing content analysis algorithms in a standard plugin format, that can be used by several host applications, including audio editors and batch audio processors.</gtr:exploitationPathways><gtr:id>304ECE5D-0C59-41E0-B1E1-0B21CD6CDB18</gtr:id><gtr:outcomeId>r-2233785644.3913527760bd5c</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.omras2.org</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B8E0B875-D57D-4686-A51C-7A0C1D67DE18</gtr:id><gtr:title>Content-Based Music Information Retrieval: Current Directions and Future Challenges</gtr:title><gtr:parentPublicationTitle>Proceedings of the IEEE</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c22771ab31e9e0f3a420753ef98952b9"><gtr:id>c22771ab31e9e0f3a420753ef98952b9</gtr:id><gtr:otherNames>Casey M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d05a05a72eb2f2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53D8CE76-6D4D-4365-8503-5551BCA01CE5</gtr:id><gtr:title>Semantics for Music Analysis through Linked Data: How Country is My Country?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cfee2fb14b0a517816212684336ce9ea"><gtr:id>cfee2fb14b0a517816212684336ce9ea</gtr:id><gtr:otherNames>Page K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-8957-2</gtr:isbn><gtr:outcomeId>doi_53d057057930fac9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E0B31ED-CEB9-4317-887E-A02595F6CE58</gtr:id><gtr:title>Report for Core Experiment on Dynamic Volume Change Representation for IM AF</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f26a9c5500f499fe8aa8c09c3b3bb40"><gtr:id>3f26a9c5500f499fe8aa8c09c3b3bb40</gtr:id><gtr:otherNames>Y. Zhu (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_263438468363ddb316</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5BFEE1E-87F6-4103-952E-242BCF6B3B06</gtr:id><gtr:title>Evaluation of the Audio Beat Tracking System BeatRoot</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6d76a8ab4323a3f8dd805c24e141802"><gtr:id>e6d76a8ab4323a3f8dd805c24e141802</gtr:id><gtr:otherNames>Dixon S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd20322c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D4DBD65F-BA92-4348-8BFD-F6B26922888D</gtr:id><gtr:title>Music Information Retrieval Using Social Tags and Audio</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/500eb111ee39c0d64026f276b07d6dae"><gtr:id>500eb111ee39c0d64026f276b07d6dae</gtr:id><gtr:otherNames>Levy M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d05e05e9dac73f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0DF10949-1009-44B0-9299-EC96A4AB41D3</gtr:id><gtr:title>An Overview of Semantic Web Activities in the OMRAS2 Project</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2fd53c70d717490415b8aa5cf9c39f79"><gtr:id>2fd53c70d717490415b8aa5cf9c39f79</gtr:id><gtr:otherNames>Fazekas G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd88bc44</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CDA28BF8-875B-417F-ADC5-7B93382E1C60</gtr:id><gtr:title>Estimation of harpsichord inharmonicity and temperament from musical recordings.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6d76a8ab4323a3f8dd805c24e141802"><gtr:id>e6d76a8ab4323a3f8dd805c24e141802</gtr:id><gtr:otherNames>Dixon S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>doi_53d06e06eb273c95</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E8A76F62-3222-4BAA-8220-9693F015CF74</gtr:id><gtr:title>The Temperament Police: The Truth, The Ground Truth, and Nothing but the Truth</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1112f209625f61ce481326c2f7f08219"><gtr:id>1112f209625f61ce481326c2f7f08219</gtr:id><gtr:otherNames>Simon Dixon (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_42890127961406c3c2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D9233D27-0FEE-4075-84E4-83DC5C4D7B3B</gtr:id><gtr:title>Analysis and Exploitation of Musician Social Networks for Recommendation and Discovery</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b2cb8aff4f61c237a650fc39b2510b0"><gtr:id>8b2cb8aff4f61c237a650fc39b2510b0</gtr:id><gtr:otherNames>Fields B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d05e05ea1136fe</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>152E4CF1-1305-4EC0-B4C7-F7C4F330BC7F</gtr:id><gtr:title>A New Interactive MPEG Format for the Music Industry</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/14077ab52e29aed689a0033c67cd3397"><gtr:id>14077ab52e29aed689a0033c67cd3397</gtr:id><gtr:otherNames>Panos Kudumakis (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_3331434531140089b2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BADF7FD2-C514-4963-900E-5C8EEEB352CF</gtr:id><gtr:title>Linked Data and You: Bringing Music Research Software into the Semantic Web</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de82f753a4c19dbf093d1cd4c2aecc91"><gtr:id>de82f753a4c19dbf093d1cd4c2aecc91</gtr:id><gtr:otherNames>Cannam C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd6e2e4e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9191C672-19EC-4E44-8D78-5F86C01D9276</gtr:id><gtr:title>Knowledge Representation Issues in Musical Instrument Ontology Design</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41ba96eda0bf1419e4bbab8819fa9df6"><gtr:id>41ba96eda0bf1419e4bbab8819fa9df6</gtr:id><gtr:otherNames>Sefki Kolozali (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_169768060814066224</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FABDCE1D-8ED9-499F-AFDB-02C71C96E61C</gtr:id><gtr:title>Source-filter modeling in sinusoid domain</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76bd81c0e8c87b01d63241a7398dd7a2"><gtr:id>76bd81c0e8c87b01d63241a7398dd7a2</gtr:id><gtr:otherNames>Wen Xue (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_22628849831385f0ee</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>789A582B-8297-4696-B60B-BFD16EA507D8</gtr:id><gtr:title>Analysis of Minimum Distances in High-Dimensional Musical Spaces</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c22771ab31e9e0f3a420753ef98952b9"><gtr:id>c22771ab31e9e0f3a420753ef98952b9</gtr:id><gtr:otherNames>Casey M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d05c05ce95d53b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>58A3D447-876F-452C-9BE3-A65064B354F4</gtr:id><gtr:title>High precision frequency estimation for harpsichord tuning classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97108c66f07effc5e61dfe5e5b783580"><gtr:id>97108c66f07effc5e61dfe5e5b783580</gtr:id><gtr:otherNames>Tidhar D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-4295-9</gtr:isbn><gtr:outcomeId>doi_53d0580582e78bf4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F5EF71D-DD7D-4897-89A0-A215C73D3B86</gtr:id><gtr:title>The Studio Ontology Framework</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/60befb604e2925462a994f8118e3cbf1"><gtr:id>60befb604e2925462a994f8118e3cbf1</gtr:id><gtr:otherNames>Gyorgy Fazekas (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_766834706813e170cc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4BA9B92F-F779-4BDB-A285-E08D5C32DC66</gtr:id><gtr:title>Influences of Signal Processing, Tone Profiles, and Chord Progressions on a Model for Estimating the Musical Key from Audio</gtr:title><gtr:parentPublicationTitle>Computer Music Journal</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2cd4e3c9d93908c015ae7d1f0a60322"><gtr:id>d2cd4e3c9d93908c015ae7d1f0a60322</gtr:id><gtr:otherNames>Noland K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d076076d1e76f1</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5DF6A0E1-FBDA-4AB7-909C-48076E18CE05</gtr:id><gtr:title>Modern Methods for Musicology: Prospects, Proposals, and Realities</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4139d7095d654a60ab4a53f1c69ed09b"><gtr:id>4139d7095d654a60ab4a53f1c69ed09b</gtr:id><gtr:otherNames>Crawford, Tim; Gibson, Lorna</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-0-7546-7302-6</gtr:isbn><gtr:outcomeId>i_86586634253c03d11c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53798BBE-8398-43B4-9C75-FCE167507A28</gtr:id><gtr:title>The MPEG Interactive Music Application Format Standard [Standards in a Nutshell</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2db80d2d8620e7c67334224b98db23e4"><gtr:id>2db80d2d8620e7c67334224b98db23e4</gtr:id><gtr:otherNames>Jang I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d05b05bb11aec3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CBC9D881-84E2-42F8-BA4B-36A11A26C675</gtr:id><gtr:title>TempEst: Harpsichord Temperament Estimation in a Semantic Web Environment</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97108c66f07effc5e61dfe5e5b783580"><gtr:id>97108c66f07effc5e61dfe5e5b783580</gtr:id><gtr:otherNames>Tidhar D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd662282</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>075A5226-6E5D-4E25-AA14-E161EA5F9F51</gtr:id><gtr:title>Approximate Note Transcription for the Improved Identification of Difficult Chords</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a084fad2768a1789e1e895736466a842"><gtr:id>a084fad2768a1789e1e895736466a842</gtr:id><gtr:otherNames>Matthias Mauch (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_572612201213f022f2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F73DC648-1225-4BB9-98A9-C8EA61EB016B</gtr:id><gtr:title>Composite spectrogram using multiple Fourier transforms</gtr:title><gtr:parentPublicationTitle>IET Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df41536d06389e06951d1e04c1f46382"><gtr:id>df41536d06389e06951d1e04c1f46382</gtr:id><gtr:otherNames>Wen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d031031e48c58a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B13E365F-8743-44A4-B866-20D908565B8D</gtr:id><gtr:title>Automatic chord transcription from audio using computational models of musical context</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f654beb046a9f57fbf818f988b037766"><gtr:id>f654beb046a9f57fbf818f988b037766</gtr:id><gtr:otherNames>Mauch Matthias</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_888521967263eb86da</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EE871B73-F5D4-4A6E-AB96-4ED6A0B02B91</gtr:id><gtr:title>On the Characterization of Slowly Varying Sinusoids</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Audio, Speech, and Music Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df41536d06389e06951d1e04c1f46382"><gtr:id>df41536d06389e06951d1e04c1f46382</gtr:id><gtr:otherNames>Wen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d076076823847d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A489734-A47C-4212-B9F9-7B458CB53A59</gtr:id><gtr:title>Learning Latent Semantic Models for Music from Social Tags</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/500eb111ee39c0d64026f276b07d6dae"><gtr:id>500eb111ee39c0d64026f276b07d6dae</gtr:id><gtr:otherNames>Levy M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd33ffaa</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B3359DB-4DF0-455C-B2B2-16845CC07C2C</gtr:id><gtr:title>Towards a Distributed Research Environment for Music Informatics and Computational Musicology</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6d76a8ab4323a3f8dd805c24e141802"><gtr:id>e6d76a8ab4323a3f8dd805c24e141802</gtr:id><gtr:otherNames>Dixon S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd9158a6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>560D30EC-17E7-4AFB-AE61-57EB0B3117C9</gtr:id><gtr:title>Using Audio Analysis and Network Structure to Identify Communities in On-line Social Networks of Artists</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d324516da5e56c8feab082f736272c70"><gtr:id>d324516da5e56c8feab082f736272c70</gtr:id><gtr:otherNames>Kurt Jacobson (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_895588060713eca67c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46E579F1-EB9F-479D-8C24-4CFE9E0283BE</gtr:id><gtr:title>Simultaneous Estimation of Chords and Musical Context From Audio</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ba0eefbf6e56d17315a1570e42ce7dbe"><gtr:id>ba0eefbf6e56d17315a1570e42ce7dbe</gtr:id><gtr:otherNames>Mauch M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d05c05ced4aa3e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43629825-91EB-4BAC-951E-23E41110C09D</gtr:id><gtr:title>Investigating Music Collections at Different Scales with AudioDB</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/37010addbf3b6c9d58f22506e9fe34b9"><gtr:id>37010addbf3b6c9d58f22506e9fe34b9</gtr:id><gtr:otherNames>Rhodes C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd53f234</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0B1ACC2F-3AB7-4342-8989-C01DBB9A62B5</gtr:id><gtr:title>Structural Segmentation of Musical Audio by Constrained Clustering</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/500eb111ee39c0d64026f276b07d6dae"><gtr:id>500eb111ee39c0d64026f276b07d6dae</gtr:id><gtr:otherNames>Levy M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d05c05ce688a1d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9C320655-B4D4-4878-B551-35BC01F537ED</gtr:id><gtr:title>Improving Music Genre Classification Using Automatically Induced Harmony Rules</gtr:title><gtr:parentPublicationTitle>Journal of New Music Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e194d7894b0a94988dd28113116f7ec9"><gtr:id>e194d7894b0a94988dd28113116f7ec9</gtr:id><gtr:otherNames>Anglade A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03c03cd80691c</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E017614/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>