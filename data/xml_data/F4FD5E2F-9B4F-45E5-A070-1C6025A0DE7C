<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B04F1DCA-5CFC-4F69-9602-D85517726870"><gtr:id>B04F1DCA-5CFC-4F69-9602-D85517726870</gtr:id><gtr:name>Helsinki University of Technology</gtr:name><gtr:address><gtr:line1>PO Box 1000</gtr:line1><gtr:line2>FIN-02015 HUT</gtr:line2><gtr:line3>Finland</gtr:line3><gtr:region>Outside UK</gtr:region><gtr:country>Finland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5195F775-F0E4-443A-AE02-3E6B51BB33A7"><gtr:id>5195F775-F0E4-443A-AE02-3E6B51BB33A7</gtr:id><gtr:name>Phonak Hearing Systems</gtr:name><gtr:address><gtr:line1>Laubisrutistrasse 28</gtr:line1><gtr:line4>8712 Stafa</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Switzerland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/EFCAC93C-F04E-44A2-BBB7-F84F5A766633"><gtr:id>EFCAC93C-F04E-44A2-BBB7-F84F5A766633</gtr:id><gtr:firstName>Guy</gtr:firstName><gtr:surname>Brown</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG009805%2F1"><gtr:id>F4FD5E2F-9B4F-45E5-A070-1C6025A0DE7C</gtr:id><gtr:title>Perceptual constancy in real-room listening by humans and machines</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G009805/1</gtr:grantReference><gtr:abstractText>Our perceptual 'systems' allow us reliably to judge properties of things in the real world under diverse conditions, as they exhibit 'constancy'. For example, a white surface in dim light can be distinguished from a black surface in bright light, even though the luminance of the two surfaces might be the same. Although constancy is clearly vital for survival, and has been extensively studied in vision, it has not been investigated in hearing very much. This lack of knowledge probably accounts for the poor performance of the current generation of artificial listening devices, which are becoming increasingly important in hearing aids, as well as in other applications of automated speech recognition. We aim to measure the different listening conditions effected by real rooms and then to investigate constancy in hearing with perceptual experiments. This information will then be incorporated into prototype artificial-listening devices, which will be tested for their effectiveness in dealing with the real world conditions that human hearing seems to cope with so exquisitely.</gtr:abstractText><gtr:fund><gtr:end>2012-04-19</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-10-20</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>128310</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Modelling the role of the auditory efferent system in the recognition of noisy and reverberberant speech</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D375ABB0-F449-4020-8B02-FD30E06D1B7C</gtr:id><gtr:impact>A talk to the universities of Oldenburg and Magdeburg, Germany, reviewing our work on noise-robust and reverberation-robust front-end processors for automatic speech recognition, based on models of auditory efferent function.</gtr:impact><gtr:outcomeId>r-2588152496.6919350bcf500a</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2011</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project investigated perceptual compensation for the effects of room reverberation on speech recognition, using experiments with both human listeners and machine systems (auditory models and automatic speech recognition (ASR) systems). Our key findings were:



Perceptual data on compensation for the effects of reverberation in a 'sir-stir' identification task was closely matched by a computer model of auditory efferent processing. In the computer model, the peripheral auditory response was suppressed by efferent activity, which in turn was determined by the dynamic range of the preceding auditory nerve response. The model therefore suggests that perceptual compensation for the effects of reverberation is underlain, at least in part, by auditory mechanisms responsible for the monitoring and control of dynamic range.



The computer model was able to explain Watkins' finding that perceptual compensation in the 'sir-stir' task was not affected by time-reversing the speech preceding the test word, but that compensation was abolished when the room impulse response was reversed. Time reversal of the speech preceding the test word (the 'context') did not substantially affect the dynamic range of the auditory nerve response, so our model made the same predictions in the forward-speech and reverse-speech conditions. For these experimental stimuli, however, reversing the room impulse response increased the dynamic range of the context prior to the test word. This caused a reduction in the efferent response and removed the compensation effect.



We have demonstrated that perceptual compensation for reverberation was apparent for human listeners in a consonant identification task using naturalistic speech. Test words included a wider range of consonants (/t/, /k/, /p/), six vowels, and were spoken by both male and female voices in realistically variable speech utterances. Compensation was apparent in the pattern of confusions made by listeners, as quantified in terms of the relative information transmitted. When more reverberation was added to the test word only, listeners confused many consonants. These confusions were largely resolved when similar reverberation was added to the speech preceding the test word.



Using this (/t/, /k/, /p/) consonant identification task, we confirmed Watkins' finding that perceptual compensation is abolished when the room impulse response is time-reversed. We found also that the reverberation tail of the test word contributes to perceptual compensation under certain conditions. Compensation was reduced when the reverberation tail of the test word was partially removed by gating, but this effect was only apparent when the context preceding the test word did not itself promote compensation. Further, we investigated the time course of perceptual compensation using reverberant contexts of gradually increasing duration and found that compensation increased with increasing context duration up to 500 ms, the maximum time course considered. 



A reverberation-robust ASR system was developed in which different statistical models of speech were selected according to the dynamic range of the speech signal. The model showed a close match to the confusion patterns made by human listeners in our perceptual data.



Finally, principles derived from our perceptual experiments (within-band processing and the role of reverberation tails) were implemented in a 'missing feature' ASR system. In this approach, a number of acoustic cues (including those derived from a model of binaural processing) were employed to identify time-frequency regions of speech that were relatively uncorrupted by reverberation. The 'clean' regions were used directly for recognition, whereas the true values of the corrupted regions were imputed (reconstructed) from statistical models of speech. The ASR system gave a good performance on the CHiME challenge, an evaluation corpus in which the speech is corrupted by reverberation and environmental noise.</gtr:description><gtr:exploitationPathways>Hearing impaired listeners have particular problems understanding speech in reverberant environments. Our computer models and psychophysical data could be used to improve the performance of hearing aids and cochlear implants in reverberant conditions. The results of this research will be of interest to researchers developing reverberation-robust automatic speech recognition systems, which wish to include human-like processing in their algorithms. Our computer model makes predictions that can be tested by workers in auditory neurophysiology.</gtr:exploitationPathways><gtr:id>1339F5BF-B1DE-495C-8EBB-A00EBAFAC011</gtr:id><gtr:outcomeId>r-4156930096.5936203776f0f74</gtr:outcomeId><gtr:sectors><gtr:sector>Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.dcs.shef.ac.uk/~guy/constancy/index.htm</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>7C4DA1E9-DB80-4DFC-827F-3C4BF6319CB9</gtr:id><gtr:title>Low-level and high-level models of perceptual compensation for reverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_8453498034cad33f6c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>113F1259-AE09-463E-AFBC-5708A7063270</gtr:id><gtr:title>A computer model of perceptual compensation for reverberation: evaluation on a consonant identification task</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/418bb88d3fc0dd1154e531314786a4fc"><gtr:id>418bb88d3fc0dd1154e531314786a4fc</gtr:id><gtr:otherNames>Guy Brown (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_5552596913cad35f9c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB4ACB66-166D-4499-B99A-A3CD1D43359C</gtr:id><gtr:title>A computer model of perceptual compensation for reverberation based on auditory efferent suppression</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_2758913286cad35452</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AEAC753A-984F-421C-8E9A-B5B9001B6F04</gtr:id><gtr:title>Perceptual compensation for effects of reverberation in speech identification: A computer model based on auditory efferent processing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_569229737913d79a98</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E937C160-F190-4A2F-BD4B-07F2413C78CA</gtr:id><gtr:title>Mask estimation and imputation methods for missing data speech recognition in a multisource reverberant environment</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a895c84cedee64c62c1b64d80c2290fa"><gtr:id>a895c84cedee64c62c1b64d80c2290fa</gtr:id><gtr:otherNames>Keronen S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53cfebfeb85e8a1d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>71F2DDC2-3C49-4B27-933F-5C56A2BE3BB8</gtr:id><gtr:title>Perceptual compensation for reverberation: human identification of stop consonants in reverberated speech contexts</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_9403509737cad3c3ec</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>480B876E-2E51-4947-924A-AD7B9ED94FAF</gtr:id><gtr:title>A computer model of auditory efferent suppression: implications for the recognition of speech in noise.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/927de28339d4fea28f000fb150e2a503"><gtr:id>927de28339d4fea28f000fb150e2a503</gtr:id><gtr:otherNames>Brown GJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>doi_53d06e06e896b251</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>16971BCC-06C7-4BDB-8558-D01CAD9F1DB1</gtr:id><gtr:title>Abstracts of the British Society of Audiology Short Papers Meeting on Experimental Studies of Hearing and Deafness</gtr:title><gtr:parentPublicationTitle>International Journal of Audiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f17cdb3b8df0f84cd0e505c9a3fbced"><gtr:id>3f17cdb3b8df0f84cd0e505c9a3fbced</gtr:id><gtr:otherNames>Furness D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d085085c374f81</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A80F772D-A207-4FE4-823F-C210AA97A5A4</gtr:id><gtr:title>Consonant confusions provide further evidence that time-reversed rooms disturb compensation for reverberation</gtr:title><gtr:parentPublicationTitle>7th Forum Acusticum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/36a2e20998edad9ae81c8b8246381e20"><gtr:id>36a2e20998edad9ae81c8b8246381e20</gtr:id><gtr:otherNames>Beeston A.V.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545262922eda65.98627697</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0EB81376-1632-4993-A689-4D72D382EC33</gtr:id><gtr:title>Modelling reverberation compensation effects in time-forward and time-reversed rooms</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_439171298313d799c6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>42B95CAD-9BC1-41A2-9788-E3C7FA5EE4AD</gtr:id><gtr:title>A computational model of binaural speech recognition: Role of across-frequency vs. within-frequency processing and internal noise</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7fe600e39a904c30369e2a8a1730eb26"><gtr:id>7fe600e39a904c30369e2a8a1730eb26</gtr:id><gtr:otherNames>Palom?ki K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d00b00b65d4316</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9826DE06-3A75-4F42-9897-98F322D36479</gtr:id><gtr:title>Perceptual compensation for the effects of reverberation on consonant identification: evidence from studies with monaural stimuli.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b8f4a622fe26617a52916def7a46e074"><gtr:id>b8f4a622fe26617a52916def7a46e074</gtr:id><gtr:otherNames>Beeston AV</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>544fd101678265.14605780</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B2FE428-0BF0-4BC1-A3FE-70067F568A0C</gtr:id><gtr:title>A computer model of perceptual compensation for reverberation based on auditory efferent suppression</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>r_4488347298cad34958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>89B541E0-4728-4B00-8737-E483542E2E9D</gtr:id><gtr:title>Compensation for the effects of reverberation on automatic speech recognition: a perceptually-inspired approach based on weighting of parallel acoustic models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/418bb88d3fc0dd1154e531314786a4fc"><gtr:id>418bb88d3fc0dd1154e531314786a4fc</gtr:id><gtr:otherNames>Guy Brown (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>r_2896750143cad4d728</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15D61A00-DFA4-472C-A98A-821B705BE98D</gtr:id><gtr:title>Mask estimation and sparse imputation for missing data speech recognition in multisource reverberant environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b656b5655a8ba94c51eb4ed5e001cba"><gtr:id>7b656b5655a8ba94c51eb4ed5e001cba</gtr:id><gtr:otherNames>Heikki Kallasjoki (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_175535209613e22576</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2CB88EDE-F186-4027-B2E9-A2C511F4890E</gtr:id><gtr:title>Recognition of reverberant speech by missing data imputation and NMF feature enhancement</gtr:title><gtr:parentPublicationTitle>REVERB challenge workshop in conjunction with ICASSP 2014 and HSCMA 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/19ce24d37e133c45f0ad25e758dd751f"><gtr:id>19ce24d37e133c45f0ad25e758dd751f</gtr:id><gtr:otherNames>Kallasjoki, H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545263858b2944.03921483</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F4213E5-16DF-4541-9BA8-B4C21123657C</gtr:id><gtr:title>Perceptual compensation for adverse effects of room reverberation on speech recognition: A model based on auditory efferent processing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1548903c2470df58d3e8b33612fb9f2"><gtr:id>b1548903c2470df58d3e8b33612fb9f2</gtr:id><gtr:otherNames>Amy Beeston (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_9343640953cad2f700</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G009805/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>