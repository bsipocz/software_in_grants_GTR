<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B3980ADD-58CF-4A26-B922-E005960588A6"><gtr:id>B3980ADD-58CF-4A26-B922-E005960588A6</gtr:id><gtr:firstName>Per Ola</gtr:firstName><gtr:surname>Kristensson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH027408%2F2"><gtr:id>F81DC7AC-169A-4146-AEAE-3C5B4182C019</gtr:id><gtr:title>Text Entry by Inference: Eye Typing, Stenography, and Understanding Context of Use</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/H027408/2</gtr:grantReference><gtr:abstractText>My research is based on the observation that our daily interaction with computers is highly redundant. Some of these redundancies can be modelled and exploited by intelligent user interfaces. Intelligent text entry methods use AI techniques such as machine learning to exploit redundancies in our languages. They enable users to write quickly and accurately, without the need for a key press for every single intended letter.In this programme I propose to develop two new intelligent text entry methods. The first is a system that enables disabled users to communicate efficiently using an eye-tracker. The second system is a novel intelligent text entry method that is inspired by stenography.In addition, I propose to explore text entry methods' broader context. The research literature has concentrated on inventing text entry methods that promise high entry rates and low error rates. Now that we have text entry methods that have reasonably high entry rates it is time to complement this objective function by discovering other aspects of text entry. I propose to use social-science techniques, such as diary and field-studies, to understand how users would prefer to use text entry methods in the wild. System 1: Eye-typing by inferenceThis is a system that will potentially increase the entry rate in eye-typing systems. Current eye-typing systems are inherently slow (due to the dwell timeouts), and users perceive them as frustrating. I propose to build a system that enables users to eye-type without the need for a dwell timeout at all. Potentially, my method will be faster than any other eye-tracker based method in the world.With my proposed system users write words by directing their gaze at the intended letter keys, in sequence. Users' intended words are transcribed when they look at a result area positioned above the keyboard. Users can write more than one word. They can also write sequences or words, or even stop short within a word. They may go to the spacebar key between words but this is not strictly necessary for the system to be able to correctly infer users' intended words.System 2: Stenography by inferenceThis system will be a stenography system for pen or single-finger input. The primary application is mobile text entry. However, I strive to create a system that to some extent can replace the desktop keyboard, should users so desire. Potentially it will be faster than any other pen-based text entry method.The idea behind this method is to enable users to write words quickly by gesturing patterns they have previously learned. Such open-loop recall from muscle-memory is much faster than the closed-loop visually-guided motions users are required to perform when they tap on, for example, an on-screen keyboard. My proposed system will enable users to quickly and accurately articulate gestures for individual words. These gestures will be fixed for a particular word. That is, each word is associated with a single (prototypical) unique gestural pattern. A user's input gesture is recognised by a pattern recognizer. The word whose closest pattern best match the user's input gesture will be outputted by the system as the user's intended word.Understanding the broader context of text entryThe last component of my proposed programme serves to contribute new perspectives to the text entry research field. As previously discussed, context of use is largely unexplored in text entry. I intend to explore this topic using a range of qualitative methods. I intend to perform interviews, conduct field studies (e.g. studying participants trying a prototype mobile speech recognizer at a caf), and diary-studies. The latter will be conducted with a system that provides users of a choice of a few text entry methods that I hypothesize will be useful for different situations. I also intend to read literature on design and architecture to further my understanding of the complete design space of text entry.</gtr:abstractText><gtr:potentialImpactText>Research dissemination The research results will be submitted for publications in the best relevant fora. I will first aim for the top-tier multi-disciplinary journals because both of the proposed systems have a high chance of breaking new ground and interest the general research community. System 1 (eye-typing by inference) have a high probability to break record-speeds of writing with the eyes only (currently held by Dasher), and System 2 (stenography by inference) has a high probability to break the record speeds of pen-based and single-finger based input (currently held by ShapeWriter). For justifications behind my estimations see the system descriptions outlined earlier. I also aim to continually publish intermediate research results in the human-computer interaction literature: CHI, IUI and UIST, and/or the HCI-journals Human-Computer Interaction, ACM Transactions on Computer-Human Interaction, and others. Transfer of knowledge to the general public My experience with previous press coverage of my research is that despite all the previous press articles, what contributed the most to reaching the general public was the release of ShapeWriter on Apple's AppStore for the iPhone. Learning from this experience I propose make software available for download as soon as it is practical to do so. I will start a website with a blog that explains how the systems work and provides users with the possibility to download the software. Technology transfer I will investigate the opportunity to patent isolated or entire parts of systems 1 and 2. I am an inventor or co-inventor of six patent applications and have so far been granted two. I have also co-founded a technology start-up based on my previous research results (ShapeWriter, Inc.), and I have experience in attracting capital, talking to enterprise customers, and all the other (extreme) challenges in actually making a technical start-up company work. I have also some experience of the technology-transfer services available within the University of Cambridge. I was an Executive Committee Member of Cambridge University Entrepreneurs 2008 - 2009, and I have attended seminars organised by Cambridge Enterprises (technology transfer body at Cambridge). It should be remembered that there are many ways to make technology transfer work. I want to have an academic career and I am not interested in starting another company unless an extraordinary opportunity arises. I do think there is a huge possibility to improve society by providing users with more efficient text entry methods, particularly for users who prefer to avoid the desktop computer. However, impact on society (number of customers) and business opportunity (market size) are not the same things. I know from my work on ShapeWriter, Inc. that it is extremely hard to make money from licensing to mobile phone manufacturers. At the same time, while enthusiasts are willing to pay for better text entry, the majority will not pay retail for a new text entry method. A possible exception is the market for accessibility software. However, I think it is morally objectionable to sell tax-funded research outcomes at high prices for disabled users. Therefore, I am probably going to release the software under an open source license, if this ends up being feasible. I predict two problems with the open source release scheme. First, there needs to be an active developer community that maintains the software. I will try to solve this by either attracting third-party developers, or by merging my project into a larger project that has an established developer community. Second, users need to be aware that my software exists. I will tackle this by putting up a website, uploading demonstration videos to websites, participating in university outreach efforts, and by demonstrating my systems at scientific conferences.</gtr:potentialImpactText><gtr:fund><gtr:end>2013-05-27</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-03-28</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>183157</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Parts of the research outcomes, in particular keyboard optimisation and langauge modelling, has been featured in the international press, such as press coverage by BBC Scotland and radio interviews.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>D5100086-CDD2-4294-BE9E-AA49F6165810</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56e2faa6833548.66595105</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The main contributions have been in text entry and intelligent user interfaces, in particular:

* We have discovered a novel method for creating in-domain training data that allows us to create efficient statistical language models for AAC devices. AAC devices are used by nonspeaking motor-disabled individuals to communicate. If the statistical models are poor, the text entry rate is very low, which is frustrating for the end-users.

* We have discovered a new methodology for more accurately evaluating text entry methods by allowing users to compose text rather than engage in the traditional transcription task. We have further shown how this new methodology can be made as reliable as the traditional transcription-task. This opens up for better text entry evaluations that better reflect real-word needs.

* We have discovered that dwell-free eye-typing is empirically faster than traditional eye-typing and via a mathematical model demonstrated that for any realistic choice of behavioural parameters, dwell-free eye-typing must be faster than traditional eye-typing. Eye-typing is used by severely motor-disabled users to communicate via their eyes. Any speed-up increases quality of life.

* We have developed technology for proximity-aware interfaces that allow the user interface to adapt depending on the user's distance to it and made it work in collaborative environments with multiple people.

* We have developed new attention-aware display technology that potentially results in less distractions in a multi-display environment.

* We have identified an empirical link and a mathematical &amp;quot;law&amp;quot; that can estimate the probability that users will subjectively detect a difference in performance in a user interface as a function of the objective difference in performance. This would potentially allow user interface designers to make better informed trade-off decisions when they have to decide between an interface with an objectively higher performance but with (for example) worse aesthetics. It may very well be that users will not notice the performance differential and the more aesthetic interface (in this example) could be preferable.</gtr:description><gtr:exploitationPathways>There are two ways forward:

1. The empirical findings have led to implications for design which can be used by interface designers as solution principles for new user interface designs.

2. The technical systems research can inform engineers and computer scientists on how to build non-trivial intelligent user interfaces.</gtr:exploitationPathways><gtr:id>E4FE3230-1E28-4B27-9D93-ABFE4A9B9B1C</gtr:id><gtr:outcomeId>56e2fe5cb88893.04620291</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C32C5E9A-5E80-4FCE-83E9-117DECFFFA08</gtr:id><gtr:title>Complementing text entry evaluations with a composition task</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Computer-Human Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f8649914a2705eef000a6cdaf303dde4"><gtr:id>f8649914a2705eef000a6cdaf303dde4</gtr:id><gtr:otherNames>Vertanen K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675fe6b29f29</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F2FE35A-B7CF-487F-ABBD-34820EA97EC9</gtr:id><gtr:title>Estimating and using absolute and relative viewing distance in interactive systems</gtr:title><gtr:parentPublicationTitle>Pervasive and Mobile Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5732350034a429073cb1495b53e711a2"><gtr:id>5732350034a429073cb1495b53e711a2</gtr:id><gtr:otherNames>Dostal J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_53d0050058e8bcd8</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H027408/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>AEC62E13-CCF3-4039-A153-2E97DB66398F</gtr:id><gtr:grantRef>EP/H027408/1</gtr:grantRef><gtr:amount>246359.63</gtr:amount><gtr:start>2010-06-01</gtr:start><gtr:end>2011-03-28</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>F81DC7AC-169A-4146-AEAE-3C5B4182C019</gtr:id><gtr:grantRef>EP/H027408/2</gtr:grantRef><gtr:amount>183157.23</gtr:amount><gtr:start>2011-03-28</gtr:start><gtr:end>2013-05-27</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>