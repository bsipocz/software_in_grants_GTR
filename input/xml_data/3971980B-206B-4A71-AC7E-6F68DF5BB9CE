<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/52FDE897-E62F-4388-A588-9382D1E7C2D3"><gtr:id>52FDE897-E62F-4388-A588-9382D1E7C2D3</gtr:id><gtr:name>Sheffield Hallam University</gtr:name><gtr:department>Faculty of Arts Computing Eng and Sci</gtr:department><gtr:address><gtr:line1>City Campus</gtr:line1><gtr:line2>Howard Street</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S1 1WB</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/52FDE897-E62F-4388-A588-9382D1E7C2D3"><gtr:id>52FDE897-E62F-4388-A588-9382D1E7C2D3</gtr:id><gtr:name>Sheffield Hallam University</gtr:name><gtr:address><gtr:line1>City Campus</gtr:line1><gtr:line2>Howard Street</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S1 1WB</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C7FD454E-D359-48AF-95FD-3E1375802686"><gtr:id>C7FD454E-D359-48AF-95FD-3E1375802686</gtr:id><gtr:name>South Yorkshire Fire and Rescue</gtr:name><gtr:address><gtr:line1>Edlington Fire Station</gtr:line1><gtr:line2>Edlington Lane</gtr:line2><gtr:line3>Warmsworth</gtr:line3><gtr:postCode>DN12 1DA</gtr:postCode><gtr:region>Yorkshire and the Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4D6A8C0E-1BC0-4091-80A6-1A5E0463DCB4"><gtr:id>4D6A8C0E-1BC0-4091-80A6-1A5E0463DCB4</gtr:id><gtr:name>Thales Netherland</gtr:name><gtr:address><gtr:line1>Haakbergerstraat 49</gtr:line1><gtr:postCode>7554 PA</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Netherlands</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/25C0D19F-053F-4DDA-B059-B1B1E11C0696"><gtr:id>25C0D19F-053F-4DDA-B059-B1B1E11C0696</gtr:id><gtr:name>Guide Dogs</gtr:name><gtr:address><gtr:line1>Hillfields</gtr:line1><gtr:line2>Burghfield Common</gtr:line2><gtr:postCode>RG7 3YG</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/8A029AF7-FB41-4DA3-BF2D-D4A2FF5DA1B2"><gtr:id>8A029AF7-FB41-4DA3-BF2D-D4A2FF5DA1B2</gtr:id><gtr:firstName>Jacques</gtr:firstName><gtr:otherNames>sybrandus</gtr:otherNames><gtr:surname>Penders</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7E000E35-4C69-4891-8C93-D03DBD7F022B"><gtr:id>7E000E35-4C69-4891-8C93-D03DBD7F022B</gtr:id><gtr:firstName>Alan</gtr:firstName><gtr:otherNames>Franciszek</gtr:otherNames><gtr:surname>Holloway</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/00339880-9E1B-4DE9-BB5D-D90C7B412C10"><gtr:id>00339880-9E1B-4DE9-BB5D-D90C7B412C10</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:otherNames>Eland</gtr:otherNames><gtr:surname>Jones</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/E1C0BF72-D60C-4DB1-B76B-DF057B78BE93"><gtr:id>E1C0BF72-D60C-4DB1-B76B-DF057B78BE93</gtr:id><gtr:firstName>C</gtr:firstName><gtr:surname>Roast</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B6799569-566C-4837-A88D-5B7050CB01A6"><gtr:id>B6799569-566C-4837-A88D-5B7050CB01A6</gtr:id><gtr:firstName>Alexander Heathcliff</gtr:firstName><gtr:surname>Reed</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI028757%2F1"><gtr:id>3971980B-206B-4A71-AC7E-6F68DF5BB9CE</gtr:id><gtr:title>REINS</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I028757/1</gtr:grantReference><gtr:abstractText>The REINS project is to design and investigate haptic communicational interfaces (reins) between a human agent and a mobile robot guide. The reins will facilitate joint navigation and inspection of a space under conditions of low visibility (occurring frequently in fire fighting). The focus is on haptic and tactile human robot cooperation as it has been found that a limited visual field and obscured cameras adds to the distress of humans working under pressure. Humans naturally interact with animals using tactile feedback in scenarios such as working with guide dogs and horse riding; the REINS project aims to extend this practice to human robot interaction. Expertise from a number of different disciplines - design, engineering, robotics, and communication - will be brought to bear on the problem of designing a communicational interface which will be both sufficiently robust for the relevant physical environment and sufficiently flexible to allow for the on-the-spot exercise of human judgement and creativity.Inspired by the use of a harness for a guide dog and also the rein to ride or drive a horse, the REINS project will investigate and experiment with haptic interfaces for human-robot cooperation. The low/no visibility constraint ensures the focus is on the tactile and haptic aspects only. Currently, robots do not sufficiently enhance human confidence. In human-robot cooperation, the human (by nature) will try to 'read' the situation, and anticipate the movements of the robot companion. The robot is provided with an impedance filter and the rein enables the human to feel the robot's movements and behaviour. Experiences with remotely controlling a robot which is not directly visible show that 'operators spent significantly more time gathering information about the state of the robot and the state of the environment than they did navigating the robot'.The REINS project aims to map the communicational landscape in which humans (fire fighters, but also the visually impaired) might be working with robots, with the emphasis on tactile and haptic interaction. We adapt a semi-autonomous mobile robot for navigation in front of a human. The robot provides rich sensory data and is enabled to try the mechanical impedances of the objects it encounters. We also design and implement a soft rein (rope), a wireless rein and a stiff rein (inspired by the lead for guide dog) enabling the human to use the robot to actively probe objects. The project thus creates the means to explore the haptic Human-Robot Interaction landscape. We will work from an integrationist perspective in which the communicator is not a mere user of pre-existing signs but a sign-maker; the signs emerge in the ongoing coordination and integration of activities adapted to the particular circumstances. We review the communicational landscape occurring within a team of (human) fire fighters and in addition review literature on working guide dogs and horse riding. A research question is whether the information should be explicitly encoded as messages or can remain implicit.In the initial phase of the project the robot is adapted and the first prototypes of the reins are implemented; the emphasis in this phase is on providing rich data to the human. The second phase is dedicated to surveying the communicational landscape. The human-robot team will navigate a known environment with low visibility where unknown obstacles may occur. At least two different types of reins are applied: one requires that messages are explicitly coded, while the other propagates the information implicitly. Based on experiences in the first trials the reins might be adapted to improve their usability. Professional fire fighters will be the first group of subjects to try the reins, later on also volunteers experienced with guide dogs may join the experimentations.</gtr:abstractText><gtr:potentialImpactText>The REINS project studies tactile and haptic interaction in low visibility conditions. The project focuses on the emergence of the navigational abilities of a human robot team, much of it will depend on the confidence the human has in the robot. WHO: South Yorkshire Fire and Rescue (letter of support) are involved in the project. The project studies fire fighting practice and performs experiments targeted at a fire fighting environment. A direct impact is that fire fighters become better aware of their (implicit) practices. In case the project succeeds and robots are indeed applied, they will reduce the risks for fire fighters and thus enhance and improve the rescue of citizens and thus safety of society. Fire brigades within the UK cooperate on a very amicable level, colleagues meet at the national Fire Service College, also regularly regional and national events are organised and informally colleagues exchange their latest experiences. Thus possibly applicable knowledge obtained in REINS will propagate to a national level. The consortium engages with the Guide Dogs for the Blind Association (letter of support). When the technology has matured (and REINS will contribute to that) robots will become useful to assist people with handicaps in their daily routines. Moreover, robots could generally support patients and carers or even takeover certain caring tasks. The consortium engages with Thales NL/UK, manufactures in electronic equipment (letter of support). Direct human robot interaction is still in its infancy. Thales is looking for new ways for interfacing people and machines; new technologies 'for humans-in-the-loop of complex systems-of-systems'. The human factors research of the Thales group is based in the Netherlands, however when usable the knowledge passes on to products and devices (including robots) developed by Thales UK. Short term South Yorkshire Fire and Rescue (SYFR) are very happy to facilitate and assist in the experimental work of the Reins project. SYFR have cooperated with Sheffield Hallam in the Guardians and ViewFinder projects. Reflecting on this they stated: 'As a Fire Brigade we do not have the means to be at the forefront of science and technology developments. However, we are looking at technology to help us improve our service. Being involved in these projects has made our officers better aware of available and up-coming technologies and directly helps us when evaluation new FireFighting technology'. When and if appropriate the Guide Dogs for the Blind Association has offered to arrange volunteers to test a robotic solution as an alternative for a guide dog. Long term In fire fighting, risk assessments is extremely important but very time consuming. Robots with a mobile detection unit would certainly speed this up also more risk can be taken and contamination and in particular decontamination would not be as big an issue as it is related to human beings. The application of robots in Fire and Rescue operations will improve data collection and thus speed-up and improve decision making and therefore adds to saving lives but also to saving property. Appropriate human robot interfaces are essential for introducing robots into daily human life. once this issue is reasonably solved robots can be applied for (partially) handicapped people and also for the elderly. The Guide Dogs for the Blind Association is keen to evaluate the developed solutions for this purpose. Staff development The PDRAs and PhD students will work together with professional fire fighters as their end-users. Thus they will have to listen to and try to understand the viewpoints of people who are non-professionals in the scientific/technical field of the PDRAs/PhDs. Thus they develop skills and experiences applicable in any situation where knowledge and high tech are to be transferred to society.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-03-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>455966</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Action for Involvement, talk and expert for discussion</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D21AB5EC-567B-4A79-A486-425E39BBB6E5</gtr:id><gtr:impact>Lively discussions on whether people wanted to be assisted by a robot

Organiser intend to organise a more assistive robotics for the elderly event</gtr:impact><gtr:outcomeId>5433fed118ca67.87344986</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at the RE.WORK Technology Summit</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>DAD715EC-A3E3-4CBE-85E9-AB3542206330</gtr:id><gtr:impact>Several lengthy discussions with participants

No direct impacts yet</gtr:impact><gtr:outcomeId>54340037693129.77666588</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk, British Computing Society, Yorkshire branch</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>4488579B-7410-4F91-AB34-86F1F6A3C0CC</gtr:id><gtr:impact>Some lively discussion afterwards.

No direct impacts</gtr:impact><gtr:outcomeId>5433fd506ea7c4.39779019</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Search and Rescue Robotics @ Sheffield Hallam University</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B760822D-2E09-4F81-86BA-7918299A8D02</gtr:id><gtr:impact>Presentation to a Japanese robotics delegation organised by Industrial Maths KTN.</gtr:impact><gtr:outcomeId>r-9173119719.3014620c0e3932</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research or patient groups</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk: 'Haptic Interfaces for a Robot Guide'  for Science &amp; Technology Group of the Sheffield University of the Third Age</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>06032CC1-5028-4DAF-AF94-CEC810C5F429</gtr:id><gtr:impact>About 30 mostly retired scientists,



One lady (not retired) came back with a request for discussing a new project.</gtr:impact><gtr:outcomeId>5433fc6ddce1c1.63190663</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Visit to primary School, group 4</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>8E77BF65-92D9-4575-A967-05211C4B74A4</gtr:id><gtr:impact>85 pupils attended the talk (5 minutes) and the demonstrations.


The children were very interested in the robots as long as they did not get too close to them.</gtr:impact><gtr:outcomeId>5458b0af3c5b83.79867544</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Apply similar technology for supporting patients with dementia (see added publication).</gtr:description><gtr:id>B666A21B-1BCF-433B-9BE2-0F52FB4F4B4D</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56cb1631899e50.57216263</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The aim of the project is to understand how a human is guided by another intelligent agent when the vision is impaired. In that scenario, haptics becomes a natural solution. Therefore KCL presents identification of abstracted dynamics of human control policies and human responses in haptic perceptions. 
The SHU team aims at exploring how a human and a semi-autonomous robot can develop cooperation using a haptic interface in low visibility conditions. 

1) Human-human guidance (KCL)

Follower-Model Generation
First, Experiments were conducted to extract the control policies in guiding and following when a blindfolded human is guided by another human by using a hard rein. The key findings from human demonstration experiments are, 
? By modelling how state maps into action by a Nth order state dependent discrete linear controller, it was found that the guider gave more emphasis on 3rd order predictive model and the follower gave more emphasis on 2nd order reactive model. The model order and reactive/predictive nature will give an insight of human behaviour in guiding and following when the vision is impaired.
? Studying of how the guider may modulate the pulling force in response to the confidence level of the follower in order to study how the above control policy interact with the follower in an arbitrary path tracking task is necessary to understand the follower's trust on the guider. Modelling the voluntary movements of the follower as a damped inertial system. It was found that damping coefficient is most sensitive to explain the confidence level of the follower.

Follower-Model Validation 
Secondly, extracted control policies were implemented on planner 1-DoF robotic arm to test its validity and stability. Moreover the human response patterns and behavioural characteristic were studied when the blindfolded human's most dominant arm was perturbed by the robotic arm to guide them in to a desired angular position. They key findings from human robot demonstration experiments are,
? Studying the human movements in leftward and rightward directions convey how humans understand the perceiving information to move another point when the robotic arm uses to generate a tug force to perturb the humans' most dominant arm in low visibility conditions. The asymmetry was noticed in human behavioural metrics in leftward/rightward movements. The extracted behavioural metrics present characteristics and limitations of human movements when the haptic perturbation is given from different directions.
? Haptic perception of external perturbations depends on involuntary muscle recruitment patterns to stabilize the hand.

2) Robot-Human Guidance (SHU)

Inspired by how a visually impaired person is collaborating with a guide dog, we distinguish between the navigation task and the task of guiding.
The guide dog just guides while the human being makes the navigation decisions (which direction to go to reach the goal). In addition, exploration of the (unknown) environment is an additional task in search and rescue.

The focus of the REINS project is on locomotion guidance - the (autonomous) robot leading the human along a safe path - and the human using the robot for exploring the close by environment.

The project has experimented with 1) a wireless rein, 2) a flexible rein (rope) and 3) a stiff rein.
A basic requirement for guidance as well as exploration is that the human being 'knows' where the robot is. A stiff rein provides that information implicitly. If a flexible rein is tense it also provides a good feeling (implicit information) of direction however when slack occurs there is no feedback. A wireless rein does not provide any implicit information; all feedback (including the position of the robot) has to be made explicit.

The project continues to investigate using a fixed rein, which not necessarily requires a set of a-priory fixed codes. We aim for so-called non-performative feedback; we are not looking for coded messages, a coded message would be for example: 'two taps mean go right'.</gtr:description><gtr:exploitationPathways>Most promising is to use and apply our findings for devices that are being used in normal conditions (visibility), for instance to assist elderly people.

We gave some demonstrations for people diagnosed with dementia; they turned out to be very receptive to the idea of allowing a robot in their domestic/residential space. (Our initial expectation was that they would be reluctant and negative.)</gtr:exploitationPathways><gtr:id>96B372BD-AEB0-4401-ABA7-B7B582E52579</gtr:id><gtr:outcomeId>544e55ce42dbc4.88730458</gtr:outcomeId><gtr:sectors><gtr:sector>Healthcare,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>471205E7-09E2-4EBA-96B5-045D39CA228B</gtr:id><gtr:title>An Optimal State Dependent Haptic Guidance Controller Via a Hard Rein</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de8d7ef43e9831350360a7971c5b8c7a"><gtr:id>de8d7ef43e9831350360a7971c5b8c7a</gtr:id><gtr:otherNames>Anuradha  Ranasinghe (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_690168700413d882a0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5EEBCEB1-2846-44FC-BBB3-95F2D3FD4607</gtr:id><gtr:title>Design of a Mechanical Impedance Filter for Remote Haptic Feedback in Low-Visibility Environment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bbf208e866cb092ff5ca6f9410bab31c"><gtr:id>bbf208e866cb092ff5ca6f9410bab31c</gtr:id><gtr:otherNames>Alireza Janani (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_729336297813d7452a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>63D45408-474B-4603-A76A-BC124327F4DE</gtr:id><gtr:title>Designing an interface for Haptic human robot interaction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f80e27e830a199412ec30332ee56f21"><gtr:id>5f80e27e830a199412ec30332ee56f21</gtr:id><gtr:otherNames>Jacobus Penders (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_545489003213e4bc0a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BC086F94-2B6E-4F94-B115-9EA8454104F4</gtr:id><gtr:title>A two party haptic guidance controller via a hard rein</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f3f470591ae9226e38f94f10e7f3b7b5"><gtr:id>f3f470591ae9226e38f94f10e7f3b7b5</gtr:id><gtr:otherNames>Anuradha Ranasinghe (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_722291664113d88354</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EECFC788-AABA-4638-8E9D-3A24B014BFF7</gtr:id><gtr:title>Enhancing trust and confidence in human robot interaction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f80e27e830a199412ec30332ee56f21"><gtr:id>5f80e27e830a199412ec30332ee56f21</gtr:id><gtr:otherNames>Jacobus Penders (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_423008814613e4ba84</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CD4467FB-53C2-4634-B389-08022F895CF6</gtr:id><gtr:title>Towards Human Technology Symbiosis in the Haptic Mode</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b0212c7409bc294538c8e67f044cf61e"><gtr:id>b0212c7409bc294538c8e67f044cf61e</gtr:id><gtr:otherNames>Peter Jones (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_188365138314013222</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60C75653-66EF-4A83-BB99-C54F026BBC2E</gtr:id><gtr:title>Dementia and Robotics: People with Advancing Dementia and Their Carers Driving an Exploration into an Engineering Solution to Maintaining Safe Exercise Regimes.</gtr:title><gtr:parentPublicationTitle>Studies in health technology and informatics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/336a411a37514e324952331d51f90684"><gtr:id>336a411a37514e324952331d51f90684</gtr:id><gtr:otherNames>Cooper C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0926-9630</gtr:issn><gtr:outcomeId>58c2c4a8ab5079.77337425</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6627C8C6-4CE7-43B1-8B40-301B7171E8BC</gtr:id><gtr:title>Following a Robot using a Haptic Interface without Visual Feedback</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2693389dfb1a0f7dbe7315e09b0695fa"><gtr:id>2693389dfb1a0f7dbe7315e09b0695fa</gtr:id><gtr:otherNames>Ayan Ghosh (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_358424152513d8c3be</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I028757/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>35</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>