<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A3C215EE-5645-4543-ACD7-878BFB368AC7"><gtr:id>A3C215EE-5645-4543-ACD7-878BFB368AC7</gtr:id><gtr:firstName>Panos</gtr:firstName><gtr:surname>Kudumakis</gtr:surname><gtr:orcidId>0000-0003-0518-4198</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0C941CCA-2B6E-4ADC-B915-3543BAEF5164"><gtr:id>0C941CCA-2B6E-4ADC-B915-3543BAEF5164</gtr:id><gtr:firstName>Chris</gtr:firstName><gtr:surname>Cannam</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/058B7945-94E9-450F-A970-E88BA39F33D9"><gtr:id>058B7945-94E9-450F-A970-E88BA39F33D9</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Johnathan</gtr:otherNames><gtr:surname>Bryan-Kinns</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH043101%2F1"><gtr:id>6E9FAB73-85BA-469D-88C0-1AEBEB07B849</gtr:id><gtr:title>Sustainable Software for Digital Music and Audio Research</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H043101/1</gtr:grantReference><gtr:abstractText>The aim of this proposal is to provide a Service to support the development and use of software, data and metadata to enable high quality research in the thriving UK audio and music research community.Researchers in this area come from a wide range of backgrounds including: signal processing, electronics, computer science, music, information sciences, dance &amp;amp; performance, and data sonification, but have a common interest in the use of audio and music in their work. Through this project, we want to create a climate where researchers think beyond the research they are undertaking at the present, towards the impact that it could have on other researchers, but right from the start. Through helping researchers to strongly connecting their work with that of others, each will see that their work should be reusable: my research helps your research. In many of the fields within this community, researchers do not have the skills or desire to write their own code, or piece together other code, but benefit greatly from straight out of the box software. Even where they do write their own code, they work on different platforms and use a wide variety of batch and real-time environments (Matlab, Python, Max/MSP, SuperCollider, VST plugins). While there may be potentially useful software in other fields that could help them with their research, they are often not be aware of its existence, and it is often not available for the platform or environment they use.Other specific software- and data-related difficulties faced by researchers in this community include:* Software designed for legacy platforms (Sun SPARC, NeXTSTEP) and/or no longer maintained;* PhD students graduate or staff move: their web pages containing original software and/or data lost;* Code for some systems never released (not considered a priority for researchers, or code not considered ready );* Software and/or datasets in slightly different versions (due to error corrections or enhancements);* Copyright issues of datasets (e.g. audio of Beatles tracks cannot be placed on the web).The proposed Service to the UK music and audio research community will consist of several elements: * A software engineering resource to engineer cross-platform robust software from UK research prototypes; * A software tailoring programme to modify or adapt existing tools for use by other UK researchers; * A source repository with version control, to hold source code for software tools being developed in the community; * A dataset repository to hold and maintain key experimental datasets and metadata (e.g. human-generated labels); * Advice &amp;amp; training in the use of available software, and best practice in developing sustainable software. * Engagement and outreach to the research community and beyondAn important aspect of the Service will be planning for sustainability, so that the software developed during its lifetime, and the Service itself, can be sustained beyond the end of the funded period.</gtr:abstractText><gtr:potentialImpactText>Potential beneficiaries of this project outside of the academic research community include anyone who could benefit from access to high quality software based on the latest research in audio and music. Examples from various sectors are given below. Commercial private sector: * Commercial companies designing audio equipment, through easier access to new audio research; * Musicians and sound artists, through their ability to use new research methods and processes for new creative outputs. * Computer games companies, through better access and use audio and music research outputs for improved game music and audio, or for new types of audio or music-based computer games * Hearing aid and cochlear implant manufacturers, through access to new research applicable to hearing improvement * Audio archiving companies, through access to the latest algorithms and methods for music information retrieval * Television and radio companies, through ability to use latest research in the design of new technology-based programmes Policy-makers and others in government and government agencies: * Research funders, through improved flow of research outputs to research users and other researchers, and thus more effective use of research funds; and increased across-the-board impact of research in audio and digital music * Police and security services, through access to algorithms for analysing and separating audio signals Public sector, third sector and others: * Healthcare workers who work with music and audio, such as music therapists, through new music and audio visualization tools * Museums, through software to measure and visualise acoustic properties of objects * Computing museums and digital libraries, through sharing of experiences with emulation and virtualization of software * Libraries, through improved open access to research results (including software) for the benefit of their users; and access to improved ways to catalogue and search audio and music information based on the latest research * Standards organizations, through access to high quality examples of research methods on which to based forthcoming standards * Science promotion organizations, through availability of high-quality usable research software attractive to people who may be interested in science Wider public: * People interested in exploring music or other audio recordings at home, school, college or university, using the latest research, either for educational or general interest purposes * Teachers in schools, colleges or universities who want to use research-generated software for teaching audio or music. * People such as programming hobbyists, who wish to participate in open-source research software development and maintenance, through contribution to bug fixing, upgrading, etc. of software development activities promoted by the Service. These may have an interest in audio and music, but that would not be a precursor to involvement. * Audiences of creative output involving audio and music, through availability of new creative outputs or technology facilitated by audio and digital music research Researchers employed on the project, and others undertaking training or other interaction: * Improved skills in research software development and research methodologies, which may be transferred into e.g. the commercial private sector on completion of the project</gtr:potentialImpactText><gtr:fund><gtr:end>2014-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>947057</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Annual SoundSoftware Workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>C252AF47-C7FE-43E2-9DEC-E8A4A621DC8F</gtr:id><gtr:impact>An annual series of one-day workshops on Software and Data for Audio and Music Research, held on Queen Mary University of London.</gtr:impact><gtr:outcomeId>56d497fb620943.60101756</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.soundsoftware.ac.uk/soundsoftware2014</gtr:url><gtr:year>2012,2013,2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Software Carpentry Autumn School 2010</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>C3DECDB3-772D-4728-923D-16178D3F9D3F</gtr:id><gtr:impact>22 PhD students and researchers attended a one-week Autumn School (London, UK, 1-5 November 2010) to learn about software development skills required to build reliable research software quickly and with a minimum of effort, and so maximize the impact of their research.</gtr:impact><gtr:outcomeId>56d496ce9be569.43142986</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.soundsoftware.ac.uk/autumnschool2010review</gtr:url><gtr:year>2010</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2800000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>FP7 Marie Curie Initial Training Network</gtr:description><gtr:end>2018-09-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>607290</gtr:fundingRef><gtr:id>6129ECF5-59FE-48A0-AA5A-4B028E5DD51D</gtr:id><gtr:outcomeId>568be1bcb9eb51.53728992</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2014-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>2980000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>H2020-ICT-2015 Audio Commons</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>688382</gtr:fundingRef><gtr:id>7A57AA23-01F8-4DC0-A536-434E70F4C685</gtr:id><gtr:outcomeId>568bdebdb40db2.04379356</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3800000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>H2020 Marie Sklodowska-Curie Action (MSCA) Innovative Training Network</gtr:description><gtr:end>2018-12-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>642685</gtr:fundingRef><gtr:id>F9D04682-AE0B-45EF-9EF0-58D5BBBEFA55</gtr:id><gtr:outcomeId>568bdf68c296c2.55896834</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1275401</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Making Sense of Sounds</gtr:description><gtr:end>2018-12-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N014111/1</gtr:fundingRef><gtr:id>C752444A-5899-4BB2-892E-FE4F11D05F08</gtr:id><gtr:outcomeId>568bdd81e07af0.10810814</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The SoundSoftware code site has assisted with the development of a range of software such as Sonic Visualiser. Sonic Visualiser is available for Linux, Mac OS/X and Windows, and has been downloaded over 330,000 times. In a survey of over 800 users (11 Oct 2010 to 25 Apr 2013), 49% reported that they were non-academic users, using it as a professional (9%) or for personal use (40%). Usage is truly international, with users from 66 countries including the USA (32%), UK (25%), Germany (10%) and France (8%).</gtr:description><gtr:id>2E742964-A758-4C8D-B8D2-A7C3B5CAF41D</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5463665bd7ddc6.84229631</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project produced several key outcomes, including:
 * We were instrumental in bringing Software Carpentry to the UK, as a programme and community for training researchers to develop reliable software. This is relevant to academics outside our own field and of significance to UK research in general;
 * We developed the SoundSoftware code site, which continues to be a centre for audio and music research software
 * We developed and maintained numerous audio analysis plugins for wider use, including submitting them for the annual MIREX evaluation, managing builds and distribution, and providing language interfaces for using them in Java and Python as well as C++
 * We developed the broadly useful application Tony for singing-pitch tracking and analysis</gtr:description><gtr:exploitationPathways>Researchers in the UK and internationally who use audio and music research can benefit from the research software produced by the project, and the code development approaches that we have been promoting.
The commercial private sector, public sector and wider public can also benefit from the availability of digital music tools.</gtr:exploitationPathways><gtr:id>8F594B92-743C-409A-908D-9DA7B3C15611</gtr:id><gtr:outcomeId>56d495923d5516.23785236</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education</gtr:sector></gtr:sectors><gtr:url>http://www.soundsoftware.ac.uk/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>EasyMercurial is a simple user interface for the Mercurial distributed version control system. It's free, open source, and cross-platform.</gtr:description><gtr:id>E55BBA73-6439-4440-9E73-99AC4D5F31E9</gtr:id><gtr:impact>Supported learning of software development techniques and easy access to version control repositories, particularly those hosted at the SoundSoftware code site. This application has seen some tens of thousands of downloads and has been positively received by users we have engaged with.</gtr:impact><gtr:outcomeId>5463a1d1a60591.33408874</gtr:outcomeId><gtr:title>EasyMercurial</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://easyhg.org/</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Tony is a software program for high quality scientific pitch and note annotation in three steps: automatic visualisation/sonification, easy correction, and export.

First, Tony automatically analyses the audio to visualise pitch tracks and notes from monophonic recordings . As a Tony user, you can play back pitches and notes alongside the recording, which makes it easy to spot the inevitable extraction errors.

Then you can use several tools that make correction of such errors a breeze, including alternative pitch track selection, octave shift, and easy note split, merge and deletion.

Finally, you can export pitch track and note track in .csv (comma-separated values) format, or simply save a session file to continue annotating another time.</gtr:description><gtr:id>A0A4D908-92C2-4411-96C2-925693753512</gtr:id><gtr:impact>Has supported development and publication of an annotated multi-track test audio dataset, and is likely to be used in future work of this kind.</gtr:impact><gtr:outcomeId>5463a0759a1914.43685275</gtr:outcomeId><gtr:title>Tony: a tool for melody annotation</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/tony</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>SMALLbox is a foundational framework for processing signals using adaptive sparse structured representations. The main aim of SMALLbox is to become a test ground for exploration of new provably good methods to obtain inherently data-driven sparse models, which are able to cope with large-scale and complicated data. The main focus of research in the area of sparse representations is in deveoping reliable algorithms with provable performance and bounded complexity. Yet, such approaches are simply inapplicable in many scenarios for which no suitable sparse model is known. Moreover, the success of sparse models heavily depends on the choice of a &amp;quot;dictionary&amp;quot; to reflect the natural structures of a class of data. Inferring a dictionary from training data is a key to the extension of sparse models for new exotic types of data.

SMALLbox was developed under an EU grant, with development support and facilitation from the SoundSoftware project (EPSRC).</gtr:description><gtr:id>59EEF5C3-7BC6-460E-8B23-37E2C9DAA047</gtr:id><gtr:impact>Through this work, the SoundSoftware project supported the publication and collection in a reusable form of substantial research work in sparse representation techniques.</gtr:impact><gtr:outcomeId>5463a35aa89ff8.46412135</gtr:outcomeId><gtr:title>SMALLbox: Sparse Representation and Dictionary Learning evaluation toolbox</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/smallbox</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>20A8229F-E9D3-4AD2-B8F7-F5E79F6CA5A4</gtr:id><gtr:title>Improving instrument recognition in polyphonic music through system integration</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c511b3f7514c82954e8e1c2eac88927"><gtr:id>8c511b3f7514c82954e8e1c2eac88927</gtr:id><gtr:otherNames>Giannoulis D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545ce4eee9eab6.09523276</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B444FE37-EB06-443A-B934-97AFE1A2236D</gtr:id><gtr:title>Score-Informed Source Separation for Musical Audio Recordings: An overview</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ecbadd3514247d9dd2617ec865425671"><gtr:id>ecbadd3514247d9dd2617ec865425671</gtr:id><gtr:otherNames>Ewert S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>568ac018eddd75.58104781</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CC12A1FD-CE11-4031-A573-CFEBDB8D18A0</gtr:id><gtr:title>Computer-aided Melody Note Transcription Using the Tony Software: Accuracy and Efficiency</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/387aac94452da898ebf2da2e66e0c532"><gtr:id>387aac94452da898ebf2da2e66e0c532</gtr:id><gtr:otherNames>Mauch, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d436485a8aa9.14795036</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C29D9D2-1BB3-450C-979B-EA7575798286</gtr:id><gtr:title>Polyphonic piano transcription using non-negative Matrix Factorisation with group sparsity</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845955cb409b459776fd0fe8bfe01bf7"><gtr:id>845955cb409b459776fd0fe8bfe01bf7</gtr:id><gtr:otherNames>O'Hanlon K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545ce4ea793070.33843732</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FAC2E5CD-387B-442A-801C-F5B39DBFA840</gtr:id><gtr:title>MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research</gtr:title><gtr:parentPublicationTitle>15th International Society for Music Information Retrieval Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f5d0a27c6279559e227d5ff295010a4"><gtr:id>8f5d0a27c6279559e227d5ff295010a4</gtr:id><gtr:otherNames>Bittner, R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54639f6d0fceb7.20160246</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7BC083C4-4039-43C7-BB57-BAF9D69EAB71</gtr:id><gtr:title>Detection and Classification of Acoustic Scenes and Events</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675ee81f0180</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C43F1987-7F39-4B40-BC3C-FEA5F20CCB93</gtr:id><gtr:title>Sound Software: Towards software reuse in audio and music research</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de82f753a4c19dbf093d1cd4c2aecc91"><gtr:id>de82f753a4c19dbf093d1cd4c2aecc91</gtr:id><gtr:otherNames>Cannam C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>545ce4e6c88451.67026992</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EA314342-16F1-490E-8D8B-DF32313A034B</gtr:id><gtr:title>Best practices for scientific computing.</gtr:title><gtr:parentPublicationTitle>PLoS biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/deb5f0f607f9a5b24127fdedd8ca5fb7"><gtr:id>deb5f0f607f9a5b24127fdedd8ca5fb7</gtr:id><gtr:otherNames>Wilson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1544-9173</gtr:issn><gtr:outcomeId>545ce4f52acd43.76620280</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1369F12C-4C4F-4F57-9911-FDD21561D297</gtr:id><gtr:title>Software techniques for good practice in audio and music research</gtr:title><gtr:parentPublicationTitle>134th Audio Engineering Society Convention 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4b411e8033856bfead458237efbef0a9"><gtr:id>4b411e8033856bfead458237efbef0a9</gtr:id><gtr:otherNames>Figueira L.A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545ce6088aef57.81358358</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H043101/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E05CFE0B-163D-412D-A3C2-28E89B2CA336</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Software Engineering</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>