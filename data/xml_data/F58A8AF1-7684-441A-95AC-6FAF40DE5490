<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/EC23DA53-CA73-4104-A3F6-2A9523484E69"><gtr:id>EC23DA53-CA73-4104-A3F6-2A9523484E69</gtr:id><gtr:name>Queen's University of Belfast</gtr:name><gtr:department>Electronics Electrical Eng and Comp Sci</gtr:department><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Belfast</gtr:line4><gtr:line5>County Antrim</gtr:line5><gtr:postCode>BT7 1NN</gtr:postCode><gtr:region>Northern Ireland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EC23DA53-CA73-4104-A3F6-2A9523484E69"><gtr:id>EC23DA53-CA73-4104-A3F6-2A9523484E69</gtr:id><gtr:name>Queen's University of Belfast</gtr:name><gtr:address><gtr:line1>University Road</gtr:line1><gtr:line4>Belfast</gtr:line4><gtr:line5>County Antrim</gtr:line5><gtr:postCode>BT7 1NN</gtr:postCode><gtr:region>Northern Ireland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/DA9C079F-08C2-4B63-8771-1899C76D54DD"><gtr:id>DA9C079F-08C2-4B63-8771-1899C76D54DD</gtr:id><gtr:firstName>Roger</gtr:firstName><gtr:surname>Woods</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/35ED2FEC-2EF7-4C82-AE41-F4D3A36B4FF3"><gtr:id>35ED2FEC-2EF7-4C82-AE41-F4D3A36B4FF3</gtr:id><gtr:firstName>John</gtr:firstName><gtr:otherNames>Patrick</gtr:otherNames><gtr:surname>McAllister</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/47AF5EBC-BACC-4E2E-846A-2BCC2DDEF382"><gtr:id>47AF5EBC-BACC-4E2E-846A-2BCC2DDEF382</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>Martin</gtr:otherNames><gtr:surname>McCourt</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2BA536B4-D78D-4E94-B962-FF855E258EF2"><gtr:id>2BA536B4-D78D-4E94-B962-FF855E258EF2</gtr:id><gtr:firstName>Ming</gtr:firstName><gtr:surname>Ji</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD048605%2F1"><gtr:id>F58A8AF1-7684-441A-95AC-6FAF40DE5490</gtr:id><gtr:title>SHARES - System-on-chip Heterogeneous Architecture Recognition Engine for Speech</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D048605/1</gtr:grantReference><gtr:abstractText>The availability of viable, robust speech recognition systems has the potential to revolutionalise the way that people interact with mobile technology. This implies moving beyond simple call home type commands, to being able to dictate arbitrary, extensive e-mails to your mobile device and to reliably and efficiently access its increasingly complex features using natural speech. This will unlock the potential of next generation portable technology to the widest range of potential users in many important application scenarios e.g. for emergency services and military environments as well as time-efficient business and consumer usage. The current issue is, however, that the increasing algorithmic complexity needed to meet user expectations for naturalness and robustness far exceeds the processing and power capabilities forecast for current embedded processor technology. New architectures are therefore needed to radically advance the pace of state-of-the-art recognition technology for mobile and embedded devices.Commercial speech recognition engines for mobile applications are typically small footprint versions of desktop solutions, with the recognition functionality for acceptable quality highly constrained to the processing and power budget available on any given embedded platform. Applications are typically constrained to a few commands and name or song lists. In comparison, state-of-the art research systems on natural unconstrained speech run up to 200-times slower than real-time on 2.8 GHz Xeon processors. In addition, algorithmic research to maintain recognition accuracy in acoustically noisy operating environments, considered essential to widespread adoption of recognition technology, points towards even greater complexity. The gap between algorithmic requirements and the processing and power capability of conventional processor platforms is thus growing even further.For large vocabulary continuous speech recognition (LVCSR) engines, decoding the most likely sequence of words is essentially an extremely large scale search problem over all possible word combinations. To cope with the huge size of the potential search space, search networks created dynamically during decoding were, until recently, considered the only viable approach to realise large vocabulary recognition. Static networks were too big for all but more constrained vocabulary tasks. However, in a significant departure from accepted wisdom, full expansion of large vocabulary static search networks prior to decoding has been importantly demonstrated using the Weighted Finite State Transducer (WFST). The WFST structure creates considerable potential for achieving efficient regularised decoding architectures, which we intend to exploit. To our knowledge, we would be the first to specifically exploit the Weighted Finite State Transducer network decoding framework in novel hardware architectures for low power large complexity speech recognition.</gtr:abstractText><gtr:fund><gtr:end>2010-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>503467</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>80000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Demonstration system for real-time large vocabulary continuous speech recognition</gtr:description><gtr:end>2010-12-02</gtr:end><gtr:fundingOrg>Invest Northern Ireland</gtr:fundingOrg><gtr:fundingRef>PoC132</gtr:fundingRef><gtr:id>E1F368A8-42E8-4A7E-BDBF-6089AD1B8A11</gtr:id><gtr:outcomeId>5ec8d2665ec8d27a</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The research has been directly employed in a start-up company proposal called MVR (Mobile Voice Recognition). The company won the Northern Ireland Science Park (NISP) &amp;pound;25k HiTech competition.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>526479EC-75A4-4D04-86CC-217B7D7AA246</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>546a47ce55d222.89298705</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>The present disclosure relates to improvements in or relating to pattern recognition, and in particular to a novel systems and methods for pattern recognition, and to devices incorporating such systems and methods. In particular, a spectral analysis is performed on a candidate pattern to obtain a set of observation vectors. The observation vectors are decoded to produce a candidate match between the observation vectors and entries from a knowledge source. The step of decoding the vectors comprises modelling the knowledge source as a weighted finite state transducer (WFST) and the step of decoding comprises modelling the propagation of tokens through nodes of the WFST for observation vectors derived from successive frames of the candidate pattern. For each successive propagation, a list of tokens is sorted in order of their associated cost, and a pruning step is applied to remove tokens from said sorted list which go beyond a pre-determined cost threshold, before performing the next iteration.</gtr:description><gtr:grantRef>EP/D048605/1</gtr:grantRef><gtr:id>ED978159-7F3C-4CE7-A89A-233587FACB4E</gtr:id><gtr:impact>The work became part of a business proposition for a company MVR. Ltd.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>m-2032245604.351391876673c4e</gtr:outcomeId><gtr:patentId>PCT/GB2011/052436</gtr:patentId><gtr:protection>Patent application published</gtr:protection><gtr:title>Pattern recognition</gtr:title></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>The result of the project was the development of a software core that is unique in the fact that it provides desktop performances for large vocabulary speech recognition on embedded platforms with limited resource. This has been made possible by adopting a novel approach where most of the computation is done off-line instead of on-the-fly as it is the case in traditional approaches.</gtr:description><gtr:exploitationPathways>The topic formed the basis of the Queen's University Impact document which has been formally published and a Impact talk given to the Public entitled &amp;quot;Local Talent, Global Impact&amp;quot; on 29th May 2013. A direct Proof of Concept (PoC132) was funded by InvestNI resulting in a patent application and a detailed commercialisation study. A number of follow-on contacts have been made with a number of companies. The project won the 2011 HiTech award at the Northern Ireland Science Park (NISP) &amp;pound;25k awards for the most promising ideas.</gtr:exploitationPathways><gtr:id>D0658426-274D-4055-9842-A22F119034C8</gtr:id><gtr:outcomeId>r-3512736077.89793677733b1c</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.ecit.qub.ac.uk/Research/WirelessCommunicationSystems/Projects/SHARES-System-on-chipHeterogeneousArchitectureRecognitionEngineforSpeech/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C6E0E3FB-342A-4FEF-8D42-37BD0A5C812E</gtr:id><gtr:title>GPU acceleration of automated speech recognition for mobile devices</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/244bbc3b2e7339451e5a795a3bc3ce4e"><gtr:id>244bbc3b2e7339451e5a795a3bc3ce4e</gtr:id><gtr:otherNames>Veitch R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0435-2</gtr:isbn><gtr:outcomeId>doi_53d059059813a962</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AB1FA2A8-5667-4C38-B7AB-6929D32C6D2D</gtr:id><gtr:title>Replacing Uncertainty Decoding with Subband Re-estimation for Large Vocabulary Speech Recognition in Noise</gtr:title><gtr:parentPublicationTitle>INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba17a0bca94c6eb4f13f13cffb2fd49"><gtr:id>dba17a0bca94c6eb4f13f13cffb2fd49</gtr:id><gtr:otherNames>Lv, J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_197206357713e3f7f2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D2536746-B604-42DF-99AE-843AB2A53E8A</gtr:id><gtr:title>Optimization of Weighted Finite State Transducer for Speech Recognition</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Computers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b749297f66ba64272e49e2285a481d9d"><gtr:id>b749297f66ba64272e49e2285a481d9d</gtr:id><gtr:otherNames>Aubert L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d05d05d1a851c2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C20C59BA-C2E2-4496-89D2-3AD19DC2618A</gtr:id><gtr:title>Acceleration of HMM-based speech recognition system by parallel FPGA Gaussian calculation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/244bbc3b2e7339451e5a795a3bc3ce4e"><gtr:id>244bbc3b2e7339451e5a795a3bc3ce4e</gtr:id><gtr:otherNames>Veitch R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-6309-1</gtr:isbn><gtr:outcomeId>doi_53d05c05c70a7298</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3AA5BA8-31E7-4961-9458-A98425C79723</gtr:id><gtr:title>FPGA Implementation of a Pipelined Gaussian Calculation for HMM-Based Large Vocabulary Speech Recognition</gtr:title><gtr:parentPublicationTitle>International Journal of Reconfigurable Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/244bbc3b2e7339451e5a795a3bc3ce4e"><gtr:id>244bbc3b2e7339451e5a795a3bc3ce4e</gtr:id><gtr:otherNames>Veitch R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d0760768c32573</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2F8A8D5A-6B95-4A7F-A813-73BC9BB41651</gtr:id><gtr:title>Adapting noisy speech models ? Extended uncertainty decoding</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2751d64b0863f20bf475b3b5c56c2838"><gtr:id>2751d64b0863f20bf475b3b5c56c2838</gtr:id><gtr:otherNames>Lu J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-4295-9</gtr:isbn><gtr:outcomeId>doi_53d0580582c9c57d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C768227E-52BE-42CF-9214-61CD4C9802FC</gtr:id><gtr:title>Noise Compensation and Missing-Feature Decoding for Large Vocabulary Speech Recognition in Noise</gtr:title><gtr:parentPublicationTitle>INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba17a0bca94c6eb4f13f13cffb2fd49"><gtr:id>dba17a0bca94c6eb4f13f13cffb2fd49</gtr:id><gtr:otherNames>Lv, J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_593541822813e3f6a8</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D048605/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>15BC6F17-6453-42B4-836A-01286E6D8068</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Comput./Corpus Linguistics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>1E31C833-3A35-4F54-A499-31D0C245B5D5</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>System on Chip</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>