<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Statistical Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E843A18C-C593-4E79-B0F4-02FCE63BE7E2"><gtr:id>E843A18C-C593-4E79-B0F4-02FCE63BE7E2</gtr:id><gtr:name>Zoological Soc London Inst of Zoology</gtr:name><gtr:address><gtr:line1>Nuffield Laboratories</gtr:line1><gtr:line2>Regents Park</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>NW1 4RY</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/42F75266-8B03-4CB4-B423-89816E77E737"><gtr:id>42F75266-8B03-4CB4-B423-89816E77E737</gtr:id><gtr:name>Jacobs University Bremen</gtr:name><gtr:address><gtr:line1>PO Box 750</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9"><gtr:id>F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9</gtr:id><gtr:name>Microsoft Research Ltd</gtr:name><gtr:address><gtr:line1>21 Station Road</gtr:line1><gtr:postCode>CB1 2FB</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A0957E9A-8F03-4853-9846-510D5C21CA5B"><gtr:id>A0957E9A-8F03-4853-9846-510D5C21CA5B</gtr:id><gtr:firstName>Kate</gtr:firstName><gtr:otherNames>Elizabeth</gtr:otherNames><gtr:surname>Jones</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/AF6F1E66-72CA-45CC-85EA-60FCD5F61D57"><gtr:id>AF6F1E66-72CA-45CC-85EA-60FCD5F61D57</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Girolami</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/77201EFF-0EDB-4AF0-8DC6-8908902F301D"><gtr:id>77201EFF-0EDB-4AF0-8DC6-8908902F301D</gtr:id><gtr:firstName>Gabriel</gtr:firstName><gtr:otherNames>Julian</gtr:otherNames><gtr:surname>Brostow</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK015664%2F1"><gtr:id>FC2AB483-BAF1-4068-98A1-D34558490149</gtr:id><gtr:title>ENGAGE : Interactive Machine Learning Accelerating Progress in Science, An Emerging Theme of ICT Research</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K015664/1</gtr:grantReference><gtr:abstractText>Our vision is to establish and lead a new theme in ICT research based on Interactive Machine Learning (IML). Our expansion of IML will give scientists and non-ICT specialists unprecedented access to cutting-edge Machine Learning algorithms by providing a human-computer interface by which they can directly interact with large scale data and computing resources in an intuitive visual environment. In addition, the outcome of this particular project will have a direct transformative impact on the sciences by making it possible for non-programming individuals (scientists), to create systems that semi-automatically detect objects and events in vast quantities of A) audio and B) visual data. By working together across two parallel, highly interconnected streams of ICT research, we will develop the foundations of statistical methodology, algorithms and systems for IML. As an exemplar, this project partners with world leading scientists grappling with the challenge of analysing enormous quantities of heterogeneous data being generated in Biodiversity Science.</gtr:abstractText><gtr:potentialImpactText>This research project ultimately will have a broad impact across a range of disciplines and contribute to the strategic development of the EPSRC portfolio in ICT. By the nature of research that reaches across a number of disciplines within ICT and beyond to other sciences, there is a range of beneficiaries as detailed below.

Machine Learning
The ML academic and industrial community will benefit from the investigation of the proposed methods, and systems for IML, which have far wider impact than the application areas being targeted by this research. While Stream.A. of the proposal will develop, analyse and apply new ML methodologies specifically within the IML framework, there is considerable hope that this will lead to further substantial cross-fertilisation of ideas within the ML research community pertaining to user interaction and the formal quantification of information and uncertainty inherent in the synthesis of user and system as a whole. 

Computer Vision
Only a few individual groups in the Computer Vision community have put effort into building interactive systems. Dissemination of our findings and prototypes from this project will help focus the CV community on challenges beyond the typical objectives of &amp;quot;just&amp;quot; making algorithms run real-time: the labeler(s) providing the training data must be modeled just like other variables. Most importantly, to maintain our field's track record of transferring technology beyond academia, we must plan ahead to models and algorithms that perform online learning as specialized users become more sophisticated and demanding. 

Communications and Engagement

The project will directly impact ICT and scientific communities via workshops, publications, public software releases, and the training of highly qualified personnel. Further details about academic value are explained in the main proposal, so the focus here is on impact beyond the ICT academic sphere.

As a major new area of ICT is being established, it is important that a community is built to foster the interface between the various disciplines that this research in IML will have impact on, to share early results, stimulate enquiry and adoption of the research results, as well as to encourage a wider community to engage in this area of research. To engage researchers and users of IML systems, we will organize two qualitatively different styles of workshops. The first set of workshops will be carried out in the top venues for ML (NIPS), CV (ICCV), and Human Computer Interaction (CHI). These workshops will perform the crucial task of bridging the separate research communities to create the strong inter-community bonds necessary for long-term research in IML. 

The second style of workshop will be a hands-on workshop to introduce our IML tools to non-programming scientists who can apply them in their own work. These workshops will be modelled after similar, highly successful endeavours supporting open-source software by the Blender Foundation, a non-profit corporation that has created a number of computer animated short films.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-01-06</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>674580</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Our vision is to establish and lead a new theme in ICT research based on Interactive Machine Learning (IML). Our expansion of IML will give scientists and non-ICT specialists unprecedented access to cutting-edge Machine Learning algorithms by providing a human-computer interface by which they can directly interact with large scale data and computing resources in an intuitive visual environment. In addition, the outcome of this particular project will have a direct transformative impact on the sciences by making it possible for non-programming individuals (scientists), to create systems that semi-automatically detect objects and events in vast quantities of A) audio and B) visual data.</gtr:description><gtr:exploitationPathways>By working together across two parallel, highly interconnected streams of ICT research, we will develop the foundations of statistical methodology, algorithms and systems for IML. As an exemplar, this project partners with world leading scientists grappling with the challenge of analysing enormous quantities of heterogeneous data being generated in Biodiversity Science.</gtr:exploitationPathways><gtr:id>A10F18BF-FBA1-4A4E-B4B2-9C1C5A1B79DF</gtr:id><gtr:outcomeId>56defba43b8e77.08269350</gtr:outcomeId><gtr:sectors><gtr:sector>Agriculture, Food and Drink,Government, Democracy and Justice</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>467E062D-B510-49AF-B9AD-BADE19DACB98</gtr:id><gtr:title>Markov Chain Monte Carlo from Lagrangian Dynamics.</gtr:title><gtr:parentPublicationTitle>Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb50b2c3e4e325d75a56c0b839a8e54b"><gtr:id>cb50b2c3e4e325d75a56c0b839a8e54b</gtr:id><gtr:otherNames>Lan S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1061-8600</gtr:issn><gtr:outcomeId>5675e9c5cca3d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52D065CB-808F-4178-8477-E53E97687BA6</gtr:id><gtr:title>Interpretable Transformations with Encoder-Decoder Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dedc8dfcfa05fba6a5b96bd79a1d91f"><gtr:id>1dedc8dfcfa05fba6a5b96bd79a1d91f</gtr:id><gtr:otherNames>Worrall D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aaa9758e5f873.90378801</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8744CC72-57F2-46BC-BFF7-6187F38DFDE1</gtr:id><gtr:title>Becoming the expert - interactive multi-class machine teaching</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0cc11a9fdef43ccd3002b1dd9383443"><gtr:id>f0cc11a9fdef43ccd3002b1dd9383443</gtr:id><gtr:otherNames>Johns E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9c8291a5193.87648754</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>431492EC-B334-49B4-9333-43E60CC6C97C</gtr:id><gtr:title>Acoustic identification of Mexican bats based on taxonomic and ecological constraints on call design</gtr:title><gtr:parentPublicationTitle>Methods in Ecology and Evolution</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b71c5b164e945887aa80af26b6dcb191"><gtr:id>b71c5b164e945887aa80af26b6dcb191</gtr:id><gtr:otherNames>Zamora-Gutierrez V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d434de33b90.63165936</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94317ED8-51D8-4A1B-8796-71937C3BA18C</gtr:id><gtr:title>A comparative evaluation of stochastic-based inference methods for Gaussian process models</gtr:title><gtr:parentPublicationTitle>Machine Learning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0994dc6d8fb1ec5f6d4ec08f30249e6b"><gtr:id>0994dc6d8fb1ec5f6d4ec08f30249e6b</gtr:id><gtr:otherNames>Filippone M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dd7f0811b126.87487015</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>04ACDA30-5102-49FC-A756-076CCDD89CF5</gtr:id><gtr:title>Bat detective-Deep learning tools for bat acoustic signal detection.</gtr:title><gtr:parentPublicationTitle>PLoS computational biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b14858a357a2e51a1e4e7224130e349"><gtr:id>8b14858a357a2e51a1e4e7224130e349</gtr:id><gtr:otherNames>Mac Aodha O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>1553-734X</gtr:issn><gtr:outcomeId>5aaa9687785d74.38196787</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D40C78E2-1951-4EC2-9FB3-F5DAF8ACA514</gtr:id><gtr:title>Online learning with (multiple) kernels: a review.</gtr:title><gtr:parentPublicationTitle>Neural computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23e98178c0742a548277673d9392dcbf"><gtr:id>23e98178c0742a548277673d9392dcbf</gtr:id><gtr:otherNames>Diethe T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0899-7667</gtr:issn><gtr:outcomeId>56dd7f5b7ea336.63775118</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B31E9637-0E2F-4753-BD41-F711F06E4AF5</gtr:id><gtr:title>Biases of acoustic indices measuring biodiversity in urban areas</gtr:title><gtr:parentPublicationTitle>Ecological Indicators</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1356d9188d13c10a854093a6c18269c1"><gtr:id>1356d9188d13c10a854093a6c18269c1</gtr:id><gtr:otherNames>Fairbrass A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a352bac5a8db3.13060859</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>91DE120A-84DD-4DE7-B5C2-DC32E2A4F0FD</gtr:id><gtr:title>Forecasting the combined effects of climate and land use change on Mexican bats</gtr:title><gtr:parentPublicationTitle>Diversity and Distributions</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b71c5b164e945887aa80af26b6dcb191"><gtr:id>b71c5b164e945887aa80af26b6dcb191</gtr:id><gtr:otherNames>Zamora-Gutierrez V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a352c6d02e770.09342517</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F39CF6A1-D650-4C10-B41B-3306EEC7FF53</gtr:id><gtr:title>Putting the Scientist in the Loop -- Accelerating Scientific Progress with Interactive Machine Learning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ff51beb4a33e863f949a5b4c2437462"><gtr:id>2ff51beb4a33e863f949a5b4c2437462</gtr:id><gtr:otherNames>Aodha O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d9c82939d320.83417881</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB26BDED-021A-40BF-8A3E-78536E632813</gtr:id><gtr:title>Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ff51beb4a33e863f949a5b4c2437462"><gtr:id>2ff51beb4a33e863f949a5b4c2437462</gtr:id><gtr:otherNames>Aodha O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56d9c828ce4238.88565212</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>63B5B39C-B11C-4E7F-806C-9C05D34BCB17</gtr:id><gtr:title>QuantiFly: Robust Trainable Software for Automated Drosophila Egg Counting.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5757ac37df6126bc5f5c2b6f086b6e17"><gtr:id>5757ac37df6126bc5f5c2b6f086b6e17</gtr:id><gtr:otherNames>Waithe D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>56d9c828911397.93545747</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>93165595-81EE-4A58-AB7A-6098EE5CC9D2</gtr:id><gtr:title>Structured Prediction of Unobserved Voxels from a Single Depth Image</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c3b15c2b4b54095a6b102d484d5cb3b7"><gtr:id>c3b15c2b4b54095a6b102d484d5cb3b7</gtr:id><gtr:otherNames>Firman M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7ca5143b964.32952787</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9BD7A2D4-43B1-4FBC-BFFB-2F134C6A8580</gtr:id><gtr:title>Hierarchical Subquery Evaluation for Active Learning on a Graph</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ff51beb4a33e863f949a5b4c2437462"><gtr:id>2ff51beb4a33e863f949a5b4c2437462</gtr:id><gtr:otherNames>Aodha O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d9c828b195c9.28055038</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K015664/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>FC2AB483-BAF1-4068-98A1-D34558490149</gtr:id><gtr:grantRef>EP/K015664/1</gtr:grantRef><gtr:amount>674580.03</gtr:amount><gtr:start>2013-02-01</gtr:start><gtr:end>2014-01-06</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>FA40B4DB-E290-45C2-B66D-0D1510016CFF</gtr:id><gtr:grantRef>EP/K015664/2</gtr:grantRef><gtr:amount>518914.1</gtr:amount><gtr:start>2014-01-06</gtr:start><gtr:end>2017-03-06</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>