<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Experimental Psychology</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/05999628-A43E-40B9-801A-B882BAA5170D"><gtr:id>05999628-A43E-40B9-801A-B882BAA5170D</gtr:id><gtr:firstName>Thomas</gtr:firstName><gtr:surname>Beesley</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8DFEE9A7-0807-4943-BE1E-8FC2859139F8"><gtr:id>8DFEE9A7-0807-4943-BE1E-8FC2859139F8</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>Robert</gtr:otherNames><gtr:surname>Shanks</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FJ007196%2F1"><gtr:id>6AA6E1A6-6891-4A4B-8216-B9CA55DE6FE2</gtr:id><gtr:title>Representing and responding in the visual world: a new model of contextual cuing.</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/J007196/1</gtr:grantReference><gtr:abstractText>&lt;p>One of the most fundamental psychological functions is the ability to recognize a familiar scene and perform an action relevant to it or an action relevant to some internal goal. Yet we currently have a very limited understanding of this fundamental aspect of human behaviour.&lt;/p>

&lt;p>The project aims to test a newly proposed computational model of this type of learning.This model learns about repeating scenes by creating memories for how specific objects within the scene are arranged with respect to the target object. The specific objectives of this research are to:&lt;/p>

&lt;ul>
 

 &lt;li>Develop a model of contextual cue learning to further our understanding of the processes responsible for learning scene contingencies in the visual world. &lt;/li>

 &lt;li>Test the predictions of this model against the predictions of the Brady and Chun (2007) model using several novel experimental designs.&lt;/li>

 &lt;li>Use eye tracking technology to monitor contextual cuing, providing key data to the field to address fundamental unresolved questions about the role of attention in scene learning.&lt;/li>

 &lt;li>Disseminate the findings to the academic community via high-profile publications and conference presentations, and to the wider public via suitable press releases.&lt;/li>

&lt;/ul></gtr:abstractText><gtr:potentialImpactText>Visual context learning is a fundamental cognitive process which plays an important role in our ability to comprehend and act within the familiar scenes we experience. However, key aspects of the psychological processes responsible for this learning are poorly understood. The proposed project aims to bridge this knowledge gap with the development of a new formal model of scene learning. This will provide a testable account of a wide range of patterns of data in the contextual cuing literature and recent data from our own laboratory. We will explore the predictions of this model in a series of novel behavioural experiments.The primary beneficiaries of this research will be academic in a range of fields within the cognitive sciences. The research ideas stem from the vast literature on associative learning theory. The data will therefore be of interest to researchers with broad interests in animal and human learning, and computational models of learning. The tasks we use are taken from the implicit learning literature and therefore the data will be important for researchers with interest in the explicit/implicit learning distinctions, which are of central importance within cognitive psychology. The research extends the application of associative learning theory to theories of visual context (or scene) learning. This will of course extend the intended audience to the fields of visual perception and attention. Finally, specific attentional components will be tested using eye tracking tools. The relative novelty of these techniques within cognitive psychology means that the methodological and data analysis aspects of our research will be of great interest to researchers in a wide range of fields within cognitive psychology. We will maximise the impact of our research within these academic fields with publications in journals with broad readership (eg Journal of Experimental Psychology: General; Cognitive Science). Similarly, presentations will be given at conferences with attendance from a wide range of research fields (Meeting of the Cognitive Science Society; Annual Meeting of the Psychonomic Society; Meetings of the Experimental Psychology Society). We also believe that the research has wider societal benefits. For example, a greater understanding and formal theories of visual context learning would facilitate the development of artificial systems capable of navigation and scene recognition. The uses of these technologies could therefore include the development of robotics for military use and scene recognition in surveillance systems.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-09-02</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2012-09-03</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>163320</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our research findings have yielded 2 publications thus far in leading journals. One of these (in JEP:LMC) reports the results of our tests of theories of attention deployment in contextual cuing and is principally of interest to other researchers in the field. Our results will be of interest not only for experimental psychologists working in the area of human learning and memory, but also to researchers working on visual cognition in artificial intelligence.

The second publication (in BRM) is likely to have considerably wider impact. It describes a new algorithm we have developed for the correction of eye-tracking data that is now available for any researcher working with these important and fast-developing methods.

This research project has also consolidated a fruitful collaboration between David Shanks (PI of the project), Miguel A. Vadillo (now working at King's College London) and Tom Beesley (University of New South Wales) that remains fully active despite the end of the project. A further grant application is under development.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>F6F30149-8B34-443B-943F-57D8FC662D63</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>559cfe8c3b8363.91842884</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The present project developed and tested a new model of visual search that aimed at explaining how we learn to find objects in familiar environments. The predictions of the model have been confirmed by several experiments and computational modeling techniques. Furthermore, these predictions have also been explored using eye-tracking measures that allow us to study how people deploy their attentional resources during visual search in familiar contexts.

The conclusions of our studies have important implications for any attempt to design artificial intelligent algorithms that mimic humans' ability to deploy visual attention efficiently. They also provide a unique insight into the mechanisms of human learning and memory.

This research project has also consolidated a fruitful collaboration between David Shanks (PI of the project), Miguel A. Vadillo (now working at King's College London) and Tom Beesley (University of New South Wales) that remains fully active despite the end of the project. Thanks to the present project we have also contributed to the development of better methods for the use of eye-tracking technologies in behavioural research.</gtr:description><gtr:exploitationPathways>Our results will be of interest not only for experimental psychologists working in the area of human learning and memory, but also to researchers working on visual cognition in artificial intelligence.

Furthermore, as a result of our research we have developed a new algorithm for the correction of eye-tracking data that is now available for any researcher working with these methods. Further details about these methods and potential applications of our research can be found at the websites of the authors:

David Shanks (https://sites.google.com/site/davidshanksucl/), Miguel A. Vadillo (http://mvadillo.com), and Tom Beesley (http://tombeesley.wordpress.com/).</gtr:exploitationPathways><gtr:id>6685C4B8-5745-432C-8FF8-D2C2BA675F8B</gtr:id><gtr:outcomeId>544f9fd99c3595.02790560</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Education,Electronics</gtr:sector></gtr:sectors><gtr:url>http://discovery.ucl.ac.uk/1428694/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>3049E8B7-4121-4C07-AC44-C8193407DD98</gtr:id><gtr:title>Pre-exposure of repeated search configurations facilitates subsequent contextual cuing of visual search.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Learning, memory, and cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/48a8725095646e9568038cab5d4e1d35"><gtr:id>48a8725095646e9568038cab5d4e1d35</gtr:id><gtr:otherNames>Beesley T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0278-7393</gtr:issn><gtr:outcomeId>54369292cc78a8.54252893</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5C3433BD-7000-4EBC-B540-162B64D8715E</gtr:id><gtr:title>A simple algorithm for the offline recalibration of eye-tracking data through best-fitting linear transformation.</gtr:title><gtr:parentPublicationTitle>Behavior research methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e8e70bdd76c37d70a5b8122ff96420be"><gtr:id>e8e70bdd76c37d70a5b8122ff96420be</gtr:id><gtr:otherNames>Vadillo MA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1554-351X</gtr:issn><gtr:outcomeId>544cece6f3f756.05094516</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8F899B19-5BB0-4D24-A5C1-4F20AC0B502D</gtr:id><gtr:title>Underpowered samples, false negatives, and unconscious learning.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e8e70bdd76c37d70a5b8122ff96420be"><gtr:id>e8e70bdd76c37d70a5b8122ff96420be</gtr:id><gtr:otherNames>Vadillo MA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>589850db3b4b68.22081745</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>59328488-28C9-48F8-88D9-749A515E5E3F</gtr:id><gtr:title>Configural learning in contextual cuing of visual search.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/48a8725095646e9568038cab5d4e1d35"><gtr:id>48a8725095646e9568038cab5d4e1d35</gtr:id><gtr:otherNames>Beesley T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5898504f9cdd68.25676142</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/J007196/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>