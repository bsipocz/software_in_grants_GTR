<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E07FDFB3-EE00-4C11-B570-773B13A3EDCD"><gtr:id>E07FDFB3-EE00-4C11-B570-773B13A3EDCD</gtr:id><gtr:name>International Commission on Illumination</gtr:name><gtr:address><gtr:line1>CIE Central Bureau</gtr:line1><gtr:line2>Kegelgasse 27</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Austria</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/72AE482E-97F3-4497-AF43-5D48736CC8B4"><gtr:id>72AE482E-97F3-4497-AF43-5D48736CC8B4</gtr:id><gtr:name>Imsense Ltd</gtr:name><gtr:address><gtr:line1>Norwich Bio-Incubator</gtr:line1><gtr:line2>John Innes Centre</gtr:line2><gtr:line3>Colney Lane</gtr:line3><gtr:postCode>NR4 7UH</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2A5C593E-43C3-4E03-9530-990C61A80560"><gtr:id>2A5C593E-43C3-4E03-9530-990C61A80560</gtr:id><gtr:firstName>Graham</gtr:firstName><gtr:surname>Finlayson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH022236%2F1"><gtr:id>9E476026-32A4-40D4-9188-90EB4412F2C5</gtr:id><gtr:title>Illuminating Colour Constancy: from Physics to Photography</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H022236/1</gtr:grantReference><gtr:abstractText>In daily life, we depend on colour images which represent the real world, from photographs of key personal events to pictures of possible purchases. In general, these are poor approximations of the real thing. Our aim is to understand better how we perceive colours in the real world, and how to recreate that perception with images. Central to these aims is colour constancy, a fundamental phenomenon which keeps object colours stable even under large changes in the colour of the illumination - we see an apple as red whether it is under bluish daylight or yellowish tungsten light. Camera sensors, which faithfully record the changing light signals, do not naturally possess colour constancy. But digital cameras are often equipped with special colour balancing modules to cope with changes in lighting, and the photographs they produce may be further processed to remove colour casts. In computer vision, such 'color correction' algorithms are necessary to enable machines to use colour as a reliable cue - for example, in automated grading of manufactured goods such as tiles. Human vision and computer vision are typically studied in isolation from each other: the first aims to understand why colours appear as they do to humans, and the other to make them as useful as possible to machines, regardless of how they appear. These two goals are generally not identical, because neither human nor computer colour constancy is perfect.To bridge colour constancy from humans to machines we will perform an innovative set of experiments. First, we will systematically study illuminant metamerism. Metamerism is what makes all image reproduction work: two stimuli with vastly different colour spectra can induce the same colour percept. The light invoking a white percept on a TV has a highly spiky spectrum compared to the flat spectral reflectance of a piece of white paper in daylight. Yet, illuminants which look the same when shining on white paper can sometimes make other surfaces change appearance. We experience this phenomenon when we buy clothes which look good under the artificial shop lights but less satisfactory when we take them outdoors. We will quantify this effect for real scenes under real lights using a new 'tuneable' spectral illuminator with which we can generate any light spectrum. Our second innovation is to make use of newly available High-Dynamic-Range (HDR) displays. In contradistinction to the real world where the brightest point in the scene may be a 100000 times as bright as the darkest point, most displays struggle to produce a dynamic range of even 1000:1 and printed photographs are at most 100:1. Yet we know that colour perception depends on the overall dynamic range of the scene. The new HDR displays can output contrast ratios of 100000:1 and we will use them to measure constancy in lab conditions but with real world brightnesses. A third challenge that we face in making colour photographs match our perception of the real world is the inaccuracy of colour memory. Typically, when we view a photograph, we do not have the real thing to compare it with, but must recall the original scene from memory. The imperfections of our memory then may taint our judgment. It is well known that our memory colours for familiar objects such as sky, grass, and skin tend to be 'over-saturated' -- grass may be remembered as greener and the sky as bluer than they actually are. Thus, when we test colour correction algorithms by asking people which image they prefer, we might find that they do not prefer the one that most accurately reproduces the original scene, but instead matches their imperfect memory. We will quantify these effects of memory and preference. Finally, our research will, at all stages, consider how measured percepts of colour might be predicted by mathematical models. Ultimately, we will design algorithms to automatically see colours as we do, making for better photographs and more useful vision machines.</gtr:abstractText><gtr:potentialImpactText>Colour images are ubiquitous in daily life, used for professional, commercial, and personal purposes, yet they often fail to reproduce an object or scene as it would appear in 'real life' - in art reproductions, holiday photographs, product depictions in catalogues or websites, newscasts, and more. This failure generally arises because colour images do not successfully incorporate colour constancy. Colour constancy poses an important engineering problem for the design of devices that produce colour images -- in many domains, from digital cameras, television, remote sensing, displays, printing and industrial inspection. Indeed, Edwin Land (founder of Polaroid), argued that colour constancy was the fundamental problem in colour vision. Our research aims to provide a step change in our understanding of the theory and practice of colour constancy, advancing both models of human perception as well as colour correction algorithms for devices. It will, by definition, target a broad range of beneficiaries: both end-users such as professional and amateur photographers, designers, artists, marketing personnel, computer game designers and the public in general; and the commercial users who will ultimately deliver the improvements in colour images, such as camera and display manufacturers and imaging software companies. Our direct partners will benefit most immediately from this research programme but will also ensure its high impact. Imsense Ltd is a spin out company from the University of East Anglia exploiting Prof. Finlayson's work on dynamic range compression (www.imsense.com). The company also have an interest in other aspects of image quality, including colour constancy. Dolby, who make proprietary software packages for high dynamic range (HDR) displays, are interested in understanding how human observers see colour on HDR displays. Our data will help them make better pictures. The CIE (The International Commission on Illumination, http://www.cie.co.at/index_ie.html) are also partners on this project and they have agreed to help us disseminate our work to industry and academics through CIE Division 8 (which focuses on Imaging Technology). Professor David Brainard (U. Pennsylvania) is a world-leading authority on colour constancy who has agreed to collaborate on all aspects of the project. The impact of our research will be significantly enhanced through our links with Prof Brainard, especially for dissemination in the USA. More generally, colour constancy and colour image processing are key areas in computer vision and image processing, as well as in visual psychophysics. We wish to ensure the widest impact to these communities and beyond. As part of the project plan we will deliver both new data and data analysis tools to these communities. A variety of new algorithms will also be developed and these will also be released, as a Matlab toolbox. Another key deliverable of this project is a database of calibrated, high dynamic range, multispectral images. To our knowledge this type of image set does not yet exist and we expect many follow-on works to make use of this data. We are also planning to present our work to scientific meetings that have a broad scientific appeal including the Digital Futures meetings of the Royal Photographic Society (http://www.rps-isg.org/), selected monthly meetings of the Colour Group (www.colour.org.uk), whose audience includes graphic designers, display engineers, photographers, artists, museum curators and others, the International Colour Association congress (AIC 2013), which will be held in Newcastle, and (we hope) at a Royal Society Summer Exhibition meeting (2011/2012). See impact plan for details.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-11-14</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-05-15</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>643759</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The research project contributed to an exhibition about colour in the National Gallery:

http://www.nationalgallery.org.uk/about-us/press-and-media/press-releases/making-colour</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>B00AB7CF-D66A-4C83-BCB8-23996079BC1F</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d764677c8ab6.41462681</gtr:outcomeId><gtr:sector>Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Colour Constancy is a surprising and fundamental aspect of vision. The physics of how we see: light reflects from a scene and we measure with three cone classes (roughly, red-, green- and blue- sensors) teaches that the colors we see should depend on the colour of the light. Simply under a redder light the spectra being reflected from the scene are redder (compared with, say, a bluer light). We see constant colors because of processing undertaken by our visual system. Computer vision and digital photography seek to make their camera measurements equally colour constant. Colour constancy is a large field and has been extensively studied for 50 years.

Our research made several important discoveries. In terms of machine vision a key problem stymieing existing research is that the colour constancy afforded by an automated system tended to be tied to the image exposure. Colour constancy has two parts: determining the colour of the prevailing light and then correcting the image to remove any colour bias (due to the light). We developed leading methods for illuminant estimation and colour correction that are exposure independent.

This research complements or psychophysical work which demonstrated the human visual system has better colour constancy for lights which are typical (the yellows, whites and blues that fall along the Planckian Locus).</gtr:description><gtr:exploitationPathways>Apple, patented our work on illuminant estimation (through a commercial relationship with UEA). It is entirely possible that this work will be commercialized (though, we are not privy to how Apple use its IP)</gtr:exploitationPathways><gtr:id>83A4EE92-328D-430D-9EE1-48358CB97BC4</gtr:id><gtr:outcomeId>56d75542026cf7.87171306</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>4DDE3BD5-DC5E-4BC4-8E48-A980DC5E1046</gtr:id><gtr:title>Web-based Image Preference</gtr:title><gtr:parentPublicationTitle>Journal of Imaging Science and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da255050404c975334359f21f4f0cb44"><gtr:id>da255050404c975334359f21f4f0cb44</gtr:id><gtr:otherNames>Harris M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f95e95e81d3e5e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E693BC1-2908-4631-B873-42CECAE2CABC</gtr:id><gtr:title>The Zeta-image, illuminant estimation, and specularity manipulation</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2cacdc99d24acf09aef11e8bea3e98f3"><gtr:id>2cacdc99d24acf09aef11e8bea3e98f3</gtr:id><gtr:otherNames>Drew M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54634e38a58741.34999664</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3981BADB-3DF2-4CFC-9F49-573C0EBB1256</gtr:id><gtr:title>General Lp Constrained Approach for Colour Constancy</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>56c47916266530.99716606</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AADD3B51-9DD9-4625-A242-3935D1E892A4</gtr:id><gtr:title>Colour Constancy from Both Sides of the Shadow Edge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/77120a64bce7fd383dd90aa6fb7ba4cb"><gtr:id>77120a64bce7fd383dd90aa6fb7ba4cb</gtr:id><gtr:otherNames>Lynch S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54636c9894afc2.81878297</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C49CDF51-46C4-452F-8D6D-9ED152E83F76</gtr:id><gtr:title>Spectral sharpening by spherical sampling.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>54634e396c9e21.01482446</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2F8C3C9B-C238-4E8E-B4EC-DF8D50D35757</gtr:id><gtr:title>Corrected-Moment Illuminant Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56c46e5c537aa7.08484711</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E1E0BDF-F2AE-4C57-B4A7-3B5835981C2E</gtr:id><gtr:title>Shades of Gray: An Experiment in the Context of Human Vision</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4dadcf7f87e6b9fccc9927e1204ff71b"><gtr:id>4dadcf7f87e6b9fccc9927e1204ff71b</gtr:id><gtr:otherNames>Troconso-Rey P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56c47096e35e48.71048288</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CB6848C0-9AAD-4928-AEA2-9F2F91513F66</gtr:id><gtr:title>Corrected-Moment Illuminant Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54636c98bb2df1.57532639</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6479F26E-33FB-4C7D-B5E3-9BD2E6A7CD32</gtr:id><gtr:title>Chromatic illumination discrimination ability reveals that human colour constancy is optimised for blue daylight illuminations.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91e8064bd6dbd1db56b1eaa537ab1509"><gtr:id>91e8064bd6dbd1db56b1eaa537ab1509</gtr:id><gtr:otherNames>Pearce B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>54634e3925ed85.51022094</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B675E89F-126C-4D41-A0FF-CD8433706A39</gtr:id><gtr:title>Human colour constancy by achromatic adjustment for real scenes under multiple illuminations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41f688ee245bd3cfd4f9d980d52a3595"><gtr:id>41f688ee245bd3cfd4f9d980d52a3595</gtr:id><gtr:otherNames>Chriton S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56c471b5bd1c62.32325688</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>59ACEECF-677A-4838-9592-BE5C0711D8D3</gtr:id><gtr:title>Spectral sharpening by spherical sampling</gtr:title><gtr:parentPublicationTitle>JOURNAL OF THE OPTICAL SOCIETY OF AMERICA A-OPTICS IMAGE SCIENCE AND VISION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/85a2f8e5df7c8816f20ad5a314d10f6a"><gtr:id>85a2f8e5df7c8816f20ad5a314d10f6a</gtr:id><gtr:otherNames>Finlayson Graham D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>56c442cc201fe1.75155596</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA090F11-C157-4D15-A4B1-77688EBFF9A1</gtr:id><gtr:title>The Role of Bright Pixels in Illumination Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/725e06060725bf434d0f29650980e7c6"><gtr:id>725e06060725bf434d0f29650980e7c6</gtr:id><gtr:otherNames>Reza H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56c475434eede1.02302045</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CAEF4479-3C21-4AC9-B5B9-ED374C70A54C</gtr:id><gtr:title>Specularity, the Zeta-image, and Information-Theoretic Illuminant Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2f0c51da149e012998a6613378cf9813"><gtr:id>2f0c51da149e012998a6613378cf9813</gtr:id><gtr:otherNames>Drew MS</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56c4730ac9fbc6.35807304</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>281BB842-5381-40C9-BA0B-F82E618A5194</gtr:id><gtr:title>Revisiting Surface Colour Estimation Under Varying Illumination</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>56c47bfcc1f730.26469735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E4765F56-C82A-4BB2-832D-C424A369C07C</gtr:id><gtr:title>Spectrally Tunable LED illuminator for Vision Research</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fc6ecf3f4f71076dd6b6478dc4643ead"><gtr:id>fc6ecf3f4f71076dd6b6478dc4643ead</gtr:id><gtr:otherNames>Mackiewicz M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56c476ac8e71b2.38171062</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DD8CFDA8-D061-4A93-A7AB-D07CFF66F1C6</gtr:id><gtr:title>Colour Constancy by Illumination Matching in Real World Scenes</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91e8064bd6dbd1db56b1eaa537ab1509"><gtr:id>91e8064bd6dbd1db56b1eaa537ab1509</gtr:id><gtr:otherNames>Pearce B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56c4520eb5ff45.41756201</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C3FC7D1-3743-4FAE-A5EE-0CFC75D0F52E</gtr:id><gtr:title>A Compact Singularity Function to Predict WCS Color names and Unique Hues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8fdc63602442331c6baa650163940424"><gtr:id>8fdc63602442331c6baa650163940424</gtr:id><gtr:otherNames>Vazquez-Corra1 J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>56c47dbf8325b5.05505952</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>624B795B-EDF0-44DE-BADF-5A307735A24A</gtr:id><gtr:title>A new spectrally sharpened sensor basis to predict color naming, unique hues, and hue cancellation.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c35dfe7db36beba565b3f65be606b09b"><gtr:id>c35dfe7db36beba565b3f65be606b09b</gtr:id><gtr:otherNames>Vazquez-Corral J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>5463550d5ab408.28585203</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C41D27B2-031A-4FCF-BCA5-B64123DB5415</gtr:id><gtr:title>Achromatic adjustment outdoors and indoors using the Mirasol re?ective display</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fc6ecf3f4f71076dd6b6478dc4643ead"><gtr:id>fc6ecf3f4f71076dd6b6478dc4643ead</gtr:id><gtr:otherNames>Mackiewicz M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56c470040acc04.37823707</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13BA61C8-76CC-45C3-9952-B4290210C0EE</gtr:id><gtr:title>Color Constancy from both sides of the shadow edge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df91afe275f202e78c18478d4a8baf94"><gtr:id>df91afe275f202e78c18478d4a8baf94</gtr:id><gtr:otherNames>Lynch SE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56c46d95275db2.20513716</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C88A0F21-B5F8-4B6F-A987-492D8D730202</gtr:id><gtr:title>Compact Singularity Function to Predict WCS Color names and Unique Hues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bbfca120506d94dd548c660e26a91e72"><gtr:id>bbfca120506d94dd548c660e26a91e72</gtr:id><gtr:otherNames>Vazquez J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>56c47a70059017.44211112</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E1AFB570-1BC8-4D96-A4C6-EA8C2B7C74B6</gtr:id><gtr:title>Human colour constancy under wide-ranging illuminations: a bias for blue</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3370c71b3f2f0a9f996fb0aadbeef8c"><gtr:id>d3370c71b3f2f0a9f996fb0aadbeef8c</gtr:id><gtr:otherNames>Hurlbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56c46f647ca6f0.70856486</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>12C9F293-D63B-4334-9B60-BFD8386C6F16</gtr:id><gtr:title>On calculating metamer sets for spectrally tunable LED illuminators.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>54634e38ce6123.40700562</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B8C4322B-265F-4C25-B347-59C365D6B387</gtr:id><gtr:title>Color correction using root-polynomial regression.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5675e0caa2d73</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>50FE2658-D6C1-4864-89E6-0A6D57F06A2B</gtr:id><gtr:title>Root Polynomial Color Correction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>56c4798fe73c33.53447469</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A298CC5-1F9D-4065-A9B1-46604B69AAA6</gtr:id><gtr:title>Are Colour Constancy Mechanisms Biased for Typical Illuminations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91e8064bd6dbd1db56b1eaa537ab1509"><gtr:id>91e8064bd6dbd1db56b1eaa537ab1509</gtr:id><gtr:otherNames>Pearce B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56c472729074e5.67888884</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B596CDBE-C0BC-4CDA-AFBF-18CF0F487237</gtr:id><gtr:title>General ?
&lt;sup>p&lt;/sup> constrained approach for colour constancy</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4673-0062-9</gtr:isbn><gtr:outcomeId>54636c98e2cfa9.87702499</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E818A95B-4516-47DE-8736-C873BFC392DA</gtr:id><gtr:title>IMPROVEMENT OF COLORIZATION REALISM VIA THE STRUCTURE TENSOR</gtr:title><gtr:parentPublicationTitle>International Journal of Image and Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c88c00518f1e6da6d498cae5ccf50cd4"><gtr:id>c88c00518f1e6da6d498cae5ccf50cd4</gtr:id><gtr:otherNames>DREW M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54634e39947a10.77837763</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>35CE6F9D-D49A-4642-892A-41E4C9ACDFCA</gtr:id><gtr:title>The Reproduction Angular Error for Evaluating the Performance of Illuminant Estimation Algorithms.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>58c5275e2c0fa4.08176067</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>63B4D008-5EE2-4AB2-84BE-CCC7C164215B</gtr:id><gtr:title>Colour Constancy in Immersive Viewing</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3370c71b3f2f0a9f996fb0aadbeef8c"><gtr:id>d3370c71b3f2f0a9f996fb0aadbeef8c</gtr:id><gtr:otherNames>Hurlbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56c44b032bfdf9.78275279</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F4825EFB-8335-4BA5-AF2F-437056E9F0D4</gtr:id><gtr:title>The illumination correction bias of the human visual system</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6cd3306d60b8f40c0dec24c2d043bb4b"><gtr:id>6cd3306d60b8f40c0dec24c2d043bb4b</gtr:id><gtr:otherNames>Crichton S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56c453808f90b9.50283804</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H022236/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>