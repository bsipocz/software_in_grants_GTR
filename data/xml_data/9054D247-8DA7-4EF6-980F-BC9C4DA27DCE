<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1C0C0EC8-3AEE-4671-A717-FC834D00338C"><gtr:id>1C0C0EC8-3AEE-4671-A717-FC834D00338C</gtr:id><gtr:name>Nissan Motor Company</gtr:name><gtr:address><gtr:line1>1-1 Morinosatoaoyama, Atsugi-shi</gtr:line1><gtr:postCode>243-0123</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1C0C0EC8-3AEE-4671-A717-FC834D00338C"><gtr:id>1C0C0EC8-3AEE-4671-A717-FC834D00338C</gtr:id><gtr:name>Nissan Motor Company</gtr:name><gtr:address><gtr:line1>1-1 Morinosatoaoyama, Atsugi-shi</gtr:line1><gtr:postCode>243-0123</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5624A530-E796-4C66-B878-D1991105BCCD"><gtr:id>5624A530-E796-4C66-B878-D1991105BCCD</gtr:id><gtr:firstName>Niki</gtr:firstName><gtr:surname>Trigoni</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/4D83C9E4-588F-445A-91F8-0702B22B950C"><gtr:id>4D83C9E4-588F-445A-91F8-0702B22B950C</gtr:id><gtr:firstName>Ingmar</gtr:firstName><gtr:surname>Posner</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/C74C9612-26CA-4D91-B65F-260FD21EB7A4"><gtr:id>C74C9612-26CA-4D91-B65F-260FD21EB7A4</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>Michael</gtr:otherNames><gtr:surname>Newman</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ012017%2F1"><gtr:id>9054D247-8DA7-4EF6-980F-BC9C4DA27DCE</gtr:id><gtr:title>Intelligent Workspace Acquisition, Comprehension and Exploitation for Mobile Autonomy in Infrastructure Denied Environments</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J012017/1</gtr:grantReference><gtr:abstractText>Vehicles will only get smarter. There will always be a desire for more machine intelligence and autonomy. Our needs and expectations are ever increasing. As a result, we continue to pack more sensors and more computation into the robots that carry, transport, labour for and defend us. 

Here we interpret autonomy as a robot's ability to sense, understand and ultimately act of its own accord in its operating environment. This proposal is about giving autonomous vehicles the ability to navigate in difficult conditions over long periods of time. Conditions become &amp;quot;difficult&amp;quot; when GPS is denied or only intermittently available, when little, if anything, is known about the environment, when communications are sporadic and unreliable or when operating conditions like lighting change unpredictably. And yet, somewhat perversely, it is often in just these conditions that our need to navigate is greatest: consider, for example, the surveying of buildings in a stricken nuclear facility such as Fukushima, or the autonomous driving of cars at night in cities where GPS coverage is poor. Intelligent navigation lies at the heart of much of mobile robotics research. It finds application in remote inspection, autonomous urban driving, defence, logistics, security and space robotics.

We shall consider how machines can acquire and manage the information they need to operate persistently in workspaces of our choosing. The goal is to demonstrate that performance improves through use and over time - something that comes naturally to humans and is immensely valuable in a machine. This goal poses questions about how the computers that control robots should represent their environment in a plastic fashion - one which can be stretched and pulled into different shapes over time. We also need to consider how to enable machines to decide how to act to improve their understanding of the world - alone and in concert with other vehicles, each with different sensors and capabilities. How can vehicle sensors be calibrated transparently and continuously? How can motion be planned to maximise both the coverage of inspections and the accuracy of workspace assessments? How can successful operation be guaranteed in the presence of unreliable or short-range communications?

This interweaving of the state of the art in navigation, planning and communications management is unusual and will allow us to ask and provide answers to challenging robotics science questions which, when exploited, will have a dramatic impact on the robots that will become indispensable in the future.</gtr:abstractText><gtr:potentialImpactText>The technical specifications of processors, sensors, networks and storage devices continue to improve at a staggering rate. Driven by this trend, in recent years autonomous systems research has made enormous strides towards applications
of significant value to the public domain. Robots are in the process of revolutionising the running of ports, mines, hospitals, factories and construction sites. Agents which can operate indefinitely will provide another step-change in the impact intelligent systems have on society. It will come in the shape of smarter transport and plant maintenance, lower cost logistics, more planetary science and increased security. 

This proposal starts with state-of-the-art competencies in navigation, mapping, multi-vehicle coordination and data-flow management. It asks how these can be used in concert to address our thirst for labour-saving, efficiency-increasing, time-saving, security-giving robots and autonomous vehicles. It sets forth a research program built around the persistent acquisition, comprehension and exploitation of workspace models over indefinite periods, large scales and multiple agents.

To demonstrate impact we have several specific scenarios in mind which encompass a spectrum of autonomy:

1) A Mars rover sample-and-return mission which uses visual navigation to estimate its trajectory and, importantly, allows autonomous path retracing. In many ways planetary science epitomises impact in robotics - there is simply no alternative other than using robots. 

2) A consumer, semi-autonomous road vehicle which, for navigation, exploits environmental knowledge acquired from a survey vehicle. In the face of varying lighting and weather conditions, this will be achieved using only a single, low-cost sensor. This approach is in stark contrast to the much vaunted google cars approach, which is always dependent on expensive 3D laser sensors.

3) A heterogeneous fleet of ground vehicles deployed in a reconnaissance role. They must not only navigate in changing conditions, but also plan to maximise the accuracy of their workspace characterisations (e.g. object detection performance) as well as maintaining communications connectivity.

4) Inspection of a nuclear plant or subsea well head with a tele-operated robot. We envision assistive autonomy in which a human operator remains in control of the vehicle but receives guidance on where the camera should be pointed next to ensure a complete and thorough inspection.

We stress that our view of impact is not limited to these four scenarios - they are simply illustrative. What we propose applies to any robotics scenario in which time and time again there is value in knowing &amp;quot;what is where&amp;quot; but one cannot depend on external infrastructure. We do not insist on prior knowledge - we can use it if available but, if not, we will accumulate what is needed autonomously. We do not insist on invariant workspaces - we set out to learn scene dynamics. We do not insist on good communications or superb object detectors - we plan to exploit the platforms' mobility to stay in touch while getting the best views of the world we can.

The mixing of planning, navigation, model-learning and communications management into a single proposal is an unusual, exciting and challenging prospect. It promises impact in the transport, logistics, defence, space and nuclear domains. There will never be fewer robots and what we propose here lies at the core of robotics science and its application to real world tasks.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-10-21</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-10-22</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1093659</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Nissan Motor Company</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>Nissan Collaboration</gtr:description><gtr:id>669BAF49-3841-4232-A97B-B2DB57195CA3</gtr:id><gtr:outcomeId>b96459ceb96459e2-1</gtr:outcomeId><gtr:piContribution>0.5M GBP income raise by a collaboration with Nissan Motor Company. This resulted in a demonstration of driverless car technology to the worlds media in Feb 2012.


In October 2010 a collaborative deal was signed with Nissan Motor Company Inc. They are placing, at their own cost, an RA to work in my group and ?general support funding? for the group. Again, an I.P. deal is at the heart of the agreement
. This has resulted in a demonstration of driverless technology in the UK

See www.robotcar.org.uk</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Michael Tanner (PhD student) judged the &quot;regionals&quot; of Vex Robotics competition (primarily a STEM-outreach program) hosted at Stowe School</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>306273EC-7EA7-4096-8A8A-7E07711BD404</gtr:id><gtr:impact>From: Michael Tanner 
Subject: Vex Robotics Competition
Date: 6 February 2017 at 12:59:07 GMT
To: Paul Newman , Ingmar Posner 

[This is purely an FYI/feel-good email. No action required.]

Paul/Ingmar,

Last week I took a day off to judge the &amp;quot;regionals&amp;quot; of Vex Robotics competition (primarily a STEM-outreach program) hosted at Stowe School
 http://www.vexrobotics.com/vexedr/competition/

I thoroughly enjoyed the event and was fascinated at the creative designs students developed for this year's challenge. Here is a YouTube video showing the types of robots students develop:
 https://www.youtube.com/watch?v=FCck9_vk8H4

The students were quite diverse (boys/girls, 10 - 17 years old, hail from UK/US/China, etc.), but they all shared a deep passion regarding their respective robot designs. The level of knowledge some of the students demonstrated was impressive (e.g., describing the chemical/material property trade-offs between various plastics included in their designs). Once the students learned I was studying robotics at Oxford, I was inuidated with questions.

I went home and immeditally ordered one of Vex Robotics' cheaper toy robotics kits (http://www.vexrobotics.com/vexiq) for my daughters to play with at home.

--
Michael Tanner
Oxford Robotics Institute
Department of Engineering Science
University of Oxford
Comm +44 7514 119187</gtr:impact><gtr:outcomeId>58ca6708922090.94266661</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:url>http://www.vexrobotics.com/vexedr/competition/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPSRC Robotics, Automation &amp; Artificial Intelligence (RAAI) Theme Day</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>BA3BA51F-0F90-4CCF-A747-666AF1C2E6FD</gtr:id><gtr:impact>EPSRC are undertaking a review of our robotics, automation and artificial intelligence portfolios of relevance to Robotics and Autonomous Systems (RAS) in order to evaluate the quality and importance of EPSRC's portfolio of research and training in the area. To facilitate this we are hosting a Theme Day on the 31st January 2017 in Central London.
 
The Theme Day will involve poster presentations from holders of current and recent related grants from across the EPSRC portfolio. A panel of internationally leading experts chaired by Prof David Hogg will use the posters and discussions with attendees to draw conclusions about the portfolio as a whole. The outcomes of the review will be used to inform future strategy in the area of RAAI and will not impact on future funding decisions at a PI level. 
 
The Theme day will be an opportunity for PIs to present their research to the review panel. The day will also give attendees an opportunity to view work of relevance to RAAI from across the EPSRC portfolio and to network with leaders in the area from across the UK.
 
As a holder of such a related grant(s) (details below) we would like to invite you to attend the event.
 
Related Grant(s): EP/I005021/1, EP/J012017/1, EP/M019918/1</gtr:impact><gtr:outcomeId>58ca64c4053467.40485308</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Supporters</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Shell Eco-Marathon June / July 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BC1E18A4-C510-487A-946D-F29136E50B10</gtr:id><gtr:impact>Shell Eco-marathon challenges student teams around the world to design, build, test and drive ultra-energy-efficient vehicles.</gtr:impact><gtr:outcomeId>58c17cf1b3a989.81038712</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.youtube.com/watch?v=kU7OYLgnlkM</gtr:url><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Talk at AHRC Research Network Workshop</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>DA5231D0-9731-4B97-A1D6-1A59A6C3331D</gtr:id><gtr:impact>The action-based alternative to 3D coordinate-frame representation was the main topic of our video meetings throughout 2016, and of our workshop at St John's College, Oxford, in January 2017. This workshop brought together experts from diverse disciplines for a focussed multidisciplinary discussion across three days, testing the action-based hypothesis by assessing its philosophical, computational and neuroscientific consequences. You can see the final discussion of the workshop here. As well as discussions led by Andrew and me, the workshop featured talks by:</gtr:impact><gtr:outcomeId>58ca641fd18677.08782749</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://jamesstazicker.com/research/the-action-based-brain/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>RSS 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>078D6B24-0F0B-4082-86F9-725FC312545F</gtr:id><gtr:impact>Submission of papers to RSS 2015 - listed in publications</gtr:impact><gtr:outcomeId>56e016c05f54d2.23145652</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014,2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>ICRA 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>241F957C-4110-470F-8CCD-03163887D82C</gtr:id><gtr:impact>Submission of papers to ICRA 2015 - listed in publications</gtr:impact><gtr:outcomeId>56e016762d4489.81352038</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014,2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Radio 4 Interview - The Today Programme</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>5AB96F8F-4B54-4494-A860-BD423FF6238E</gtr:id><gtr:impact>Go to 1:34:14 for Paul's interview on the Today Programme:</gtr:impact><gtr:outcomeId>58ca652ea5ea72.19685209</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/b08hl5rt</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1655490</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Life-Long Infrastructure Free Robot Navigation</gtr:description><gtr:end>2016-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/I005021/1</gtr:fundingRef><gtr:id>9F39B9D4-E9AD-4948-821C-19B50747F85B</gtr:id><gtr:outcomeId>56e00ba802d000.54335250</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>4991610</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Mobile Robotics: Enabling a Pervasive Technology of the Future</gtr:description><gtr:end>2020-02-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/M019918/1</gtr:fundingRef><gtr:id>9B55E8E0-2942-4EFD-8F70-7C0184686270</gtr:id><gtr:outcomeId>56e00c2e57cab2.27619783</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Ian Baldwin &amp;quot;Lidar Point Cloud&amp;quot;</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>A18F9B11-6042-4D94-B020-496186021606</gtr:id><gtr:impact>Paul Newman and Ian Baldwin &amp;quot;Lidar Point Cloud&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04791ac7382.54780834</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Lidar Point Cloud</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Deep Image-based Detection</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>8211A5C0-396F-4751-B45A-30D9D64C824E</gtr:id><gtr:impact>Deep Image-based Detection</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94bd8136c90.22556150</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Deep Image-based Detection</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Real-time Remote State Visualisation</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>82C53891-1252-477A-8067-DC199CD94F1B</gtr:id><gtr:impact>Real-time Remote State Visualisation</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b92bde466.03711918</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Real-time Remote State Visualisation</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dub4</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>C1FAA266-7554-456B-B598-1ED58EBA1131</gtr:id><gtr:impact>Dub4</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94c1d4f2973.63240372</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Dub4</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Visual Odometry System licensed for Mars Mission by ESA</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>9E6461E3-D0F0-4E32-A52E-181CFAE2C708</gtr:id><gtr:impact>Visual Odometry System licensed for Mars Mission by ESA</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>5464d454358634.35270280</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>OvO</gtr:title><gtr:yearProtectionGranted>2011</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Semantic Label Projection v2</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>D3531730-5628-4D37-8CDB-F6D2DDCB188B</gtr:id><gtr:impact>Semantic Label Projection v2</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b4cb389b8.20775088</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Semantic Label Projection v2</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Parsing Traffic Lights Version 2</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>F6DA1019-0114-42F5-94E8-D97596C61861</gtr:id><gtr:impact>Parsing Traffic Lights Version 2</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b6b8b3017.90684867</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Parsing Traffic Lights Version 2</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Path Discovery using Random Forests and Dense Stereo</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>E99C4DF4-6EBA-4924-992C-5EBA429B31D9</gtr:id><gtr:impact>Path Discovery using Random Forests and Dense Stereo</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94bfe5cfb06.20667223</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Path Discovery using Random Forests and Dense Stereo</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dense Laser Stereo</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>1A6BA160-EB92-4A09-846F-26C2545004D0</gtr:id><gtr:impact>Dense Laser Stereo</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b19c27ec0.43526342</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Dense Laser Stereo</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Semi-supervised Training for deep semantic Segmentation</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>C4B8C6E0-9D8B-438F-9B1B-0CB18DF1C969</gtr:id><gtr:impact>Semi-supervised Training for deep semantic Segmentation</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94e02b23909.78604174</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Semi-supervised Training for deep semantic Segmentation</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Chris Prahacs and Paul Newman &amp;quot;NABU4 Design&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>1F54C8EE-CA73-4CE8-8041-4908AA27678D</gtr:id><gtr:impact>Chris Prahacs and Paul Newman &amp;quot;NABU4 Design&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044a39d7453.39566715</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;NABU4 Design&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Asa Eckert-Erdheim, Scott Terry, Christopher Prahacs and Paul Newman &amp;quot;NABU Man Portable Navigation and Survey Scanner&amp;quot;</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>B0E3BD3E-5B2E-481E-A104-2D84A8189252</gtr:id><gtr:impact>Asa Eckert-Erdheim, Scott Terry, Christopher Prahacs and Paul Newman &amp;quot;NABU Man Portable Navigation and Survey Scanner&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e046fc4ceb47.34714585</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;NABU Man Portable Navigation and Survey Scanner&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Vote3Deep</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>8ECD2E2E-C246-461A-9BF6-DDE33FDF7F33</gtr:id><gtr:impact>Vote3Deep</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94c603df216.11371563</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Vote3Deep</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Scene Prior Builder v2</gtr:description><gtr:grantRef>EP/J012017/1</gtr:grantRef><gtr:id>32B99179-6CD3-4BA8-88BC-03C3994FC27B</gtr:id><gtr:impact>Scene Prior Builder v2</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94c41090493.08958717</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Scene Prior Builder v2</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>Vehicles will only get smarter. There will always be a desire for more machine intelligence and autonomy. Our needs and expectations are ever increasing. As a result, we continue to pack more sensors and more computation into the robots that carry, transport, labour for and defend us. Here we interpret autonomy as a robot's ability to sense, understand and ultimately act of its own accord in its operating environment. This proposal was about giving autonomous vehicles the ability to navigate in difficult conditions over long periods of time. Conditions become &amp;quot;difficult&amp;quot; when GPS is denied or only intermittently available, when little, if anything, is known about the environment, when communications are sporadic and unreliable or when operating conditions like lighting change unpredictably. And yet, somewhat perversely, it is often in just these conditions that our need to navigate is greatest: consider, for example, the surveying of buildings in a stricken nuclear facility such as Fukushima, or the autonomous driving of cars at night in cities where GPS coverage is poor. Intelligent navigation lies at the heart of much of mobile robotics research. It finds application in remote inspection, autonomous urban driving, defence, logistics, security and space robotics. We considered how machines can acquire and manage the information they need to operate persistently in workspaces of our choosing. The goal was to demonstrate that performance improves through use and over time - something that comes naturally to humans and is immensely valuable in a machine. This goal posed questions about how the computers that control robots should represent their environment in a plastic fashion - one which can be stretched and pulled into different shapes over time - this resulted in a transformation paper called &amp;quot;plastic maps&amp;quot; by Churchill and Newman. We also needed to consider how to enable machines to operated robot. We envision assistive autonomy in which a human operator remains in control of the vehicle but receives guidance on where the camera should be pointed next to ensure a complete and thorough inspection. This has resulted in a new thread of work on dense reconstruction which will flow into the programme grant. We are due to start inspections of a fusion site within weeks. We stress that our view of impact has not been limited to these three scenarios - they are simply illustrative. What we propose applies to any robotics scenario in which time and time again there is value in knowing &amp;quot;what is where&amp;quot; but one cannot depend on external infrastructure. We do not insist on prior knowledge - we can use it if available but, if not, we will accumulate what is needed autonomously. We do not insist on invariant workspaces - we set out to learn scene dynamics and we have done so.</gtr:description><gtr:exploitationPathways>The story of EPSRC's support for research that my involvement with continues with a &amp;pound;5 million Programme Grant that has just begun (March 2015). I am already looking at how big data could be harnessed in this area and will be linking up with some new industrial names for robotics exploitation.
And finally, this has all resulted in the creation of one of the UK's most exciting spin outs Oxbotica, which seeks to apply the science of mobile autonomy, to anything that moves on land. We will start with cars.</gtr:exploitationPathways><gtr:id>0CA73F16-7721-4DD0-80E4-1F736FD6B9D8</gtr:id><gtr:outcomeId>56d9bb2ab4e7c6.83377144</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Government, Democracy and Justice,Transport</gtr:sector></gtr:sectors><gtr:url>http://mrg.robots.ox.ac.uk/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>Driverless Cars</gtr:description><gtr:geographicReach>National</gtr:geographicReach><gtr:id>248DC18C-266D-4FAF-868D-A9A366F9BBA9</gtr:id><gtr:outcomeId>5464cfbfd88fd0.06559828</gtr:outcomeId><gtr:type>Gave evidence to a government review</gtr:type><gtr:url>http://www.policyexchange.org.uk/publications/category/item/eight-great-technologies</gtr:url></gtr:policyInfluenceOutput></gtr:policyInfluenceOutputs><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>Oxbotica</gtr:companyName><gtr:description>Oxbotica is a spin-out from Oxford University's internationally acclaimed Mobile Robotics Group. We specialise in mobile autonomy, navigation and perception, and draw on our heritage of world leading research into autonomous robotics. Our solutions allow robots, vehicles, machinery and people to precisely map, navigate and actively interact with their surroundings, delivering new capability and precision to a wide range of applications. Our 3D imaging and localisation solutions operate indoors and outdoors and are suitable for use in applications ranging from hand held survey devices to autonomous vehicles.

Oxbotica was founded by Prof. Ingmar Posner and Prof. Paul Newman - leaders of Oxford University's Mobile Robotics Group (MRG). MRG has an outstanding reputation for innovation and industrial collaborations (mrg.robots.ox.ac.uk). It has licensed navigation software for use on Mars rovers, developed the UK's first self-driving car, and has been a key and influential innovator in the area of Robotics and Autonomous Systems.</gtr:description><gtr:id>083DBB3E-FB77-41F7-9E01-0D583D88BA0D</gtr:id><gtr:impact>Oxbotica will leverage the innovative and world leading outputs of the UK's premier mobile robotics group, enabling rapid commercialisation with our industry partners and further application of spin-off technologies.</gtr:impact><gtr:outcomeId>5463530f914259.91764147</gtr:outcomeId><gtr:url>http://www.oxbotica.com/</gtr:url><gtr:yearCompanyFormed>2014</gtr:yearCompanyFormed></gtr:spinOutOutput><gtr:spinOutOutput><gtr:companyName>Navenio</gtr:companyName><gtr:description>The company develops indoor positioning systems capable of localising people inside buildings, where GPS does not work. The Navenio technology relies on smartphone sensors and requires no additional infrastructure and no survey effort to be deployed in the building. It is therefore inherently scalable to a large number of buildings. The company focuses on applying the indoor positioning technology to a number of verticals, including retail and healthcare.</gtr:description><gtr:id>1DC218D4-156D-47A3-9D3C-5CCF8B66D119</gtr:id><gtr:impact>* Live deployment of the technology in NHS trust; use of the technology for allocating tasks to porters based on their location
* Current deployment and testing in several retail environments</gtr:impact><gtr:outcomeId>58c9774fe8ca86.83756618</gtr:outcomeId><gtr:url>https://www.navenio.com</gtr:url><gtr:yearCompanyFormed>2015</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>F6CB52C9-A10C-453D-BF82-2CFB4BCC9A36</gtr:id><gtr:title>Parsing outdoor scenes from streamed 3D laser data using online clustering and incremental belief updates</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Conference on Artificial Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99a95f42fec838b289cedaa0baf3873"><gtr:id>c99a95f42fec838b289cedaa0baf3873</gtr:id><gtr:otherNames>Triebel R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54635e57f3ea12.57905288</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C2FD277-6447-4F6F-91FA-7EBE7412FE22</gtr:id><gtr:title>LAPS - localisation using appearance of prior structure: 6-DoF monocular camera localisation using prior pointclouds</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/03f96d57f563aea58a8f5662d4f734f1"><gtr:id>03f96d57f563aea58a8f5662d4f734f1</gtr:id><gtr:otherNames>Stewart A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>54635c90925853.90091074</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>869E1358-6ED9-43D3-9C24-1F8D384E64C8</gtr:id><gtr:title>Laser-only road-vehicle localization with dual 2D push-broom LIDARS and 3D priors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/768077d1ee2fd994432b4d261d5af941"><gtr:id>768077d1ee2fd994432b4d261d5af941</gtr:id><gtr:otherNames>Baldwin I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1737-5</gtr:isbn><gtr:outcomeId>546393d4e30731.67467414</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7F94E3EF-A227-4418-BE3B-8E7C1E5F9132</gtr:id><gtr:title>A Framework for Infrastructure-Free Warehouse Navigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6cd0aec4b889afad2b6197888df168d9"><gtr:id>6cd0aec4b889afad2b6197888df168d9</gtr:id><gtr:otherNames>M.Gadd</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd8afdd89b0.33793322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2D2D4D2-80F5-4906-89CF-73462B8C740C</gtr:id><gtr:title>Semantic categorization of outdoor scenes with uncertainty estimates using multi-class gaussian process classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1737-5</gtr:isbn><gtr:outcomeId>doi_53d059059a255fc4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CAF896F9-B288-424D-9D1B-4B69B58E092A</gtr:id><gtr:title>Robust occupancy inference with commodity WiFi</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fe7c89fddc71b72db69dbca9d36a56be"><gtr:id>fe7c89fddc71b72db69dbca9d36a56be</gtr:id><gtr:otherNames>Lu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c96fd75fc152.43899915</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7CE17709-AC17-4E7C-82CD-E3868DD2FA4E</gtr:id><gtr:title>Efficient visual positioning with adaptive parameter learning: poster abstract</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/950f684271a537ce4db930c819254c72"><gtr:id>950f684271a537ce4db930c819254c72</gtr:id><gtr:otherNames>Wen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c96f670be0c1.20466745</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D5F8E453-8562-433D-8F28-9A20AD294A7C</gtr:id><gtr:title>Off the beaten track: Predicting localisation performance in visual teach and repeat</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bc724fef42194883dee1fe02f782ea49"><gtr:id>bc724fef42194883dee1fe02f782ea49</gtr:id><gtr:otherNames>Dequaire J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c29a5a1a37a6.43003280</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D2C984B-C118-439F-B20D-511790593CC3</gtr:id><gtr:title>Shady Dealings: Robust, Long- Term Visual Localisation using Illumination Invariance</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/04ba25d1d77a3cb587920a23010d9b89"><gtr:id>04ba25d1d77a3cb587920a23010d9b89</gtr:id><gtr:otherNames>C. McManus</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bb0d33fb05.67024211</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A6E30DC7-872C-43B6-83E9-55829DBDACB3</gtr:id><gtr:title>How was Your Day? Online Visual Workspace Summaries Using Incremental Clustering in Topic Space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2dc667d5a7b6fa34f6ac7252d1109e"><gtr:id>0b2dc667d5a7b6fa34f6ac7252d1109e</gtr:id><gtr:otherNames>R. Paul</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54635ea9896a63.10432444</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E961CA5-1798-4F6B-AFC7-0969F19694D0</gtr:id><gtr:title>Robust Indoor Positioning With Lifelong Learning</gtr:title><gtr:parentPublicationTitle>IEEE Journal on Selected Areas in Communications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c970dd4d3644.06628037</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AE7D0A44-0BAF-4029-84C5-E2C521F7742E</gtr:id><gtr:title>Visual Place Recognition: A Survey</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Robotics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/970693815b1fb0df3a351d9530aec462"><gtr:id>970693815b1fb0df3a351d9530aec462</gtr:id><gtr:otherNames>Lowry S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cdcb5bac49c6.01220371</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5B2B715F-99D2-4DB7-8021-AEF88304B05A</gtr:id><gtr:title>Practice makes perfect? Managing and leveraging visual experiences for lifelong navigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0003334c5042f8c1ccb89b16e90e8971"><gtr:id>0003334c5042f8c1ccb89b16e90e8971</gtr:id><gtr:otherNames>Churchill W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e64e731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DA48BE56-E0A0-4557-8649-714BE803FDC2</gtr:id><gtr:title>Know Your Limits: Embedding Localiser Performance Models in Teach and Repeat Maps</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7f325ae0fa56bd064194fb91d41fad46"><gtr:id>7f325ae0fa56bd064194fb91d41fad46</gtr:id><gtr:otherNames>W.Churchill</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd5a089f349.49149838</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5B6F5383-4402-43B9-A7EF-3757991786F8</gtr:id><gtr:title>Exploiting known unknowns: Scene induced cross-calibration of lidar-stereo systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f15c492bda7ffabf37da386aa31532fd"><gtr:id>f15c492bda7ffabf37da386aa31532fd</gtr:id><gtr:otherNames>Scott T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cefa025bc144.89033494</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>690DE1ED-E19A-4E8D-8686-A211BB005F16</gtr:id><gtr:title>Toward automated driving in cities using close-to-market sensors: An overview of the V-Charge Project</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/330ada88bb0cf81531e53d016c16af4b"><gtr:id>330ada88bb0cf81531e53d016c16af4b</gtr:id><gtr:otherNames>Furgale P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463966feb9471.94946845</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2DC7AE0B-CDA8-4B18-B18B-50BC2A7F8524</gtr:id><gtr:title>Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8e400243a2424a1693884d9c0e1d7bb"><gtr:id>a8e400243a2424a1693884d9c0e1d7bb</gtr:id><gtr:otherNames>Corke P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546396a0073be8.25874481</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>069ECE18-82BD-422D-8894-5B60DE7513D3</gtr:id><gtr:title>Voting for Voting in Online Point Cloud Object Detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/199e360bdea6ffd3a7cc2931022d91ab"><gtr:id>199e360bdea6ffd3a7cc2931022d91ab</gtr:id><gtr:otherNames>D.Z. Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d71afe61af47.62921600</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>034A41DF-01A0-4273-B475-635F1245B59A</gtr:id><gtr:title>Work Smart, Not Hard: Recalling Relevant Experiences for Vast-Scale but Time-Constrained Localisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6712d182c44c264bddf25cbf5d3ac2d0"><gtr:id>6712d182c44c264bddf25cbf5d3ac2d0</gtr:id><gtr:otherNames>C.Linegar</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd326a9c270.16635567</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9FE22AC6-A880-4230-8B5B-A2F7D250DA46</gtr:id><gtr:title>Cross-calibration of push-broom 2D LIDARs and cameras in natural scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23a40be487410666a3600d6fedd0afa8"><gtr:id>23a40be487410666a3600d6fedd0afa8</gtr:id><gtr:otherNames>Napier A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>54635ffc62c991.28056110</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>85C8ABFF-65E6-445C-BB8E-609E6C3FADD8</gtr:id><gtr:title>What Lies Behind: Recovering Hidden Shape in Dense Mapping</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/106ba4396dfd41a95d23cafc913ec5d4"><gtr:id>106ba4396dfd41a95d23cafc913ec5d4</gtr:id><gtr:otherNames>M. Tanner</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cdcaf144dc47.35836063</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>45B1D513-5D43-43AD-9650-0F88AF6D9464</gtr:id><gtr:title>Can priors be trusted? Learning to anticipate roadworks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c383bcdbcc9faa1eb7bf2af1eaae0cf0"><gtr:id>c383bcdbcc9faa1eb7bf2af1eaae0cf0</gtr:id><gtr:otherNames>Mathibela B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-3064-0</gtr:isbn><gtr:outcomeId>doi_53d05a05a0d49659</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>78759288-6B0D-41A8-AFB3-09E9F64E2A08</gtr:id><gtr:title>Made to Measure: Bespoke Landmarks for 24-Hour, All-Weather Localisation with a Camera</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25000155b407e30222f3ade71603c89f"><gtr:id>25000155b407e30222f3ade71603c89f</gtr:id><gtr:otherNames>C. Linegar</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dda1df13e4d2.26225570</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1B90E91B-92F9-4CB1-981E-CDED70087D38</gtr:id><gtr:title>1 year, 1000 km: The Oxford RobotCar dataset</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d81eaa1265fa908e57800b54c76f2ed"><gtr:id>5d81eaa1265fa908e57800b54c76f2ed</gtr:id><gtr:otherNames>Maddern W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c27cc5d504d7.24907285</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FC8F579-3D36-4A01-A1F7-0ECFDC06B922</gtr:id><gtr:title>Accuracy Estimation for Sensor Systems</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Mobile Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/950f684271a537ce4db930c819254c72"><gtr:id>950f684271a537ce4db930c819254c72</gtr:id><gtr:otherNames>Wen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675fc07d7cb3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3F652A51-B83E-4E0B-93BA-3A5566993870</gtr:id><gtr:title>Underground Incrementally Deployed Magneto-Inductive 3-D Positioning Network</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Geoscience and Remote Sensing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/93758d890023f9bdf7b3a4159df60850"><gtr:id>93758d890023f9bdf7b3a4159df60850</gtr:id><gtr:otherNames>Abrudan T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c96e614b55e9.67903180</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>244A1923-60DC-4266-8A3E-568E32B51B9F</gtr:id><gtr:title>Identification and mitigation of non-line-of-sight conditions using received signal strength</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>58c973caae81f9.24133656</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9E8E0F2F-4082-4A68-9E31-92FB9F88AD4B</gtr:id><gtr:title>Scene Signatures: Localised and Point-less Features for Localisation</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/04ba25d1d77a3cb587920a23010d9b89"><gtr:id>04ba25d1d77a3cb587920a23010d9b89</gtr:id><gtr:otherNames>C. McManus</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bfd800d113.48364243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CFCA1E75-E082-4D11-943A-F9F919255A50</gtr:id><gtr:title>Learning on the Job: Improving Robot Perception Through Experience</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9704a5bf53a6b9a7ebadc9eb1c60dcb"><gtr:id>c9704a5bf53a6b9a7ebadc9eb1c60dcb</gtr:id><gtr:otherNames>C. Gurau</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d8317b3723b8.68836447</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>78977981-84B4-4BB1-A779-546DB9954B0B</gtr:id><gtr:title>Driven Learning for Driving: How Introspection Improves Semantic Mapping</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2343ce5ef4d904042c621e68ac7ea4b4"><gtr:id>2343ce5ef4d904042c621e68ac7ea4b4</gtr:id><gtr:otherNames>R. Triebel</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463b8f0647bf1.84488224</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>141ABDEF-CD17-4005-90B2-AADC1F273B9D</gtr:id><gtr:title>Leveraging Experience for Large-Scale LIDAR Localisation in Changing Cities</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3b45bf9aefd7dd4350652cdca8eb71b0"><gtr:id>3b45bf9aefd7dd4350652cdca8eb71b0</gtr:id><gtr:otherNames>W.Maddern</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf0e4ac06648.52365652</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52020FD7-0C52-4E5C-9A09-63809865A461</gtr:id><gtr:title>Illumination Invariant Imaging: Applications in Robust Vision-based Localisation, Mapping and Classification for Autonomous Vehicles</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0e561e1b5a65aac31882ff2578f944"><gtr:id>9a0e561e1b5a65aac31882ff2578f944</gtr:id><gtr:otherNames>W. Maddern</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463be94eaf5b9.21328333</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D7A0EA2A-EB58-41BA-ACAC-F8FA4FCD3F67</gtr:id><gtr:title>Accurate Positioning via Cross Modality Training</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/410511ea2aebc0921c9a43d1ffeefcf7"><gtr:id>410511ea2aebc0921c9a43d1ffeefcf7</gtr:id><gtr:otherNames>Papaioannou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c971d99a0177.26785364</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7DAC42B2-8B20-4162-914C-17A7A55CB2B3</gtr:id><gtr:title>Made to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7edd86cccca4e312dfa9a996d0086e8f"><gtr:id>7edd86cccca4e312dfa9a996d0086e8f</gtr:id><gtr:otherNames>Linegar C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c29b91362656.57827231</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C6DEE4CB-9C49-4074-85D2-7C5A4000B73E</gtr:id><gtr:title>Integrating Metric and Semantic Maps for Vision-Only Automated Parking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fab37417ff0fe90f72f20fb2d4056fde"><gtr:id>fab37417ff0fe90f72f20fb2d4056fde</gtr:id><gtr:otherNames>H.Grimmett</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd77e328df3.85094601</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B430297B-B5FA-4FAF-950F-F5C6C1E9799B</gtr:id><gtr:title>Gaussian Process Regression for Fingerprinting based Localization</gtr:title><gtr:parentPublicationTitle>Ad Hoc Networks</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/22e459c4c42360108e16cea2f07aec37"><gtr:id>22e459c4c42360108e16cea2f07aec37</gtr:id><gtr:otherNames>Kumar S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c96ed0a350e7.81545590</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>40D86E20-AA5E-48A8-9DF9-401D9F88FBF7</gtr:id><gtr:title>Opportunistic Radio Assisted Navigation for Autonomous Ground Vehicles</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/950f684271a537ce4db930c819254c72"><gtr:id>950f684271a537ce4db930c819254c72</gtr:id><gtr:otherNames>Wen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e06740404892.48452458</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E97342BA-1829-45AA-B496-18F79BAC761E</gtr:id><gtr:title>Introspective classification for robot perception</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/38888467eee7d5e3ccaa70308f092111"><gtr:id>38888467eee7d5e3ccaa70308f092111</gtr:id><gtr:otherNames>Grimmett H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b21fd485c496.61135682</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A7621E3-C265-4CBF-B6D7-A75B2FDE24A7</gtr:id><gtr:title>LAPS-II: 6-DoF day and night visual localisation with prior 3D structure for autonomous road vehicles</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d81eaa1265fa908e57800b54c76f2ed"><gtr:id>5d81eaa1265fa908e57800b54c76f2ed</gtr:id><gtr:otherNames>Maddern W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bed2d4b615.61788181</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>08E732E0-17FB-4955-B485-EB941CA7BD96</gtr:id><gtr:title>Lost in translation (and rotation): Rapid extrinsic calibration for 2D and 3D LIDARs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d81eaa1265fa908e57800b54c76f2ed"><gtr:id>5d81eaa1265fa908e57800b54c76f2ed</gtr:id><gtr:otherNames>Maddern W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e7920ed</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1578AACD-210C-4743-A25E-9CB88C7D0B61</gtr:id><gtr:title>Lost in Translation (and Rotation): Fast Extrinsic Calibration for 2D and 3D LIDAR's</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0e561e1b5a65aac31882ff2578f944"><gtr:id>9a0e561e1b5a65aac31882ff2578f944</gtr:id><gtr:otherNames>W. Maddern</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54635f8f5a7b30.56658829</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F0E99281-9283-4E45-A112-B71ACFD32E97</gtr:id><gtr:title>How was your day? Online visual workspace summaries using incremental clustering in topic space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e8dbd10</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F1283B87-9513-494B-9EED-7FE557F14D48</gtr:id><gtr:title>Wrong Today, Right Tomorrow: Experience-Based Classification for Robot Perception</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/747401b3013552fe5f6631b04c9db4f0"><gtr:id>747401b3013552fe5f6631b04c9db4f0</gtr:id><gtr:otherNames>J. Hawke</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b22057de3a07.72480693</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A22F0C28-9A6E-4540-A6B5-1A94E64539F7</gtr:id><gtr:title>Continually improving large scale long term visual navigation of a vehicle in dynamic urban environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0003334c5042f8c1ccb89b16e90e8971"><gtr:id>0003334c5042f8c1ccb89b16e90e8971</gtr:id><gtr:otherNames>Churchill W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-3064-0</gtr:isbn><gtr:outcomeId>doi_53d05a05a0de6306</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF6E40F5-B2D8-4830-A61C-12007E92FA98</gtr:id><gtr:title>Modelling observation correlations for active exploration and robust object detection</gtr:title><gtr:parentPublicationTitle>Journal of Artificial Intelligence Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a0012aa1098b6748fd7047d2c03eccc"><gtr:id>6a0012aa1098b6748fd7047d2c03eccc</gtr:id><gtr:otherNames>Velez J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>10769757</gtr:issn><gtr:outcomeId>54635f229755f6.99964985</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>18B9F725-31B6-4292-9B42-AD5E58713674</gtr:id><gtr:title>Fusion of Radio and Camera Sensor Data for Accurate Indoor Positioning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/410511ea2aebc0921c9a43d1ffeefcf7"><gtr:id>410511ea2aebc0921c9a43d1ffeefcf7</gtr:id><gtr:otherNames>Papaioannou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58c972762e25a8.41485975</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C30EA26C-BC12-4F05-AA93-9E6E90950357</gtr:id><gtr:title>Continuous vehicle localisation using sparse 3D sensing, kernelised r&amp;eacute;nyi distance and fast Gauss transforms</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4b2cc33c47b63271465857847ab49319"><gtr:id>4b2cc33c47b63271465857847ab49319</gtr:id><gtr:otherNames>Sheehan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54635fca398aa4.27081142</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38455C48-3BE5-4E86-BCE0-0FA5DB157649</gtr:id><gtr:title>Experience-based navigation for long-term localisation</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0003334c5042f8c1ccb89b16e90e8971"><gtr:id>0003334c5042f8c1ccb89b16e90e8971</gtr:id><gtr:otherNames>Churchill W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463ba58823241.16628291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D1A45DAE-C35B-4834-8D90-8DAC21DF17A0</gtr:id><gtr:title>Poster: WiFi sensors meet visual tracking for an accurate positioning system</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/410511ea2aebc0921c9a43d1ffeefcf7"><gtr:id>410511ea2aebc0921c9a43d1ffeefcf7</gtr:id><gtr:otherNames>Papaioannou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58c9736c2deae1.96463127</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0C47924E-4579-4BC3-9846-981A137E675C</gtr:id><gtr:title>Indoor Tracking Using Undirected Graphical Models</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Mobile Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c9706c0047a9.97738533</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>56F87C42-0327-4E1A-96F8-ACE08961EB67</gtr:id><gtr:title>Visual Precis Generation using Coresets</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2dc667d5a7b6fa34f6ac7252d1109e"><gtr:id>0b2dc667d5a7b6fa34f6ac7252d1109e</gtr:id><gtr:otherNames>R. Paul</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bcd6d96254.57495148</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>823EAD66-D8D4-4B51-8B84-57F5AE64FAFE</gtr:id><gtr:title>Robust pedestrian dead reckoning (R-PDR) for arbitrary mobile device placement</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58c97231ca32e8.58517702</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A1DB194A-2FF7-42EF-B473-F4C6F1F60368</gtr:id><gtr:title>Robust Direct Visual Localisation using Normalised Information Distance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d776fe9b6b1f44f3d9975f92058d77a"><gtr:id>0d776fe9b6b1f44f3d9975f92058d77a</gtr:id><gtr:otherNames>G. Pascoe</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdcd24d4ca12.64682054</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BD54E58-228B-4261-964B-4F6D5261E4F8</gtr:id><gtr:title>FARLAP: Fast Robust Localisation using Appearance Priors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e01a290a4bdc93ddd06e50485738cc55"><gtr:id>e01a290a4bdc93ddd06e50485738cc55</gtr:id><gtr:otherNames>G.Pascoe</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd63ced4506.40589149</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>025B82E7-E698-4D06-A1E8-7C203B3257FF</gtr:id><gtr:title>Dense and Swift Mapping with Monocular Vision</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1ccae6aef918f3fbe42a6fe9bac41466"><gtr:id>1ccae6aef918f3fbe42a6fe9bac41466</gtr:id><gtr:otherNames>P.Pinies</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd056a306f0.25480340</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BCEEF98D-1822-4A5F-9717-BF88213E9838</gtr:id><gtr:title>Demo: Lightweight continuous indoor tracking system</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58c973265fcb04.10308655</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>554A65FC-27C8-495D-9D1A-309DC6B9C1A3</gtr:id><gtr:title>Road vehicle localization with 2D push-broom LIDAR and 3D priors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/768077d1ee2fd994432b4d261d5af941"><gtr:id>768077d1ee2fd994432b4d261d5af941</gtr:id><gtr:otherNames>Baldwin I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058ec2193b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCB52AFB-1F3F-4CB8-99DF-00A1AB3A0F78</gtr:id><gtr:title>Generation and exploitation of local orthographic imagery for road vehicle localisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23a40be487410666a3600d6fedd0afa8"><gtr:id>23a40be487410666a3600d6fedd0afa8</gtr:id><gtr:otherNames>Napier A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-2119-8</gtr:isbn><gtr:outcomeId>doi_53d05a05a25ab89c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF45570F-6F7F-41C4-90EA-BC56DFDED7CC</gtr:id><gtr:title>What could move? Finding cars, pedestrians and bicyclists in 3D laser data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c8216ad8b0edf7c3fbae30aac4d3a4d9"><gtr:id>c8216ad8b0edf7c3fbae30aac4d3a4d9</gtr:id><gtr:otherNames>Dominic Zeng Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e83c959</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52B2BDEC-DB9C-4767-A3B3-A4780786EA94</gtr:id><gtr:title>Can Priors Be Trusted? Learning to Anticipate Roadworks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3bf846e4d9a713e3488d94692757d4fa"><gtr:id>3bf846e4d9a713e3488d94692757d4fa</gtr:id><gtr:otherNames>B. Mathibela</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546392a4bf52e7.01525304</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B33A5E7-97F5-4091-B75D-352F3CF149FD</gtr:id><gtr:title>Can Priors Be Trusted? Learning to Anticipate Roadworks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3bf846e4d9a713e3488d94692757d4fa"><gtr:id>3bf846e4d9a713e3488d94692757d4fa</gtr:id><gtr:otherNames>B. Mathibela</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546392a62f6366.39626497</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F95C8FCF-8B36-4ECA-A0DD-9DE3F0EB8D0F</gtr:id><gtr:title>Distraction suppression for vision-based pose estimation at city scales</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e069a2c6809b1f71a6ee517bc39e329f"><gtr:id>e069a2c6809b1f71a6ee517bc39e329f</gtr:id><gtr:otherNames>McManus C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>5463602cefb931.94565918</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B383DA73-3B1F-41B4-91D9-18D10513DC30</gtr:id><gtr:title>Comparison of Accuracy Estimation Approaches for Sensor Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/950f684271a537ce4db930c819254c72"><gtr:id>950f684271a537ce4db930c819254c72</gtr:id><gtr:otherNames>Wen H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4799-0206-4</gtr:isbn><gtr:outcomeId>56e069ae7edec8.06719322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F4D5B8E-B1AD-44B9-9F1A-92068A3D35CD</gtr:id><gtr:title>Sensor network algorithms and applications.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd2ae002450f7c0d6a78bb00e2b1eab1"><gtr:id>dd2ae002450f7c0d6a78bb00e2b1eab1</gtr:id><gtr:otherNames>Trigoni N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1364-503X</gtr:issn><gtr:outcomeId>58c968f115ad61.15540256</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F0F4FBD6-E3A8-4C7F-93E7-F9D48B4016FC</gtr:id><gtr:title>Lighting Invariant Urban Street Classification</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26ae2050de62c6cd2df470ceb394513a"><gtr:id>26ae2050de62c6cd2df470ceb394513a</gtr:id><gtr:otherNames>B. Upcroft</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bc38623b70.09410676</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DFEB54C6-8D0D-412B-9C04-350C63390C63</gtr:id><gtr:title>Learning place-dependant features for long-term vision-based localisation</gtr:title><gtr:parentPublicationTitle>Autonomous Robots</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e069a2c6809b1f71a6ee517bc39e329f"><gtr:id>e069a2c6809b1f71a6ee517bc39e329f</gtr:id><gtr:otherNames>McManus C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675fb7173a86</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87D04720-E504-4EAA-8502-ECB017261E5E</gtr:id><gtr:title>Practice Makes Perfect? Managing and Leveraging Visual Experiences for Lifelong Navigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1aff0f72170e6247247271291d864f9f"><gtr:id>1aff0f72170e6247247271291d864f9f</gtr:id><gtr:otherNames>W. Churchill</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546372e30b5707.03634710</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F1F70251-3B84-4872-BD6B-D283CEA852B6</gtr:id><gtr:title>Knowing when we don't know: Introspective classification for mission-critical decision making</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/38888467eee7d5e3ccaa70308f092111"><gtr:id>38888467eee7d5e3ccaa70308f092111</gtr:id><gtr:otherNames>Grimmett H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>546394f0efafb3.22756684</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F369467-308C-427B-80E2-E0D724C9AA75</gtr:id><gtr:title>Non-Line-of-Sight Identification and Mitigation Using Received Signal Strength</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Wireless Communications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e04e6544bfe8.21209915</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>364DA53C-3B6F-419C-A7EE-74981ED9DC40</gtr:id><gtr:title>A New Approach to Model-Free Tracking with 2D Lidar</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1927e29ec944c44da9b234a935eb55a7"><gtr:id>1927e29ec944c44da9b234a935eb55a7</gtr:id><gtr:otherNames>D. Z. Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463b6841b8270.84806145</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FA6E862-FCCC-4E15-A803-0C712BC1764C</gtr:id><gtr:title>Identification and mitigation of non-line-of-sight conditions using received signal strength</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56e069149cb961.30869376</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1D656215-0CE7-4495-A727-D65DAE52F944</gtr:id><gtr:title>Lightweight map matching for indoor localization using conditional random fields (BEST PAPER)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05377ddcf907e45fc7b507d5eb5e53df"><gtr:id>05377ddcf907e45fc7b507d5eb5e53df</gtr:id><gtr:otherNames>Xiao Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58c972d138eaa5.56708221</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D5B310F9-8754-4887-8A8B-4A829873F9AF</gtr:id><gtr:title>Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e01a290a4bdc93ddd06e50485738cc55"><gtr:id>e01a290a4bdc93ddd06e50485738cc55</gtr:id><gtr:otherNames>G.Pascoe</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cef83d458312.17903598</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J012017/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>95</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>34B6BDD6-DA02-4CA0-A969-29D50394A953</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Networks &amp; Distributed Systems</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>95</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>