<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FD66C955-5923-42A3-A9A8-8D66CA2E5942"><gtr:id>FD66C955-5923-42A3-A9A8-8D66CA2E5942</gtr:id><gtr:name>Home Office Sci Dev't Branch</gtr:name><gtr:address><gtr:line1>Home Office Sci Dev't Branch Langhurst</gtr:line1><gtr:line2>Langhurst House</gtr:line2><gtr:line3>Langhurstwood Road</gtr:line3><gtr:line4>Horsham</gtr:line4><gtr:line5>West Sussex</gtr:line5><gtr:postCode>RH12 4WX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2"><gtr:id>E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2</gtr:id><gtr:firstName>Barry-John</gtr:firstName><gtr:surname>Theobald</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/46117A48-1C74-4CC8-911D-1BEF2878020D"><gtr:id>46117A48-1C74-4CC8-911D-1BEF2878020D</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Harvey</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/4A387D6B-E08F-4191-86CB-E960B8C48229"><gtr:id>4A387D6B-E08F-4191-86CB-E960B8C48229</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:surname>Cox</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE028047%2F1"><gtr:id>83C84AC4-6753-4787-B94D-24C5F2E31849</gtr:id><gtr:title>LILiR2 - Language Independent Lip Reading</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E028047/1</gtr:grantReference><gtr:abstractText>It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading, particularly in non-laboratory environments. This project will collect data for lip-reading and use it to build automatic lip-reading systems: machines that convert videos of lip-motions into text. To be effective such systems must accurately track the head over a variety of poses; extract numbers, or features, that describe the lips and then learn what features correspond to what text. To tackle the problem we will need to use information collected from audio speech. So this project will also investigate how to use the extensive information known about audio speech to recognise visual speech.The project is a collaboration between the University of East Anglia who have previously developed state-of-the-art speech reading systems; the University of Surrey who built accurate and reliable face and lip-trackers and the Home Office Scientific Branch who wish to investigate the feasibility of this approach for crime fighting.</gtr:abstractText><gtr:fund><gtr:end>2010-09-29</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-05-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>391814</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2580000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Home Office</gtr:description><gtr:end>2013-03-02</gtr:end><gtr:fundingOrg>Home Office</gtr:fundingOrg><gtr:id>6B9166B1-B9DB-41BD-8823-F6F4A1A81063</gtr:id><gtr:outcomeId>5ed306c85ed306dc</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>228000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Home Office</gtr:description><gtr:end>2013-03-02</gtr:end><gtr:fundingOrg>Home Office</gtr:fundingOrg><gtr:fundingRef>7020739</gtr:fundingRef><gtr:id>A665212F-E11C-427B-AF1F-014A766F72BA</gtr:id><gtr:outcomeId>r-4490613876.36755906a6b0f8</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2010-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our work is formed the basis for a number of public talks, press pieces and is currently under-going some proof-of-concept commercialisation.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>EAB0B897-3EA5-47B3-9B86-AEEEE09C9710</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54575f189efdf5.58053667</gtr:outcomeId><gtr:sector>Aerospace, Defence and Marine,Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading. This project collected new datasets for lip-reading and used these to build automatic lip-reading systems: machines that convert videos of lip-motions into text. It also compared human performance against automatic performance on the same dataset. 



To be effective at automatic lip reading, systems must accurately track the head over a variety of poses; extract features, that describe the lips and then learn what features correspond to what text. To this end the project developed a state-of- the-art facial feature tracking system that could track any set of facial features on any person, in any environement in real- time. This tracking system has resulted in high quality international publications and interest from a variety of industrial sectors from government through to the post production industries. The project also developed several feature extraction/representations that could be used in the recognition of words and a general recognition framework for lip- reading. It made significant advances in person dependant recognition with accuracies approaching the level of speech recognition and it made significant new progress in the more challenging problem of person independent recognition i.e recognising people speaking who have never been seen by the system before. 



The project also developed approaches to language identification which allows the recognition of language to be performed just by the motion of the lips and the recognition of expression and non verbal communication, the subtle facial expressions that humans use intuitively to supplement the information provided by a speaker about subtle aspects of communication such as their interest in a topic of conversation.</gtr:description><gtr:exploitationPathways>Our findings have been used as input to at least four subsequent grants on lip-reading. 

After much subsequent work (not funded by EPSRC) we have moved to more robust tracking, off-axis lip-reading, and some evidence of person-independence (the key problem not tackled in this grant).</gtr:exploitationPathways><gtr:id>33CA5206-0223-4151-A072-A87840E186E0</gtr:id><gtr:outcomeId>r-7681991278.829044779a9504</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Healthcare,Government, Democracy and Justice,Retail,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>3A692DC2-19AC-43B1-8665-43DB3381AC3B</gtr:id><gtr:title>In Pursuit of Visemes</gtr:title><gtr:parentPublicationTitle>Proceedings on Auditory-Visual Speech Processing (AVSP)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a2a680f7660e59df3322c8c5043da158"><gtr:id>a2a680f7660e59df3322c8c5043da158</gtr:id><gtr:otherNames>Hilder S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>56a91dad2ee1e0.93730248</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4B30BBBF-E201-496F-876D-46A08688EEF3</gtr:id><gtr:title>Is automated conversion of video to text a reality?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce0d37f6c831118dcb2b42a83ae26a0a"><gtr:id>ce0d37f6c831118dcb2b42a83ae26a0a</gtr:id><gtr:otherNames>Bowden R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56a91daf912504.40732944</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61A776FA-2936-4FD0-807B-83AFC91FFD9E</gtr:id><gtr:title>Some observations on computer lip-reading: moving from the dream to the reality</gtr:title><gtr:parentPublicationTitle>SPIE Proceedings 9253a: Optics and Photonics for Counterterrorsim, Crime Fighting and Defence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56a91dae8b56f0.96264203</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B3C3E73F-2706-4F71-A217-E3D785DD7114</gtr:id><gtr:title>Speaker independent visual-only language identification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd7bf73c013c8ba3590cb756919497ab"><gtr:id>dd7bf73c013c8ba3590cb756919497ab</gtr:id><gtr:otherNames>Newman J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-4295-9</gtr:isbn><gtr:outcomeId>doi_53d0580582c09631</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96A296DF-3396-4395-AC7B-8E4C0FF750EB</gtr:id><gtr:title>In pursuit of visemes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d87ca8b13584a58c99d59a74d733b041"><gtr:id>d87ca8b13584a58c99d59a74d733b041</gtr:id><gtr:otherNames>Barry-John Theobald</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_9751893356cac4dac6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD279924-CC0F-4490-9A16-C90E694C0B83</gtr:id><gtr:title>Improving visual features for lip-reading</gtr:title><gtr:parentPublicationTitle>Proceedings International Conference on Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>56a91dad58a0b4.18216183</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3932C1A0-0C31-4FA1-A687-077400937D76</gtr:id><gtr:title>The challenge of multispeaker lip-reading</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d87ca8b13584a58c99d59a74d733b041"><gtr:id>d87ca8b13584a58c99d59a74d733b041</gtr:id><gtr:otherNames>Barry-John Theobald</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>r_8820792115cac7ca92</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A1AD1E06-32D0-409A-9C1E-E02034798061</gtr:id><gtr:title>Comparing visual features for lipreading</gtr:title><gtr:parentPublicationTitle>Proceesings International Conference on Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>56a91dad031303.68200305</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>78F15AF1-79BA-4C9B-8514-2FB5CF517A36</gtr:id><gtr:title>Resolution limits on visual speech recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of IEEE International Conference on Image Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56a91dae5d8ae9.44063272</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CC3D8510-4944-4FB0-AF2B-A997EA438F12</gtr:id><gtr:title>Insights into machine lip reading</gtr:title><gtr:parentPublicationTitle>2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56a91dae0cd430.57501050</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>401FDE90-7A66-4A1F-8224-A47668820607</gtr:id><gtr:title>Is automated conversion of video to text a reality?</gtr:title><gtr:parentPublicationTitle>Proceedings SPIE 8546, Optics and Photonics for Counterterrorism, Crime Fighting and Defence VIII</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce0d37f6c831118dcb2b42a83ae26a0a"><gtr:id>ce0d37f6c831118dcb2b42a83ae26a0a</gtr:id><gtr:otherNames>Bowden R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56a91dae33ed13.44516079</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C82EDA30-91C7-40CC-9A11-988B4F853A07</gtr:id><gtr:title>Insights into machine lip reading</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>56a91dafb8a6c5.83802806</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A8B0353E-E276-46B9-958C-4F349513E6C3</gtr:id><gtr:title>Recent developments in automated lip-reading</gtr:title><gtr:parentPublicationTitle>Optics and Photonics for Counterterrorism and Crime Fighting IX</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce0d37f6c831118dcb2b42a83ae26a0a"><gtr:id>ce0d37f6c831118dcb2b42a83ae26a0a</gtr:id><gtr:otherNames>Bowden R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56a91dada582e5.16639742</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D2F03CF-9D7F-4E3B-9BD5-3D3768F80AB3</gtr:id><gtr:title>Limitations of visual speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d87ca8b13584a58c99d59a74d733b041"><gtr:id>d87ca8b13584a58c99d59a74d733b041</gtr:id><gtr:otherNames>Barry-John Theobald</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_3413325717cac56676</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB766CD3-3FDB-4573-AD27-75F9837C7781</gtr:id><gtr:title>Which phoneme-to-viseme maps best improve visual-only computer lip-reading?</gtr:title><gtr:parentPublicationTitle>Advances in Visual Computing: Proceedings 10th International Symposium, ISVC 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56a91daeb3dfd1.33853131</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BACEE22-C90A-43AE-BD89-C5BDE15DB668</gtr:id><gtr:title>Improving Lip-reading performance for robust audiovisual speech recognition using dynamic neural networks</gtr:title><gtr:parentPublicationTitle>Proceedings of the 1st Joint Conference on Facial Analysis, Animation and Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1ae4d389e225931ba4910982d1908b98"><gtr:id>1ae4d389e225931ba4910982d1908b98</gtr:id><gtr:otherNames>Thangthai K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a91daf3c3d95.61734051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BF97BF7A-B035-489D-8892-FFB27B5038F2</gtr:id><gtr:title>Finding phonemes: improving machine lip-reading</gtr:title><gtr:parentPublicationTitle>Proceedings of the 1st Joint Conference on Facial Analysis, Animation and Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a91daf145371.82340447</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>959EE8C7-256E-4DAB-83DB-A193CDB9CB7C</gtr:id><gtr:title>Speaker-independent machine lip-reading with speaker-dependent viseme classifiers</gtr:title><gtr:parentPublicationTitle>Proceedings of the 1st Joint Conference on Facial Analysis, Animation and Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a91daede3618.66834928</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>637575E8-A265-44AC-AB90-D2829D78A052</gtr:id><gtr:title>View Independent Computer Lip-Reading</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1659-0</gtr:isbn><gtr:outcomeId>56a91db01c0997.60213764</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B632F9FA-DAA1-4B13-BA7F-E0D19EC56EA1</gtr:id><gtr:title>Robust facial feature tracking using selected multi-resolution linear predictors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f1d770686de08b20027dea3bcaef3cb1"><gtr:id>f1d770686de08b20027dea3bcaef3cb1</gtr:id><gtr:otherNames>Ong E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4420-5</gtr:isbn><gtr:outcomeId>doi_53d05805866241b7</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E028047/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>