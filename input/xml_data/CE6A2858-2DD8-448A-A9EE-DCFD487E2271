<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>UCL Interaction Centre</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/19224F62-8033-4766-A080-460A6F3EBBDC"><gtr:id>19224F62-8033-4766-A080-460A6F3EBBDC</gtr:id><gtr:firstName>Amanda</gtr:firstName><gtr:otherNames>Clare</gtr:otherNames><gtr:surname>Williams</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/098AF839-3F94-4990-87CB-3D0D0BAA0D76"><gtr:id>098AF839-3F94-4990-87CB-3D0D0BAA0D76</gtr:id><gtr:firstName>Nadia</gtr:firstName><gtr:otherNames>Luisa</gtr:otherNames><gtr:surname>Berthouze</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH017178%2F1"><gtr:id>CE6A2858-2DD8-448A-A9EE-DCFD487E2271</gtr:id><gtr:title>Pain rehabilitation: E/Motion-based automated coaching</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H017178/1</gtr:grantReference><gtr:abstractText>Almost 1 in 7 UK citizens suffer from chronic pain, much of which is mechanical lower back pain with no treatable pathology. Pain management rehabilitative programs suffer from two shortcomings: (a) there are too few resources in the health care system to treat all patients face-to-face; (b) current approaches fail to integrate treatment of interrelated physiological and psychological factors. Combining expertise from engineering, clinical health sciences, and industry through a multidisciplinary team of investigators and advisors, this proposal seeks to address both shortcomings by (a) developing a set of methods for automatically recognising audiovisual cues associated with pain, behavioural patterns typical of pain, and affective states influencing pain and activity, and (b) integrating these methods into an interactive computer system that will provide appropriate feedback and prompts to the patient based on his/her behaviour measured during self-directed exercise and fitness-building sessions. This intelligent system will enable and motivate patients to continue their progress in extending activity inside (and in the longer term, outside) the clinical environment and thus will facilitate their reintegration into social and working life.In doing so, the project aims to make major contributions in a number of research areas. First, the project will significantly expand the state of the art in the field of emotion recognition by extending current methods in affective body gesture recognition, facial expression recognition and affective vocalisation recognition to deal with naturalistic rather than acted emotional expressions. This entails theoretical and practical contributions to important challenges such as detection and tracking of human behavioural cues in real world unconstrained environments, spatiotemporal analysis of complex dynamic stimuli such as non-linguistic vocalisations, facial expressions, and body movements, and spatiotemporal fusion of multimodal data streams in which constraints of synchronisation are relaxed. Second, the project will advance our understanding of how affect and pain-related moods such as fear, depression, and frustration in particular, interact with motor behaviour to not only modify pain expressions, but also to produce guarded movements that can exacerbate pain or worsen disability. This, in turn, will contribute to the body of work on cognitive behavioural models of pain to improve the diagnosis and management of pain behaviours. Finally, the project will contribute a novel user-centred approach to developing patients' understanding of their body movement during self-directed therapy; it will also identify what type of feedback best promotes engagement and encourages persistence, and offsets the negative effects of pain-related anxiety, frustration, and low-mood. Patients, clinicians and members of the advisory team will be periodically involved in the design of the interface and the testing of the system through design workshops. The system will eventually be made available for long term testing to the Pain Management Centre of the National Hospital of Neurology and Neurosurgery, London.</gtr:abstractText><gtr:fund><gtr:end>2015-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-05-04</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1154531</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Keynote, Disruptive, Madrid, Spain (Industry)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>2A15D884-5323-4A2B-B9DA-55E903B3A8ED</gtr:id><gtr:impact>Invited to provide the industry with an understanding of affective-aware technology in 5 years time.

Exposure to EMo&amp;amp;Pain project and to the team</gtr:impact><gtr:outcomeId>58c983d29cd2f9.58127641</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Putting people at the centre of digital health. UCL Festival for Digital Health</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>E5CEC6E0-D526-41E7-A023-E624B2F72DF3</gtr:id><gtr:impact>About 100 people attended this event I co-organized and chaired at the UCL Festival for Digital Health, 30th June 2014. This event comprised short
of presentations and interactive demonstrations of work at our group. The event sparkled discussion on understanding healthcare and human error,
on sensing motion/emotion in healthy and chronic pain populations and on feedback to increase motivation, to promote behaviour change and to
provoke changes in body perception.

Apart from the informative nature of this activity, through this event we broaden our research network, and started new collaborations.</gtr:impact><gtr:outcomeId>5462309970a7a7.91341714</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.fdh.ucl.ac.uk/event/human-factors-digital-health/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote on pain and technology at the SMART Summer School on Computational Social and Behavioural Science, Paris, France</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>88449CC7-F3BE-4E7F-BFFE-AE744AABC618</gtr:id><gtr:impact>Key note at summer school. It led students to consider body (and not just face) as a modality for affect recognition.
Received invitation to give a kynote at Pain Face Day 2017</gtr:impact><gtr:outcomeId>58ca71407d5dc2.53553918</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote, Deep Learning Summit, London, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F65F2756-B3C7-45B8-A86F-A7E41957FB66</gtr:id><gtr:impact>Introducing the importance of body movement as an affective modality for affective-aware technology design</gtr:impact><gtr:outcomeId>58c98439d920e1.65483408</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Science Museum - Wearables event</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>8A5E396C-F739-4186-9F40-617FF28B633B</gtr:id><gtr:impact>The event generated discussion and engagement and people reported a better understanding of wearables for other conditions other than general activity tracking.</gtr:impact><gtr:outcomeId>56d46c4fd5dc37.01884205</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presented at the British Pain Society Technology day</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>9CDE23AD-8B50-48C7-9150-D0C6EF24E3A1</gtr:id><gtr:impact>Presented at the Technology day for British Pain Society. Raised visibility of our research and discussion around methods.</gtr:impact><gtr:outcomeId>56db0125cc9a63.46221177</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Big Bang Fair</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>4D3E2CFE-B71B-481D-B6AD-9898432D87E9</gtr:id><gtr:impact>The fairs are part of a UK-wide programme led by EngineeringUK to bring science and engineering to life for young people.</gtr:impact><gtr:outcomeId>56d46bc9ecbf49.23648345</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>British HCI Conference Demo</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>B139A299-6815-4F70-9F1D-B7B243E46DBF</gtr:id><gtr:impact>The Go-with-the-flow app was selected to be presented at the British HCI Conference with attendees from academia and industry.</gtr:impact><gtr:outcomeId>56dd8ef5c40789.65191466</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TEDxStMartins Talk</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6E606755-FDC3-4E23-8A49-BEE4A97CD003</gtr:id><gtr:impact>TEDxStMartins Talk.

The talk presented the work Berthouze's team is doing on automatic emotion recognition from body expressions. TEDx talks are on an invitation-basis only and they have high visibility. 

http://tedxcentralsaintmartins.com/videos/

http://www.youtube.com/watch?v=PXTrBqSw4-A</gtr:impact><gtr:outcomeId>r-4450904302.718960c0b8278</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://tedxcentralsaintmartins.com/videos/</gtr:url><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Bright Club stand up comedy set</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>AAF4B12F-91FF-4DA9-AEB8-CB4D5B3C6EDD</gtr:id><gtr:impact>The set was done to engage audiences with our research through stand up comedy.</gtr:impact><gtr:outcomeId>56dd8c3f9946b8.89813887</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presented at Being Human Festival</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>2C968F7F-21B1-4F5B-BEE5-6C9C4B6FBF18</gtr:id><gtr:impact>To explore the 'sounds that move us' through multisensory science and philosophy. Featuring talks by musicians and researchers, demonstrations of sonic illusions, digital mapping, and even 'sonic shoes', this evening of talks, experiments and interactive demonstrations invites you to explore the ways in which music and sounds speak to more than just our ear: http://beinghumanfestival.org/

Run by the School of Advanced Study in partnership with the British Academy, the Arts and Humanities Research Council and the Wellcome Trust, Being Human is the UK's only national festival of the humanities. It aims to provide opportunities for non-academic audiences to experience how the humanities can inspire and enrich our everyday lives.</gtr:impact><gtr:outcomeId>56d5b34681dd38.83067135</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>814800</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:department>Seventh Framework Programme (FP7)</gtr:department><gtr:description>Ubi-Health Project funded by EC Marie Curie IRSES Program</gtr:description><gtr:end>2016-08-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:id>3A4794F8-DD10-4B62-8FF6-8B10099FC924</gtr:id><gtr:outcomeId>56db4edaac6db9.14487945</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2014-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>548743</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>HUMAN: FOF-04-2016 - Continuous adaptation of work environments with changing levels of automation in evolving production systems - as coI</gtr:description><gtr:end>2019-09-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>723737</gtr:fundingRef><gtr:id>0461EC45-FE34-4F0F-8E5D-A000F585254D</gtr:id><gtr:outcomeId>58be8a953c6277.04967486</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>16000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>RosetreesTrust</gtr:description><gtr:end>2016-03-02</gtr:end><gtr:fundingOrg>Rosetrees Trust</gtr:fundingOrg><gtr:id>B3D9518C-BFFD-4370-9A4B-0A4A23D23529</gtr:id><gtr:outcomeId>545d1b8a1c6950.24134571</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2014-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>4000</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>UCL Enterprise Scholarship</gtr:description><gtr:end>2015-12-02</gtr:end><gtr:fundingOrg>UCL Advances</gtr:fundingOrg><gtr:id>438F6E82-5661-4C6F-BE5F-BEAA57FB64F0</gtr:id><gtr:outcomeId>56d0de828051c2.06518101</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>- Clinicians specialized in other conditions than musculoskeletal chronic pain has seen the potential for adopting the technology to treat other conditions. Very preliminary assessment of the use of the Go-with-the-flow device has been carried out with people with CRPS at the Royal National Orthopedic Hospital with positive feedback from patients. We are discussing applying for a grant to extend the design of the device to this population.

- Various companies have shown interest in integrating the device with their software for chronic pain.

- The Go-with-the-Flow device was also used during workshops with the theater producer Rachel Bagshaw during the creation of the theatrical piece (National Theatre Studio) on chronic pain: &amp;quot;Where we are, where we are going&amp;quot;. The aim of the theatrical piece is to facilitate understanding of what chronic pain is and what it means to live with chronic pain. 

- The sonification framework is currently being tested and extended in the context of studies for helping children with autism learn and control their body movement (in collaboration with CICESE, Mexico)</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>FC859FDE-11D1-4CE2-9EFD-96DDE51E3967</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54623473498153.30305205</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>1) A multifaced first of a kind dataset has been acquired and compiled to underpin the design of much needed affect-aware physical rehabilitaiton systems for chronic lower back pain. The corpus contains facial expressions (multiple high level definition video cameras), voice, full body movement data (17 gyros), muscle activity (from EMG) gathered from people with chronic pain and healthy participants during physical rehabilitation activity. This corpus will be made available to the research community (upon request) as soon as full clearance is obtained (Aung et al., 2016). 

2) Automatic recognition models of perceived pain levels (Olugbade et al., 2014,2015) and, pain-related behaviour (Aung et al., 2013, 2014, 2016) have been proposed and tested on the above dataset using body movement data and muscle activity data. Such algorithms can be used to personalize the run-time support that full-body sensing technology can provide to people with chronic pain during physical rehabilitation. 

3) New algorithms (Romera-Paredes et al., 2013-2014) based on the multi task learning paradigm have been proposed and have contributed to the issue of person specific effects (idiosyncrasy) typical of datasets containing data from various subjects expressing emotional states in their unique ways. This is particular important when dealing with limited sample from each user (e.g., a new user using the system)

4) An extended set of qualitative studies with people with chronic pain (at the Pain Centre at National Hospital for Neurology and Neurosurgery (NHNN) and in the homes) and physiotherapists has led to a better understanding of people's needs and strategies to facilitate physical activity despite pain (Singh et al., 2014). These findings have led to reconsider the role of technology in chronic pain physical rehabilitation. We have proposed that, whereas in acute pain and induced pain, technology is being used to provide fun and facilitate attention away from pain, in chronic pain technology needs to help people to overcome also and first of all their psychological barriers. We have proposed new personalized sonification mechanisms to help people overcome the psychological barriers they meet while engaging in physical activity and when pain is a constant threat. Sonification is here designed as a way to increase patients' awareness of movement (devoid of pain) and related processes (breathing) rather than simply correct movement. The aim is to increase confidence and self-efficacy.Through body movement and sound, patients can design calibrated aural space within which they can exercise and explore their body. Results showed increase awareness and confidence in movement and increased perceived self-efficacy (Singh et al., 2015). 

5) We have currently designed 3 platforms 

(a) full-body sensing (motion capture) platform for an in-depth study of movement in pain;
(b) a Kinect-based application (in collaboration with the University of Genoa) for a more coarse detection of body movement; and 
(c) finally a wearable smartphone-based device with a minimal number of sensors (Singh et al., 2014; Digital Health Prize 2nd prize (judged as the most innovative app - the 1st prize went to ready to deploy apps). 

All three platforms integrate movement and breathing sensors (designed by my team) with the first two also including EMG sensors. A key aim of the smartphone-based device is to bring physical rehabilitation away from mere physical activity sessions toward daily life functioning by facilitating awareness and management of body physical and emotional resources. We are currently extending the wearable device with a larger set of movement sensors and EMG to facilitate not only exercising but also everyday functioning. 

6) Investigated the use of the designed technology to support not only exercise but also functional activity in the home. A qualitative study with people with chronic pain over 10 days shows how the sonification and calibration framework designed for exercise indeed help people to develop strategies to facilitate everyday activity and apply self-management strategies to function in their social role. The results were accepted for publication at CHI'17

7) Developed a low-cost reduced sensor network wearable prototype for pain, confidence and anxiety detection (based on body movement and muscle activity sensors) in exercise and functioning activity. The prototype is used to understand automatic detection in everyday life. The work is under review in at IEEE Transactions on Affective Computing journal.</gtr:description><gtr:exploitationPathways>1. Our dataset will be released (as soon as full clearance is obtained) to foster new research in the pain bevahiour detection from face expression, body motion, emg and voice. Currently open only to collaborators
We are also making aspects of our software available to others. Ethical approval has been obtained to open the sonification app to the public. This will be our next step.

2. We are also applying for more funding to bring the current wearable to closer to commercialization. 

3. The sonification framework is being tested in a novel context: facilitate body movement control in children with autism (collaboration with CICESE, Mexico)

4. The work has informed the design of affect-aware technology for stroke</gtr:exploitationPathways><gtr:id>3BE75916-DEE7-4A3D-AD53-6808739E868B</gtr:id><gtr:outcomeId>546236e4cd8258.72620776</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.emo-pain.ac.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Dataset of chronic pain and healthy subjects undergoing typical movement exercises with pain related behaviour labels. Four synchronised sensing modalities: motion capture, EMG, facial video and acoustics.</gtr:description><gtr:id>4F1EF74C-A05B-40F9-AF1F-4B8B01306810</gtr:id><gtr:impact>This multifacted dataset will serve to foster research in chronic pain related behaviour mostly within the affective computing community and machine learning community
As full clearance will be obtained this will be open to the full research community.</gtr:impact><gtr:outcomeId>546231f5120c56.63732198</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Emo-Pain Corpus</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Go-with-the-flow is a wearable Smartphone app that senses movement and related physiological processes (e.g., breathing) and transforms these signals into sound. Smartphone sensors track movement and Arduino-based respiration chest belts detect breathing patterns related to anxiety. It was developed as part of my PhD on the Emotion and Pain project. The device has been designed from user studies with people with chronic pain and with physiotherapists and psychologists (Singh et al., 2014) and validated through control studies and home studies. The description of the device is described in related publications (Singh et al., 2015).</gtr:description><gtr:id>FD208AC6-BA8D-47D5-BE6E-24A1AE6F4094</gtr:id><gtr:impact>Go-with-the flow was hailed as the most innovative app at the Festival of Digital Health, 2014 and we won the Runners Up prize. The event celebrated the cutting edge of health apps in the fields of gamification and self-tracking and included a live showcase of apps and games demonstrating innovation from UCL academics and SMEs. More details here.

http://www.fdh.ucl.ac.uk/event/gamification-self-tracking-health-wellbeing/

Subsequently I also won people's choice award for presenting our work at the Grace Hopper London Colloquium 2015.

http://academy.bcs.org/content/london-hopper-colloquium.

We were also finalists at the Social Innovators Challenge that was orgainsed and hosted by Healthbox, UCLB and Numbers4Good, in collaboration with Janssen Healthcare Innovation and Trafford Housing Trust, with support from the Cabinet Office

http://www.healthsocialinnovators.org/</gtr:impact><gtr:outcomeId>56db791b0028e7.98580587</gtr:outcomeId><gtr:title>Go-with-the-flow</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The software extracts kinematics and muscle activity features from data captured by movement and EMG sensors and detect chronic pain-related behavior during a set of specific physical rehabilitation exercises. It can be used to personalize the support needed by the patient during physical rehabilitation.
The description of the features and the models is provided in the related publications (Aung et al., 2013-2016) at www.emo-pain.ac.uk. We are now improving the software to use cheaper and more easily available sensors. This version will be open-source.</gtr:description><gtr:id>3B161FCC-C014-4C6A-8B9C-2ACE381E3492</gtr:id><gtr:impact>The software is not yet used in the domain but it has led to new interest by companies and other researcher groups.</gtr:impact><gtr:outcomeId>56dc0f5e8b9dc4.83192068</gtr:outcomeId><gtr:title>Automatic recognition of pain-related behaviour during physical rehabilitation</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The software extracts kinematics and muscle activity features from data captured by movement and EMG sensors and predict pain level (3 levels) during physical rehabilitation exercise. It can be used to provide personalized support to people with chronic pain while they are engaged in physical rehabilitation. The description of the features and of the models are provided in Olugbade et al., (2014, 2015)
We are currently refining it to be able to process information from a wearable device based on cheaper and more ready available sensors to be deployed in the home.</gtr:description><gtr:id>0A9370FF-5E1C-4356-981B-DCB31AF8A26E</gtr:id><gtr:impact>Not yet used in the domain but led to interest from industry and connection with other research groups to further advance it.</gtr:impact><gtr:outcomeId>56dc4fe7349d72.78408908</gtr:outcomeId><gtr:title>Automatic detection of pain levels from body movement and muscle activity in physical rehabilitation</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Software publicly available from my emopain papers now is https://github.com/bernard24/ConvexTensor . Particularly that related to the paper &amp;quot;A New Convex Relaxation for Tensor Completion&amp;quot;.</gtr:description><gtr:id>4932FECC-3B2B-43C3-9DC6-36580A715CB0</gtr:id><gtr:impact>Other researchers have built on this to implement their own approaches, and to compare to this one.</gtr:impact><gtr:outcomeId>56dd55197fb672.42036919</gtr:outcomeId><gtr:title>Software publicly available from my emopain papers</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/bernard24/ConvexTensor</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>A49C2A2F-8D8B-47EF-B12F-F14F52D16DFD</gtr:id><gtr:title>Emotion recognition by two view SVM_2K classifier on dynamic facial expression features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eeed92cffe6e84d4d4f20c649670257b"><gtr:id>eeed92cffe6e84d4d4f20c649670257b</gtr:id><gtr:otherNames>Meng H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4244-9140-7</gtr:isbn><gtr:outcomeId>545d1a9af06041.23683945</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF4F65D8-89BE-43CA-8445-D0FEF5771C60</gtr:id><gtr:title>A One-Vs-One classifier ensemble with majority voting for activity recognition</gtr:title><gtr:parentPublicationTitle>ESANN 2013 proceedings, 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bacdb1f6c25df4d2bbd9b8189f74d3c"><gtr:id>4bacdb1f6c25df4d2bbd9b8189f74d3c</gtr:id><gtr:otherNames>Romera-Paredes B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d1d195ca642.66696764</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C8DB25D-18FB-4020-9F72-3E0393E66293</gtr:id><gtr:title>Multilinear multitask learning</gtr:title><gtr:parentPublicationTitle>30th International Conference on Machine Learning, ICML 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bacdb1f6c25df4d2bbd9b8189f74d3c"><gtr:id>4bacdb1f6c25df4d2bbd9b8189f74d3c</gtr:id><gtr:otherNames>Romera-Paredes B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d1a9a2e9904.59187714</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46C7F937-CF8F-4A7C-A377-D20BF43D1F86</gtr:id><gtr:title>Exploiting Unrelated Tasks in Multi-Task Learning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0e43fce9cff2cf930ae3bfa04a61d564"><gtr:id>0e43fce9cff2cf930ae3bfa04a61d564</gtr:id><gtr:otherNames>Bernardinio Romera-Paredes (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_382291365613d9845c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF743299-D623-463D-A6B8-8A9E1A772AFA</gtr:id><gtr:title>Multilinear multitask learning</gtr:title><gtr:parentPublicationTitle>30th International Conference on Machine Learning, ICML 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bacdb1f6c25df4d2bbd9b8189f74d3c"><gtr:id>4bacdb1f6c25df4d2bbd9b8189f74d3c</gtr:id><gtr:otherNames>Romera-Paredes B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d1d13ed71a1.02206243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>737E56BF-3D96-4118-8DA3-431B07AFC291</gtr:id><gtr:title>Pain level recognition using kinematics and muscle activity for physical rehabilitation in chronic pain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a427e76b8b8b8e9497ec8907f75166c1"><gtr:id>a427e76b8b8b8e9497ec8907f75166c1</gtr:id><gtr:otherNames>Olugbade T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d4278462a510.04999840</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6722F5C4-04B4-4073-99C0-91995132681D</gtr:id><gtr:title>A New Convex Relaxation for Tensor Completion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f38223fd289a59f65918a0b120b72459"><gtr:id>f38223fd289a59f65918a0b120b72459</gtr:id><gtr:otherNames>Romera-Paredes B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dac42f56faf7.47525141</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>10E04F0C-5928-4978-8D0A-44E9484D520F</gtr:id><gtr:title>Musically Informed Sonification For Self-directed Chronic Pain Physical Rehabilitation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd769c0768f3371b9252b54127433b17"><gtr:id>bd769c0768f3371b9252b54127433b17</gtr:id><gtr:otherNames>Newbold J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a0eabd6fd6d1.85605291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>91C88F2B-E445-469E-B37C-4A587A8F9E9F</gtr:id><gtr:title>Exploiting unrelated tasks in multi-task learning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f38223fd289a59f65918a0b120b72459"><gtr:id>f38223fd289a59f65918a0b120b72459</gtr:id><gtr:otherNames>Romera-Paredes B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56dad121d58aa7.03911102</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A28A049F-800C-4137-8DCE-187776A38CC1</gtr:id><gtr:title>Human Observer and Automatic Assessment of Movement Related Self-Efficacy in Chronic Pain: from Exercise to Functional Activity</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a427e76b8b8b8e9497ec8907f75166c1"><gtr:id>a427e76b8b8b8e9497ec8907f75166c1</gtr:id><gtr:otherNames>Olugbade T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a995944329e48.86616845</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6F9DEAD4-846A-4B65-8BAD-1091AC850A4F</gtr:id><gtr:title>: Tracking, Analysis and Sonification of Movement and Breathing to Build Confidence in Activity Despite Chronic Pain</gtr:title><gtr:parentPublicationTitle>Human-Computer Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62669b53e98481724294af0f7a2ae396"><gtr:id>62669b53e98481724294af0f7a2ae396</gtr:id><gtr:otherNames>Singh A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d42680176006.75578412</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2735DE2E-7A7A-4F6B-BB91-D74FDD410981</gtr:id><gtr:title>A One-Vs-One classifier ensemble with majority voting for activity recognition</gtr:title><gtr:parentPublicationTitle>ESANN 2013 proceedings, 21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bacdb1f6c25df4d2bbd9b8189f74d3c"><gtr:id>4bacdb1f6c25df4d2bbd9b8189f74d3c</gtr:id><gtr:otherNames>Romera-Paredes B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d1a9b489298.81635634</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C9B91F6-F5C9-468B-8A71-F4065E942016</gtr:id><gtr:title>One size fits none! Making affective state a key variable of behaviour change technology for chronic pain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62669b53e98481724294af0f7a2ae396"><gtr:id>62669b53e98481724294af0f7a2ae396</gtr:id><gtr:otherNames>Singh A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5461fd34e95077.34671398</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>93684E89-21D0-46A2-8D3D-7978FBEEB814</gtr:id><gtr:title>Body Tracking in Healthcare</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ab13d18fbf099ba071d1ffa24e5941e2"><gtr:id>ab13d18fbf099ba071d1ffa24e5941e2</gtr:id><gtr:otherNames>O'Hara Kenton</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:isbn>9781627059503</gtr:isbn><gtr:outcomeId>56def117f1fa02.94031848</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>839347C0-E055-4667-AD14-F517880E887B</gtr:id><gtr:title>User needs for technology supporting physical activity in chronic pain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f112daaaa59be6a1e4a176c2b1f6bbf"><gtr:id>8f112daaaa59be6a1e4a176c2b1f6bbf</gtr:id><gtr:otherNames>Swann-Sternberg, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56daff15ceadf9.00142396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4ABF9F5D-CDDC-4FAC-99EE-D6346FE911A0</gtr:id><gtr:title>Affective State Level Recognition in Naturalistic Facial and Vocal Expressions.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eeed92cffe6e84d4d4f20c649670257b"><gtr:id>eeed92cffe6e84d4d4f20c649670257b</gtr:id><gtr:otherNames>Meng H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>545d1a9b220462.29450857</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>80ABE63C-5D3A-4334-8BCC-09220859C43E</gtr:id><gtr:title>Impact of pain behaviors on evaluations of warmth and competence.</gtr:title><gtr:parentPublicationTitle>Pain</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4fc4d38d36222fde99a03b9353f4153a"><gtr:id>4fc4d38d36222fde99a03b9353f4153a</gtr:id><gtr:otherNames>Ashton-James CE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0304-3959</gtr:issn><gtr:outcomeId>545d1a9b94d1e5.86021924</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>463A0781-213F-4BFE-A568-5C130A628A6B</gtr:id><gtr:title>Walk a mile in my shoes: reflecting on studies with people with chronic pain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62669b53e98481724294af0f7a2ae396"><gtr:id>62669b53e98481724294af0f7a2ae396</gtr:id><gtr:otherNames>Singh A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5461fc9fc9bbd9.26521291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>23AD1E0F-F078-4FF3-A126-E2A5982D8B7B</gtr:id><gtr:title>Transfer learning to account for idiosyncrasy in face and body expressions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f38223fd289a59f65918a0b120b72459"><gtr:id>f38223fd289a59f65918a0b120b72459</gtr:id><gtr:otherNames>Romera-Paredes B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>5461ff43e78c61.36767197</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>647E1F60-5843-4BC0-B815-FA284B05ACEA</gtr:id><gtr:title>Mind the Gap: A SIG on bridging the gap in research on body sensing, body perception and multisensory feedback</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a995f63aff49c4643f524638f7773833"><gtr:id>a995f63aff49c4643f524638f7773833</gtr:id><gtr:otherNames>Singh, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dafdcd20f702.08474617</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B376515-B238-4B53-AE7B-5D50CF498B21</gtr:id><gtr:title>Time-Delay Neural Network for Continuous Emotional Dimension Prediction From Facial Expression Sequences.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eeed92cffe6e84d4d4f20c649670257b"><gtr:id>eeed92cffe6e84d4d4f20c649670257b</gtr:id><gtr:otherNames>Meng H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>56db5a70b38972.05393744</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0D3D084F-BD45-4A23-8008-76E4B1702518</gtr:id><gtr:title>Tracking, Analysis and Sonification of Movement and Breathing for Supporting Physical Activity in Chronic Pain Using The Go-with-the-flow Framework</gtr:title><gtr:parentPublicationTitle>Frontiers in Public Health</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/faedaca0abf5c1f7a81a77b89f20d1b7"><gtr:id>faedaca0abf5c1f7a81a77b89f20d1b7</gtr:id><gtr:otherNames>Aneesha S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>22962565</gtr:issn><gtr:outcomeId>56dd927e6b5e10.41107294</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43E2C279-8D69-4C38-8B88-8DC18D186047</gtr:id><gtr:title>Multilinear-MultiTask Learning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aaec3678204f780753e9231ae2094c6f"><gtr:id>aaec3678204f780753e9231ae2094c6f</gtr:id><gtr:otherNames>Bernardino Romera-Paredes (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_894228828613d987d6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C3A422E-5378-4D7B-870B-16E1852C4180</gtr:id><gtr:title>Interactive Technology to Support Physical Activity in People With Chronic Musculoskeletal Pain: What Users Want.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a995f63aff49c4643f524638f7773833"><gtr:id>a995f63aff49c4643f524638f7773833</gtr:id><gtr:otherNames>Singh, A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56db023513c811.89016871</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8F170585-0B0D-4C1A-9B53-65A11D597C77</gtr:id><gtr:title>The Automatic Detection of Chronic Pain-Related Expression: Requirements, Challenges and the Multimodal EmoPain Dataset</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aff0b9f9080aa0f1db5c2bf5ca4b438f"><gtr:id>aff0b9f9080aa0f1db5c2bf5ca4b438f</gtr:id><gtr:otherNames>Aung M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dac1debc8536.11882825</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD368524-26E6-4AF5-A7FE-C8EE08DF8DA5</gtr:id><gtr:title>Emotion and pain: interactive technology to motivate physical activity in people with chronic pain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62669b53e98481724294af0f7a2ae396"><gtr:id>62669b53e98481724294af0f7a2ae396</gtr:id><gtr:otherNames>Singh A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54620123a843e1.30253266</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A066F140-A89E-4183-A966-1EE9626375AF</gtr:id><gtr:title>Identifying Pain Behaviour for Automatic Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9d7385de0ac3c2bef12ee44c2ff8936d"><gtr:id>9d7385de0ac3c2bef12ee44c2ff8936d</gtr:id><gtr:otherNames>Cella, M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56daffe9bc0444.39901509</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>382DF31B-3DC8-42DD-8C6B-B1E609A5FD23</gtr:id><gtr:title>Getting RID of pain-related behaviour to improve social and self perception: A technology-based perspective</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aff0b9f9080aa0f1db5c2bf5ca4b438f"><gtr:id>aff0b9f9080aa0f1db5c2bf5ca4b438f</gtr:id><gtr:otherNames>Aung M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56dac1de7718d2.99581582</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H017178/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>67</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>FD25826C-8B50-43A3-8871-3FF08D051906</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Biomechanics &amp; Rehabilitation</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>34</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>