<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:department>Sch of Engineering and Informatics</gtr:department><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0E29AFB5-3434-4892-A705-18C6478731FE"><gtr:id>0E29AFB5-3434-4892-A705-18C6478731FE</gtr:id><gtr:name>Plessey Semiconductors Ltd</gtr:name><gtr:address><gtr:line1>Tamerton Road</gtr:line1><gtr:line2>Roborough</gtr:line2><gtr:postCode>PL6 7BQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CF7F58C1-4B26-4CBA-9FE8-BA1DD8457CB7"><gtr:id>CF7F58C1-4B26-4CBA-9FE8-BA1DD8457CB7</gtr:id><gtr:name>Unilever UK Central Resources Ltd</gtr:name><gtr:address><gtr:line1>Quarry Road East</gtr:line1><gtr:line2>Bebington</gtr:line2><gtr:postCode>CH63 3JW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/DE48007C-ABDF-43E5-AC5E-35C4B5753B33"><gtr:id>DE48007C-ABDF-43E5-AC5E-35C4B5753B33</gtr:id><gtr:firstName>Daniel</gtr:firstName><gtr:surname>Roggen</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN007816%2F1"><gtr:id>6FE14811-5A8D-4BAD-8EB0-F5D42E37F92F</gtr:id><gtr:title>Lifelearn: Unbounded activity and context awareness</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N007816/1</gtr:grantReference><gtr:abstractText>The Wearable Computing market is expected to explode, as evidenced in 2014 and early 2015 with a plethora of new products primarily in the sports and fitness domain. Business Insider in 2013 estimated that 300 million units would be shipped by 2018. What makes wearables (and similarly mobile phones) unique is their contextual intelligence: they use sensors to infer users' context, such as location, activities or social interactions. This contextual intelligence allows a fitness tracker to detect by itself that the user is running, walking, or doing push-ups. 

We are motivated by the vision of pervasive &amp;quot;wearable smart assistants&amp;quot; that provide situated contextual support in daily life. They may act as &amp;quot;memory reminders&amp;quot; for people with dementia, or encourage healthy behaviours through supportive prompts presented at the right time (e.g. to fight obesity, diabetes, cardiovascular diseases).

This project deals with the heart of any such assistive technology: the ability to recognise general human activities and context from sensors. Current methods can only recognise pre-defined or &amp;quot;closed sets&amp;quot; set of activities and context. This is insufficient for the scenarios outlined above. In such applications, the set of relevant activities is not necessarily known at design-time, as different users tend to have different routines, routines may change as users change interests, and activities may be performed differently, for instance after an injury. Therefore the set of relevant activities and contexts is potentially unbounded and is said to be &amp;quot;open-ended

The project investigates the methods required to recognise an &amp;quot;open-ended&amp;quot; set of activities and contexts from existing wearables, such as a smartwatch and a mobile phone, following lifelong learning principles. In other words, the system should discover that a user engages in a new activity, even if it was not initially programmed with the knowledge of that activity.

We develop new open-ended learning techniques that can model changing number of classes at runtime. These methods run on a recognition infrastructure comprising software on the wearable devices and on a server. The infrastructure will be made open-source to benefit other projects. We develop methods that discover reoccurring wearable sensor patterns. Repeating patterns may correspond to new activities or contexts. Therefore they are modelled using open-ended learning techniques. Finally, we develop methods to decide whether a discovered pattern is meaningful and what it represents. This is achieved by involving the user and occasionally requesting to provide information about his/her current activity. We compare different feedback options that minimise the number of interruptions and the complexity of the queries. Overall, the system is evaluated on existing data as well as on a new long-term dataset collected within this project.


Our approach is novel and timely. Performance increases in activity recognition are incremental and the inability to deal with unknown activities is most critical for large-scale deployments in daily life scenarios. This project addresses this fundamental limit. This is timely given raising costs of healthcare and calls to rely on technology to address this issue. The outcomes of this project along understanding daily human behaviour may lead to new smart assistants that could help support independent living or assist users in following healthy behaviour change. The outcomes may also find their use in psychology research and in the area of sustainable innovation, such as the assessment of consumer-product interaction and behaviour change initiatives. As such the project has clear societal benefits.


This project is supported by our partners Unilever and Plessey Semiconductors, respectively interested in consumer behaviour research and new products in the healthcare domain.</gtr:abstractText><gtr:potentialImpactText>This project is motivated by the vision of pervasive &amp;quot;smart assistants&amp;quot; that provide situated contextual support in daily life, such as &amp;quot;memory reminders&amp;quot; for persons with dementia. At the heart of such technology lies the ability to recognise general human activities and context. This is the key challenge that this project addresses. In the longer term, the outcomes may become fundamental to address societal challenges in assisted living, health monitoring, and smart assistance.

*Economic and industrial impact

This project develops technologies along mobile, wearable, Internet of Things and behavioural analytics. There are major UK technology companies in the field (e.g. ARM, Imagination Technologies) as well as US technology companies with UK branches (e.g. Qualcomm). Behavioural analytics is beneficial for targeted advertisement or consumer behaviour analysis. The UK has several major companies active in health, nutrition and care (e.g. GSK, Unilever, Body Shop) where behavioural analysis can be used to understand product usage and design new products. 

Our project partners Unilever and Plessey Semiconductors illustrate the interest in the technology we develop, which they aim to use respectively for better consumer behaviour research and to create new microelectronics products. 

We will reach out to industries through the UK Design Forum (a UK microelectronics forum where companies and academics meet) and the Knowledge Transfer Network of Innovate UK. The PI has also contacts to leading researchers active in companies (Intel, Google, Qualcomm, etc.).

*Societal and quality of life benefits

In the longer term, understanding human daily routines from sensors enables new interventions and services in the field of healthcare. It can support longer independent living; activity monitoring may be used to detect onset of depression; cues may be provided in relevant contextual situations to support a desired lifestyle change (e.g. to fight obesity, diabetes or cardiovascular disease). 
The impacts can be major in terms of quality of life for the concerned users. They will also benefit public bodies involved in healthcare (UK NHS, charities) and can increase effectiveness of public services and support evidence-based policies.

*Wider public

There is a growing interest in the general public in &amp;quot;quantified self&amp;quot;, i.e. keeping track of one's own routines. Within this project we will inform the general public as to how these technologies operate, their future potential (e.g. with the outcomes of this project), and their possible downsides (e.g. along privacy). We will reach-out to the wider public through talks at existing meetup events and university open-doors (see pathway to impact for details).

*Skilled workforce
The team (postdoc funded by this project and students indirectly associated to it) will gain useful and transferable skills, especially analytical, presentation and organisational skills, that are applicable to several employment sectors:
- big data analytics
- machine learning
- mobile and wearable sensing and applications
- sensing technologies and internet of things 
- client/server architectures
- behaviour analytics

*Impact timescales
The infrastructure we develop in the project will be immediately available after the end of the project and may have immediate impact for companies already active in wearable sensing and our project partners. 

Open-ended awareness is a novel paradigm that will take more time to be exploited. Based on previous experience we envision uptake in the scientific community within 3 years from the project start (through the workshops we will organise). Wider industry awareness and uptake may be 3-5 years after the project completion.

*Summary

We disseminate our work through high-impact publications and conferences and will reach out to selected industries directly to achieve impact both in the UK and internationally.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98520</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>4th workshop on human activity sensing corpus and applications: towards open-ended context awareness, Ubicomp</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EB972666-2553-4069-B231-AF16598B0BDA</gtr:id><gtr:impact>This workshop was designed to promote the notion of &amp;quot;unbounded&amp;quot;/&amp;quot;open-ended&amp;quot;/&amp;quot;lifelong learning&amp;quot; in human activity/context awareness in the wearable, mobile and ubicomp community. This was organised as a special call for topics within the Human Activity Sensing Corpus and Applications (HASCA( workshop at Ubicomp 2016. Papers were peer reviewed for inclusion in the adjunct proceedings of HASCA. 
This actual workshop comprised a keynote talk from Dr Mario Fritz from Max Plank on &amp;quot;scalable learning and perception&amp;quot; followed by presentations of the accepted peer-reviewed contributions. 
A key outcome from this workshop was the crystallisation of the notion of &amp;quot;unbounded&amp;quot;/&amp;quot;open-ended&amp;quot;/&amp;quot;lifelong learning&amp;quot; in in human activity/context awareness in the wearable, mobile and ubicomp communities, as illustrated by several papers tackling this in various ways. This is a marked change from the 3rd edition of HASCA (2015) which did not have a special call for contributions on unbounded activity/context recognition and welcomed primarily dataset-related contributions.</gtr:impact><gtr:outcomeId>58c6611d9fd3d8.82506876</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://hasca2016.hasc.jp/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>168044</gtr:amountPounds><gtr:country>China, People's Republic of</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Huawei Technologies (direct funding)</gtr:description><gtr:end>2017-12-02</gtr:end><gtr:fundingOrg>Huawei Technologies</gtr:fundingOrg><gtr:id>FF86C7F3-F866-44A0-8747-FB4863E7CE8E</gtr:id><gtr:outcomeId>58c65d68935314.55153493</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We demonstrated for the first time that Deep Learning techniques are suitable for activity recognition from wearable sensor data, despite the comparatively smaller amount of available training data in contrast to other fields such as speech and image recognition.

So far, the suitability of our deep learning techniques is being further investigated with a view towards industrial applications in the domain of mobile computing and hardware-assisted dedicated learning circuits.

The main challenges to overcome to achieve impact related to the computational cost and benefit tradeoff of the method. Further advances along performance increase or computational cost reduction will enable inclusion into novel context-aware consumer products.</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>1A6D4044-D46D-4E34-99C5-E2E156095BCF</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>58c66cf1c46ad9.50785245</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Human activity and context recognition from mobile, wearable and ambient sensors is fundamental to enable next generation of assistive technologies in a wide range of domains including industrial applications, assisted living, personalised healthcare, sports safety and performance.

Until now, most activity recognition systems were limited to recognising specific pre-defined activities. This fundamentally limits the scope of applications, in particular when attempting to provide assistance in everyday life where a potentially unbounded set of activities and contexts can be encountered. This is the case, for example, if one were to develop an activity diary as a memory prosthesis (e.g. for people with dementia).

This project develops approaches to recognise an unbounded or open-ended set of activities automatically by a combination of novel machine learning techniques, motif discovery and assumptions about human behaviour. This has the potential to significantly expand the set of assistive systems which we can devise.

At this stage we demonstrated the ability to significantly increase recognition performance by using novel Deep Learning techniques. Furthermore, we demonstrated that the signal features these deep learning models learn are generic and can be transferred across domains. This is a fundamental step towards unbounded activity recognition as features must be generic enough to recognise an open set of activities. 

So far, the most significant achievements of the award are:
- A demonstration that deep learning techniques can be applied to human activity/context recognition in mobile and wearable computing, despite the previously held belief that the amount of training data would be inadequate in comparison to the data available in speech and image recognition.
- A demonstration that deep learning techniques clearly outperform previously proposed systems for wearable activity/context recognition
- An insight that the kernels of deep convolutional networks do learn fundamental characteristics of sensor signals which specialise in the higher levels of the architecture. 
- A demonstration that transfer learning is possible by transferring convolutional kernels across models. Convolutional kernels at the lowest layers can be shared across multiple problem domains, with the benefit of computational cost savings. Convolutional kernels at the higher level tend to specialise to a problem domain and therefore they tend to be more suitable for transfer when the source problem displays higher variability compared to the target domain.
- Overall, deep convolutional networks appear to learn fundamental features of wearable/mobile sensor signals and can be transferred to similar domains. This can be leveraged for unbounded activity recognition by subsequent motif discovery techniques.</gtr:description><gtr:exploitationPathways>The findings of our research can be taken forward as follows:
- Our deep learning architecture can be reused for other problems of activity and context recognition, with likely significant performance increase over previous methods
- The reference recognition architecture which we released publicly can be used as a baseline to benchmark new methods
- The system we developed can be exploited in embedded architectures, sensor hubs and mobile systems to provide activity recognition and context awareness.

We expect companies active in mobile and wearable technologies, processor architectures, sensor technologies and others to be especially interested to bring this research forward.</gtr:exploitationPathways><gtr:id>DC3EFB66-CAAC-4EBE-8370-864600B02346</gtr:id><gtr:outcomeId>58c6748647bcd8.74390076</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:sectors><gtr:url>http://www.sussex.ac.uk/strc/research/wearable</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>EPSRC Associate College invitation</gtr:description><gtr:geographicReach>National</gtr:geographicReach><gtr:id>E6ECED8E-04F4-4DA6-959A-6F8B34DB2266</gtr:id><gtr:outcomeId>58c66ab118e640.58092698</gtr:outcomeId><gtr:type>Participation in a advisory committee</gtr:type></gtr:policyInfluenceOutput></gtr:policyInfluenceOutputs><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>We released the reference implementation of the deep learning framework used in the Sensors and ISWC articles to the scientific community, including pre-trained models. This has made our implementation of deep learning for activity recognition in wearable computing a reference implementation which allows other scientist to compare their system against this baseline.</gtr:description><gtr:id>28A0489F-C05F-41E5-89C6-BDD992B4FB50</gtr:id><gtr:impact>The following are notable impacts:

 - a large number of researchers contacted us for further queries on the code (67 individual queries) 

- Two companies contacted us for follow-up collaborations where work is under progress 

- 36 citations to the Sensors article after one year.</gtr:impact><gtr:outcomeId>58c66633674e36.74339419</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Deep learning for activity recognition reference implementation</gtr:title><gtr:type>Data analysis technique</gtr:type><gtr:url>https://github.com/sussexwearlab/DeepConvLSTM</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>We released the reference implementation of the deep learning framework used in the Sensors and ISWC articles to the scientific community, including pre-trained models. This has made our implementation of deep learning for activity recognition in wearable computing a reference implementation which allows other scientist to compare their system against this baseline.</gtr:description><gtr:id>EDDFBD8B-DE96-4A01-BC97-9DC41E3148E3</gtr:id><gtr:impact>The following are notable impacts:
- a large number of researchers contacted us for further queries on the code (67 individual queries)
- Two companies contacted us for follow-up collaborations where work is under progress
- 36 citations to the Sensors article after one year.</gtr:impact><gtr:outcomeId>58c665932e8766.53822019</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Deep learning for activity recognition reference implementation</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>https://github.com/sussexwearlab/DeepConvLSTM</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>We released the reference implementation of the deep learning framework used in the Sensors and ISWC articles to the scientific community, including pre-trained models. This has made our implementation of deep learning for activity recognition in wearable computing a reference implementation which allows other scientist to compare their system against this baseline.</gtr:description><gtr:id>F3613690-8BC7-4B08-B94E-CE38573EB74A</gtr:id><gtr:impact>The following are notable impacts: 

- a large number of researchers contacted us for further queries on the code (67 individual queries) 

- Two companies contacted us for follow-up collaborations where work is under progress 

- 36 citations to the Sensors article after one year.</gtr:impact><gtr:outcomeId>58c66a17b54458.02054833</gtr:outcomeId><gtr:title>Deep learning for activity recognition reference implementation</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/sussexwearlab/DeepConvLSTM</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>24551192-706B-433F-A202-48CA2C5AA85D</gtr:id><gtr:title>Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition.</gtr:title><gtr:parentPublicationTitle>Sensors (Basel, Switzerland)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d699101702bd3ea9f2c9e9cc440be5df"><gtr:id>d699101702bd3ea9f2c9e9cc440be5df</gtr:id><gtr:otherNames>Ord??ez FJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1424-8220</gtr:issn><gtr:outcomeId>58c65b1561fb20.98230209</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N007816/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>13B6D44B-6EAF-464B-A4CC-1C08F8DDE5A0</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Mobile Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>