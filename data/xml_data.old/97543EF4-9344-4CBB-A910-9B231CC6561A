<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/F9F1D136-12E3-4BE4-9668-0C9BC4A7C1BF"><gtr:id>F9F1D136-12E3-4BE4-9668-0C9BC4A7C1BF</gtr:id><gtr:name>Bangor University</gtr:name><gtr:department>Sch of Psychology</gtr:department><gtr:address><gtr:line1>Bangor University</gtr:line1><gtr:line4>Bangor</gtr:line4><gtr:line5>Gwynedd</gtr:line5><gtr:postCode>LL57 2DG</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F9F1D136-12E3-4BE4-9668-0C9BC4A7C1BF"><gtr:id>F9F1D136-12E3-4BE4-9668-0C9BC4A7C1BF</gtr:id><gtr:name>Bangor University</gtr:name><gtr:address><gtr:line1>Bangor University</gtr:line1><gtr:line4>Bangor</gtr:line4><gtr:line5>Gwynedd</gtr:line5><gtr:postCode>LL57 2DG</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/78308CAA-8483-45C0-A280-0AB4B6AD9D35"><gtr:id>78308CAA-8483-45C0-A280-0AB4B6AD9D35</gtr:id><gtr:name>Medical Research Council</gtr:name><gtr:address><gtr:line1>Polaris House</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:postCode>SN2 1FL</gtr:postCode><gtr:region>South West</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/638C8CA2-B22C-443B-BFD0-C3EEB6F0FF0B"><gtr:id>638C8CA2-B22C-443B-BFD0-C3EEB6F0FF0B</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:surname>Downing</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/9E890CFA-58CE-4FE5-966B-C1A35CD45276"><gtr:id>9E890CFA-58CE-4FE5-966B-C1A35CD45276</gtr:id><gtr:firstName>Steven</gtr:firstName><gtr:otherNames>Paul</gtr:otherNames><gtr:surname>Tipper</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FF028652%2F1"><gtr:id>97543EF4-9344-4CBB-A910-9B231CC6561A</gtr:id><gtr:title>Mirror systems and the perception and generation of action: Multivariate fMRI studies</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/F028652/1</gtr:grantReference><gtr:abstractText>&lt;p>The overall goal of this research is to investigate the brain systems that underlie the basic human ability to understand the overt behaviour of other individuals. Multivariate analyses will be applied to fMRI data obtained while human subjects observe and perform actions under a variety of conditions. The specific objectives are:&lt;/p>

&lt;ol>
 

 &lt;li>To test for evidence of a common neural code for perception and production of action. If the voxel-by-voxel spatial variations in activity in parietal and premotor regions reflects the distribution of the underlying neural network(s), then there should be a strong similarity between the local patterns of fMRI activity elicited by viewing an action and by performing the same action.&lt;/li>

 &lt;li>To examine whether population codes in the same brain area represent different views of actions.&lt;/li>

 &lt;li>To examine local patterns of fMRI activity in parietal and frontal action systems to determine whether they distinguish transitive (object-directed) and intransitive (non-object-directed) classes of actions.&lt;/li>

 &lt;li>To test the hypothesis that when attention is withdrawn from visual action, the population codes distinguishing between different forms of action become more diffuse, resulting in less selective patterns of neural activity.&lt;/li>
&lt;/ol></gtr:abstractText><gtr:fund><gtr:end>2012-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2008-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>399514</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Multivoxel pattern analysis reveals mirror-like responses in human parietal and occipitotemporal cortex for transitive and intransitive actions</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:id>CC54E3F3-A784-47DC-896D-00FA4F2D9170</gtr:id><gtr:impact>Presented at Workshop on Concepts, Actions, and Objects: Functional and Neural Perspectives</gtr:impact><gtr:outcomeId>r-2622480096.8752062ab81fa4</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our fMRI approaches to study these vision-action systems are the most sophisticated so far. Unlike previous MVPA studies, each participant's data was analyzed with a whole-cerebrum information mapping (&amp;quot;searchlight&amp;quot;) approach. Furthermore, in contrast to the volume-based approach used by most MVPA &amp;quot;searchlight&amp;quot; studies to date, we used surface-based reconstructions of the cortex. This approach improves both the classification accuracy and spatial specificity of the resulting information maps. The development of such new techniques are driving the progress of cognitive neuroscience, and hence we expect them to be utilized by others over the coming years. A matlab toolbox that emerged from the research (http://surfing.sourceforge.net.) has been downloaded over 120 times.</gtr:description><gtr:id>9E4633B5-0B35-4BBC-9BF0-3A6299CDB183</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5447de3636adb2.83883171</gtr:outcomeId><gtr:sector>Other</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The evidence that observation of an action activates similar motor states in the viewer has remained equivocal in human fMRI studies. Although similar neural areas are activated both when doing an action and when observing another person perform the action, it remained possible that completely different neural populations were activated within these common areas. We have now provided the strongest data so far from fMRI to show that the representations of doing action and those of viewing action are the same, in that the neural population code representing a specific produced action is able to classify the same action when it is merely viewed. (Oosterhoff et al, 2010, Journal of Neurophysiology, 104 (2), 1077-1089).

From these basic discoveries we then asked other questions concerning the vision-action systems. For example, the classic findings in monkey point to the ventral premotor cortex F5 as key to the joint represention of produced and observed actions. However, we demonstrate that this is not necessarily the case in the human brain where parietal cortex appears to be the more important region for representing vision of, and production of, action. Indeed, the ventral prefrontal region plays a much more restricted role, only representing actions when viewed from a 1st person perspective (Oosterhof, et al, 2012, Journal of Cognitive Neuroscience, 24(4), 975-989). 

We extended the investigation of the relationship between 1st and 3rd person action representations in a study of learning. Again, using fMRI, we demonstrated that when actions were experienced from a 1st person perspective they could generalize to 3rd person representations during a later viewing task. However, the form of the prior experience was critical: merely viewing 1st person actions produced minimal learning. Rather, production of action, even without vision of the action, was necessary and sufficient to produce learning (Wiggett et al, in press, Social Neuroscience,). We also confirmed that these learning processes mediating the links between vision and production of action were automatic (Wiggett et al, 2011, Brain &amp;amp; Cognition, 76, 87-96), and we developed new baseline measures to detect excitatory and inhibitory processes during interactions between vision of, and production of, action (Wiggett et al, under review, Experimental Brain Research).

Two further key issues were engaged. Again, in the monkey the mirror cells that activate when the animal produces and also when observing the same actions, appear to require action to be directed to an object (transitive action). They do not activate when an action is produced but no object is visible (intransitive action). However, we demonstrated MVPA representions of viewed and produced actions whether an object was acted on or not (Oosterhoff et al, 2010). A further issue concerned imagery. Whether imagery of an action is a representation mediating vision and production of an action has remained an unresolved issue. We were able to demonstrate that MVPA representations of specific imagined actions can classify those same actions when they are produced, suggesting that in principle imagery does play an important role in cross-modal vision-action representations (Oosterhoff et al, in press, NeuroImage,).</gtr:description><gtr:exploitationPathways>We expect our research to have a significant impact over the coming years. The study of mirror systems that propose common representations of produced action and vision of action is a very large and vibrant research field. Our work has made important contributions. Perhaps most importantly, there have been increasing questions concerning the existence of such mirror systems in humans. We have provided the strongest fMRI data to date that such cross-modal vision-action representations exist. This work is already being cited. Two other key issues have concerned the relationship between the neural sites in monkey and human. Our work shows that the analogue to the ventral premotor (F5) region in monkey is NOT the main site for such processes in human, where parietal cortex plays a more dominant role. This is a significant challenge to accepted views and will have to be considered in future model development. A further key issue concerns the role of visual imagery as a process mediating the link between producing an action and the observation of the action. We demonstrated that in principle imagery does play a role, and this will also have to be taken in to account by those developing models of mirror systems.

The other main contributions where we expect significant impact are methodological. Our fMRI approaches to study these vision-action systems are the most sophisticated so far. Unlike previous MVPA studies, each participant's data was analyzed with a whole-cerebrum information mapping (&amp;quot;searchlight&amp;quot;) approach. Furthermore, in contrast to the volume-based approach used by most MVPA &amp;quot;searchlight&amp;quot; studies to date, we used surface-based reconstructions of the cortex. This approach improves both the classification accuracy and spatial specificity of the resulting information maps. The development of such new techniques are driving the progress of cognitive neuroscience, and hence we expect them to be utilized by others over the coming years.</gtr:exploitationPathways><gtr:id>74154DB6-6227-422B-A802-F1AEFC2C7E82</gtr:id><gtr:outcomeId>5447e4213fc6a0.51053334</gtr:outcomeId><gtr:sectors><gtr:sector>Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>CDAEC253-3150-4799-8BA6-46186B19F2F8</gtr:id><gtr:title>Doing, seeing, or both: effects of learning condition on subsequent action perception.</gtr:title><gtr:parentPublicationTitle>Social neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a17df6c1ad6f5bbe712e9d373fdd90ed"><gtr:id>a17df6c1ad6f5bbe712e9d373fdd90ed</gtr:id><gtr:otherNames>Wiggett AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1747-0919</gtr:issn><gtr:outcomeId>pm_53cc008c008962536</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3E8385D7-4F33-4AB2-9D30-90C3BE61FA47</gtr:id><gtr:title>Is the extrastriate body area involved in action perception?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/009f3cc6d56c449b3a01e10027c232f7"><gtr:id>009f3cc6d56c449b3a01e10027c232f7</gtr:id><gtr:otherNames>Alison Wiggett (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>r_9617971265cb51977c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F634898A-873A-4C6B-BC60-88C49F0C8F51</gtr:id><gtr:title>Facilitation and interference in spatial and body reference frames.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a17df6c1ad6f5bbe712e9d373fdd90ed"><gtr:id>a17df6c1ad6f5bbe712e9d373fdd90ed</gtr:id><gtr:otherNames>Wiggett AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>pm_53cc019c0190213e6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE588A9D-222F-43FE-A122-3CE7AB3F964E</gtr:id><gtr:title>A comparison of volume-based and surface-based multi-voxel pattern analysis.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dae4aaa1816a50179a8fd0a2b1f57396"><gtr:id>dae4aaa1816a50179a8fd0a2b1f57396</gtr:id><gtr:otherNames>Oosterhof NN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>pm_53cbfc7bfc7252715</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FAE977A-D21A-47FE-9E07-22CCACBC9C86</gtr:id><gtr:title>Surface-based information mapping reveals crossmodal vision-action representations in human parietal and occipitotemporal cortex.</gtr:title><gtr:parentPublicationTitle>Journal of neurophysiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dae4aaa1816a50179a8fd0a2b1f57396"><gtr:id>dae4aaa1816a50179a8fd0a2b1f57396</gtr:id><gtr:otherNames>Oosterhof NN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0022-3077</gtr:issn><gtr:outcomeId>pm_53cbfc3bfc3d9f38f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92FFF624-251D-4095-BD23-B1F4C1C279DA</gtr:id><gtr:title>Visuo-motor imagery of specific manual actions: a multi-variate pattern analysis fMRI study.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dae4aaa1816a50179a8fd0a2b1f57396"><gtr:id>dae4aaa1816a50179a8fd0a2b1f57396</gtr:id><gtr:otherNames>Oosterhof NN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>pm_53cc00dc00dc59582</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E7EABDB8-6F86-4662-B1F9-4E3319516078</gtr:id><gtr:title>Multivoxel pattern analysis reveals mirror-like responses in human parietal and occipitotemporal cortex</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e28978ef76efd4e7077eaffc846d51f1"><gtr:id>e28978ef76efd4e7077eaffc846d51f1</gtr:id><gtr:otherNames>Nick Oosterhof (author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>1900-01-01</gtr:date><gtr:outcomeId>r_7211660572cbdcd21a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4EC102CD-1251-4234-B5AD-33C36A22171C</gtr:id><gtr:title>Representation of action in occipito-temporal cortex.</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a17df6c1ad6f5bbe712e9d373fdd90ed"><gtr:id>a17df6c1ad6f5bbe712e9d373fdd90ed</gtr:id><gtr:otherNames>Wiggett AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>pm_53cbfcbbfcbee9cd3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A418DD2A-A1B0-4307-B782-6085111C6E1C</gtr:id><gtr:title>Learning associations between action and perception: effects of incompatible training on body part and spatial priming.</gtr:title><gtr:parentPublicationTitle>Brain and cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a17df6c1ad6f5bbe712e9d373fdd90ed"><gtr:id>a17df6c1ad6f5bbe712e9d373fdd90ed</gtr:id><gtr:otherNames>Wiggett AJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0278-2626</gtr:issn><gtr:outcomeId>pm_53cbfe1bfe1fb658e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8414C458-1723-4AB5-95DE-FC344D6DEEB7</gtr:id><gtr:title>Viewpoint (in)dependence of action representations: an MVPA study.</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dae4aaa1816a50179a8fd0a2b1f57396"><gtr:id>dae4aaa1816a50179a8fd0a2b1f57396</gtr:id><gtr:otherNames>Oosterhof NN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>pm_53cbffebffe395656</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/F028652/1</gtr:identifier><gtr:identifier type="RES">RES-062-23-0980</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>