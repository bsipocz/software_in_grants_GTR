<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Experimental Psychology</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A2400336-DA21-4940-A557-1928AD378EC6"><gtr:id>A2400336-DA21-4940-A557-1928AD378EC6</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>Patrick</gtr:otherNames><gtr:surname>Vinson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FK001337%2F1"><gtr:id>EE6EA9D6-D571-4EE6-A087-61C7979BE29D</gtr:id><gtr:title>Making sense from the hands and mouth: multimodal integration in spoken and signed languages</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/K001337/1</gtr:grantReference><gtr:abstractText>&lt;p>Successful communication involves integrating multiple types of information. In sign languages, linguistic information is provided by the hands, but also by the body, face and mouth. In spoken languages, speech sounds are combined with visible mouth movements and gestures. Such information from different sources is tightly synchronised, facilitating comprehension. Most studies of signed and spoken language, however, focus on a single dominant modality (hands or speech), and therefore little is known about how the different types of information are integrated in comprehending language.&lt;/p>

&lt;p>This project investigates the integration of mouth and hands in signed and spoken languages, testing how integration affects language comprehension in British Sign Language (BSL) and English, and the extent to which common neural systems subserve integration for signed and spoken languages.This will be achieved by digitally manipulating BSL and English video materials, creating stimuli with mismatches in content or timing of information from the hands and mouth (eg saying &amp;quot;drive&amp;quot; but gesturing &amp;quot;twist&amp;quot;). Behavioural experiments will test the consequences of different types of mismatches on comprehending language; functional MRI studies will investigate which neural systems are involved in integrating information from the hands and mouth in the the two languages. &lt;/p></gtr:abstractText><gtr:potentialImpactText>The general BSL community
The proposed research has important consequences especially for the Deaf community in the UK as primary users of a minority language (BSL) in a majority language environment (English). To date there has been little research on the role of mouthings in BSL, and existing research has largely targeted academic audiences. As a result there are widely varying opinions in the community about the nature of mouthing beyond the appreciation that it is the product of language contact: whether it should be considered a full-fledged part of BSL, or an optional form of simultaneous bilingual expression possible when mouth patterns are not specified in BSL (i.e. a flexible communication tactic to be deployed only in some contexts). As a result, the treatment of mouthing in BSL educational contexts is highly variable, with only implicit attention to mouthing except in situations where specific mouth gestures should accompany particular signs. Findings from the proposed research clarifying the role of mouthing in BSL has potential impact on those groups outside of academia who have specific vested interest in knowing more about the language: BSL tutors, students, interpreters, interpreter trainers, communication support workers, etc (as well as international users with interest in other sign languages). 

Educational practice
This project can also inform educational practice: finding a high degree of integration between hand and mouth in sign comprehension implies that substantial benefits could be gained by explicit treatment of mouthing in sign language education. Furthermore, explicit attention to English-derived mouthing that accompanies BSL may offer further benefits to English literacy education for deaf children, supplementing instruction that focuses purely upon English phonological awareness with additional links to the English-derived mouthings that accompany many BSL signs. Potential users of research findings are individuals involved in education of deaf children (whether in deaf schools or mainstream education), as well as other educational environments in which multimodal approaches may provide benefits.

Language rehabilitation
Evidence that gestures can facilitate communication across various contexts, along with results of the proposed studies related to integration of audio-visual speech and gesture both have implications for rehabilitation. It may be especially promising to consider how gesture combined with visible speech can benefit in cases of language difficulty. Although a few studies have tested the efficacy of gesture-based interventions, such approaches have been considered only to a very limited extent despite growing awareness that the multimodal nature of spoken language should be considered in atypical language contexts. Potential users in this domain are speech and language therapists.

Communication technology
Effective communication in noisy environments has long been a goal of the communications industry, and the boom in video communication now permits spoken language to be accompanied by video in numerous cases. Findings of strong integration and comprehension benefit for gestures accompanying speech would imply that inclusion of hands and body in spoken video messages could facilitate video communication beyond head-alone video messages. In BSL, a better understanding of how mouthing is integrated in comprehension should provide substantial improvements in automatic sign language recognition technology as well as the development of effective sign language avatars. In both domains, it is recognised that mouthing is essential to successful performance, yet implementation of mouthing lags far behind. Results from the proposed research will inform these domains especially by identifying the conditions under which integration is most effective, thus having relevance for individuals involved in applying speech/sign language recognition technology.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2013-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>156076</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Invited talk (Northumbria)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>65090333-2D37-44A0-93B6-4A4E1657EDAE</gtr:id><gtr:impact>Invited lecture, Northumbria University Department of Psychology seminar. Presented research findings on speech and gesture to audience of approx 25 academic staff and postgraduate students. Subsequent discussion included sharing experiences and suggestions of good practice in related studies being planned or carried out.</gtr:impact><gtr:outcomeId>583ec0c00ed0b0.29895095</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Psychology Taster lectures</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>344BA6A5-0968-4BEF-ABE9-14FCE79B694D</gtr:id><gtr:impact>Approx 300 prospective applicants to UCL's BSc Psychology programme (mostly aged 16-17) and approx 200 parents/guardians attended one of six Psychology Taster lectures delivered by David Vinson (topic: &amp;quot;language, communication and nonverbal expression&amp;quot;) as part of UCL's Open Days in June and September 2016. Subsequent Q&amp;amp;A and discussion revealed increased awareness of gestures and other modes of nonverbal expression as key to understanding language.</gtr:impact><gtr:outcomeId>58132d73291c56.11423814</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Undergraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UCL New Psychology modules</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6D1F133B-FAD8-4711-B877-DAC248956CFE</gtr:id><gtr:impact>Two new UCL Psychology modules were created by David Vinson and Jeremy Skipper: &amp;quot;Multimodal Language and Cognition&amp;quot; at the postgraduate level (25 students in two terms) and &amp;quot;Language in Context&amp;quot; designed for final year Psychology undergraduates (10 students in the inaugural term). Both were directly inspired by the research programme concerning co-speech gestures and the multidimensional nature of sign language articulation; employing active learning methods to deeply explore current research on the subject in a student-led discussion format. Student evaluations were overwhelmingly positive, with the undergraduate module being rated as &amp;quot;excellent&amp;quot; (maximum possible) by all students who completed the survey.</gtr:impact><gtr:outcomeId>581332f15cd032.72531590</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.institute-for-multimodal-communication.org/education/</gtr:url><gtr:year>2014,2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UCL 2nd Year Psychology Teaching</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>70B275FD-47C2-4C6E-8634-9AF1FDF55F55</gtr:id><gtr:impact>The two primary topic areas of the research (co-speech gestures and multiple dimensions of sign language production) and primary research findings were each incorporated as stand-alone lecture topics into UCL's Psychology undergraduate module &amp;quot;Language and Cognition&amp;quot; in 2014-15 and 2015-16 academic years (module convenor &amp;amp; lecturer: David Vinson). This module is required for second-year students on BSc Psychology and BSc Psychology and Language Sciences, reaching 320 undergraduate students in the two years offered so far. Student feedback on these two topics was exclusively positive and several students subsequently carried out their major research projects on these subjects.</gtr:impact><gtr:outcomeId>58132fe08484e3.09067736</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Undergraduate students</gtr:primaryAudience><gtr:year>2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Rethinking Macroeconomics Conference</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>13A0918D-A958-4BC5-A7D2-2ECB0FD2D31A</gtr:id><gtr:impact>Expert panelist in a conference intended &amp;quot;to start a discussion about what needs to be done to transform macroeconomics back into a useful policy science&amp;quot; (put on by National Institute of Economic and Social Research), offering perspectives from psychology. Audience (approx 70) represented academia, journalism, policy, business. Subsequent questions and discussions offered interdisciplinary suggestions of how theories of human motivation, behaviour and language use may relate to phenomena that economic models seek to explain or predict.</gtr:impact><gtr:outcomeId>583ec3b21b22c3.53113056</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.niesr.ac.uk/events/rethinking-macroeconomics-conference</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Live Science programme (Science Museum, London)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8616F66E-D6B9-4B31-92F7-4C794BAF1C29</gtr:id><gtr:impact>363 visitors to the Science Museum London (Age ranges 6-75) participated in bite-size versions of experiments on the integration of speech and gesture, as part of a larger project &amp;quot;What's in a word?&amp;quot; conducted in the museum's Who Am I? Gallery (June-August 2014). Each person was individually debriefed, opening up discussion about the role of gestures in language understanding.

Followup mailouts are still in preparation; we hope to engage visitors in further dialogue about their experience.</gtr:impact><gtr:outcomeId>544fbf12e67e26.92918502</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk (Kaiserslautern)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>C3C38AC3-3B70-4EBB-B10A-550CB78F335C</gtr:id><gtr:impact>Introduced gesture research to an academic audience unaware of the topic and its relevance. Possible collaboration is being explored and a pilot study planned; currently exploring possibilities for a visiting studentship in the topic area.

Possible visiting postgraduate studentship / exchange arrangement is in the works.</gtr:impact><gtr:outcomeId>5582bb3b4c2475.30069789</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>183455</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>Marie Sklodowska-Curie individual fellowship (as Mentor to the Fellow)</gtr:description><gtr:end>2018-10-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>H2020-MSCA-IF-2015 / 702655: &quot;INCREASE&quot;</gtr:fundingRef><gtr:id>F17F0013-3D97-41BA-AF1A-969CD6270311</gtr:id><gtr:outcomeId>5813291aa2fac6.18048657</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>While the primary impacts of the research have been academic, potential for impact on society more broadly has been achieved to some extent through highlighting the importance of visual/nonverbal elements of communication in supplementing what is expressed via language alone. One means by which this has been accomplished so far is by outreach activities to the general public: The first of these was during the project itself: dinteractive experiments and one-on-one debriefing sessions at the Science Museum London which directly reached more than 350 visitors. After data collection was complete, we presented similar issues and research findings to nearly 500 members of the public (prospective university applicants and their parents) who attended one of six Psychology taster lectures at UCL's summer Open Days for prospective applicants.

Additionally, the specific research programme has led directly to a dramatically increased presence of these subject areas as part of the core BSc Psychology curriculum at UCL: adding specific lectures based on these subjects to modules taken by all students studying Psychology at UCL (more than 300 students since the addition of these content areas), as well as creating specifically focused in-depth modules allowing students with particular interests around these subjects to study them far more deeply (35 students so far). Although this development occurs in an academic setting, the increased salience of these concepts in the programme of study can be said to affect society outside the academy, as the majority of our graduates go on to graduate level work outside university settings.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>AA82554A-CE2B-4016-A961-3DC67A18CF73</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d98853790e18.35032220</gtr:outcomeId><gtr:sector>Education,Other</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Understanding speech and gestures:
We created audio-visual materials of an actor speaking and gesturing, and developed methods to seamlessly combine speech and gestures from different video clips. This allowed us to advance on previous studies about comprehending speech and gesture together, in which a speaker's face was unnaturally obscured or removed from video clips.

In a series of computerised experiments we found that even when the face is visible and the movements correspond to the speech that is heard, comprehenders cannot help paying attention to mismatching gestures, even when they are irrelevant to the task.
When both speech and gestures are relevant to a decision, speech is a more important cue for native speakers. However, non-native speakers were more reliant on gesture in the same conditions, showing how language experience can modulate the balance of the different cues.

When we manipulated the relative timing of speech and gestures using similar video manipulation methods, we found largely the same effects, with only very slight benefits to whichever mode started first, likely due to preparatory information that is visible while a gesture is being formed, and mouth movements that are visible before speech is heard.

Finally, in a functional MRI study, we explored the brain networks involved in understanding speech and gestures together, and those that are active when the two cues provide conflicting meanings (also including video clips with audio-visual speech only, and with gesture only). The key finding of this study was a left-lateralised network incorporating areas often associated with language processing (middle/posterior temporal, parietal, and left inferior frontal gyrus) which was active when speech and gesture occurred together but further elevated when speech and gesture were in conflict, or only one of the two cues was present.

Understanding hands and mouth in sign language comprehension:
We used the same video manipulation methods and conducted a largely parallel set of studies in British Sign Language, presenting silent video clips in which the body and hands from one video were recombined with the head and face from another, to assess how mouth movements affect comprehension of signs in deaf and hearing participants.

In a first experiment with native BSL signers, incongruent mouth patterns interfered with participants' judgments, even when they were irrelevant to the task (participants were told &amp;quot;just pay attention to the hands&amp;quot;), showing that signers cannot help but pay attention to the mouth movements that accompany signs. This effect was comparable for deaf and hearing native signers.

A follow-up study showed a strong imbalance between the two cues when both were relevant to the task: information on the hands was much more important, and participants paid much more attention to it, than mouth movements. This is likely the product of visual and iconic differences: mouth patterns are small, fast-moving and bear little resemblance to the concepts being expressed, vs. the visible movements of the hands and arms, and sign forms which often are highly iconic (depicting aspects of shape, action or movement). Again, no difference was found between deaf and hearing participants with comparable BSL skill: despite very different language experience, and in most instances explicit instruction on speechreading English, mouth patterns in BSL appear to be processed the same regardless of hearing status.

Finally, a functional MRI study was conducted, with similar design to the English study described above but manipulating hands and mouth patterns in BSL. In contrast to the English study in which strong, clear effects were observed for conflicting speech and gesture, conflict between hands and mouth had little discernable effect on neural activity - likely because the hands are much more important a cue for comprehension. Increased activity in temporal-parietal regions for signs with mouth patterns, compared to the same signs with a still face, may be consistent with improved semantic retrieval when mouth movements are consistent with a sign, as is often the case in natural sign production.</gtr:description><gtr:exploitationPathways>The findings especially highlight the importance of visual information accompanying speech in order for it to be most effectively understood: not just facial movements but the gestures that speakers make when they reflect meaningful aspects of their messages. In many cases this suggests taking more advantage of audio-visual modes of communication especially in noisy environments or when comprehenders may not be fluent users of a language.

For signed languages the findings suggest that mouth patterns are used by comprehenders: not just naturally available in many communicative contexts but contributing to their understanding of a message. 

The findings together may be taken forward by working to understand how the various information available to comprehenders, &amp;quot;verbal&amp;quot; and &amp;quot;nonverbal&amp;quot; alike, contributes to their understanding in different contexts.</gtr:exploitationPathways><gtr:id>DC8BF1D5-3BAC-4994-8C79-F68C3172AD92</gtr:id><gtr:outcomeId>56aba32e72af93.74725408</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Other</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2149EB1B-8133-471A-A980-36569987CB54</gtr:id><gtr:title>Comprehending Sentences With the Body: Action Compatibility in British Sign Language?</gtr:title><gtr:parentPublicationTitle>Cognitive science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1d4d810c00c9786602250a8ffc832033"><gtr:id>1d4d810c00c9786602250a8ffc832033</gtr:id><gtr:otherNames>Vinson D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0364-0213</gtr:issn><gtr:outcomeId>581342e5bb1616.76381711</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E5335C1-693E-4C59-8984-B5CE0F268636</gtr:id><gtr:title>Integration of &amp;quot;secondary&amp;quot; information during comprehension: co-speech gestures and mouth patterns accompanying signs.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>561d13e4706928.34454268</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>07CAC9AC-FA28-4BF2-8A01-0442736A487F</gtr:id><gtr:title>Integration Of Secondary Information During Comprehension: Mouthings In British Sign Language</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0ce699f39d569a06a246999fb9660e2"><gtr:id>f0ce699f39d569a06a246999fb9660e2</gtr:id><gtr:otherNames>Perniss P</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56a8a480ac2d15.40936489</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BBF61CA-8483-428F-8079-4A9E31124C74</gtr:id><gtr:title>Language as a multimodal phenomenon: implications for language learning, processing and evolution.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/82c0206b9d679e1f86dd4f95bcae8838"><gtr:id>82c0206b9d679e1f86dd4f95bcae8838</gtr:id><gtr:otherNames>Vigliocco G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>58134251ed75d4.66343138</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95F06BEA-6A61-42EE-B521-D352EDB1925B</gtr:id><gtr:title>Integration of &amp;quot;secondary&amp;quot; information during comprehension: Co - speech gestures in English and mouth patterns accompanying British Sign Language signs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>561d13802ec615.95505614</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A66369D-CD07-4336-A816-BBDFB18E98B4</gtr:id><gtr:title>The role of left superior parietal lobule in sign language production: A TMS study with British Sign Language</gtr:title><gtr:parentPublicationTitle>The Fifth Annual Meeting of the Society for the Neurobiology of Language (poster presentation)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>544fbbe8bd5df6.55096928</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6548F069-40EE-45A8-9587-6F8BF43A7C8F</gtr:id><gtr:title>The role of left superior parietal lobule in sign language production: A TMS study with British Sign Language</gtr:title><gtr:parentPublicationTitle>International Workshop on Language Production 2014 (poster presentation)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>544fbb74be1f23.22232961</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98036A1F-1038-4F0C-831E-B7D6D97D6148</gtr:id><gtr:title>Hand-mouth integration in comprehension: The role of mouthings in British Sign Language (BSL)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0ce699f39d569a06a246999fb9660e2"><gtr:id>f0ce699f39d569a06a246999fb9660e2</gtr:id><gtr:otherNames>Perniss P</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>561d143506b007.29876325</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCDD9CCD-6591-42EA-A448-5CFE8A5EE2BB</gtr:id><gtr:title>Integration of &amp;quot;secondary&amp;quot; information during comprehension: co-speech gestures and mouth patterns accompanying signs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>54c0cdb3b21a06.26089220</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3398B019-AA17-48E0-B4EA-FEEBE7F299E6</gtr:id><gtr:title>Integration of &amp;quot;secondary&amp;quot; information during comprehension: Co-speech gestures and mouth patterns accompanying signs</gtr:title><gtr:parentPublicationTitle>Architectures and Mechanisms for Language Processing (AMLaP) Conference XX 2014 (Poster presentation)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>544fbc7909eba8.89500429</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3BB02643-1A73-4B0A-96A1-C349C34B6C36</gtr:id><gtr:title>Semantic integration of speech and iconic gestures: bringing the face into the picture</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56a8a4376188e5.80974848</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7AA54C69-09EC-4E18-8A80-59A7255EC373</gtr:id><gtr:title>A faster path between meaning and form? Iconicity facilitates sign recognition and production in British Sign Language</gtr:title><gtr:parentPublicationTitle>Journal of Memory and Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1d4d810c00c9786602250a8ffc832033"><gtr:id>1d4d810c00c9786602250a8ffc832033</gtr:id><gtr:otherNames>Vinson D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>551a65ec704985.17988248</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>57B288B9-5D82-44E8-8DE8-D765468AB032</gtr:id><gtr:title>Integration of &amp;quot;secondary&amp;quot; information during comprehension: mouthings in British Sign Language</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7794e9d6a3b9eb334652c8dffb6eec44"><gtr:id>7794e9d6a3b9eb334652c8dffb6eec44</gtr:id><gtr:otherNames>Vinson DP</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>5582ba3ab09d43.70794651</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/K001337/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>7B8CEF0E-4DA8-4D0E-812A-5B92663E5EB9</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psycholinguistics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>