<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Neuroscience Physiology and Pharmacology</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/4E1C3E62-C548-4AA5-A35F-58E429B4959A"><gtr:id>4E1C3E62-C548-4AA5-A35F-58E429B4959A</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Pope</gtr:otherNames><gtr:surname>Bassett</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/107BCFB3-3CEF-4C2F-A5F8-558AC7AFC041"><gtr:id>107BCFB3-3CEF-4C2F-A5F8-558AC7AFC041</gtr:id><gtr:firstName>Francesca</gtr:firstName><gtr:surname>Cacucci</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FR009872%2F1"><gtr:id>B180BE51-421E-4FA4-9676-E69ABDB907AD</gtr:id><gtr:title>Sensation to Cognition: Vestibular Integration and Directional Navigation</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/R009872/1</gtr:grantReference><gtr:abstractText>The vestibular system not only provides us with our sense of balance, but it helps us to understand the space around us. Starting from a set of tiny accelerometers inside the skull, the vestibular system detects our head movements and compares them to perceived movements in the world around us. By keeping track of our intended movements and the way the world seems to respond to them, the brain can assemble a mental picture of space that we use to organise our behaviour and remember how to get to the places we go.
Our research seeks to understand the steps that happen between our initial detection of movement in the vestibular system, and the spatial abilities it ultimately allows. To do so, we focus on the most basic spatial construct, our sense of direction. If we rotate in place with our eyes closed, we know intuitively when we've returned to the starting point, and that same feeling allows us to walk around a city and keep track of the direction we started from. We have this sense of direction because the brain keeps a running tally of all the angular movements that the vestibular system detects, adding and subtracting from it as we change direction in a process called path integration. 
Keeping our sense of direction up to date seems to be the job of specialised cells in the brain called Head Direction (HD) cells. A given HD cell fires only when the head is pointed in a particular direction. When the head moves and points in a different direction, the neural signal moves to a different HD cell that corresponds with the new direction, which will remain active, even in the dark, until the next head turn. We want to understand how HD cells take movement signals from the vestibular system and turn them into directional signals, which they then pass on to parts of the brain that represent other features of space. 
To update the head's current direction, the size of its last rotation, its angular displacement, must be known. The vestibular system detects head acceleration, and passes along a signal that codes velocity. Do HD cells then do a velocity-to-direction conversion, or do they just respond to an already-converted displacement signal? We plan to answer this question by recording the activity of the neurons which lie between the vestibular and the HD systems, to see where their signal falls along the transition between acceleration and direction.
We also want to understand how HD cells guide our navigational behaviour. Even though HD cells seem to act like a built-in compass, when we observe them as an animal is trying to solve a directional spatial problem, the behaviour and the cell activity don't always match. We want to solve this puzzle by recording HD cells while the animal solves the simplest possible spatial problem involving only their most recent rotation, and nothing else. In humans, this is done by rotating subjects in the dark, and then asking them to use a joystick to steer their rotating chair back to the starting point. We can ask a mouse to do the same thing by giving it a sweetened milk reward only at a specific, learned direction, so that when the mouse licks the reward spout, it is telling us which direction it thinks it is facing. At the same time, we can give the mice visual signals that don't match the vestibular rotation signals. When visual and vestibular signals disagree, the brain recalibrates the vestibular system to make it line up with vision again. This re-tuning means that the HD cells would receive a different input signal than usual, and the mice's judgements of rotation will reflect the changes in the HD cell firing that result.
Finally, we will a use a novel technique that will allow us to visualise HD cells in the living brain. We will image the activity of HD cells while the animal walks in a virtual reality environment, and then add visual-vestibular mismatch. This will tell us if the HD neural circuit rearranges itself to accommodate the re-tuning that follows.</gtr:abstractText><gtr:technicalSummary>The proposed research will investigate the multi-stage signal transformations that convert a sensory signal from the vestibular periphery to a cognitive representation of rotational space. Head Direction (HD) cells appear in numerous, connected populations in the limbic thalamus and cortex, and fire as a function of an animal's directional bearing in the environment. Any single HD cell in a population is associated with a specific direction, and preferred directions are distributed among cells so as to collectively represent the complete surround. HD cells continue to fire in their preferred directions in the dark, and yet rotations of prominent visual landmarks following disorienting interventions prompt commensurate rotations of directional firing. Therefore, the directional signal represents an ongoing estimate of angular bearing, updated by self-movement, but flexibly anchored to environmental cues. 

Vestibular lesions are devastating to HD cell activity, so the input signal to the HD cell network is widely thought to be angular head velocity. Multiple synapses lie between the vestibular nuclei and HD cells, however, allowing for significant signal transformation prior to entering the HD network. We will characterise the input signal to HD cells using vestibular systems control methods, and test the hypothesis that HD cells and oculomotor neurons share a common input signal related to angular displacement. Vestibular signalling is highly plastic under visual-vestibular mismatch conditions, which we hypothesise must therefore also affect HD cell updating. We will use vestibular plasticity paradigms to alter the HD representation and rotational path-completion tasks to assess their behavioural impact. Finally, we will image HD cells in the anterior thalamus of mice in a VR environment under control and visual-vestibular mismatch conditions, to identify anatomical network topography and the limits of its re-configurability under plasticity-inducing conditions.</gtr:technicalSummary><gtr:potentialImpactText>We propose to study how vestibular information is transformed and integrated into the cognitive representation of heading direction. The main beneficiaries of the research outcomes resulting from this project will be: academic, industry and the wider public.

Academic researchers directly benefiting from the work outlined in the proposal are: systems and behavioural neuroscientists interested in how complex cognitive functions arise from transformation of sensory inputs. In particular, our research promises to solve the long-standing problem of how sensory information (in our case, vestibular) is integrated to obtain the read out of a complex spatial construct (sense of direction). 

Training/skills development: The proposal is based on novel approaches combining vestibular physiology paradigms with in vivo electrophysiological and optical imaging recordings of neurons. As such the PDRA/co-applicant (JB) will acquire new skills and forge a new avenue of research. JB intends to capitalise on this by using the results of experimental work of this proposal as a springboard for fellowship application in year 2 of this grant lifetime.

Industry beneficiaries will be those designing autonomous mobile agents, which need to learn to navigate and keep track of their bearing in complex environments (robots, self-driving vehicles). Moreover, our research will directly benefit industry partners which design virtual reality set ups for both human and animal navigation, and companies which specialise in developing new research tools.

Public beneficiaries will be mainly school children and other individuals which we will reach in order to spark their interest in the origin of our sense of direction, and how this is intimately connected to our vestibular end organs.</gtr:potentialImpactText><gtr:fund><gtr:end>2020-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2018-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>471157</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">BB/R009872/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>790AD28C-6380-4025-83C2-6881B93C4602</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal behaviour</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E457FFDE-A4C1-4907-AE12-A394D95A3AE5</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Cognitive Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F439A20B-A9B0-4A68-B703-7F6AE7570E39</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems neuroscience</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>