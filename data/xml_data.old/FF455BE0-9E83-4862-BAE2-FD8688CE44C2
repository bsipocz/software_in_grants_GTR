<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:department>Sch of Biological Sciences</gtr:department><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/988D371B-F21D-42BF-97B2-5CDA4D7C4749"><gtr:id>988D371B-F21D-42BF-97B2-5CDA4D7C4749</gtr:id><gtr:firstName>Slawomir</gtr:firstName><gtr:otherNames>Jaroslaw</gtr:otherNames><gtr:surname>Nasuto</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ003077%2F1"><gtr:id>FF455BE0-9E83-4862-BAE2-FD8688CE44C2</gtr:id><gtr:title>Brain-Computer Interface for Monitoring and Inducing Affective States</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J003077/1</gtr:grantReference><gtr:abstractText>Brain Computer Interfaces (BCI) allow for modification of subject's environment or control of external devices by the power of thought alone. They achieve this by analysis of small electrical potentials generated by the subject's brain while its owner is thinking. Emotions involve specific mental states hence also involve particular patterns of brains electrical activity. The proposed research will build innovative intelligent BCI systems that can monitor our emotions, and modify them automatically and adaptively via controlled computer music generation system. Creation of such systems would advance our understanding of fundamental relationships between the subjective emotions, corresponding brain states and characteristics of music that can induce very vivid and powerful emotions in humans. Such systems can be used for treatment of emotional/mood disorders such as depression so are of direct benefit to society and NHS. In addition, they are of interest for healthy subjects as means of relaxation or perhaps by enhancement of gaming experience. Thus the proposed project could also lead to interesting developments in the entertainment industries such as the gaming industry.</gtr:abstractText><gtr:potentialImpactText>This research will contribute towards the enhancement of the quality of life, health in the society and the economic competitiveness of the entertainment industry in the UK, including the gaming industries and music. Its interdisciplinary nature will also benefit several academic stakeholders as outlined in the Academic Beneficiaries section.
BCI technology to monitor and induce affective states will impact on the health sector by providing a valuable tool in preventive and therapeutic programmes for health problems such as stress, depression, anxiety and other conditions related to affective states. Stress is an endemic problem in our society and economy. According to a study by the mental health charity Mind (http://www.mind.org.uk/), every year UK businesses lose &amp;pound;26 billion and 70 million working days because of stress. Excessive stress leads to anxiety and more severe cases of depression. Stress may cause suppression of the immune system, insomnia, fatigue, and affect adversely productivity. Persistent cases may lead to social withdrawal and family dysfunction. The project impact will be enhanced by liaising with the health sector in order to raise their awareness on this projects goals and prepare the ground for taking our systems out of the lab into the real world of special needs as soon as we have a working prototype.
Activities that will enhance impact on creative industry include organisation of the Workshop during Peninsula Arts Contemporary Music Festival in Plymouth which will enable us to spread awareness about the technology developed in the music industry.
The project will positively impact public understanding of science via press exposure campaign, use of internet media (dedicated WWW page, youtube etc) for dissemination of the information about the project as well as by using the exploitation channels available at both universities.
Lastly, in additional to the usual channels of academic dissemination the impact on academia will be enhanced bythe organisation of the International Workshop at Reading as well as by providing a an excellent training for highly qualified interdisciplinary research staff.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-08-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>509001</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>interview for spotify</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FD90C5A1-0FD6-4CBA-A325-A7FC24F41C18</gtr:id><gtr:impact>A company preparing promotional material for Spotify for CES2017, a global consumer electronics and consumer technology tradeshow, performed interview with me on 'power of audio'.</gtr:impact><gtr:outcomeId>58bf17babb76b5.29872877</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Guardian feature</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9A702966-F452-40C3-81D8-1F3C1AB8E756</gtr:id><gtr:impact>The Guardian made a feature on the music and and tis effect on emtion and the brain. The feature was centred around a visit by a pop musician, Tiny Tempah, to the University of Reading where we discussed the project and performed a scan of his brain while he was listening to various musical pieces.</gtr:impact><gtr:outcomeId>58bf1674cf0851.12060872</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.theguardian.com/lifeandstyle/2016/sep/12/mind-blowing-music-tinie-tempahs-brain-scan</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Neural Engineering Transformative Technologies (NETT) summer school</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>8079243E-8130-4C2A-AF36-26F9CFF546A4</gtr:id><gtr:impact>Talk raised awareness about the research at the crossroads of music and neural engineering.

Increased awareness on state of the art research on music and neural engineering.</gtr:impact><gtr:outcomeId>54610dd596dfb1.97693579</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC News visited the ICCMR and interviewed Professor Miranda about the application of Music Neurotechnology in his piece Activating Memory</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>4D7166A5-26D1-4079-B4F1-2DA3087B9210</gtr:id><gtr:impact>the interview was aired on national radio.

the information about our research reached wider audience.</gtr:impact><gtr:outcomeId>54610ec7a464c6.83739909</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2011,2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Engineer previewed the project, including interviews with principal investigators Eduardo Miranda and Slawomir Nasuto.</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>4D8F403D-44E4-4459-B254-2A71ED6F16C9</gtr:id><gtr:impact>Further commentary from record producer Steve Levine (the Beach Boys, Culture Club) on some of the real-world applications for the results were discussed.

Raised awareness on the research in general public.</gtr:impact><gtr:outcomeId>5461105495cbb0.59860237</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2011</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Dr E. Roesch and Prof SJ Nasuto interviewed for a Feature programme in French TV channel Encyclo devoted to BCI technology</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3262EC7C-FD6D-4E84-B588-71DEE890B355</gtr:id><gtr:impact>Raised the awarness in assistive technologies and Brain COmputer Interfaces

The programme has not been aired yet.</gtr:impact><gtr:outcomeId>54611110edec00.56091891</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Professor Miranda Symphony piece of Minds Listening which was premiered at the Peninsual Arts Contemporary Music Festival in Plymouth</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>70A69585-C31F-4EE1-8569-E3CE1783B3E3</gtr:id><gtr:impact>audience could experience music created with the use of the technology developed partly through the project.

Stimulated public understanding and appreciation of the research.</gtr:impact><gtr:outcomeId>54610fbaae70a4.27830202</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The project generated quite a lot of interest in the general public via outreach activities, most recently the Guardian feature involving a pop musician Tiny Tempah. We are also entering the stage where we actively explore the pottential deployment of systems build around affective generation of music in therapeutic/ambulatory context and we have been engaging with clinical researchers to pursue these avenues.</gtr:description><gtr:id>32B6C3F4-FED5-4BC0-B815-0D48C63FD930</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56ddce9bc6b5f0.94279975</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Music induced emotions can be predicted using a combination of brain activity and acoustic features.

We have also proposed a novel method for automated identification of neural correlates of continuous variables.

We developed and evaluated an affective brain-computer music interface for modulating the affective states of its users. The system was evaluated in a longitudinal study 
and was able to detect its users' affective states with up to 65% accuracy and to modulate their affective states significantly above chance level. the system represents one of the first demonstrations of an online affective brain-computer music interfaces.</gtr:description><gtr:exploitationPathways>These results allow for more systematic incorporation of music as a viable feedback channel in Brain Computer Interfaces.

Also, the method for identification of neural correlates of continuous variables might be of interests to BCI practitioners designing new BCI systems but also for researchers trying to characterise the neural processes associated with some cognitive functions.</gtr:exploitationPathways><gtr:id>9E73ACD7-FEB5-4910-9BBF-625BE8A376B9</gtr:id><gtr:outcomeId>56ddcdc84df3a7.66285499</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare,Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>4C66CB09-0670-4035-BBF0-6756FF69D2F7</gtr:id><gtr:title>Affective brain-computer music interfacing.</gtr:title><gtr:parentPublicationTitle>Journal of neural engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1741-2552</gtr:issn><gtr:outcomeId>585d506ad4e041.54289912</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5DC9A57B-A18D-44DF-8E36-1EB31868F60A</gtr:id><gtr:title>Dynamic game soundtrack generation in response to a continuously varying emotional trajectory</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3cf5cd802f563f9377bacca8a23898a"><gtr:id>d3cf5cd802f563f9377bacca8a23898a</gtr:id><gtr:otherNames>Williams D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddc86750b959.31664048</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B547C7B0-0003-4E68-983E-9E357CC54DB6</gtr:id><gtr:title>An investigation into the use of six facially encoded emotions in brain-computer interfacing</gtr:title><gtr:parentPublicationTitle>Brain-Computer Interfaces</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d341676da89.79041716</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>091349D1-9FBF-456A-A8B5-BE08ABFC61FB</gtr:id><gtr:title>Brain-computer music interfacing for continuous control of musical tempo</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/daae3998c9c7407251714d5447ea76b7"><gtr:id>daae3998c9c7407251714d5447ea76b7</gtr:id><gtr:otherNames>Daly, I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546109cb4b64a5.96066846</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DCCC55D2-E81D-4362-8341-B210111CFD57</gtr:id><gtr:title>A Perceptual and Affective Evaluation of an Affectively -Driven Engine for Video Game Soundtracking</gtr:title><gtr:parentPublicationTitle>ACM Computers in Entertainment</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/747822dd7a8484820a1b7c0be773dbc9"><gtr:id>747822dd7a8484820a1b7c0be773dbc9</gtr:id><gtr:otherNames>WILLIAMS D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c13892c926d3.98159553</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CC7294AB-293F-46EA-A526-A7090C3877AC</gtr:id><gtr:title>Music-induced emotions can be predicted from a combination of brain activity and acoustic features.</gtr:title><gtr:parentPublicationTitle>Brain and cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0278-2626</gtr:issn><gtr:outcomeId>5675ec9f41a65</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A5B4690A-E450-4C1E-8962-EF6ECD176185</gtr:id><gtr:title>Neural correlates of emotional responses to music: an EEG study.</gtr:title><gtr:parentPublicationTitle>Neuroscience letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0304-3940</gtr:issn><gtr:outcomeId>54610725c6bf34.73565891</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2377D8C1-862E-444F-B43C-6204F6F50DBC</gtr:id><gtr:title>Automated artifact removal from the electroencephalogram: a comparative study.</gtr:title><gtr:parentPublicationTitle>Clinical EEG and neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1550-0594</gtr:issn><gtr:outcomeId>543272f3c2fad0.99805601</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FF798DE7-262C-4B9D-A308-04A72507FB92</gtr:id><gtr:title>An affective brain-computer music interface</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56ddc95a17ffa2.71408713</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FACA459C-663B-45A5-A841-5DB3A7664AE2</gtr:id><gtr:title>Artificial affective listening towards a machine learning tool for sound-based emotion therapy and control</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a87b72e6271ee5a7fce20ab4f5289b78"><gtr:id>a87b72e6271ee5a7fce20ab4f5289b78</gtr:id><gtr:otherNames>Kirke, A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54610ab889ae90.99402092</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C2BF27E6-3591-4273-8FE6-26AC3EC364E8</gtr:id><gtr:title>Testing for significance of phase synchronisation dynamics in the EEG.</gtr:title><gtr:parentPublicationTitle>Journal of computational neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0929-5313</gtr:issn><gtr:outcomeId>5432749fbd2628.47456543</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A36A7CA-7C8F-469B-A2E5-7235DCA6C1E0</gtr:id><gtr:title>Investigating affect in algorithmic composition systems</gtr:title><gtr:parentPublicationTitle>Psychology of Music</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3cf5cd802f563f9377bacca8a23898a"><gtr:id>d3cf5cd802f563f9377bacca8a23898a</gtr:id><gtr:otherNames>Williams D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54610557a15c70.00643757</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B172A4A-6F06-48D6-9D08-140EFAA5F66A</gtr:id><gtr:title>Towards human-computer music interaction: Evaluation of an affectively-driven music generator via galvanic skin response measures</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddc673a09577.74142885</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>499A001F-91B3-461C-9533-729508454129</gtr:id><gtr:title>Autocorrelation based EEG dynamics depicting motor intention</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08cf0f8d9e986ec62aa0d40cc84ff96b"><gtr:id>08cf0f8d9e986ec62aa0d40cc84ff96b</gtr:id><gtr:otherNames>Wairagkar M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56ddc9c52ac414.24503605</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>93184CCA-F201-4946-80AE-F94444A760C5</gtr:id><gtr:title>Emotion and anticipation in an enactive framework for cognition (response to andy clark).</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f0271030c4f3484539a1d7d46dde627"><gtr:id>5f0271030c4f3484539a1d7d46dde627</gtr:id><gtr:otherNames>Roesch EB</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>543272f4383154.14977381</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6189A296-724B-4EA6-9DFA-EDF533F0AC57</gtr:id><gtr:title>Directed Motor-Auditory EEG Connectivity Is Modulated by Music Tempo.</gtr:title><gtr:parentPublicationTitle>Frontiers in human neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b9e9728aae08720735ff521f4532ca86"><gtr:id>b9e9728aae08720735ff521f4532ca86</gtr:id><gtr:otherNames>Nicolaou N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1662-5161</gtr:issn><gtr:outcomeId>5aa6d678620138.34658089</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7C3CB1B5-1326-40F6-877A-5A7C8383E9FD</gtr:id><gtr:title>Changes in music tempo entrain movement related brain activity</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/daae3998c9c7407251714d5447ea76b7"><gtr:id>daae3998c9c7407251714d5447ea76b7</gtr:id><gtr:otherNames>Daly, I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54610912682b67.79638752</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A8940897-D489-4526-AFC2-7E6ADD2D1385</gtr:id><gtr:title>Investigating Perceived Emotional Correlates of Rhythmic Density in Algorithmic Music Composition</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Applied Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3cf5cd802f563f9377bacca8a23898a"><gtr:id>d3cf5cd802f563f9377bacca8a23898a</gtr:id><gtr:otherNames>Williams D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675fc5ab3456</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EBEFB9F4-8B62-4F67-88DD-4FB2CFC37103</gtr:id><gtr:title>Automated identification of neural correlates of continuous variables.</gtr:title><gtr:parentPublicationTitle>Journal of neuroscience methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0165-0270</gtr:issn><gtr:outcomeId>doi_55f9749746621b1c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>099A5EB0-3FF3-43C1-9682-DA054EEF8749</gtr:id><gtr:title>Single tap identification for fast BCI control.</gtr:title><gtr:parentPublicationTitle>Cognitive neurodynamics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1871-4080</gtr:issn><gtr:outcomeId>543272f3273179.98089532</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>02BBF32B-A66C-43DB-83B7-07DEA6A78E9C</gtr:id><gtr:title>Identifying music-induced emotions from EEG for use in brain-computer music interfacing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddc673c570a7.53453985</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9503DF5D-ECEA-44F4-8888-DE747934E894</gtr:id><gtr:title>Evaluating Perceptual Separation in a Pilot System for Affective Composition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/93cd2ca6651198abea632bf1fabf3474"><gtr:id>93cd2ca6651198abea632bf1fabf3474</gtr:id><gtr:otherNames>Williams, D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54610872873c00.09963470</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3CBF5F30-79CA-41E8-92BD-F9E3BEEE76E2</gtr:id><gtr:title>Personalised, Multi-modal, Affective State Detection for Hybrid Brain-Computer Music Interfacing</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76c6242cb5a578b95426f037da1f4a08"><gtr:id>76c6242cb5a578b95426f037da1f4a08</gtr:id><gtr:otherNames>Daly I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a9959bb3d4284.31269039</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C489E6FD-96D5-433A-A163-77B89DDDDB19</gtr:id><gtr:title>Towards Affective Algorithmic Composition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/93cd2ca6651198abea632bf1fabf3474"><gtr:id>93cd2ca6651198abea632bf1fabf3474</gtr:id><gtr:otherNames>Williams, D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54610a5408b335.53041154</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E0762AE5-2A7B-43EA-A993-561C72D03141</gtr:id><gtr:title>Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Applied Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3cf5cd802f563f9377bacca8a23898a"><gtr:id>d3cf5cd802f563f9377bacca8a23898a</gtr:id><gtr:otherNames>Williams D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa6d67830c643.55300473</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J003077/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>