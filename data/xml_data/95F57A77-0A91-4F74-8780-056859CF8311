<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C3DA1BEA-518B-4CF9-81B3-BD8D12D82A93"><gtr:id>C3DA1BEA-518B-4CF9-81B3-BD8D12D82A93</gtr:id><gtr:name>Apple</gtr:name><gtr:address><gtr:line1>1 Infinite Loop</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/43F70FE4-97E9-460D-BE06-F2FCD4B47699"><gtr:id>43F70FE4-97E9-460D-BE06-F2FCD4B47699</gtr:id><gtr:name>Datacolor</gtr:name><gtr:address><gtr:line1>5 Princess Road</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9E6E35FF-910B-43FD-9549-2333EABB1F32"><gtr:id>9E6E35FF-910B-43FD-9549-2333EABB1F32</gtr:id><gtr:name>Buhler Sortex Ltd</gtr:name><gtr:address><gtr:line1>Atlantis Avenue</gtr:line1><gtr:postCode>E16 2BF</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CF7F58C1-4B26-4CBA-9FE8-BA1DD8457CB7"><gtr:id>CF7F58C1-4B26-4CBA-9FE8-BA1DD8457CB7</gtr:id><gtr:name>Unilever UK Central Resources Ltd</gtr:name><gtr:address><gtr:line1>Quarry Road East</gtr:line1><gtr:line2>Bebington</gtr:line2><gtr:postCode>CH63 3JW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2A5C593E-43C3-4E03-9530-990C61A80560"><gtr:id>2A5C593E-43C3-4E03-9530-990C61A80560</gtr:id><gtr:firstName>Graham</gtr:firstName><gtr:surname>Finlayson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ005223%2F1"><gtr:id>95F57A77-0A91-4F74-8780-056859CF8311</gtr:id><gtr:title>Rank based spectral estimation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J005223/1</gtr:grantReference><gtr:abstractText>The colours, or RGB pixels, recorded by a digital camera are the result of the interaction of the prevailing light in the scene striking and being reflected by objects and the characteristics of the camera itself. The complexity is such that different cameras see differently and no cameras see the world exactly as we do. You will have noticed this when looking at photos where sometimes the colours don't look right or the pictures captured by one camera look 'better' than another. Moreover, sometimes we see colours change dramatically. We have all probably observed that white clothes can look bluish under ultra violet light (say in a night club). But, in fact the colours we see change subtly, all the time, as we move from one light to another (which is why it is always a good idea to check the colour of your clothes outside the shop). Here, even small changes can lead to poor customer satisfaction or, potentially, in a medical imaging application the wrong diagnosis.

Good pictures, by which we might mean accurate 'colour measurement' are possible if we know the spectral colour characteristics of a camera and/or the spectrum of light in a scene. While we can, in principle, measure these quantities the measurement is not easy to do so and is expensive (not easy as it requires considerable (Physics) lab time and expensive because spectral measurement devices cost many thousands of pounds). When measurement is not feasible, there do in fact exist methods for estimating (say) the spectrum of light in a scene. Yet, these methods only tend work if the camera is accurately calibrated first (a sort of chicken and the egg situation). Our 'Rank Based Spectral Estimation' Project aims to make it much easier to calibrate a camera or measure the illuminant in situ (and as such also make it easier to measure reflectance too)

So, how does our method work. Well suppose we gave you 50 grey tiles all of which appeared to have a different brightness. It would be an easy task for you to rank them from darkest to brightest. But, now suppose we change the colour of the light. Depending on the spectral shape of the grey reflectances, the ranking order can change (sometimes considerably). No problem, it is a simple matter to reorder the tiles. Remarkably, for specially chosen reflectances, the rank order will strongly correlate with the spectral shape of the light. Thus a simple ranking experiment gives us a strong clue to the colour of the light. (And, if we knew the colour of the light we could, for example predict whether the colour of our clothes might change when we go outdoors.)

The Rank Based Spectral Estimation project aims to take this simple ranking idea and provide simple, and accurate, estimation tools for deriving the spectral shape of the prevailing light, the spectral characteristics of a camera and the spectral reflectances of surfaces. At the heart of our method is a specially designed reflectance target containing many reflectances (whose design is part of the proposed research). Ranking these reflectances will allow us to accurately estimate the light spectrum and the spectral attributes of a camera. Accurate spectral estimates are required in many applications from photography, through, visual inspection to forensic imaging and telepresence (e.g. remote diagnosis).

Remarkably, we believe the methods we develop will also prove useful in understanding how we see. Indeed, it is very likely that you see the world a little differently than I do. Yet estimating an individual's spectral response is notoriously difficult. To the extent it can be done at all, it requires many hours of (tedious) detailed visual experiments. Through ranking it will be possible to uncover an observers spectral response (technically called 'colour matching curves') quickly and simply. We simply ask the observer to carry out a simple ranking of the kind mentioned above.</gtr:abstractText><gtr:potentialImpactText>Rank-based Spectral Estimation is a new paradigm for estimating the spectrum of the light, the spectral characteristics of the camera and for making spectral reflectance estimation easier. RBSE will be developed directly with our project partners who have each identified an estimation problem they are interested in solving where the traditional (expensive and time consuming) measurement approaches are difficult to use. With Apple we will develop and evaluate RBSE for the spectral estimation of camera spectral sensitivities paying special attention to the problem of the 'batch to batch' variation' in the spectral response of cameras. Unilever has a long-standing interest in modelling the appearance of physical objects which, hitherto, has relied on lab-based measurements. Through RBSE we aim to help Unilever make in situ appearance measurements. The interest of Datacolor is color measurement. They are the leading manufacturer of colour measurement devices. Through our collaboration we will be able to assess how well RBSE can approximate physical measurement. Finally, with Buhler-Sortex we will investigate the color tolerances needed for stable accept/reject decisions in industrial inspection. RBSE will be used to help calibrate Sortex machines. 

Of course through our extensive dissemination plans, we will also publish our developed algorithms and data. So, any industry that has a need for spectral estimation will be able to prototype our technology (including, in medical and forensic imaging). More generally, we will extend http://www.cvrl.org where Prof Stockman provides access to human colour vision datasets as well as making a dedicated project website. These resources will include tutorial articles to ensure that the general public can engage with our research.

Both PIs on this grant have an extensive track record of communicating their research to standards bodies. This includes CIE technical committees TC 8-02 and TC8-07 in Image Technology and TC 1-72 on measurement of appearance. Prof Stockman has also worked closely on the definition of a new standard on cone response functions (TC 1-36: Fundamental Chromaticity Diagram with Physiologically Significant Axes). Also, Teresa Goodman chairs TC 2-65: Methods of Characterizing Spectrophotometers. Prof Finlayson has also worked with ISO TC 42 (Photography) for which the Society of Imaging Science and Technology (for which Finlayson serves as a Vice President) has an oversight and administration role.

The project also provides unrivaled training opportunities for the PDRA (Dr Vazquez) and the UEA funded research student funded alongside this grant. It is our plan that both will be trained in optical radiation measurement and, under the supervision of Teresa Goodman at NPL. And both will will be trained in psychophysical experimentation under the primary supervision of Prof. Stockman at UCL. Dr Vazquez will also spend time working directly in the labs of Buhler Sortex and Unilever (see their letter of support). The whole project team will benefit from the large and varied integration with industry. The whole team will have the opportunity to see their research transferred and then exploited in an industrial context.

In 2013 the Quadrennial meeting of the International Colour Society meets in Newcastle. This is a forum for scientists, industrialists, practionners and the lay public to meet and discuss colour. We will make a coordinated series of presentations as well as a purpose built exhibit at this meeting. In 2008, NPL (leading from research with partners including Unliever) had a very successful exhibit at the Royal Society Summer Exhibition on the 'Physics of Perception'. We will submit a proposal to exhibit for summer 2014. More generally the team has a history of tutorial presentations to organisations such as 'The Colour Group of Great Britain' and 'The Institute of Electrical Engineers' and meetings of the RPS Imaging Science Group.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>465217</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We have two patents

&amp;quot;Rank-based Sp[ectral Estimation&amp;quot; and
&amp;quot;Rank-based Radiometric Calibration&amp;quot;

UEA is in discussion with Spectral Edge Ltd (a UEA spinout from Finlayson's lab) to license/assign the latter (or both) patents</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>68B8D368-00EC-4F35-B5AB-930DE0954072</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56e08515d8d1c5.79829397</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Patent filed at PCT - publication number: GB201420876

How cameras process information is proprietary and this know how does not appear in the academic journals. Yet, how we transform raw images to pleasant photographs represents valuable knowledge and would help pump prime any company working in the imaging area. This patent provides techniques for understanding and uncovering camera processing as a simple parametric model (with few parameters)</gtr:description><gtr:grantRef>EP/J005223/1</gtr:grantRef><gtr:id>7874873C-21A3-496D-991E-31F002931922</gtr:id><gtr:impact>The grant is still running. We have proposed possible routes to commercialization (including a spin out company). But, this is for the future.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e00186a8ff88.14933038</gtr:outcomeId><gtr:patentId>GB201420876</gtr:patentId><gtr:protection>Patent application published</gtr:protection><gtr:title>Method and system for determining parameters of an image processing pipeline of a digital camera</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Patent granted in the US (US9001214), patent applications under prosecution in other territories.

The most common imaging devices we all have are mobile phones. So, plausibly we could use our phones to make measurements of visual appearance. Yet, understanding the meaning of the measurements a phone makes means we need a model of how the camera samples light. This in turn is predicated on the devices spectral sensitivities. This patent teaches how to estimate device spectral sensitivities independent of the processing pipeline implemented on the device/</gtr:description><gtr:grantRef>EP/J005223/1</gtr:grantRef><gtr:id>0F8FD0DE-76DE-48F2-9058-05943FEF363B</gtr:id><gtr:impact>The grant is still running. This IP could be commercialized in a number of ways including in a spin-out company.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e000e0adeda3.21524312</gtr:outcomeId><gtr:patentId>US9001214</gtr:patentId><gtr:protection>Patent granted</gtr:protection><gtr:title>Spectral Estimation Method, System and Reference Target Design Method</gtr:title><gtr:yearProtectionGranted>2011</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>Image formation in machine and humans is at the first stage linear. Yet, post the sensor level most processing is non-linear. In both man and machine often we have access to the non-linear outputs but would like to relate these to the linear physical quantities being measured.

In machine vision, and published, we have demonstrated ranking as a powerful method to infer the linear spectral sensitivities of a camera and the processing steps of colour correction and tone mapping. In human vision we have demonstrated - soon to be published - how ranking judgements can be used to provide plausible computational mechanisms for understanding early colour coding (how do we scale, redness, blueness, yellowness etc)

The research is on-going and we expect to provide tools for colour measurement (without calibration). To the extent we succeed here there may be opportunities to commercialize this work.</gtr:description><gtr:exploitationPathways>At the university's request we discussed the commercial opportunities coming out of this research. We have, as a consequence, filed 2 patents.</gtr:exploitationPathways><gtr:id>12F2292C-0FCF-46DD-8980-20D358E741A7</gtr:id><gtr:outcomeId>56d751b27dc123.88903754</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>https://spectralestimation.wordpress.com/phd-project/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D71B5F3A-8CE3-4C42-8C5B-A7FC55AD8EAE</gtr:id><gtr:title>Rank-based camera spectral sensitivity estimation.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>585d4862c33c23.65121187</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>401CC7B6-A4A7-4168-93E1-D00C443B2482</gtr:id><gtr:title>Reference data set for camera spectral sensitivity estimation.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc31c241f59a0322d8e059933403e50"><gtr:id>1dc31c241f59a0322d8e059933403e50</gtr:id><gtr:otherNames>Darrodi MM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>56c43dc565de31.55426988</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>880E7F83-EB30-47DA-A24F-30CB748178F9</gtr:id><gtr:title>Estimating human colour sensors from rankings</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/beeb9f42b3759b7bf3b95cb35859476c"><gtr:id>beeb9f42b3759b7bf3b95cb35859476c</gtr:id><gtr:otherNames>Darrodi M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c5269d81d0f1.64713808</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95117A05-FD39-4DDF-A57E-06C4A68DFA7E</gtr:id><gtr:title>Extended Linear Color Correction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c8f3cbc7f24e0be23bd21afcbdd10e4f"><gtr:id>c8f3cbc7f24e0be23bd21afcbdd10e4f</gtr:id><gtr:otherNames>Finlayson GD; Johnson G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c523ade40e97.03634165</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2B04D91A-2798-49EB-BBA9-8FAA719EAB60</gtr:id><gtr:title>The alternating least squares technique for nonuniform intensity color correction</gtr:title><gtr:parentPublicationTitle>Color Research &amp; Application</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>54634e38f2d4a4.70175493</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C0FADA67-BCDF-4CC4-9F87-A162E6E1BDEF</gtr:id><gtr:title>Method for hue plane preserving color correction.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fc6ecf3f4f71076dd6b6478dc4643ead"><gtr:id>fc6ecf3f4f71076dd6b6478dc4643ead</gtr:id><gtr:otherNames>Mackiewicz M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>585d4872449458.39064090</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB265DA3-48AD-49CF-A16D-46A6187C55F9</gtr:id><gtr:title>Estimating individual cone fundamentals from their color-matching functions.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd7723305fa62ef10fc7d1268800a260"><gtr:id>fd7723305fa62ef10fc7d1268800a260</gtr:id><gtr:otherNames>Andersen CF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>58c52520431c31.18524124</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5A830D0F-0F08-4561-8631-D5EEBD4F2FCF</gtr:id><gtr:title>A Ground Truth Data Set for Nikon Camera's Spectral Sensitivity Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/beeb9f42b3759b7bf3b95cb35859476c"><gtr:id>beeb9f42b3759b7bf3b95cb35859476c</gtr:id><gtr:otherNames>Darrodi M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56c46b48ea7c61.41963532</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>997418C0-D974-4BD3-AE4D-F340DFBE0095</gtr:id><gtr:title>Rank-Based Camera Spectral Sensitivity Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56c469fac5add4.70711597</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J005223/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>