<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Dept of Computing</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/89833E38-8292-47CC-82C3-394FE46D889E"><gtr:id>89833E38-8292-47CC-82C3-394FE46D889E</gtr:id><gtr:name>University of Arizona</gtr:name><gtr:address><gtr:line1>617 N. Santa Rita Ave.</gtr:line1><gtr:line4>Tucson</gtr:line4><gtr:line5>Arizona</gtr:line5><gtr:postCode>AZ 85721</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/28289324-275A-4FA6-9B10-201B07A6DC41"><gtr:id>28289324-275A-4FA6-9B10-201B07A6DC41</gtr:id><gtr:name>Dimensional Imaging Ltd</gtr:name><gtr:address><gtr:line1>1 Ainslie Road</gtr:line1><gtr:postCode>G52 4RU</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3C3AFB38-848D-4372-BC6E-7CDAB3AFC876"><gtr:id>3C3AFB38-848D-4372-BC6E-7CDAB3AFC876</gtr:id><gtr:name>US Custom and Border Protection</gtr:name><gtr:address><gtr:line1>McClelland Hall 427</gtr:line1><gtr:line2>P O Box 210108</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2BA3C484-FA19-4E5D-BBB7-2FCFAD740307"><gtr:id>2BA3C484-FA19-4E5D-BBB7-2FCFAD740307</gtr:id><gtr:name>The Home Office</gtr:name><gtr:address><gtr:line1>3rd floor, Seacole Bldg</gtr:line1><gtr:line2>2 Marsham Street</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW1P 4DF</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/956E999D-84FF-4CBE-8DDF-49169EA9E142"><gtr:id>956E999D-84FF-4CBE-8DDF-49169EA9E142</gtr:id><gtr:name>Carnegie Mellon University</gtr:name><gtr:address><gtr:line1>5000 Forbes Avenue</gtr:line1><gtr:line4>Pittsburgh</gtr:line4><gtr:line5>PA 15213</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/562FAE6C-A85A-45A8-B90C-4F2F79902BAC"><gtr:id>562FAE6C-A85A-45A8-B90C-4F2F79902BAC</gtr:id><gtr:firstName>Maja</gtr:firstName><gtr:surname>Pantic</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DC72772A-60E1-4295-8D44-47F448452680"><gtr:id>DC72772A-60E1-4295-8D44-47F448452680</gtr:id><gtr:firstName>Daniel</gtr:firstName><gtr:surname>Rueckert</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/85C74800-12C4-4460-B46C-76652A525C86"><gtr:id>85C74800-12C4-4460-B46C-76652A525C86</gtr:id><gtr:firstName>Stefanos</gtr:firstName><gtr:surname>Zafeiriou</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ017787%2F1"><gtr:id>ECFFF4E5-BCF8-4AAA-9879-F6A43A949725</gtr:id><gtr:title>Analysis of Facial Behaviour for Security in 4D (4D-FAB)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J017787/1</gtr:grantReference><gtr:abstractText>The overall aim of the project is the development of automated tools for automatic spatio-temporal analysis and understanding of human subtle facial behaviour from 4D facial information (i.e. 3D high-quality video recordings of facial behaviour). Two exemplar applications related to security issues will be specifically addressed in this proposal: (a) person verification (i.e. using facial behaviour as a biometric trait), and (b) deception indication. 

The importance of non-obtrusive person verification and deception indication is undisputable - every day, thousands of people go through airport security checkpoints, border crossing checkpoints, and other security screening points. Automated, unobtrusive monitoring and assessing of deceptive behaviour will form a valuable tool for end users, such as police, justice and prison services. This is in particular important as currently only informal interpretations for detecting deceptive behaviour are used. In addition, the development of alternative methods for person verification that are not based on physical traits only but on behavioural, easily observable traits like facial expressions, would be of great value for the development of multimodal biometric system. Such multi-modal biometric systems will be of great interested to government agencies such as the Home Office or the UK Border agency.

For automatic deception indication we propose to develop methodologies for detecting 4D micro-expressions and their dynamics being typical of deceptive behaviour as reported by research in psychology. For automatic person identification we propose to increase the robustness of static face- image-based verification systems by including facial dynamics as an additional biometric trait. The underlying motivation is that the dynamic 4D facial behaviour is very difficult to imitate and , hence, it has natural resilience against spoof attacks. 

The project focuses on 3D video recordings rather than on 2D video recordings of facial behaviour due to two main reasons: (1) increased robustness to changes in head-pose, and (2) ability to spot subtle changes in the depth of facial surface such as jaw clench and tremor appearance on the cheeks, which are typical of deceptive behaviour and cannot be spotted in 2D images. The research on 3D facial dynamics is now made possible by the tremendous advance of sensors and devices for the acquisition of 3D face video recordings.

The core of the project will deal with both the development of 4D-FAB research platform containing tools for human subtle facial behaviour analysis in 4D videos and the development of annotated data repository consisting of two parts: (1) annotated 4D recordings of deceptive and truthful behavior, and (2) annotated 4D recordings of subjects uttering a sentence, deliberately displaying certain facial actions and expressions, and spontaneously displaying certain facial actions and expressions. The work plan is oriented around this central goal of developing 4D-FAB technology and is carried out in 3 work packages described in the proposal.

A team of 3 Research Associates (RAs), led by the PIs, and having the background in computer vision and machine learning, will develop 4D-FAB technology. The team will be closely assisted by 6 members of the Advisory Board:
Prof. Burgoon, University of Arizona, advising on psychology of deception and credibility
Prof. Cohn, Pittsburgh University / Carnegie Mellon University, advising on face perception and facial behaviometrics
Prof. Nunamaker, Director of BORDERS, US Nat'l Center for Border Security and Immigration, advising on making 4D-FAB useful for end users in security domain
Dr Hampson, Head of Science &amp;amp; Technology, OSCT, Home Office, advising on making 4D-FAB useful for end users
Dr Cohen, Director of United Technologies Research Centre Ireland, advising on making 4D-FAB useful for end users 
Dr Urquhart, CEO of Dimensional Imaging, advising on 4D recording setup design</gtr:abstractText><gtr:potentialImpactText>The overall aim of the project is the development of automated tools for automatic spatio-temporal analysis and understanding of human facial behaviour from 4D facial information (i.e. 3D high-quality video recordings of facial behaviour). Two exemplar applications related to security issues will be specifically addressed in this proposal: (a) person verification (i.e. facial behaviour as a form of behaviometrics), and (b) deception indication. 

The importance of non-obtrusive person verification and deception indication is undisputable - every day, thousands of people go through access control points and security screening checkpoints. Automated, unobtrusive person verification and assessing of deceptive behaviour will form a valuable tool for end users, such as police, justice and prison services. Such systems will be of great interest to government agencies such as the Home Office and Border Agency (both being project partners in this proposal).

While the proposal focuses on applications in the area of security, the technology developed will have numerous applications beyond this. Human behaviour understanding plays a critical role underlying the development and design of ICT systems in a human-centred manner, built for humans based on human behaviour models. Engineering ICT systems with the capability to sense and understand unstructured human user's behaviour is a challenge that goes beyond today's systems engineering paradigm, which can free computer users from the classic keyboard and mouse and enable technologies like ambient intelligence and ubiquitous computing. Other potential benefits from efforts to automate the analysis of facial expressions are varied and numerous and span fields as diverse as:
(1) cognitive sciences - automated tools would speed up tremendously current research processes as they would replace the current lengthy and tedious manual analysis of the studied behaviour.
(2) medicine - remote monitoring of conditions like pain and depression, remote assessment of drug effectiveness, computer-based remote treatment of facial paralysis, etc., would be possible, leading to more advanced personal wellness technologies than those available today. 
(3) transportation - automatic assessment of the driver's stress level, detection of micro sleeps, and spotting driver's puzzlement, would be facilitated, enabling a next generation in-vehicle assisting technology. 
(4) education - automatic assessment of student's interest level, puzzlement, and enjoyment would become possible, facilitating development of truly intelligent tutoring systems.

We believe that the technology developed in this project has very high potential for commercialization. In particular, the developed image acquisition technology will be of substantial interest beyond the area of facial expression analysis, e.g. in healthcare and creative industries. The PIs have already extensive experience in close collaborations with industry, in particular in the healthcare domain, as well as in setting up spin-off companies. We will use our previous experiences to work in collaboration with industry to exploit opportunities for commercialisation of the developed technology. To ensure the potential for commercial exploitation we will protect the developed IP where appropriate (e.g. via patents, if and when appropriate, before dissemination to the community). 

To ensure appropriate involvement of end users in the proposed research we assembled an advisory team of potential users and industrial collaborators interested in the technology developed in this project. To ensure engagement with the wider community we also work in close collaboration with the Institute for Security Science and Technology at Imperial College London, that focuses on innovation in homeland and national security.

We will also disseminate our research to a wider audience through activities such as participation at the Royal Society Summer Exhibition, Meetings, etc.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1082116</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The database and publicly available software developed by 4DFAB are used by many start-up companies to build facial landmark localisation/tracking methodologies (these methodologies are the corner stone for many automatic facial analysis tasks, including, face recognition, facial expression recognition etc.).</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>4364BB92-463A-4E62-B6BC-4BF35922C33D</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56dd8b908611c9.20830926</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The key findings until now are the following:
(a) We have developed the first ever benchmarks for training and assessing the performance of facial landmark detection/tracking algorithms.
(b) We used the data to build the largest to date (over 10K people) generic 3D morphable model. We have also built models tailored to various groups (e.g., ethnicity, age etc.)
(c) We built methods for putting 3D faces in correspondence, which are currently used to put in correspondence thousands of 3D frames
(d) We used models for 3D face reconstruction in arbitrary conditions, as well as extraction of dense facial motion.</gtr:description><gtr:exploitationPathways>The databases and models developed through the project are currently used by the majority of the state-of-the-art in facial landmark localisation and tracking.</gtr:exploitationPathways><gtr:id>C6F1F42C-C027-4F02-B371-E7143FBD7904</gtr:id><gtr:outcomeId>56dd8ad959a6f4.72180695</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://ibug.doc.ic.ac.uk/resources</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The database provides the first collection of annotated, with regards to 68 landmarks, facial data in unconstrained conditions. The database contains more than 12,000 annotated images.</gtr:description><gtr:id>4467E46C-9280-42A4-A576-D2FABC086E32</gtr:id><gtr:impact>The database was used in the two first challenges in the field (the first held in conjuction with one of the top conferences of the field, i.e. ICCV 2013, and the second was held in a special issue of a journal, Image and Vision Computing). Currently 300W is the de-facto standard for assessing the performance of facial landmark localisation methodologies. It is used by the majority of the state-of-the-art methods reporting results in the best venues in the field (i.e., CVPR, ICCV, ECCV etc.). The paper describing the database has received more than 100 citations.</gtr:impact><gtr:outcomeId>56dd7eeff2db33.31628046</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>300W: Database of Facial Landmarks &quot;in-the-wild&quot;</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The database contains per-frame annotations of 110+ facial videos (1 min+ each) captured in unconstrained conditions. Each facial frame was annotated with regards to 68 landmarks (in total more than 150,000 frames). The database is used for assessing the performance of facial landmark tracking methodologies (or general deformable object tracking methodologies).</gtr:description><gtr:id>CE7ECA45-EE26-4F34-8A35-EF9F746FFC4D</gtr:id><gtr:impact>The database was used in the first benchmark for facial landmark tracking &amp;quot;in-the-wild&amp;quot;. The benchmark was used to run the first competition/challenge in conjunction with one of the top venues in the field, i.e. ICCV 2015. The database is currently used by many state-of-the-art facial landmark localisation methods.</gtr:impact><gtr:outcomeId>56dd84ac2d1768.05731430</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>300 VW: Annotated database for facial landmark tracking &quot;in-the-wild&quot;</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://ibug.doc.ic.ac.uk/resources/300-VW/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The Menpo project provides publicly available open source tools for annotating and building 3D Morphable Models (3DMMs). In particular, it contains a very high quality and easy to use annotation tool, as well as many different methods for building 3DMMs (including non-ridig ICP routines, Active Appearance Models, warping etc.).</gtr:description><gtr:id>ACA5B47C-32CA-4430-9017-64BF2D10B7F4</gtr:id><gtr:impact>Menpo project was used to develop the first 3DMM built from 10,000 people. This is, to the best of our knowledge, the largest scale
3D Morphable Model ever constructed, containing statistical information from a huge variety of the human population (different age, gender and ethnicity groups).</gtr:impact><gtr:outcomeId>56dd87e0877f32.11599325</gtr:outcomeId><gtr:title>Menpo: Tools for 3D Morphable Model Construction</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.menpo.org/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>680B81F6-E22A-4FDB-8D16-7CE4947274DD</gtr:id><gtr:title>Recovering Joint and Individual Components in Facial Data</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a6ef0d427c9c2.78948997</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26613377-42B9-4457-9F80-23604D667C82</gtr:id><gtr:title>A Comprehensive Performance Evaluation of Deformable Face Tracking &amp;quot;In-the-Wild&amp;quot;</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe6b18d26a7.50252400</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E6FC0884-0488-4201-B7A6-58FC65FF060A</gtr:id><gtr:title>300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_323198801313dba5fc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38CC025C-3217-46EF-A477-5DCD40B2316C</gtr:id><gtr:title>Context-Sensitive Conditional Ordinal Random Fields for Facial Action Intensity Estimation</gtr:title><gtr:parentPublicationTitle>The IEEE International Conference on Computer Vision (ICCV) Workshops. Sydney, Australia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ca97c3da5025fae211a933687e55edf"><gtr:id>2ca97c3da5025fae211a933687e55edf</gtr:id><gtr:otherNames>Rudovic O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_151796310413f4de96</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C03FD499-155E-493A-BE4F-F7FAE438DFD1</gtr:id><gtr:title>Principal Component Analysis With Complex Kernel: The Widely Linear Model</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Neural Networks and Learning Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/921ee4104a2ac03f7504bb8722813893"><gtr:id>921ee4104a2ac03f7504bb8722813893</gtr:id><gtr:otherNames>Papaioannou A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f9539537f5bd3e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B1184D74-F143-42CC-83D1-32B63CA5FA2B</gtr:id><gtr:title>300 Faces In-The-Wild Challenge: database and results</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd6c8ba1dc78.51247278</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25B869DA-DEC4-4435-9A1C-11BB2994731A</gtr:id><gtr:title>Bayesian Active Appearance Models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75a8af6c7150b2e8892e9f6969aed3d6"><gtr:id>75a8af6c7150b2e8892e9f6969aed3d6</gtr:id><gtr:otherNames>Alabort-i-Medina J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8ebdf9d7.35033822</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF88941C-EB22-4134-82B3-A86BF093F3AA</gtr:id><gtr:title>RAPS: Robust and Efficient Automatic Construction of Person-Specific Deformable Models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8f3a4ab2.11896200</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BD99CB45-110D-4E55-84F1-9BD51F4AFFD3</gtr:id><gtr:title>Active Pictorial Structures</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8caf46f9.96164221</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15B63A5A-CDC9-49B9-B0AB-C78EE381D29D</gtr:id><gtr:title>Feature-based Lucas-Kanade and active appearance models.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5675e44db1412</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>62D98ABC-26CF-4FA8-BF85-F96D39C126D6</gtr:id><gtr:title>On One-Shot Kernels: Explicit Feature Maps and Properties</gtr:title><gtr:parentPublicationTitle>Proceedings of IEEE Int'l Conf. on Computer Vision (ICCV 2013)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_732846955514072d26</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>31B9C425-59C0-4ABC-BDCC-AFCAC8561C0F</gtr:id><gtr:title>Merging SVMs with Linear Discriminant Analysis: A Combined Model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1f84361373d5c3b485d1b108d9f514e0"><gtr:id>1f84361373d5c3b485d1b108d9f514e0</gtr:id><gtr:otherNames>Nikitidis S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8f1c2906.64004560</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F379DF48-B551-40EA-AC45-B72802DA032A</gtr:id><gtr:title>Mnemonic Descent Method: A Recurrent Process Applied for End-to-End Face Alignment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1efa9a163072268cf3c2b3988f9e1d45"><gtr:id>1efa9a163072268cf3c2b3988f9e1d45</gtr:id><gtr:otherNames>Trigeorgis G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6b9a5703068.42629211</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>71D7821F-CB2D-4F52-B9DC-E620E3B830FE</gtr:id><gtr:title>Facial landmarking for in-the-wild images with local inference based on global appearance</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e104c917c1340b94c797cc0de9587047"><gtr:id>e104c917c1340b94c797cc0de9587047</gtr:id><gtr:otherNames>Martinez B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>doi_55f9539537ec2a31</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D952A54F-4D65-4B22-97FC-C888C87F73D8</gtr:id><gtr:title>A 3D Morphable Model Learnt from 10,000 Faces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d317ccc9946eec073ba6ef8df6b93001"><gtr:id>d317ccc9946eec073ba6ef8df6b93001</gtr:id><gtr:otherNames>Booth J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6b9a4e440e1.68620051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E7FC3407-575C-47DB-A902-5E15D560B25A</gtr:id><gtr:title>From Pixels to Response Maps: Discriminative Image Filtering for Face Alignment in the Wild.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4654574b83d68a748a38245b2e83cd24"><gtr:id>4654574b83d68a748a38245b2e83cd24</gtr:id><gtr:otherNames>Asthana A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5675eb554e00b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D243064D-B7C3-4CF2-BCC3-09025CF67CCA</gtr:id><gtr:title>Online kernel slow feature analysis for temporal video segmentation and tracking.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d185abab3392493297236503003181c4"><gtr:id>d185abab3392493297236503003181c4</gtr:id><gtr:otherNames>Liwicki S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5675e3abbf604</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CE76B096-2E60-45DE-A76B-E317AFD8CA86</gtr:id><gtr:title>Nonnegative Decompositions for Dynamic Visual Data Analysis.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5a578af79efd01.08341960</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F709C72D-60DC-41C8-AE3C-C417F45C2F67</gtr:id><gtr:title>Kernel-PCA Analysis of Surface Normals for Shape-from-Shading</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0cb2fc699d369822b5b95df5b09184"><gtr:id>9a0cb2fc699d369822b5b95df5b09184</gtr:id><gtr:otherNames>Snape P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8edbd317.19451958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E67ECDAE-8B97-4368-BE04-F81A5660BDB2</gtr:id><gtr:title>3D facial geometric features for constrained local model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aaf109908e93f5f256369e9311e71523"><gtr:id>aaf109908e93f5f256369e9311e71523</gtr:id><gtr:otherNames>Cheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8da2a1f1.66368425</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>91BC5DF7-2008-453D-A205-CAE4A01B7004</gtr:id><gtr:title>Deep Analysis of Facial Behavioral Dynamics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a578af1920252.39207058</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6962F808-2BA9-4689-A949-97E61833F06F</gtr:id><gtr:title>Active nonrigid ICP algorithm</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c231bbf66924a2e2cd3b7fca00a8df93"><gtr:id>c231bbf66924a2e2cd3b7fca00a8df93</gtr:id><gtr:otherNames>Shiyang Cheng</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8cec62a2.37261718</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F8CF772F-A7EC-43F6-9FCD-1B1C7E879DAB</gtr:id><gtr:title>Robust Canonical Correlation Analysis: Audio-visual fusion for learning continuous interest</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/939cacce23215069c7d1f76270ca6e7b"><gtr:id>939cacce23215069c7d1f76270ca6e7b</gtr:id><gtr:otherNames>Nicolaou M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8e134d67.27066996</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26CF0854-DCCB-4536-BDC5-BB2CE63001FE</gtr:id><gtr:title>Online learning and fusion of orientation appearance models for robust rigid object tracking</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a8100cb550cfc8870c1dbf69c343419"><gtr:id>2a8100cb550cfc8870c1dbf69c343419</gtr:id><gtr:otherNames>Marras I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f974974e81b0a9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E75F7FCA-9333-407E-A556-9558F4B0DB47</gtr:id><gtr:title>Deep Canonical Time Warping for simultaneous alignment and representation learning of sequences.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1efa9a163072268cf3c2b3988f9e1d45"><gtr:id>1efa9a163072268cf3c2b3988f9e1d45</gtr:id><gtr:otherNames>Trigeorgis G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5a578b13a82c77.80038489</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F02B52BF-59DC-4450-BCEE-D98F0F7CFD14</gtr:id><gtr:title>Learning Slow Features for Behaviour Analysis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_305230184213ed2dcc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9C81940E-C4E4-49E9-BFB5-9ACFF4938876</gtr:id><gtr:title>Incremental Face Alignment in the Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4654574b83d68a748a38245b2e83cd24"><gtr:id>4654574b83d68a748a38245b2e83cd24</gtr:id><gtr:otherNames>Asthana A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8f79da64.81319214</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>348B1B47-6978-4F39-BAFB-82812C07AE6C</gtr:id><gtr:title>The First Facial Landmark Tracking in-the-Wild Challenge: Benchmark and Results</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/11792d8e57ee799267adfbebcc11e298"><gtr:id>11792d8e57ee799267adfbebcc11e298</gtr:id><gtr:otherNames>Shen J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8c341628.73268373</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>77FB21B6-4AE2-4C84-9B7E-7EC9BDC6D88D</gtr:id><gtr:title>Slow features nonnegative matrix factorization for temporal data decomposition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8e7b2282.35070243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3EA9EF22-795E-4CBA-A3DF-092E9EFDF700</gtr:id><gtr:title>Automatic construction Of robust spherical harmonic subspaces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0cb2fc699d369822b5b95df5b09184"><gtr:id>9a0cb2fc699d369822b5b95df5b09184</gtr:id><gtr:otherNames>Snape P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8ccc5004.14051213</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>305CFDD5-8D69-4226-AEE0-48A299032B18</gtr:id><gtr:title>Automatic Construction of Deformable Models In-the-Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8f5ab026.55534284</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B8F5F25-6673-442B-8D36-F9B4B847DDA1</gtr:id><gtr:title>Variational Infinite Hidden Conditional Random Fields.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/81c8b1c09327365e2835514731ffd170"><gtr:id>81c8b1c09327365e2835514731ffd170</gtr:id><gtr:otherNames>Bousmalis K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5675eb459f8a1</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6520EC37-00D9-4B1B-918A-00696AB2C66F</gtr:id><gtr:title>Robust Correlated and Individual Component Analysis.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e897a2380019384ddfcbdd79bef8f519"><gtr:id>e897a2380019384ddfcbdd79bef8f519</gtr:id><gtr:otherNames>Panagakis Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>56dd6c8b80dbd0.61248892</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE59DA80-50D6-4B82-9570-A0C8FD941281</gtr:id><gtr:title>Estimating Correspondences of Deformable Objects &amp;quot;In-the-Wild&amp;quot;</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07c2a4af9f42a88efefb696767d62dd9"><gtr:id>07c2a4af9f42a88efefb696767d62dd9</gtr:id><gtr:otherNames>Zhou Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6b9a5482cf6.85280042</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DFA47BD2-9EF2-41C3-A355-C0F8EE4F2D66</gtr:id><gtr:title>A Joint Discriminative Generative Model for Deformable Model Construction and Classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a8100cb550cfc8870c1dbf69c343419"><gtr:id>2a8100cb550cfc8870c1dbf69c343419</gtr:id><gtr:otherNames>Marras I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a578adc1555b2.50149142</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F39280A-04AC-4BED-BBB2-BE8CA6CFE754</gtr:id><gtr:title>Joint Unsupervised Deformable Spatio-Temporal Alignment of Sequences</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bec4b48c5532.49789269</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BBD3C14-5A7A-457A-9F42-A03F84B8F81A</gtr:id><gtr:title>HOG active appearance models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675f20d842ab</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F02458DE-9681-4E8D-AC32-1D1A734B38FA</gtr:id><gtr:title>Probabilistic Slow Features for Behavior Analysis.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on neural networks and learning systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2162-237X</gtr:issn><gtr:outcomeId>56d722b8011cb1.11484602</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9FAEC51C-DD12-4336-ACE0-EFC0E95C8FCC</gtr:id><gtr:title>Optimal UV spaces for facial morphable model construction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d317ccc9946eec073ba6ef8df6b93001"><gtr:id>d317ccc9946eec073ba6ef8df6b93001</gtr:id><gtr:otherNames>Booth J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8de16af1.58673924</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E5A9AB1-D209-42FB-870E-C18C96376627</gtr:id><gtr:title>Deep Canonical Time Warping</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1efa9a163072268cf3c2b3988f9e1d45"><gtr:id>1efa9a163072268cf3c2b3988f9e1d45</gtr:id><gtr:otherNames>Trigeorgis G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6b9a5220d88.47670359</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>23811869-D886-4FFC-A117-6BE1B5664644</gtr:id><gtr:title>A robust similarity measure for volumetric image registration with&amp;Acirc;&amp;nbsp;outliers</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0cb2fc699d369822b5b95df5b09184"><gtr:id>9a0cb2fc699d369822b5b95df5b09184</gtr:id><gtr:otherNames>Snape P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d53ba65f3a4.84386632</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AD86F194-450F-48ED-82CA-6CD4C7134434</gtr:id><gtr:title>Unifying holistic and Parts-Based Deformable Model fitting</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75a8af6c7150b2e8892e9f6969aed3d6"><gtr:id>75a8af6c7150b2e8892e9f6969aed3d6</gtr:id><gtr:otherNames>Alabort-i-Medina J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d6b783e2156.85355173</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EAE35B5F-2B49-4C25-8309-B6585DA14086</gtr:id><gtr:title>Face Flow</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0cb2fc699d369822b5b95df5b09184"><gtr:id>9a0cb2fc699d369822b5b95df5b09184</gtr:id><gtr:otherNames>Snape P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8c8ea848.75636594</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCD2A3E5-2155-437B-B387-A10B5D87F194</gtr:id><gtr:title>A Deep Matrix Factorization Method for Learning Attribute Representations.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1efa9a163072268cf3c2b3988f9e1d45"><gtr:id>1efa9a163072268cf3c2b3988f9e1d45</gtr:id><gtr:otherNames>Trigeorgis G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>585d41e36584e9.44579125</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4ABC1979-232B-4FFC-9A1B-01D2EDF3135E</gtr:id><gtr:title>Context-Sensitive Conditional Ordinal Random Fields for Facial Action Intensity Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ca97c3da5025fae211a933687e55edf"><gtr:id>2ca97c3da5025fae211a933687e55edf</gtr:id><gtr:otherNames>Rudovic O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>585d6b8ce27698.55604649</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AB193A49-FEC7-4035-BC63-A289F1FAFEE8</gtr:id><gtr:title>Offline Deformable Face Tracking in Arbitrary Videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8c4fe656.18315667</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F35DC5C-984C-4A2B-BAED-69466386252C</gtr:id><gtr:title>Active Orientation Models for Face Alignment In-the-Wild</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Information Forensics and Security</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e677ef7dfabf32bb2e2bff02fb394403"><gtr:id>e677ef7dfabf32bb2e2bff02fb394403</gtr:id><gtr:otherNames>Tzimiropoulos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675f20e59e2f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E0B3FAC4-D4BE-4136-8FFD-77048949DE72</gtr:id><gtr:title>Large Scale 3D Morphable Models</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d317ccc9946eec073ba6ef8df6b93001"><gtr:id>d317ccc9946eec073ba6ef8df6b93001</gtr:id><gtr:otherNames>Booth J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a352a881c6141.51411841</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A7A56348-E47D-4D0D-9DC5-B1D48B2DC88F</gtr:id><gtr:title>Decision Level Fusion of Domain Specific Regions for Facial Action Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8eed2e88ba85cfe5e39476ae0da4c7d3"><gtr:id>8eed2e88ba85cfe5e39476ae0da4c7d3</gtr:id><gtr:otherNames>Jiang B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54648d02d60450.24387583</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7CD676FD-A764-400D-B111-A3F78C8FCEBB</gtr:id><gtr:title>A survey on mouth modeling and analysis for Sign Language recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8d151131.82060171</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55CFE610-038F-4CB9-9D02-01947EE58628</gtr:id><gtr:title>Robust Statistical Frontalization of Human and Animal Faces</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bec3e07e4144.17962904</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F5A1204-1538-463C-A024-0E958DC9EE93</gtr:id><gtr:title>A survey on face detection in the wild: Past, present and future</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f4dceb137</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C53E2515-DF38-4B46-A45B-5F9C69D44813</gtr:id><gtr:title>Robust Statistical Face Frontalization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd6c8c6ef118.77079880</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD730398-A789-4F28-8F91-62E54668FA41</gtr:id><gtr:title>Statistical non-rigid ICP algorithm and its application to 3D face alignment</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aaf109908e93f5f256369e9311e71523"><gtr:id>aaf109908e93f5f256369e9311e71523</gtr:id><gtr:otherNames>Cheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58bec2d768a569.74041875</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AA21E92-9B72-4193-93B7-37653B1CF876</gtr:id><gtr:title>300 W: Special issue on facial landmark localisation &amp;quot;in-the-wild&amp;quot;</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d68169f48a9.71662734</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>665A8A19-63CF-499F-8424-CFF1C77D8835</gtr:id><gtr:title>Deep Canonical Time Warping for simultaneous alignment and representation learning of sequences.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1efa9a163072268cf3c2b3988f9e1d45"><gtr:id>1efa9a163072268cf3c2b3988f9e1d45</gtr:id><gtr:otherNames>Trigeorgis G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5aa9b75e5b7551.24090744</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3AD9444D-6A1C-4CDD-94D2-1757C9E7CE08</gtr:id><gtr:title>A Unified Framework for Compositional Fitting of Active Appearance Models</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75a8af6c7150b2e8892e9f6969aed3d6"><gtr:id>75a8af6c7150b2e8892e9f6969aed3d6</gtr:id><gtr:otherNames>Alabort-i-Medina J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d6775405595.95912393</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6997B8F4-4F49-4201-A128-E941145FBA76</gtr:id><gtr:title>Full-Angle Quaternions for Robustly Matching Vectors of 3D Rotations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d185abab3392493297236503003181c4"><gtr:id>d185abab3392493297236503003181c4</gtr:id><gtr:otherNames>Liwicki S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dd6c8e9ab3c2.44610493</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J017787/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>