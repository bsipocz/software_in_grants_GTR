<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/D55C2D8D-9CB5-4A25-B643-B16C3E6D0C7A"><gtr:id>D55C2D8D-9CB5-4A25-B643-B16C3E6D0C7A</gtr:id><gtr:firstName>Dietmar</gtr:firstName><gtr:surname>Heinke</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FC533011%2F1"><gtr:id>5DCD2586-A560-4D7F-B770-E806C2732177</gtr:id><gtr:title>Towards a human-inspired control architecture for visually-guided action</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/C533011/1</gtr:grantReference><gtr:abstractText>Despite the increasing sophistication of robotic systems, humans remain superior to robots in an impressive number of ways - not least when it comes to acting in natural environments. The proposed project aims at understanding a set of basic processes that are crucial for such interactions, with this understanding tested through the development of a computer model that is evaluated relative to human behaviour. The process to be modelled concern our ability to shape an action to a task-relevant object present in a complex (multi-object) environment. The proposed model is based on the premise that successful human actions depend on two forms of information: the abstract function of the objects (this is something to cut with) and the visual properties of the objects that are associated with action (that the handle but not the blade can be grasped). Psychological evidence also indicates that the intended action for a given task can influence how human attention is directed. Selection of the object to act upon is not separated from the selection of which action to make. To capture this, the selection of objects and the selection of actions will be modelled as interactive process.Prior work by the applicant provides a promising backdrop to the project. In particular, a model of object selection has been generated (the SAIM model) which has been successfully applied to a large body of psychological data. A second model has simulated how actions are select to single objects (the NAM model). In this project the models will be merged together, to simulate interactions between perceptual selection and action. The model will be realised using a neural-like architecture, and tested in comparison to human data. In the longer term, the model will provide an architecture for real-time control of a robot arm operating in a complex environment</gtr:abstractText><gtr:fund><gtr:end>2009-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>126580</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Contemporary approaches to visually-guided robotics typically follow a two-stage approach: First the problem of object recognition is addressed. Second, and independent of the perceptual process, actions are selected and parameterized. However, recent psychological evidence shows that human actions are not only guided by such an indirect processing route, but also are directly guided by visual information (affordance-based processing). In previous work we developed a computational model of this dual route architecture (Naming and Action model (NAM)). 

However, NAM's functionality is very limited. Especially NAM lacks the ability to generate action parameters, e.g. target locations for movements. In order to add this functionality to NAM we developed the Selective Attention for Action model (SAAM). For SAAM we chose as prototypical action parameter contact points on flat objects produced when humans aim to perform a transporting action. SAAM generates these contact points with the help of a soft-constraint satisfaction approach in a connectionist framework. The first set of constraints takes into account the anatomy of the hand, e. g. maximal possible distances between fingers. The second set of constraints (geometrical constraints) considers suitable contact points on objects by using simple edge detectors. The third set of constraints ensures that the model generates only one location per finger. Interestingly, the third constraint enables SAAM to generate contact points on only one object in scenes with multiple objects. This result mimics attentional behaviour which is guided by action-relevant properties of objects (hence, the name of the model). Interestingly, this type of attentional guidance has been found in recent experimental studies with humans. Also SAAM's reaction times (the time it takes to determine the contact points) vary depending on the visual input. Reaction times are shorter when the object is easy to grasp compared to when the object affords a more difficult grasp. Finally, it is important to note that SAAM can be adapted to generate grasp responses for a robot hand by implementing the anatomy of a robot hand in the second constraint. 



To verify SAAM's simulation results we designed a novel experimental procedure. In this procedure we place an object in front of participants and ask them to lift the object and hold it in front of a camera to document the participants' grasp. We were able to show that human grasps match the contact points generated by SAAM. Moreover, we measured the participants' reaction times (the time it takes participants to begin reaching for the object). The results indicate that the easiness of grasping objects, e.g. a bar vs. a pyramid, influences reaction times, in line with the model's predictions. Hence, SAAM is a viable model of how humans calculate contact points for stable grasps on flat objects.

Finally, it should be noted that the network architecture of SAAM was inspired by my work on visual attention. In this line of work I examined the role of attention in different tasks: visual search in hierarchical patterns; visual search across time and space; and translation-invariant object identification in multiple object scenes. The corresponding connectionist models consist of a combination of excitatory and inhibitory connections. Interestingly, even though these models realize seemingly very different selection mechanisms, e.g. selecting objects vs. selecting contact points, the only major difference between the network architectures lies in the topology of the excitatory connections. These connections operate as &amp;quot;functional wiring&amp;quot; implementing the task at hand. Hence the architecture behind these models may represent a general neural processing principle that can be employed in a variety of tasks. Future research, e.g. the analysis of neural topologies in the brain, needs to collects supporting evidence for this speculation.





Another interesting implication of our work results from the fact that SAAM's architecture is very similar to an earlier model developed in my lab, the Selective Attention for Identification model (SAIM, Heinke &amp;amp; Humphreys, 2003). SAIM implements translation-invariant object recognition in multiple object scenes. SAIM can model a wide range of experimental evidence on attention and its disorders. 

In contrast to SAAM SAIM selects locations in one object. However, like in SAAM, SAIM employs a soft-constraint satisfaction to implement its operation. The constraints are: bright locations (a simple model assumption), a spatial cluster of bright locations and selection of only one of such cluster of locations. Interestingly the selection constraints in the two networks are implemented by the same inhibitory connections. And the two &amp;quot;functional&amp;quot; constraints, hand anatomy and spatial clusters, are implemented with excitatory connections. Only, in terms of these excitatory connections the two networks differ. Hence, the architecture of SAAM and SAIM is the same and only in the exact way the excitatory connections are wired the very different functionality of the two networks is implemented. Hence, it can be speculate whether the architecture of SAAM and SAIM represent a general principle of neural information processing. Future research needs to establish whether this is the case, e.g. by analysing neural pathways.</gtr:description><gtr:exploitationPathways>The architecture developed in this project may lead to novel ways of controlling robots. Collaborative projects with robotics experts at the University of Birmingham are currently discussed.</gtr:exploitationPathways><gtr:id>A5F2D100-D663-4925-B01C-B557EC970F51</gtr:id><gtr:outcomeId>r-223386502.10144603779791b0</gtr:outcomeId><gtr:sectors/><gtr:url>http://www.comp-psych.bham.ac.uk/projects.php</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9B773C19-29C4-4BD7-8911-16FD0D645B51</gtr:id><gtr:title>Attention and spatial resolution: a theoretical and experimental study of visual search in hierarchical patterns.</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/937157831b83c89e0b0fb0de5caeb136"><gtr:id>937157831b83c89e0b0fb0de5caeb136</gtr:id><gtr:otherNames>Deco G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>doi_53d0390395e1721b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5EBF1236-9C72-499B-9DC2-DDCBE3D97ED2</gtr:id><gtr:title>Suppressive effects in visual search: A neurocomputational analysis of preview search</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af48f807bd3829edbde954e3a26ff9a0"><gtr:id>af48f807bd3829edbde954e3a26ff9a0</gtr:id><gtr:otherNames>Mavritsaki E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53d0020021bd2a4c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A7E5F89D-08AC-48F8-A62E-5D48C07E864F</gtr:id><gtr:title>Modeling visual Affordances: The Selective Attention for Action Model (SAAM)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a00fc44041cd2167e401f7714926dcb1"><gtr:id>a00fc44041cd2167e401f7714926dcb1</gtr:id><gtr:otherNames>C Boehme</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_353748542213d9e438</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE6CE85C-6A7C-476B-8739-54DDCF133C75</gtr:id><gtr:title>Computational Modelling in Behavioural Neuroscience: Closing the gap between neurophysiology and behaviour</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ace87e9c8340dc75a613cefdc00867d8"><gtr:id>ace87e9c8340dc75a613cefdc00867d8</gtr:id><gtr:otherNames>D Heinke</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_792132578813a9f98a</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/C533011/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>65</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EFFEC6B1-6BC8-4C9D-9D77-02CEF5E4E301</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Biomedical neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>35</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>