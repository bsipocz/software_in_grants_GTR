<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/E4599F86-E5D9-40AD-A427-4782E12EA8BF"><gtr:id>E4599F86-E5D9-40AD-A427-4782E12EA8BF</gtr:id><gtr:name>Swiss Federal Institute of Technology in Lausanne (EPFL)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/B0DA50CA-D11E-4251-9678-4AA2F93DB545"><gtr:id>B0DA50CA-D11E-4251-9678-4AA2F93DB545</gtr:id><gtr:name>Polytechnic University of Catalonia</gtr:name><gtr:address><gtr:line1>Polytechnic University of Catalonia</gtr:line1><gtr:line2>C/ Jordi Girona, 31</gtr:line2><gtr:line4>Barcelona</gtr:line4><gtr:line5>E-08034</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Spain</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2A80FFDA-3B8B-43BA-80C3-3AA850B49BA1"><gtr:id>2A80FFDA-3B8B-43BA-80C3-3AA850B49BA1</gtr:id><gtr:name>University of the West of England</gtr:name><gtr:department>Faculty of Environment and Technology</gtr:department><gtr:address><gtr:line1>Coldharbour Lane</gtr:line1><gtr:line2>Frenchay</gtr:line2><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS16 1QY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2A80FFDA-3B8B-43BA-80C3-3AA850B49BA1"><gtr:id>2A80FFDA-3B8B-43BA-80C3-3AA850B49BA1</gtr:id><gtr:name>University of the West of England</gtr:name><gtr:address><gtr:line1>Coldharbour Lane</gtr:line1><gtr:line2>Frenchay</gtr:line2><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS16 1QY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E4599F86-E5D9-40AD-A427-4782E12EA8BF"><gtr:id>E4599F86-E5D9-40AD-A427-4782E12EA8BF</gtr:id><gtr:name>Swiss Federal Institute of Technology in Lausanne (EPFL)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B0DA50CA-D11E-4251-9678-4AA2F93DB545"><gtr:id>B0DA50CA-D11E-4251-9678-4AA2F93DB545</gtr:id><gtr:name>Polytechnic University of Catalonia</gtr:name><gtr:address><gtr:line1>Polytechnic University of Catalonia</gtr:line1><gtr:line2>C/ Jordi Girona, 31</gtr:line2><gtr:line4>Barcelona</gtr:line4><gtr:line5>E-08034</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Spain</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/81B0E5AB-71BA-4A8D-9912-13A037BAEE49"><gtr:id>81B0E5AB-71BA-4A8D-9912-13A037BAEE49</gtr:id><gtr:firstName>Anthony</gtr:firstName><gtr:surname>Pipe</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/18A929A7-A706-478C-909A-2D17C9E74AE2"><gtr:id>18A929A7-A706-478C-909A-2D17C9E74AE2</gtr:id><gtr:firstName>Praminda</gtr:firstName><gtr:surname>Caleb-Solly</gtr:surname><gtr:orcidId>0000-0001-8821-0464</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2391F821-F225-41AD-AD8E-D08135F51203"><gtr:id>2391F821-F225-41AD-AD8E-D08135F51203</gtr:id><gtr:firstName>Sanja</gtr:firstName><gtr:surname>Dogramadzi</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7573DA52-BF4E-442E-BD56-B6FDD7101133"><gtr:id>7573DA52-BF4E-442E-BD56-B6FDD7101133</gtr:id><gtr:firstName>Alexander</gtr:firstName><gtr:surname>Lenz</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN021703%2F1"><gtr:id>FF57F6A2-B935-4B2B-8AB3-7A350C8A8C8F</gtr:id><gtr:title>I-DRESS</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N021703/1</gtr:grantReference><gtr:abstractText>The main objective of the project is to develop a system that will provide proactive assistance with dressing to disabled users or users such as high-risk health-care workers, whose physical contact with the garments must be limited during dressing to avoid contamination. The proposed robotic system consists of two highly dexterous robotic arms, sensors for multi-modal human-robot interaction and safety features.
The system will comprise three major components, each of radical impact to the field of assistive service robotics: (a) intelligent algorithms for user and garment detection and tracking, specifically designed for close and physical human-robot interaction, (b) cognitive functions based on the multi-modal user input, environment modelling and safety, allowing the robot to decide when and how to assist the user, and (c) advanced user interface that facilitates intuitive and safe physical and cognitive interaction for support in dressing. The consortium consisting of three partners provides the expertise for the main lines of research required by the project: CSIC-UPC will work on perception and human-robot interaction, IDIAP will contribute to robot learning, and UWE-BRL will provide the expertise in safety and interface design. 
The developed interactive system will be integrated on commercial WAM robotic arms and validated through experimentation with users and human factor analysis in two assistive-dressing scenarios. Additionally, developed robot safety features and the learning by demonstration algorithms will be implemented on a Baxter robot, thus ensuring general applicability and easier acceptance of the project results by both industry and scientific community.</gtr:abstractText><gtr:potentialImpactText>Robot safety is the key issue that must be solved if robots are to leave the research labs and be allowed to enter all areas of society. A lack of widely adopted standards for personal care and medical robots is currently being considered by special working groups within the ISO - ISO TC 184/SC 2 and IEC SC62A. A new ISO 13482 safety standard for personal care robots was published in 2013; these types of robots involve close robot-human interaction for providing personal services. Dr Dogramadzi is actively involved in these working groups and the results of I-DRESS will contribute to further developments and considerations of the standard committees. Leaving the robot system in the homes with users is beyond the scope of this project, due to health and safety governance procedures. However, we will attempt to test the system with healthy adults in the laboratory environment. 
The use of robot learning from demonstration techniques will allow the developed system to automatically adapt to the specific needs of each user (which can be changing with time). This enlarges the scope of possible applications as well as the potential markets, since the dressing assistant can learn rapidly what it is expected to do, without requiring the manual reprogramming of the robot through a computer language. This versatility is expected to be crucial for such challenging physical human-robot interaction.
The technological and HRI advancements made as part of the proposed scenarios can be extended to use of similar robot configurations to support other activities of daily living. The ensuing independence for individuals, previously reliant on carers and others, will mean greater autonomy to participate in society and the workplace. Even in cases where the number of carers is reduced from two to one for personal care assistance, there will be a significant healthcare saving and more effective use of precious human resources. 
The impact of the human factors and interface design research proposed here is of crucial importance in facilitating understanding of mixed initiative human-robot interaction multi-modal dialogue. Together with extending application to people with accessibility needs where one or more communication modalities might be impaired, this research will produce pragmatic solutions for the use of robotics in the assistive care sector.

Final project developments will be done under the Open Source policies, and the use of the ROS tools will ensure their reusability. The aim of the project is to produce transferable skills, that once developed can be used as a module by other research groups or in industry. The dissemination of project results and strategies for contacting with various stakeholders is described in the section 3.2.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>305522</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Swiss Federal Institute of Technology in Lausanne (EPFL)</gtr:collaboratingOrganisation><gtr:country>Switzerland, Swiss Confederation</gtr:country><gtr:description>IDIAP</gtr:description><gtr:id>07A7E81A-B4A1-47A2-8B3E-FA835C095BBA</gtr:id><gtr:impact>This is a collaboration of 3 robotics institutes in Europe.</gtr:impact><gtr:outcomeId>58c7cc48db7005.90522412-1</gtr:outcomeId><gtr:partnerContribution>Each partner has a clearly defined set of objectives in this project that lead to a final intergration.</gtr:partnerContribution><gtr:piContribution>I-DRESS is a CHIST-ERA project and has been created with two European partners - IDIAP and Institut de Robotics industrial in Barcelona.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Polytechnic University of Catalonia</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:description>Universitat Politecnica de Catalunya, IRI</gtr:description><gtr:id>80C610F3-6A9F-458E-BA03-C269E36D9980</gtr:id><gtr:impact>All partners are from robotics field and are jointly developing a new paradigm of assisstive robotics.</gtr:impact><gtr:outcomeId>58c7f6efa96ca1.23154706-1</gtr:outcomeId><gtr:partnerContribution>The project partners are investigating different objectives that will be integrated towards the end of its duration.</gtr:partnerContribution><gtr:piContribution>This project is a collaboration between 3 robotics institutes in Europe.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>IEEE UK RAS Chapter Symposium</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>88A51670-9966-4D78-B696-A58CFF7078E3</gtr:id><gtr:impact>About 50 academia and industry representatives gathered and a group of academics presented their work. I-DRESS outcomes have been presented and reached about 25 postgraduate and undergraduate students.</gtr:impact><gtr:outcomeId>58c85a3c341160.26073957</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPSRC RAAI event</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>93456A0A-0375-45BE-8D4D-E8E46CBA1E00</gtr:id><gtr:impact>This event attracted academics and industry in UK involved in EPSRC research projects. This encouraged discussions and dissemination of good practice and professional knowledge.</gtr:impact><gtr:outcomeId>58c859973b8f45.69675897</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have conducted a series of human-robot and human-human experiments to analyse salient points in this, close and safety critical, human-robot interaction. Different interaction modalities that do not include vision have been tested to prove success of a dressing scenario. Using vision is typically unacceptable due to privacy issues but also the slower processing times.</gtr:description><gtr:exploitationPathways>Exploring safety in dressing scenarios is transferable to other close Human-Robot interactions where the robot assists the human in carrying out a particular task.</gtr:exploitationPathways><gtr:id>B22D1E34-6C2E-4224-A4EF-869FF2EA00C2</gtr:id><gtr:outcomeId>58c86af48787c0.97233196</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>80A28990-1A3C-40F2-A3D0-C3A9C64D0E8B</gtr:id><gtr:title>What's &amp;quot;up&amp;quot;? - Resolving interaction ambiguity through non-visual cues for a robotic dressing assistant</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7f422f06b3c836d83698de0dbc7a5024"><gtr:id>7f422f06b3c836d83698de0dbc7a5024</gtr:id><gtr:otherNames>Chance G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa54474c537b0.21622073</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>09F8C489-B43E-4174-89E1-CDA688E282BD</gtr:id><gtr:title>A Quantitative Analysis of Dressing Dynamics for Robotic Dressing Assistance</gtr:title><gtr:parentPublicationTitle>Frontiers in Robotics and AI</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7f422f06b3c836d83698de0dbc7a5024"><gtr:id>7f422f06b3c836d83698de0dbc7a5024</gtr:id><gtr:otherNames>Chance G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa5458e96dcb0.69629610</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>667E3B4C-A39F-4F7C-8A66-673D4BFF418B</gtr:id><gtr:title>An assistive robot to support dressing - strategies for planning and error handling</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7f422f06b3c836d83698de0dbc7a5024"><gtr:id>7f422f06b3c836d83698de0dbc7a5024</gtr:id><gtr:otherNames>Chance G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7c8b8675791.68110704</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N021703/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>