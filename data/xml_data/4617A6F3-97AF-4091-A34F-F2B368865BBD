<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B6B6F6F8-2492-4C78-B98C-71372DA3ABC3"><gtr:id>B6B6F6F8-2492-4C78-B98C-71372DA3ABC3</gtr:id><gtr:firstName>Anthony</gtr:firstName><gtr:surname>Hunter</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN008294%2F1"><gtr:id>4617A6F3-97AF-4091-A34F-F2B368865BBD</gtr:id><gtr:title>Framework for Computational Persuasion</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N008294/1</gtr:grantReference><gtr:abstractText>Persuasion is an activity that involves one party trying to induce another party to believe something or to do something. It is an important and multifaceted human facility. Obviously, sales and marketing is heavily dependent on persuasion. But many other activities involve persuasion such as a doctor persuading a patient to drink less alcohol, a road safety expert persuading drivers to not text while driving, or an online safety expert persuading users of social media sites to not reveal too much personal information online. As computing becomes involved in every sphere of life, so too is persuasion a target for applying computer-based solutions.

Many of the current persuasion technologies for behaviour change (e.g. for encouraging healthier life styles) are based on some combination of questionnaires for finding out information from users, provision of information for directing the users to better behaviour, computer games to enable users to explore different scenario concerning their behaviour, provision of diaries for getting users to record ongoing behaviour, and messages to remind the user to continue with the better behaviour.

Interestingly, argumentation is not central to the current manifestations of persuasion technologies. The arguments for good behaviour seem either to be assumed before the user accesses the persuasion technology (e.g. when using diaries, or receiving email reminders), or arguments are provided implicitly in the persuasion technology (e.g. through provision of information, or through game playing). 

So explicit consideration of arguments and counterarguments are not supported with existing persuasion technologies. Yet in real-world persuasion, in particular in applications such as behaviour change, presenting convincing arguments, and presenting counterarguments to the user's arguments, is critically important. For example, for a doctor to persuade a patient to drink less alcohol, the doctor has to give good arguments why it is better for the patient to drink less, and for how it is possible. 

In this project, we intend to bring argumentation into a new generation of persuasion technologies. An automated persuasion system (APS) is a system that can engage in a dialogue with a user (the persuadee) in order to persuade the persuadee to do (or not do) some action or to believe (or not believe) something. To do this, an APS aims to use convincing arguments in order to persuade the persuadee. 

The dialogue may involve moves including queries, claims, and importantly, arguments that are presented according to some protocol. The dialogue may be asymmetric since the kinds of moves that the APS can present may be different to the moves that the persuadee may make. For instance, the persuadee might be restricted to only making arguments by selecting them from a menu (in order to obviate the need for natural language processing of arguments being entered). In the extreme, it may be that only the APS can make moves. Whether an argument is convincing depends on the context and on the characteristics of the persuadee. An APS maintains a model of the persuadee, and this is harnessed by the strategy of the APS in order to choose good moves to make in the dialogue.

Computational persuasion is the study of formal models of dialogues involving arguments and
counterarguments, of user models, and strategies, for APSs. The overall goal of this project is to develop a formal framework for computational persuasion. This framework will extend recent developments in computational models of argument. The emphasis will be on APSs that will help users in changing behaviour (e.g. to persuade the user to drink less, or to not text while driving).</gtr:abstractText><gtr:potentialImpactText>Immediate impacts from the project (i.e. during and in the year or two after the project) will come from the development of the theoretical framework for computational persuasion, and by the case studies where we will be undertaking the first trial of argumentation technology in persuasion with users. Both the theoretical and empirical studies should be of substantial interest to the artificial intelligence community (in particular the subfield developing computational models of argument). 

Further short-term impacts in academic research (i.e. in the first three years after the end of the project) should come from uptake by researchers developing persuasion technology for behavioural change, and for researchers evaluating technology for behavioural change in domains such as healthcare and administration (e.g. encouraging healthy lifestyles, encouraging citizenship, encouraging safe driving, etc).

We will aim for medium impacts (i.e. within 5 years after the end of the project) from the research by further developing and evaluating our technology for computational persuasion in diverse applications for behaviour change. This will come through promoting our work during the project, and by seeking funds to evaluate the technology in specific domains after the end of the project (in conjunction with our collaborators in behaviour change at UCL). 

Potential areas where the project technology could be applied for behaviour (with substantial benefits to individuals and society) change include the following:
 
- healthy life styles (e.g. eating more fruit and veg, taking exercise, decreasing drinking)

- addiction management (e.g. gambling, smoking, drugs)

- weight management (e.g. addressing overweight, bulimia, anorexia) 

- treatment compliance (e.g. self-management of diabetes)

- vaccinations (e.g. encouraging uptake)

- personal finance (e.g. borrowing less, saving more)

- education (e.g. starting or continuing with a course, studying properly)

- energy efficiency (e.g. reducing domestic electricity consumption, installing home insulation)

- citizenship (e.g. voting, recycling, contributing to charities, decreasing food waste); 

- safe driving (e.g. not exceeding speed limits, not texting while driving); 

- unacceptable online behaviour (e.g. addressing racism, sexism, trolling, etc).

- antisocial behavior (e.g. addressing aggressive behaviour, vandalism)

Finally, we anticipate that there will be a medium term impact on technologies for ecommerce. Obviously persuasion is an important aspect of commerce. This is not necessary negative. Consider how a helpful shop assistant can direct a customer to a product that s/he believes is appropriate for the customer. However, computational persuasion could be abused. Therefore, we believe that an impact of this research project will be a better understanding of the potential of computational persuasion, and therefore an important starting point for researchers and policy makes wanting to develop guidelines and regulations for the appropriate use of computational persuasion in ecommerce.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>556693</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D7CECC08-1A55-4E37-85B3-CDD9FBCEB13C</gtr:id><gtr:title>Strategic Sequences of Arguments for Persuasion Using Decision Trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/752bc8be233d4e2bbe7a7de227022171"><gtr:id>752bc8be233d4e2bbe7a7de227022171</gtr:id><gtr:otherNames>Hadoux E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c15f1f013c88.75281801</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A674CD79-67CB-4847-B65A-6451ACE4D2ED</gtr:id><gtr:title>Computational Models of Argument (COMMA'16),</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d5310d02c627f78d32b96cb08b6e4c5"><gtr:id>8d5310d02c627f78d32b96cb08b6e4c5</gtr:id><gtr:otherNames>Hunter A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c160229fbf45.54130694</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0894BF0D-11CC-40A4-90B0-CF68106B4A6F</gtr:id><gtr:title>On Partial Information and Contradictions in Probabilistic Abstract Argumentation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d5310d02c627f78d32b96cb08b6e4c5"><gtr:id>8d5310d02c627f78d32b96cb08b6e4c5</gtr:id><gtr:otherNames>Hunter A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c160aa9881e2.54643716</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B565ECCF-5D79-428E-AC7A-1107A3B6225F</gtr:id><gtr:title>Optimization of dialectical outcomes in dialogical argumentation</gtr:title><gtr:parentPublicationTitle>International Journal of Approximate Reasoning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d5310d02c627f78d32b96cb08b6e4c5"><gtr:id>8d5310d02c627f78d32b96cb08b6e4c5</gtr:id><gtr:otherNames>Hunter A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d66e40a5718.28607212</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>20715673-7BE1-4268-89EF-21D2EEB3BB65</gtr:id><gtr:title>Localising iceberg inconsistencies</gtr:title><gtr:parentPublicationTitle>Artificial Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a822b62185745c233f899da2528e59b"><gtr:id>2a822b62185745c233f899da2528e59b</gtr:id><gtr:otherNames>De Bona G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fea0b8b2f23.05293606</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6C2835BD-4179-4BC9-9D4F-DCEA2AD62ED7</gtr:id><gtr:title>Two Dimensional Uncertainty in Persuadee Modelling in Argumentation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d5310d02c627f78d32b96cb08b6e4c5"><gtr:id>8d5310d02c627f78d32b96cb08b6e4c5</gtr:id><gtr:otherNames>Hunter A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c160651df238.17563485</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B94A2F2-3DE0-4AA7-A062-DEFD753B500E</gtr:id><gtr:title>Computationally Viable Handling of Beliefs in Arguments for Persuasion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/752bc8be233d4e2bbe7a7de227022171"><gtr:id>752bc8be233d4e2bbe7a7de227022171</gtr:id><gtr:otherNames>Hadoux E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c15f7f449b25.88228447</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N008294/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>76783275-A9F8-4B4E-B314-51363124259C</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Fundamentals of Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>