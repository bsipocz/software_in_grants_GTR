<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0494AD88-AA50-4F58-A6BE-1A9C56C4C0D7"><gtr:id>0494AD88-AA50-4F58-A6BE-1A9C56C4C0D7</gtr:id><gtr:name>The Royal Society</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1AF074A1-991A-47A5-B04C-BE562D179A11"><gtr:id>1AF074A1-991A-47A5-B04C-BE562D179A11</gtr:id><gtr:name>National Institute of Advanced Industrial Science and Technology</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/4A6DC0CF-2D36-4454-92D2-C175A0CC57EE"><gtr:id>4A6DC0CF-2D36-4454-92D2-C175A0CC57EE</gtr:id><gtr:name>Disney Research</gtr:name><gtr:address><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/7AFD8765-177C-4612-B0E6-DD3259AA0A8B"><gtr:id>7AFD8765-177C-4612-B0E6-DD3259AA0A8B</gtr:id><gtr:name>Marza Animation Planet</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Informatics</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0494AD88-AA50-4F58-A6BE-1A9C56C4C0D7"><gtr:id>0494AD88-AA50-4F58-A6BE-1A9C56C4C0D7</gtr:id><gtr:name>The Royal Society</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1AF074A1-991A-47A5-B04C-BE562D179A11"><gtr:id>1AF074A1-991A-47A5-B04C-BE562D179A11</gtr:id><gtr:name>National Institute of Advanced Industrial Science and Technology</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A6DC0CF-2D36-4454-92D2-C175A0CC57EE"><gtr:id>4A6DC0CF-2D36-4454-92D2-C175A0CC57EE</gtr:id><gtr:name>Disney Research</gtr:name><gtr:address><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7AFD8765-177C-4612-B0E6-DD3259AA0A8B"><gtr:id>7AFD8765-177C-4612-B0E6-DD3259AA0A8B</gtr:id><gtr:name>Marza Animation Planet</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/30959F8D-725A-4E8C-B7F4-3F054A722BC3"><gtr:id>30959F8D-725A-4E8C-B7F4-3F054A722BC3</gtr:id><gtr:name>Namco Bandai Holdings Inc</gtr:name><gtr:address><gtr:line1>9F Taiyo Seimei Shinagawa Building</gtr:line1><gtr:line2>2-16-2 Konan</gtr:line2><gtr:line3>Minato-ku</gtr:line3><gtr:postCode>108-0075</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D17D6885-5FE0-46C6-A7BF-9D449A60C213"><gtr:id>D17D6885-5FE0-46C6-A7BF-9D449A60C213</gtr:id><gtr:name>Honda Research Institute Europe GmbH</gtr:name><gtr:address><gtr:line1>Carl-Legien-Strasse 30</gtr:line1><gtr:line4>Offenbach/Main</gtr:line4><gtr:line5>D-63073</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A51380B1-3661-4185-AF46-B137C84D413B"><gtr:id>A51380B1-3661-4185-AF46-B137C84D413B</gtr:id><gtr:firstName>Subramanian</gtr:firstName><gtr:surname>Ramamoorthy</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/318CDD1C-A802-4A29-8A15-C5885F37F55D"><gtr:id>318CDD1C-A802-4A29-8A15-C5885F37F55D</gtr:id><gtr:firstName>Taku</gtr:firstName><gtr:surname>Komura</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/12AE53B4-3D4F-43DC-869E-D8E96ADE1B09"><gtr:id>12AE53B4-3D4F-43DC-869E-D8E96ADE1B09</gtr:id><gtr:firstName>Sethu</gtr:firstName><gtr:surname>Vijayakumar</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH012338%2F1"><gtr:id>639D7213-0F3F-4865-A991-5D34B29D645C</gtr:id><gtr:title>Topology-based Motion Synthesis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H012338/1</gtr:grantReference><gtr:abstractText>One of the major drivers of research in the area of humanoid robotics is the desire to achieve motions involving close contact between robots and the environment or people, such as while carrying an injured person, handling flexible objects such as the straps of a knapsack or clothes. Currently, these applications seem beyond the ability of existing motion synthesis techniques due to the underlying computational complexity in an open-ended environment. Traditional methods for motion synthesis suffer from two major bottlenecks. Firstly, a significant amount of computation is required for collision detection and obstacle avoidance in the presence of numerous close contacts between manipulator segments and objects. Secondly, any particular computed solution can easily become invalid as the environment changes. For instance, if the robot were handling an object such as a knapsack, even small deformations of this flexible object and minor changes in object dimensions (e.g., between an empty bag and a stuffed bag) might require complete re-planning in the current way of solving the problem. Similar issues arise in the area of computer animation, where there is a need for real-time control of characters - moving away from static sequences of pre-programmed motion. Although it may seem that this world is much more contained, as it is created by an animation designer, there is in fact a strong desire to create games and simulation systems where the users get to interact with the world continually and expect the animation system to react accordingly. This calls for the same sort of advances in motion synthesis techniques as outlined above.The fundamental problem lies in the representation of the state of the world and the robot. Typically, motion is synthesizes in a complete configuration or state space represented at the level of generalized coordinates enumerating all joint angles and their 3D location/orientation with respect to some world reference frame. This implies the need for large amounts of collision checking calculations and randomized exploration in a very large search space. Moreover, it is very hard to encode higher level, semantic, specifications at this level of description as the individual values of the generalized coordinates do not tell us anything unless further calculations are carried out to ensure satisfaction of relevant constraints. This is particularly inconvenient when searching for a motion in a large database. The focus of this research is to alleviate these problems by developing methods that exploit the underlying topological structure in these problems, e.g., in the space of postures. This allows us to define a new search space where the coordinates are based on topological relationships, such as between link segments. We refer to this space in terms of 'topology coordinates'. In preliminary work, we have shown the utility of this viewpoint for efficient motion synthesis with characters that are in close contacts. We have also demonstrated that this approach is more efficient for categorizing semantically similar motions. In this project, we will develop a more general framework of such techniques that will be applicable to a large class of tasks carried out by autonomous humanoid robots and virtual animated characters. Moreover, we will implement our techniques on industrially relevant platforms, through our collaborators at Honda Research Institute Europe GmbH and Namco Bandai, Japan.</gtr:abstractText><gtr:fund><gtr:end>2014-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-09-30</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>458933</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>The Royal Society</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Disney Research</gtr:description><gtr:id>EC48361E-0797-4C6E-A35E-CB09C17A5D3B</gtr:id><gtr:impact>This collaboration has just started and therefore there is no output yet. We are looking into further grants and development of IPs through this collaboration.</gtr:impact><gtr:outcomeId>546b2e02f105a9.41513494-2</gtr:outcomeId><gtr:partnerContribution>Royal Society is providing half of the PI's salary between October 2014-September 2018. Disney Research will provide the working environment for the PI in the Edinburgh office.</gtr:partnerContribution><gtr:piContribution>The PI is awarded the Royal Society Industry Fellowship for looking into techniques to make use of spatial relationships for digital acting. This project is starting on October 2014, and Taku Komura will be working half-time for Disney Research until September 2018.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Disney Research</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Collaboration with Disney Research</gtr:description><gtr:id>D3DA9932-E9E2-4A1B-86CE-729B7BFB292C</gtr:id><gtr:impact>This collaboration has just started and therefore there is no output yet. We are looking into further grants and development of IPs through this collaboration.</gtr:impact><gtr:outcomeId>546b2e02f105a9.41513494-1</gtr:outcomeId><gtr:partnerContribution>Royal Society is providing half of the PI's salary between October 2014-September 2018. Disney Research will provide the working environment for the PI in the Edinburgh office.</gtr:partnerContribution><gtr:piContribution>The PI is awarded the Royal Society Industry Fellowship for looking into techniques to make use of spatial relationships for digital acting. This project is starting on October 2014, and Taku Komura will be working half-time for Disney Research until September 2018.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Marza Animation Planet</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>Collaboration with the Film Industry (Marza Animation Planet)</gtr:description><gtr:id>9F7FDFE0-8D07-48D0-8998-0DA446C364B2</gtr:id><gtr:impact>The initial collaboration has resulted in the following publication:
Jun Saito, &amp;quot;Smooth contact-aware facial blendshapes transfer&amp;quot;, DigiPro 2013
The paper is not co-authored by the PI due to commercial reasons, but the PI has provided significant amount of ideas and effort for the core of this paper (this can be viewed in the reference). Also, this has led to the new grant starting from 2012.</gtr:impact><gtr:outcomeId>546b2934734603.98563908-1</gtr:outcomeId><gtr:partnerContribution>Jun Saito from Marza Animation Planet is currently co-supervising the PhD student Daniel Holden. He has taught basic animation techniques and programming styles in the industry to the student. Also, we are having weekly meetings to check the progress made by the student.</gtr:partnerContribution><gtr:piContribution>We have provided ideas and algorithms for mapping the facial movements from one character to multiple characters. This tool was deployed in the movie &amp;quot;Space Pirate Captain Harlock&amp;quot; released in 2013, and the details of the techniques are described in the paper &amp;quot;Smooth contact-aware facial blendshapes transfer&amp;quot;, DigiPro 2013. Also, Marza Animation Planet has provided a research fund for us to further explore the idea of using the spatial relationship for animation production. One new student, Daniel Holden, has started his PhD funded by this grant, and we are currently developing algorithms and techniques which are going to be used for the production of the film &amp;quot;Robodog&amp;quot;.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>National Institute of Advanced Industrial Science and Technology</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>Collaboration with Advanced Institute of Science and Technology, Japan</gtr:description><gtr:id>D73C40B6-C00E-4C19-8A61-750C5A5E6C9D</gtr:id><gtr:impact>The work about transferring the human motion to the humanoid robot HRP4 is published in the following conference paper: 
Shinichiro Nakaoka, Taku Komura, &amp;quot;Interaction Mesh Based Motion Adaptation for Biped Humanoid Robots&amp;quot;, IEEE Humanoids 2012. 
The system is also used in AIST in order to physically evaluate the workload of various human movements.</gtr:impact><gtr:outcomeId>546b236407c667.44403804-1</gtr:outcomeId><gtr:partnerContribution>Dr. Shinichiro Nakaoka from Japan joined our research group in School of Informatics, University of Edinburgh as a visiting researcher between 2011-12. He has developed a toolkit to transfer the human motion capture data to the humanoid robot HRP-4 in Japan. The system is brought back to Japan and is used for synthesizing motion primitives of the HRP4.</gtr:partnerContribution><gtr:piContribution>We provided the algorithm and prototype software to map the human movements to characters / robots of arbitrary topology to the research team in AIST, Japan.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>121103</gtr:amountPounds><gtr:country>France, French Republic</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Marza Studentship</gtr:description><gtr:end>2016-05-02</gtr:end><gtr:fundingOrg>Research Institute in Astrophysics and Planetology</gtr:fundingOrg><gtr:fundingRef>RB0264</gtr:fundingRef><gtr:id>166B4A31-0385-4D55-B21F-CAA46A374EE1</gtr:id><gtr:outcomeId>546b2fb39da865.76029945</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>127296</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Society Industry Fellowship</gtr:description><gtr:end>2018-09-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>IF130118</gtr:fundingRef><gtr:id>3CED598B-7749-4A73-AFA0-762E5EE68ADB</gtr:id><gtr:outcomeId>546b2ea7d73ce2.92894387</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>- One of the findings, which is to make use of the spatial relationships as a representation, was applied for the animation synthesis in the movie &amp;quot;Space Pirate Captain Harlock (2013)&amp;quot; (see DigiPro 2013: Jun Saito, &amp;quot;Smooth Contact-Aware Facial Blendshapes Transfer&amp;quot;). 
- Another finding is currently being used to edit the character movement in the movie industry (in collaboration with Marza Animation Planet, www.marza.com) for the movie &amp;quot;Robodog&amp;quot; to be released in 2015.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>2C0A30E7-0F2F-49E8-A89C-87CF30A7FCF2</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>546af3bcb61c04.31158382</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>- We have developed a representation to describe complex interactions between characters, or between a character and objects or the environment, which is applicable for animation production.
- We have developed a method to interactively control characters and robots in a dynamic environment. 
- We have developed a representation to describe the spatial relations of smooth surfaces in the 3D world. 
- We have developed a method to classify and extract similar spatial relations. 
- We have proposed a quantitative measure of how much an object is &amp;quot;wrapped&amp;quot; by a 3D surface such as a bag or cloth.
- We have proposed a method to control cloth to guide movements such as wrapping and knotting.
- We have proposed a method to compute the dense correspondense of objects with different topology</gtr:description><gtr:exploitationPathways>- Motion retargeting of characters in computer animation and games. 
- Robots to manipulate deformable objects such as cloth.
- Computing the corresponding tissues in volumetric MRI data.
- Searching a database of similar scenes.</gtr:exploitationPathways><gtr:id>333CB4C8-0C05-4CA2-8738-FC09C06E3598</gtr:id><gtr:outcomeId>546af0e6a27263.13331493</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare,Manufacturing, including Industrial Biotechology,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://homepages.inf.ed.ac.uk/tkomura/research_dir/research.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A database of human interaction with objects, captured using a magnetic motion capture system.</gtr:description><gtr:id>CB630F82-E77A-405D-BBDD-B70BC232854F</gtr:id><gtr:impact>We have made the data available to arbitrary research groups who are interested in interactions.</gtr:impact><gtr:outcomeId>546b3a078f3c17.63440004</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Edinburgh Interaction Database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.ipab.inf.ed.ac.uk/cgvu/InteractionDatabase/interactiondb.html</gtr:url></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>1E8C325A-A84E-4E2B-9EFB-0DADD1818DEF</gtr:id><gtr:title>Exploiting Causality for Selective Belief Filtering in Dynamic Bayesian Networks</gtr:title><gtr:parentPublicationTitle>Journal of Artificial Intelligence Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6983387ff1892210498a126577cd1965"><gtr:id>6983387ff1892210498a126577cd1965</gtr:id><gtr:otherNames>Albrecht S.V.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>587f3896e559b9.71124554</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A6570286-031E-4188-8814-119AA79255AF</gtr:id><gtr:title>Relationship Templates for Synthesising Scene Variations</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/43b53bb2a0f2e46e37eef18685cb1421"><gtr:id>43b53bb2a0f2e46e37eef18685cb1421</gtr:id><gtr:otherNames>Komura T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>587f3aa281ddd2.68215043</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>83F1CBC6-FCE4-4002-9622-3BB384429CD0</gtr:id><gtr:title>A Game-theoretic Model and Best-response Learning Method for Ad Hoc Coordination in Multiagent System</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/03d998144e361e5679671d5621e9fb49"><gtr:id>03d998144e361e5679671d5621e9fb49</gtr:id><gtr:otherNames>Albrecht S V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>587f3a01832a21.17012084</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94137090-2AE4-436D-8663-96A95E828F97</gtr:id><gtr:title>Lifelong Learning of Structure in the Space of Policies</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7855452b7bce571f294696cf13b5f9e6"><gtr:id>7855452b7bce571f294696cf13b5f9e6</gtr:id><gtr:otherNames>Hawasly M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>587f3c8e67a929.40837013</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F32BE245-DB7C-4A0B-8021-F0F66DCDB856</gtr:id><gtr:title>Simulating Multiple Character Interactions with Collaborative and Adversarial Goals</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Visualization and Computer Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3cb647a722ebab0233cbd6905dc97ee"><gtr:id>e3cb647a722ebab0233cbd6905dc97ee</gtr:id><gtr:otherNames>Shum H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546afa72e6fdc1.51179450</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0D4508D2-9B07-4EBB-BAFA-F3902F06A617</gtr:id><gtr:title>Motion planning and reactive control on learnt skill manifolds</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/352e27f1fb5b3bfc61b8484e62469daa"><gtr:id>352e27f1fb5b3bfc61b8484e62469daa</gtr:id><gtr:otherNames>Havoutis I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>587f399f2acaf0.76231453</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15A0F020-C574-4E22-A232-60198B76F486</gtr:id><gtr:title>Interactive partner control in close interactions for real-time applications</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Multimedia Computing, Communications, and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b45cc0e5408c0cded5ccc9991146951"><gtr:id>0b45cc0e5408c0cded5ccc9991146951</gtr:id><gtr:otherNames>Ho E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f95e95e59a4e8b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>632B137C-8649-4B26-9A12-C86F526BFB71</gtr:id><gtr:title>Generalizability of EMG decoding using local field potentials.</gtr:title><gtr:parentPublicationTitle>Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e84e4761343fea384958aeed2da88938"><gtr:id>e84e4761343fea384958aeed2da88938</gtr:id><gtr:otherNames>Krasoulis A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1557-170X</gtr:issn><gtr:outcomeId>587f3ccd724618.55413594</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DFB3C34A-C466-4EA4-8744-1EDBE7FF7ACD</gtr:id><gtr:title>Interaction capture using magnetic sensors</gtr:title><gtr:parentPublicationTitle>Computer Animation and Virtual Worlds</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d8fa54d54c7f662299ba8ad57cb5449f"><gtr:id>d8fa54d54c7f662299ba8ad57cb5449f</gtr:id><gtr:otherNames>Sandilands P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546afa73191a32.11868287</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>51B4762F-C334-4F64-B392-A0EC51848F3D</gtr:id><gtr:title>Interactive formation control in complex environments.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/878631e1253b43700a5b123a919316c6"><gtr:id>878631e1253b43700a5b123a919316c6</gtr:id><gtr:otherNames>Henry J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>546afa72454142.26468571</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D275B49-CFE1-4B7D-BB5A-D04A8A71C622</gtr:id><gtr:title>Environment-aware real-time crowd control</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/878631e1253b43700a5b123a919316c6"><gtr:id>878631e1253b43700a5b123a919316c6</gtr:id><gtr:otherNames>Henry J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>587f390db759e3.85081595</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DFD33BA9-C722-4321-A644-0DF0E3185F93</gtr:id><gtr:title>Indexing 3D Scenes Using the Interaction Bisector Surface</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f76706732cc4158a7609b4aa9382a37e"><gtr:id>f76706732cc4158a7609b4aa9382a37e</gtr:id><gtr:otherNames>Zhao X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546af643d9a813.03803043</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E70FD256-FC7D-4188-8EC1-98AC493DF3DE</gtr:id><gtr:title>Hierarchical Motion Planning in Topological Representations</gtr:title><gtr:parentPublicationTitle>2012 Robotics: Science and Systems (R:SS) Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a5d4bc1167c0310c89f3745408ce3a12"><gtr:id>a5d4bc1167c0310c89f3745408ce3a12</gtr:id><gtr:otherNames>Zarubin, D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>543bba1b392ab6.78188735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B7B49924-B6EE-4948-A518-9A26BDFF4113</gtr:id><gtr:title>Evaluation of regression methods for the continuous decoding of finger movement from surface EMG and accelerometry</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e84e4761343fea384958aeed2da88938"><gtr:id>e84e4761343fea384958aeed2da88938</gtr:id><gtr:otherNames>Krasoulis A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5660655b25c743.86504187</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39300D28-2FB7-491D-ACC8-A9F8BAA6BD54</gtr:id><gtr:title>Joint classification of actions and object state changes with a latent variable discriminative model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7a33ba08a0d206782c6b1b7ee914335"><gtr:id>d7a33ba08a0d206782c6b1b7ee914335</gtr:id><gtr:otherNames>Vafeias E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>587f396827bc02.33887619</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A80A4FFE-F54A-4039-A2DB-02EA1D542DF9</gtr:id><gtr:title>Interactive Formation Control in Complex Environments.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/878631e1253b43700a5b123a919316c6"><gtr:id>878631e1253b43700a5b123a919316c6</gtr:id><gtr:otherNames>Henry J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>5a730a57ca6525.45298470</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>03356F9C-2BD7-40F6-8C39-DC1BFFA64D0B</gtr:id><gtr:title>Harmonic parameterization by electrostatics</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f88cc0b3857e4454158c5b9ddf455220"><gtr:id>f88cc0b3857e4454158c5b9ddf455220</gtr:id><gtr:otherNames>Wang H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546af69334e614.68472589</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>29562F97-6CA2-44DB-8E2B-54B0894F5AB9</gtr:id><gtr:title>Learning in non-stationary MDPs as transfer learning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/047909fe2e69bf7d9d5940cf978b5b98"><gtr:id>047909fe2e69bf7d9d5940cf978b5b98</gtr:id><gtr:otherNames>Hassan Mahmud M M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>587f3ae8586237.13468265</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>10F3A5A9-97E6-4B34-B333-412BD95850D9</gtr:id><gtr:title>Character-Object Interaction Retrieval using the Interaction Bisector Surface</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f76706732cc4158a7609b4aa9382a37e"><gtr:id>f76706732cc4158a7609b4aa9382a37e</gtr:id><gtr:otherNames>Zhao X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a7322b9d64c98.88880306</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D15D0607-0958-4380-997A-41E1D8A725B2</gtr:id><gtr:title>Giving advice to agents with hidden goals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d8fa95808e44ed91a833bda738e25af9"><gtr:id>d8fa95808e44ed91a833bda738e25af9</gtr:id><gtr:otherNames>Rosman B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>587f3d0570ea92.30214372</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>09F7D229-962B-49A6-B50C-07D141404B3C</gtr:id><gtr:title>Manipulation of Flexible Objects by Geodesic Control</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f88cc0b3857e4454158c5b9ddf455220"><gtr:id>f88cc0b3857e4454158c5b9ddf455220</gtr:id><gtr:otherNames>Wang H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f95e95e5922e1f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EF190E80-70E7-4715-9371-E2012DA715F3</gtr:id><gtr:title>Interactive light source position estimation for augmented reality with an RGB-D camera</gtr:title><gtr:parentPublicationTitle>Computer Animation and Virtual Worlds</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2dd9df3d41f603222434924cb2726c1e"><gtr:id>2dd9df3d41f603222434924cb2726c1e</gtr:id><gtr:otherNames>Boom B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>585d5b42739199.30387282</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A372DD87-4F80-4A99-8E04-C45901A86303</gtr:id><gtr:title>Spatial relationship preserving character motion adaptation</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b45cc0e5408c0cded5ccc9991146951"><gtr:id>0b45cc0e5408c0cded5ccc9991146951</gtr:id><gtr:otherNames>Ho E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_55f952952d92b48e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AA190992-8234-4135-8965-F6F8FF45F8DA</gtr:id><gtr:title>Belief and truth in hypothesised behaviours</gtr:title><gtr:parentPublicationTitle>Artificial Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b6fd497873646f098eca35597ec8c23e"><gtr:id>b6fd497873646f098eca35597ec8c23e</gtr:id><gtr:otherNames>Albrecht S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d620924ec65.74895729</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B990C51-EEB4-468C-B927-703784897B07</gtr:id><gtr:title>An Energy-Driven Motion Planning Method for Two Distant Postures.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f88cc0b3857e4454158c5b9ddf455220"><gtr:id>f88cc0b3857e4454158c5b9ddf455220</gtr:id><gtr:otherNames>Wang H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>546afa7219c541.70825523</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>73081E5D-E00C-4098-B5BA-C3BB2CCBE44E</gtr:id><gtr:title>Interaction mesh based motion adaptation for biped humanoid robots</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35c460a3ff438cb9d342c776394e1099"><gtr:id>35c460a3ff438cb9d342c776394e1099</gtr:id><gtr:otherNames>Nakaoka S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546afa7293dec1.22133881</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BF67578F-E7F5-4BE6-8E22-DACDFDC3FAF8</gtr:id><gtr:title>Bayesian interaction shaping: learning to influence strategic interactions in mixed robotic domains</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23958b22d9a3f00b55f4b8a6880a0a9e"><gtr:id>23958b22d9a3f00b55f4b8a6880a0a9e</gtr:id><gtr:otherNames>Valtazanos A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>587f3bfcc5e180.35883706</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C126B705-4BE2-4B61-B750-64D64D6EA01E</gtr:id><gtr:title>Towards low-dimensionsal proportional myoelectric control.</gtr:title><gtr:parentPublicationTitle>Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e84e4761343fea384958aeed2da88938"><gtr:id>e84e4761343fea384958aeed2da88938</gtr:id><gtr:otherNames>Krasoulis A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1557-170X</gtr:issn><gtr:outcomeId>5660655ae7ccc2.36474380</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EBA3A5F-67B1-4D47-8BA0-6F38B7FC7E38</gtr:id><gtr:title>Real-time controllable fire using textured forces</gtr:title><gtr:parentPublicationTitle>The Visual Computer</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a732f5656e61879bf64564f0a337a730"><gtr:id>a732f5656e61879bf64564f0a337a730</gtr:id><gtr:otherNames>Lever J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>587f3b1ea17262.49377283</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B531CFBD-CF15-407A-8F07-FBA9B0BA52A4</gtr:id><gtr:title>Bayesian policy reuse</gtr:title><gtr:parentPublicationTitle>Machine Learning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d8fa95808e44ed91a833bda738e25af9"><gtr:id>d8fa95808e44ed91a833bda738e25af9</gtr:id><gtr:otherNames>Rosman B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4529927ec8.21520435</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H012338/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>53F27348-198B-4AEF-A34B-8307067F507C</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Systems engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>21CE2EA6-E7A2-4406-A045-0DA7CA19B695</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Control Engineering</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>