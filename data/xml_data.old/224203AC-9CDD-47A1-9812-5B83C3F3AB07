<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D908D947-CC7D-4CAF-A31C-EB8CF5DB38C9"><gtr:id>D908D947-CC7D-4CAF-A31C-EB8CF5DB38C9</gtr:id><gtr:name>Jiangnan University</gtr:name><gtr:address><gtr:line1>1800 Lihu Avenue</gtr:line1><gtr:postCode>214122</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9D12884B-3F95-49B0-8E1E-02200AC126C3"><gtr:id>9D12884B-3F95-49B0-8E1E-02200AC126C3</gtr:id><gtr:name>Home Office</gtr:name><gtr:address><gtr:line1>Policing &amp; Crime Reduction Group</gtr:line1><gtr:line2>Police Standards Unit, 5th Floor</gtr:line2><gtr:line3>50 Queen Annes Gate</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SW1H 9AT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/B26BF1E2-4862-4752-9E26-B0AB14D58F8F"><gtr:id>B26BF1E2-4862-4752-9E26-B0AB14D58F8F</gtr:id><gtr:name>Metropolitan Police Service</gtr:name><gtr:address><gtr:line1>Room 703</gtr:line1><gtr:line2>New Scotland Yard</gtr:line2><gtr:line3>Broadway</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SW1H 0BG</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/B3CE801D-23DD-4F60-B488-08A332CD68DA"><gtr:id>B3CE801D-23DD-4F60-B488-08A332CD68DA</gtr:id><gtr:name>Digital Barriers Ltd</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A3ED37DE-4FE9-4061-AFFA-6332B1C975AF"><gtr:id>A3ED37DE-4FE9-4061-AFFA-6332B1C975AF</gtr:id><gtr:name>3rd Forensic Ltd</gtr:name><gtr:address><gtr:line1>Dukesbridge House</gtr:line1><gtr:line2>23 Duke Street</gtr:line2><gtr:postCode>RG1 4SA</gtr:postCode><gtr:region>South East</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/EF67E19B-E192-4302-902A-C069F25BF54C"><gtr:id>EF67E19B-E192-4302-902A-C069F25BF54C</gtr:id><gtr:name>European Association for Biometrics</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D908D947-CC7D-4CAF-A31C-EB8CF5DB38C9"><gtr:id>D908D947-CC7D-4CAF-A31C-EB8CF5DB38C9</gtr:id><gtr:name>Jiangnan University</gtr:name><gtr:address><gtr:line1>1800 Lihu Avenue</gtr:line1><gtr:postCode>214122</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A3ED37DE-4FE9-4061-AFFA-6332B1C975AF"><gtr:id>A3ED37DE-4FE9-4061-AFFA-6332B1C975AF</gtr:id><gtr:name>3rd Forensic Ltd</gtr:name><gtr:address><gtr:line1>Dukesbridge House</gtr:line1><gtr:line2>23 Duke Street</gtr:line2><gtr:postCode>RG1 4SA</gtr:postCode><gtr:region>South East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9D12884B-3F95-49B0-8E1E-02200AC126C3"><gtr:id>9D12884B-3F95-49B0-8E1E-02200AC126C3</gtr:id><gtr:name>Home Office</gtr:name><gtr:address><gtr:line1>Policing &amp; Crime Reduction Group</gtr:line1><gtr:line2>Police Standards Unit, 5th Floor</gtr:line2><gtr:line3>50 Queen Annes Gate</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SW1H 9AT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B26BF1E2-4862-4752-9E26-B0AB14D58F8F"><gtr:id>B26BF1E2-4862-4752-9E26-B0AB14D58F8F</gtr:id><gtr:name>Metropolitan Police Service</gtr:name><gtr:address><gtr:line1>Room 703</gtr:line1><gtr:line2>New Scotland Yard</gtr:line2><gtr:line3>Broadway</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SW1H 0BG</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B3CE801D-23DD-4F60-B488-08A332CD68DA"><gtr:id>B3CE801D-23DD-4F60-B488-08A332CD68DA</gtr:id><gtr:name>Digital Barriers Ltd</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF67E19B-E192-4302-902A-C069F25BF54C"><gtr:id>EF67E19B-E192-4302-902A-C069F25BF54C</gtr:id><gtr:name>European Association for Biometrics</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4479E385-62A0-4CD9-B30F-0BBC29768579"><gtr:id>4479E385-62A0-4CD9-B30F-0BBC29768579</gtr:id><gtr:name>IBM United Kingdom Limited</gtr:name><gtr:address><gtr:line1>PO Box 41</gtr:line1><gtr:line2>North Harbour</gtr:line2><gtr:line4>Portsmouth</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>PO6 3AU</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F7508820-EB1A-4469-B96A-22D1FC28979B"><gtr:id>F7508820-EB1A-4469-B96A-22D1FC28979B</gtr:id><gtr:name>Home Office Science</gtr:name><gtr:address><gtr:line1>Woodcock Hill</gtr:line1><gtr:line2>Sandridge</gtr:line2><gtr:line4>St Albans</gtr:line4><gtr:postCode>AL4 9HQ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/235FCA1C-77F3-4B51-B10D-5269909BDC17"><gtr:id>235FCA1C-77F3-4B51-B10D-5269909BDC17</gtr:id><gtr:name>Cognitec Systems GmbH</gtr:name><gtr:address><gtr:line1>Grossenhainer Str. 101</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0B9506A8-D0B6-47BC-8D85-02F46A20AE10"><gtr:id>0B9506A8-D0B6-47BC-8D85-02F46A20AE10</gtr:id><gtr:name>Digital Barriers</gtr:name><gtr:address><gtr:line1>1-3 Saxton</gtr:line1><gtr:line2>Parklands, Guildford</gtr:line2><gtr:postCode>GU2 9JX</gtr:postCode><gtr:region>South East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C71485A9-841D-4C68-8ECC-B7253B09C803"><gtr:id>C71485A9-841D-4C68-8ECC-B7253B09C803</gtr:id><gtr:name>European Assoc for Biometrics EAB</gtr:name><gtr:address><gtr:line1>Pr Willem van Oranjelaan</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/562FAE6C-A85A-45A8-B90C-4F2F79902BAC"><gtr:id>562FAE6C-A85A-45A8-B90C-4F2F79902BAC</gtr:id><gtr:firstName>Maja</gtr:firstName><gtr:surname>Pantic</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/AACF5A8E-EE5A-4DD0-84EC-6F4D91859D66"><gtr:id>AACF5A8E-EE5A-4DD0-84EC-6F4D91859D66</gtr:id><gtr:firstName>Fei</gtr:firstName><gtr:surname>Yan</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/40F2DF7D-2E36-4F47-BA13-92BB9F4DBB75"><gtr:id>40F2DF7D-2E36-4F47-BA13-92BB9F4DBB75</gtr:id><gtr:firstName>William</gtr:firstName><gtr:otherNames>Jeffery</gtr:otherNames><gtr:surname>Christmas</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/27DAF832-10FB-4603-9D6D-7C410D024C97"><gtr:id>27DAF832-10FB-4603-9D6D-7C410D024C97</gtr:id><gtr:firstName>Chi Ho</gtr:firstName><gtr:surname>Chan</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/FA6E1CBA-992B-4060-8F8F-513D55A737C6"><gtr:id>FA6E1CBA-992B-4060-8F8F-513D55A737C6</gtr:id><gtr:firstName>Krystian</gtr:firstName><gtr:surname>Mikolajczyk</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/85C74800-12C4-4460-B46C-76652A525C86"><gtr:id>85C74800-12C4-4460-B46C-76652A525C86</gtr:id><gtr:firstName>Stefanos</gtr:firstName><gtr:surname>Zafeiriou</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0"><gtr:id>09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0</gtr:id><gtr:firstName>Josef</gtr:firstName><gtr:surname>Kittler</gtr:surname><gtr:orcidId>0000-0002-8110-9205</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B01BF354-7AC7-47C5-A1C9-62C10636DAA0"><gtr:id>B01BF354-7AC7-47C5-A1C9-62C10636DAA0</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:surname>Hancock</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/EF09EF02-2C6D-4167-9904-8E0D479DE8EA"><gtr:id>EF09EF02-2C6D-4167-9904-8E0D479DE8EA</gtr:id><gtr:firstName>Tae-Kyun</gtr:firstName><gtr:surname>Kim</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN007743%2F1"><gtr:id>224203AC-9CDD-47A1-9812-5B83C3F3AB07</gtr:id><gtr:title>Face Matching for Automatic Identity Retrieval, Recognition, Verification and Management</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N007743/1</gtr:grantReference><gtr:abstractText>In the past, when the majority of people were born, lived and died in the same locality where everybody knew each other, there was no need for biometrics. However, nowadays, with the society moving rapidly towards Digital Economy, and the people mobility within the country and across borders reaching unprecedented levels, efficient, robust and effective ways of recognising and verifying individuals automatically, based on biometrics, is emerging as an essential requirement and element of the fabric of the information infrastructure. Identity verification is required to facilitate commerce, and remote working, to enable access to remote services and physical sites in smart cities, as well as contributing to a safer society by fighting crime and terrorism through automatic surveillance. In this context face biometrics is a preferred biometric modality, as it can be captured unobtrusively, even without subjects' being aware of being monitored and potentially recognised. It is also the modality used by humans and thus, when needed, it supports a seamless transition and cooperation between machine and human face recognition. 

Although face biometrics is beginning to be deployed in several sectors, it is currently limited to applications where a strict control can be imposed on the process of face image capture (frontal face recognition in controlled lighting). However, automatic face recognition in uncontrolled scenarios is an unsolved problem because of the variability of face appearance in images captured in different poses, with diverse expressions, under changing illumination. Furthermore, the image variability is aggravated by degradation phenomena such as noise, blur and occlusion. 

The project will develop unconstrained face recognition technology, which is robust to a range of degradation factors, for applications in the Digital Economy and in a world facing global security issues, as well as demographic changes. The approach adopted will endeavour to devise novel machine learning solutions, which combine the technique of deep learning with sophisticated prior information conveyed by 3D face models. The scientific challenge will be to develop a face image representation, which is invariant to various imaging factors. This will necessitate gaining better understanding of the effect of natural face appearance variations and face image degradation phenomena on face image representation. The work will be carried out by a multidisciplinary team constituted by three academic partners, University of Surrey, Imperial College London and University of Stirling, which has extensive experience in biometrics and face modelling, and jointly possesses the necessary expertise, including psychology of human face perception. The research direction will be regularly reappraised and if necessary revised, with steering provided by a team of external experts representing the biometrics industry, government agencies, and potential users of the unconstrained face recognition technology. The progress of the project will be measured by extensive evaluations of the solutions developed using challenging benchmarking tests devised by the biometrics community and compared with evolving commercial offerings.</gtr:abstractText><gtr:potentialImpactText>The proposed EPSRC programme FACER2VM will generate highly visible scientific, economic and societal impact.
Advancement in the understanding of the process of human and machine face matching, and in particular how to model natural face appearance variations and how to handle sensor signal degradation phenomena will contribute to scientific knowledge with wider implications on machine perception. The scientific impact will be enhanced by vigorous dissemination activities through top ranking journal and conference publications. 

The scientific achievements will impact on education and training through the partner institutions' research led postgraduate degree programmes and their continuous professional education offerings. It will also contribute to training highly skilled researchers and engineers for the national economy.

Effective solutions to unconstrained face matching will enable the development of new applications and services of biometrics technology in Digital economy, which will foster economic impact. The expected results of the programme grant will help to enhance UK industry competitiveness in a sector forecast by TechNavio to grow in the facial recognition market alone at a CAGR of 26.6% during the period 2013-2018. The commercial impact will be promoted by the industrial partners with keen interest in exploitation of advanced face recognition technology to be developed by the project. An important role in the impact generation will be played by the European Association for Biometrics as a channel to its 65 industrial members with interests in biometrics.

The societal impact is anticipated to be multifaceted. Unconstrained face biometrics capability will significantly contribute to the government's security agenda in the framework of smart cities and national security. It can effectively facilitate access to public services. The impact will be maximized by the involvement of the Home Office CAST, and of Working Group on Biometrics in the FACER2VM user group, an organization representing diverse users of biometric technology in government departments. 

Face recognition technology also has diverse applications aiming at enhancing the quality of life of individuals, such as facilitating the personalization of devices and person specific monitoring of individuals in smart homes for all, but especially for the benefit of the aging society. 

The outreach element of FACER2VM will contribute to attracting new generations to careers in knowledge economy in general, and science and engineering in particular.</gtr:potentialImpactText><gtr:fund><gtr:end>2020-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>6104264</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Digital Barriers Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Facial biometrics technology</gtr:description><gtr:id>2B9A63A7-947E-4E37-A22F-CDA391A99A59</gtr:id><gtr:impact>Not applicable</gtr:impact><gtr:outcomeId>58b94290842431.32883985-1</gtr:outcomeId><gtr:partnerContribution>Sharing proprietary database, conducting in-house evaluation of biometric technologies developed by the project, providing guidance regarding facial recognition technology requirements.</gtr:partnerContribution><gtr:piContribution>The team has developed facial biometrics technology for unconstrained face recognition scenarios.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Home Office</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Home Office Scientific Development Branch</gtr:department><gtr:description>Surveillance</gtr:description><gtr:id>81189518-0484-4D85-906C-B0200769E635</gtr:id><gtr:impact>Not yet</gtr:impact><gtr:outcomeId>58b93d66ce7198.26811325-1</gtr:outcomeId><gtr:partnerContribution>Home Office CAST provided proprietary database of videos representative of the types of surveillance scenarios of interest to Home Office.</gtr:partnerContribution><gtr:piContribution>Investigating the use of unconstrained facial biometrics for Home Office applications including watchlist surveillance.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Metropolitan Police Service</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Crime investigation</gtr:description><gtr:id>4CC5D70F-7CC6-43D9-9BAB-18228AA27178</gtr:id><gtr:impact>Not applicable</gtr:impact><gtr:outcomeId>58b93f5864b768.04687657-1</gtr:outcomeId><gtr:partnerContribution>User specification and guidance regarding this particular type of application of biometric technologies.</gtr:partnerContribution><gtr:piContribution>Investigating the use of facial and soft biometrics in CCTV based crime investigation.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>3rd Forensic Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Soft biometrics</gtr:description><gtr:id>932FFC54-0F26-4D66-A4FD-68316A09FC92</gtr:id><gtr:impact>Not applicable</gtr:impact><gtr:outcomeId>58b93bbf0c4eb5.96109952-1</gtr:outcomeId><gtr:partnerContribution>3rdForensic provided proprietary database to facilitate the collaborative investigation, as well as user specification and guidance regarding the application of soft biometrics.</gtr:partnerContribution><gtr:piContribution>Investigating the merit of soft biometrics in searching police CCTV and other video databases for suspect re-identification and identification purposes.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Film annotation</gtr:description><gtr:id>949D8B3D-A26B-4FB7-8E4B-47D0C23640DB</gtr:id><gtr:impact>Not applicable.</gtr:impact><gtr:outcomeId>58b940bfbc7e23.45945411-1</gtr:outcomeId><gtr:partnerContribution>BBC provided sample material from their archive for the development of the technology and testing.
.</gtr:partnerContribution><gtr:piContribution>Investigating the use of facial and soft biometrics for automatic annotation of videos.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Jiangnan University</gtr:collaboratingOrganisation><gtr:country>China, People's Republic of</gtr:country><gtr:description>Long-term identification</gtr:description><gtr:id>147DCAB1-CF26-46EB-8011-CA5B56B54CB0</gtr:id><gtr:impact>Not applicable</gtr:impact><gtr:outcomeId>58c82bb95b10f0.36735796-1</gtr:outcomeId><gtr:partnerContribution>facilitating concerted effort, sharing data and resources, and joint work profiting from complementary expertise, making our research more effective.</gtr:partnerContribution><gtr:piContribution>Actively contributed to the creation of the International Joint Laboratory for Pattern Recognition and Computational Intelligence. Acting as one of its co-Directors. The laboratory has facilitated collaborative research between Jiangnan University and Surrey University and is instrumental in making both teams more productive.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>European Association for Biometrics</gtr:collaboratingOrganisation><gtr:country>Netherlands, Kingdom of the</gtr:country><gtr:description>Dissemination</gtr:description><gtr:id>DEBC398A-7B97-4997-8B68-7D91E026B5CF</gtr:id><gtr:impact>Not applicable</gtr:impact><gtr:outcomeId>58b947a22a4e18.49491525-1</gtr:outcomeId><gtr:partnerContribution>Disseminating information about the FACER2VM project and ts research outputs to the EAB membership. Providing feedback to the project.</gtr:partnerContribution><gtr:piContribution>Providing information about the FACER2VM project and its research output.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Associate Editor of (Elsevier) Image and Vision Computing Journal</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C4419677-0C7B-4D30-92F6-0BF1D661E400</gtr:id><gtr:impact>I joined the editorial board of a premium international journal in the field of computer vision.</gtr:impact><gtr:outcomeId>58ae506a84a872.70267318</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BMVA Executive Committee member</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>6943DB72-39A6-41C2-8586-07D95A0C8497</gtr:id><gtr:impact>I joined the BMVA Executive Committee for the period of Jan 2016-Dec 2018. The BMVA organises/supports various academic events and activities in the field of computer vision, and the BMVA executive committee meet to discuss regularly throughout each year.</gtr:impact><gtr:outcomeId>58ae4fa3f09b56.15229295</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Guest Editor of (Elsevier) Pattern Recognition Letters Special Issue on Personalised and Context-sensitive Interfaces in the Wild, 2016.</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BA4D0941-0B71-49BB-8B8E-056E45FD535B</gtr:id><gtr:impact>I have served as a guest editor of a special issue in Pattern Recognition Letter journal.</gtr:impact><gtr:outcomeId>58ae50f941d8a7.91759909</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>General co-chair of British Machine Vision Conference (BMVC), London, Sep 2017</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E90BCDC5-AF36-4BB5-AE2E-482B2C788476</gtr:id><gtr:impact>I co-organise this premium international conference on computer vision in London. About +350 participants and +600 high quality paper submissions are expected. The most renowned academic figures in the field are confirmed as keynotes and tutorial speakers for the event. The event this year is expected to be a unique monument in various aspects.</gtr:impact><gtr:outcomeId>58ae4e3ef3a738.43222907</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC news</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A2098827-1525-4C47-B396-2D7C1A73AA64</gtr:id><gtr:impact>Prof Josef Kittler was interviewed for an article &amp;quot;You are on camera&amp;quot; on Facial Biometrics published by BBC News Business Section.</gtr:impact><gtr:outcomeId>58ca75ef8f5e58.56539995</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/news/business-38879530</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at Deep learning summit, London, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>29DCD123-C8B2-4DE2-BA87-CA22AD5F990F</gtr:id><gtr:impact>About +300 engineers/entrepreneurs and postgraduate students attended for the summit, I gave an invited talk with a live demo on the topic of deep learning and random forest, for the applications on hand pose estimation and face recognition. The summit organisers reported very good feedback from audience on the event/topics.</gtr:impact><gtr:outcomeId>58ae474ab416e5.38090891</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Demo, Imperial College Science Festival May 2016 (500+ visitors)</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>870BBEF1-B29A-412F-A9E7-C03FA939FD87</gtr:id><gtr:impact>We did a hands-on demo for (deformable and articulated object) pose estimation at Imperial College Science Festival, May 2016. Our demo attracted/received 500+ visitors.</gtr:impact><gtr:outcomeId>58ae547052cda5.46041134</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Interview with People's Daily</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>CC74E67C-D4E2-43FA-8AB1-F495A351D5E0</gtr:id><gtr:impact>Input from an interview with Professor Josef Kittler appeared in the China's People's Daily article written to introduce the recipients of the Chinese Government Friendship Award 2016 to the readership.</gtr:impact><gtr:outcomeId>58ca742e9c91d9.09452671</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://paper.people.com.cn/rmrb/html/2017-01/18/nw.D110000renmrb_20170118_1-22.htm.</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPS London meeting</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>AEDD7B34-26AF-43F5-B956-96FFBC3A803E</gtr:id><gtr:impact>Poster describing the Stirling Face Recognition Scale at the London meeting of the Experimental Psychology Society. It generated a lot of interest.</gtr:impact><gtr:outcomeId>58c918b1b76cd4.77824187</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://www.eps.ac.uk/images/epsfiles/2017/epsjan17_programme.pdf</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote at Korea-Japan joint workshop on Frontiers of Computer Vision, Takayama, Japan</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>604E83CF-2874-4258-AFAD-5F62615FD8E6</gtr:id><gtr:impact>About +150 PG students and professors attended this conference, where I gave a keynote. This is a unique event for computer vision researchers especially to promote collaborations and knowledge-sharing between Korea and Japan. My talk on the topic of deep learning and tree structure algorithm sparked lots of questions and interests, the conference organisers reported increased attendance, and excellent feedback on my talk.</gtr:impact><gtr:outcomeId>58ae4c965def98.88033490</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at Korean Conf. on Computer Vision</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>2BBF07D6-DC2D-467D-8BEA-1DCC6BB716EE</gtr:id><gtr:impact>About +200 PG students and engineers from industry attended this event, where I gave an invited talk on the topic of deep learning and tree-structure algorithms, for hand (articulated object) pose and face (deformable object) recognition, and got lots of questions and discussions during the event.</gtr:impact><gtr:outcomeId>58ae495bafc352.30605069</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Associate Editor of IPSJ Transactions on Computer Vision and Applications (CVA), 2016-2018</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>83618C59-D045-4865-ADA3-63FD2F7334AA</gtr:id><gtr:impact>I joined the editorial board of an international journal in the field of computer vision.</gtr:impact><gtr:outcomeId>58ae515b313865.52390322</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited lecture/lab, BMVA computer vision summer school, Swansea, UK</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D11A0819-ACE2-4A09-BD29-1A7AE79584F5</gtr:id><gtr:impact>About +60 PG students attended this school, and I gave 1.5 hour lecture and 1.5 hour hands-on session, on random decision forest with deep learning. The participants expressed lots of interest on the topics and told they would use the learnt for their PG studies. The school reported very good feedback received from the students.</gtr:impact><gtr:outcomeId>58ae4ab25baf09.60710914</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BPS Cognitive Section</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EDB0A998-4921-4B3E-A9D5-E982088FC6AA</gtr:id><gtr:impact>Talk: &amp;quot;The effects of knowing&amp;quot; reporting the effects of foreknowledge of identity on ability to match faces.</gtr:impact><gtr:outcomeId>58c91794bef635.88169741</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>3D face model developed has been licensed to NAVER Corporation.</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>094EBE85-A8DF-4241-85F0-30A08F52910C</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>58b94df35496f4.07007453</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>The Surrey Textured 3D Morphable Face Model is a statistical model of the shape and texture of the human face, that we make available under a commercial licence. The model contains different mesh resolution levels of shape and texture, landmark point annotations, and metadata for texture remapping.</gtr:description><gtr:grantRef>EP/N007743/1</gtr:grantRef><gtr:id>B768CE32-9FEA-4B12-9A07-B127D9529187</gtr:id><gtr:impact>The model allows texture-based fitting for purposes such as face frontalisation, filling in missing data, improved 2D image fitting and extraction of 3D from 2D. The multiple resolution levels enable fast execution in time-critical applications like tracking.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c972b85798b5.18618454</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Surrey Textured 3D Morphable Face Model</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>As part of the programme of work aiming to develop facial biometric technology for unconstrained scenarios based on machine learning and prior knowledge, a dataset of 3D face images of Chinese ethnicity has been collected. Using this dataset, a new 3D morphable face model has been constructed. The new model is not a global face model, but conceived as a mixture of cohort models (MG3DMM) where each cohort represents different ethnicity. The key advantage of MG3DMM is in facilitating more accurate 3D model fittting to arbitrary 2D face images. This has a beneficial impact on face recognition performance.</gtr:description><gtr:exploitationPathways>3D face model licensing.</gtr:exploitationPathways><gtr:id>BEFB4E95-D1B1-42A9-B2DB-1237F0605DFD</gtr:id><gtr:outcomeId>58b953a4ee3d50.50285956</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Retail</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>BreakingNews, a novel dataset with approximately 100K news articles including images, text
and captions, and enriched with heterogeneous meta-data (such as GPS coordinates and user comments). We show this dataset
to be appropriate to explore intersection of computer vision and
natural language processing have achieved unprecedented breakthroughs in tasks like automatic captioning or image retrieval.</gtr:description><gtr:id>8270DBFB-1EA6-4116-B4FC-6404D9081E67</gtr:id><gtr:impact>This is the largest existing database for analysing visual and natural language content in form of news articles rather than short captions. The database has already been used and referenced by other research labs.</gtr:impact><gtr:outcomeId>58c7f4b4369052.62189231</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>BreakingNews: Article Annotation by Image and Text Processing</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.iri.upc.edu/people/aramisa/BreakingNews/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The Surrey 3D Morphable Face Shape Model is a multi-resolution 3D Morphable Model that we make available to the public for non-commercial purposes. The model contains different mesh resolution levels and landmark point annotations as well as metadata for texture remapping. Accompanying the model is a lightweight open-source C++ library designed with simplicity and ease of integration as its foremost goals. In addition to basic functionality, it contains pose estimation and face frontalisation algorithms.</gtr:description><gtr:id>93B7A950-248C-4280-B7C1-70FF7CFF6996</gtr:id><gtr:impact>* Using the different model resolution levels and fast fitting functionality, we have been able to use a 3D Morphable Model in time-critical applications like tracking.
* The software library makes it easy for the community to adopt the 3D Morphable Face Model in their research, and it offers a public place for collaboration.
* The model has been downloaded by more than 60 international parties.</gtr:impact><gtr:outcomeId>58c9714719d550.93178931</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Surrey 3D Morphable Face Shape Model</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>http://www.cvssp.org/faceweb/3dmm/facemodels/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>It has recently been demonstrated that local feature descriptors based on convolutional neural networks (CNN) can significantly improve the matching performance. Previous work on learning such descriptors has focused on exploiting pairs of positive and negative patches to learn discriminative CNN representations. In this code, w utilize triplets of training samples, together with in-triplet mining of hard negatives. This implementation achieves state of the art results, without the computational overhead typically associated with mining of negatives and with lower complexity of the network architecture. We compare our approach to recently introduced convolutional local feature descriptors, and demonstrate the advantages of the proposed methods in terms of performance and speed. We also examine different loss functions associated with triplets.</gtr:description><gtr:id>DDCD5907-E921-4BB5-B5CA-1B2C89D8C5FA</gtr:id><gtr:impact>The method gives state of the art results in standard matching benchmarks. It has been followed by 32 developers in Github and forked by 26 which extended this approach.</gtr:impact><gtr:outcomeId>58c7f649523ca2.76467328</gtr:outcomeId><gtr:title>TFeat</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/vbalnt/tfeat</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>eos is a lightweight 3D Morphable Face Model fitting library that provides basic functionality to use face models, as well as camera and shape fitting functionality. It's written in modern C++11/14.

Some of the functionality it provides:

* MorphableModel and PcaModel classes to represent 3DMMs
* Fast head pose, shape and facial expression fitting
* Camera pose estimation
* Shape-to-landmarks fitting
* Isomap texture extraction</gtr:description><gtr:id>A652CCA8-8D85-48B4-8B53-331DD06AED4A</gtr:id><gtr:impact>State of the art 3D from 2D extraction, face texture fusion from in-the-wild videos, 3D face super-resolution from monocular videos. The code has been followed by 280 developers on GitHub and forked by 120.</gtr:impact><gtr:outcomeId>58c977aab335b4.25667160</gtr:outcomeId><gtr:title>eos</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/patrikhuber/eos</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8ED18889-309E-469A-89A3-2B6B58D2BEF8</gtr:id><gtr:title>PD
&lt;sup>2&lt;/sup>T: Person-specific Detection, Deformable Tracking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fedba834f55.58163815</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B704827-6532-421C-92DC-1E45E8DD9246</gtr:id><gtr:title>Dynamic Attention-Controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-Set Sample Weighting</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e20f649187b603b7d0c7bf10989944f4"><gtr:id>e20f649187b603b7d0c7bf10989944f4</gtr:id><gtr:otherNames>Feng Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfaa14c36d3.07069405</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6239EA34-2324-4CA2-9ECB-30B9A28A27FF</gtr:id><gtr:title>HPatch: A benchmark and evaluation of handcrafted and learned local descriptors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/931443eb7388e9495a76a89eb97c303e"><gtr:id>931443eb7388e9495a76a89eb97c303e</gtr:id><gtr:otherNames>V. Baltnas</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c958c19eebb7.26929260</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EFF3C7DB-8055-4D5B-9D6C-2D919A4FE68F</gtr:id><gtr:title>Learning local feature descriptors with triplets and shallow convolutional neural networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/157418722878442191fa74a84a1b99f3"><gtr:id>157418722878442191fa74a84a1b99f3</gtr:id><gtr:otherNames>V. Balntas</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c9581c1318c0.16127576</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E679F694-5D7B-442D-8D5A-24336CEED5E7</gtr:id><gtr:title>3D Morphable Models as Spatial Transformer Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7d6dc5e477ff606f9f27a5c6099e6666"><gtr:id>7d6dc5e477ff606f9f27a5c6099e6666</gtr:id><gtr:otherNames>Bas A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfcfc24aa98.33502344</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0350D402-B82C-46CC-BC03-92A13FBAF5C1</gtr:id><gtr:title>Gaussian mixture 3D morphable face model</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3fef2d211056d7d7a3ee83582896265b"><gtr:id>3fef2d211056d7d7a3ee83582896265b</gtr:id><gtr:otherNames>Koppen P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2feced84ec13.71598091</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>028284E1-4B9B-4518-8CA4-CD122F072A52</gtr:id><gtr:title>Face Detection, Bounding Box Aggregation and Pose Estimation for Robust Facial Landmark Localisation in the Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e20f649187b603b7d0c7bf10989944f4"><gtr:id>e20f649187b603b7d0c7bf10989944f4</gtr:id><gtr:otherNames>Feng Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfa1d6a7540.58458093</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE64E829-85D0-4379-9BE3-4B7E19B141C7</gtr:id><gtr:title>Large Scale 3D Morphable Models</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d317ccc9946eec073ba6ef8df6b93001"><gtr:id>d317ccc9946eec073ba6ef8df6b93001</gtr:id><gtr:otherNames>Booth J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a352a891a7363.43581903</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1714B6CF-E6B3-4647-A909-CB456266686E</gtr:id><gtr:title>Dynamic Behavior Analysis via Structured Rank Minimization</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a14cdf5d4ed685cf32877d75bc5f895"><gtr:id>6a14cdf5d4ed685cf32877d75bc5f895</gtr:id><gtr:otherNames>Georgakis C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58b54a29b21b59.23683014</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C340729D-7C01-428E-B660-F745E7FC1A33</gtr:id><gtr:title>Robust Joint and Individual Variance Explained</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b691c15e09.43456574</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>79D1BAE3-001F-4A2A-9D7D-5A4ED6B0E417</gtr:id><gtr:title>Robust Statistical Frontalization of Human and Animal Faces</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bec3e07e4144.17962904</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CAA01E68-180D-49C5-9FCE-1C29AFAA3ABB</gtr:id><gtr:title>Temporal Archetypal Analysis for Action Segmentation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0efb7b5364dc9af5c3a1345e4415cfde"><gtr:id>0efb7b5364dc9af5c3a1345e4415cfde</gtr:id><gtr:otherNames>Fotiadou E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8e9b3e70fe90.28390799</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>703327F0-A128-4E73-81F3-AAC68F243CB0</gtr:id><gtr:title>3D Face Morphable Models &amp;quot;In-the-Wild&amp;quot;</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d317ccc9946eec073ba6ef8df6b93001"><gtr:id>d317ccc9946eec073ba6ef8df6b93001</gtr:id><gtr:otherNames>Booth J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b6921757d7.92200121</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D06A4AE4-B41D-4DA2-B042-9B30E969C8F3</gtr:id><gtr:title>Optical-flow features empirical mode decomposition for motion anomaly detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35cb5f3deb43c2c9b15bee9cfbd8e27e"><gtr:id>35cb5f3deb43c2c9b15bee9cfbd8e27e</gtr:id><gtr:otherNames>Ponti M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfb103e11c1.37980376</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>03EA816E-FA44-4066-AAEC-339735895319</gtr:id><gtr:title>An Anomaly Detection Approach to Face Spoofing Detection: A New Formulation and Evaluation Protocol</gtr:title><gtr:parentPublicationTitle>IEEE Access</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d92cf96059fcb25b802ebc091336e554"><gtr:id>d92cf96059fcb25b802ebc091336e554</gtr:id><gtr:otherNames>Arashloo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a352e1a5571f2.81731849</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8F4C6B18-66EA-4E30-BD20-A66466426423</gtr:id><gtr:title>BreakingNews: Article Annotation by Image and Text Processing.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c54fab3be0f3d7f735c18e0d8f639c22"><gtr:id>c54fab3be0f3d7f735c18e0d8f639c22</gtr:id><gtr:otherNames>Ramisa A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5aaa384c1bdfc3.22835583</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3F5FBAF0-C69E-45FE-8CF9-FC309ED091E1</gtr:id><gtr:title>Discriminant Incoherent Component Analysis.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a14cdf5d4ed685cf32877d75bc5f895"><gtr:id>6a14cdf5d4ed685cf32877d75bc5f895</gtr:id><gtr:otherNames>Georgakis C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>58b55578dd3e48.20196085</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1587104E-790E-4544-99E3-259C1181250A</gtr:id><gtr:title>Joint Unsupervised Deformable Spatio-Temporal Alignment of Sequences</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bec4b48c5532.49789269</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8EB4B945-66C4-47E9-A228-742689284859</gtr:id><gtr:title>Error sensitivity analysis of Delta divergence - a novel measure for classifier incongruence detection</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75600d120424e165dd3e8d1131312c1b"><gtr:id>75600d120424e165dd3e8d1131312c1b</gtr:id><gtr:otherNames>Kittler J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a9dfb77e52dc0.81994590</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3D95FE74-099A-4A39-BF58-9B70CFF1821C</gtr:id><gtr:title>A Multiresolution 3D Morphable Face Model and Fitting Framework</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75600d120424e165dd3e8d1131312c1b"><gtr:id>75600d120424e165dd3e8d1131312c1b</gtr:id><gtr:otherNames>Kittler J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:isbn>978-989-758-175-5</gtr:isbn><gtr:outcomeId>58a5dbe2c24ff0.21674890</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60DB57F8-5BEC-4EF0-A9A5-694DCCAEEE68</gtr:id><gtr:title>Marginal Loss for Deep Face Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/183ee541609d30b6de24fa5199c4ffcb"><gtr:id>183ee541609d30b6de24fa5199c4ffcb</gtr:id><gtr:otherNames>Deng J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b69095e5c6.50100322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D3FEB37D-53E2-43E0-A92D-513D8FE5A5C7</gtr:id><gtr:title>DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/416a6170845f0ba93ad42261c5e59f45"><gtr:id>416a6170845f0ba93ad42261c5e59f45</gtr:id><gtr:otherNames>Guler R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b69239f461.05972709</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>141A5FE4-E31D-4701-B676-109AE3CC832C</gtr:id><gtr:title>Real-Time 3D Face Fitting and Texture Fusion on In-the-Wild Videos</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a3bd0ce629968c023bfbaae72be9a7f"><gtr:id>6a3bd0ce629968c023bfbaae72be9a7f</gtr:id><gtr:otherNames>Huber P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe32351c913.41096154</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C765CC45-3BC1-43F7-BA63-7F757D44B60A</gtr:id><gtr:title>Unconstrained Face Detection and Open-Set Face Recognition Challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c8d6cd6bb045011c661e03a6ce837ddf"><gtr:id>c8d6cd6bb045011c661e03a6ce837ddf</gtr:id><gtr:otherNames>Gunther M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfcafe5a7a5.86468492</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C565417F-73E9-43B8-BA7A-39D52A6B7594</gtr:id><gtr:title>Disentangling the Modes of Variation in Unlabelled Data</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/737ffe0e530a693d7c0da01892f1528d"><gtr:id>737ffe0e530a693d7c0da01892f1528d</gtr:id><gtr:otherNames>Wang M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b68f822a12.48648961</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9399EB4D-D717-49CC-84C3-2E1981C0721D</gtr:id><gtr:title>A Comprehensive Performance Evaluation of Deformable Face Tracking &amp;quot;In-the-Wild&amp;quot;</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c699197c7ce9.49554486</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>02C94C87-87A4-47B9-A876-63D4B7714633</gtr:id><gtr:title>Binary Online Learned Descriptors.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62700cf1ac424352a942168a3f7254b8"><gtr:id>62700cf1ac424352a942168a3f7254b8</gtr:id><gtr:otherNames>Balntas V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5a995aaa2dd5e3.62094834</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C94118BD-319A-4442-9EE5-D031CDE22185</gtr:id><gtr:title>Side Information in Robust Principal Component Analysis: Algorithms and Applications</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f1b55240b7ea5f0a22dcf53376ebea96"><gtr:id>f1b55240b7ea5f0a22dcf53376ebea96</gtr:id><gtr:otherNames>Xue N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b69138dc96.63614111</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>552DE847-3457-4C10-8EDA-FD72D2F0B440</gtr:id><gtr:title>Deep Face Deblurring</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b68f51d652.59988761</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>431A03FD-2B71-4FBF-8735-95DC4B019A38</gtr:id><gtr:title>Convolutional Fusion Network for Face Verification in the Wild</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3181b5eb6380981af49ac1282a1054d0"><gtr:id>3181b5eb6380981af49ac1282a1054d0</gtr:id><gtr:otherNames>Xiong C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58adad0a7c9611.80068906</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ECCCDB12-8F4F-4581-B161-BE96DDC06163</gtr:id><gtr:title>Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept Detection.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/496b72b2cb5bb2cb524e10db471777b2"><gtr:id>496b72b2cb5bb2cb524e10db471777b2</gtr:id><gtr:otherNames>Koniusz P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5aaa38c3ec54e2.98230044</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D003806-F3B6-421E-8EBF-2652ED5F7D8C</gtr:id><gtr:title>Nonnegative Decompositions for Dynamic Visual Data Analysis.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5aa9b690e302b0.48328098</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92395CCF-001B-46C1-B7D1-8146F18DDB8C</gtr:id><gtr:title>AgeDB: The First Manually Collected, In-the-Wild Age Database</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d200efb191dd6360e4318a4a47f8f2cd"><gtr:id>d200efb191dd6360e4318a4a47f8f2cd</gtr:id><gtr:otherNames>Moschoglou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b68fe3c2e1.47962035</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>71F0834E-2592-4438-920B-2BB44F0F7FB8</gtr:id><gtr:title>Deformable Models of Ears in-the-Wild for Alignment and Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07c2a4af9f42a88efefb696767d62dd9"><gtr:id>07c2a4af9f42a88efefb696767d62dd9</gtr:id><gtr:otherNames>Zhou Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b690624ea6.17049349</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2B5C41FF-1E41-40C9-8B95-A70EA356E30B</gtr:id><gtr:title>A decision cognizant Kullback-Leibler divergence</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35cb5f3deb43c2c9b15bee9cfbd8e27e"><gtr:id>35cb5f3deb43c2c9b15bee9cfbd8e27e</gtr:id><gtr:otherNames>Ponti M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58a5d8548b1754.28928207</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>514DED3E-7766-4994-9C71-2CF97D16B7FD</gtr:id><gtr:title>Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9d0ebde50bca150a6f99409eae374c48"><gtr:id>9d0ebde50bca150a6f99409eae374c48</gtr:id><gtr:otherNames>Bahri M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b6916ae887.83656929</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>47799D99-33CA-44E3-B48C-0CCAB303B670</gtr:id><gtr:title>Extending non-negative matrix factorisation to 3D registered data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/668ff49876796aaabb07c6c0e1f6a907"><gtr:id>668ff49876796aaabb07c6c0e1f6a907</gtr:id><gtr:otherNames>Koppen W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a5dbe29473f3.76700711</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ADE49282-F9AD-4646-B904-82AF0EB4D82A</gtr:id><gtr:title>MAGMA: Multilevel Accelerated Gradient Mirror Descent Algorithm for Large-Scale Convex Composite Minimization</gtr:title><gtr:parentPublicationTitle>SIAM Journal on Imaging Sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b4f9e06acd7efdcf1b96f63ba15f0368"><gtr:id>b4f9e06acd7efdcf1b96f63ba15f0368</gtr:id><gtr:otherNames>Hovhannisyan V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d44a4b2dd71.06532442</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0AC2968E-09DE-4C25-843B-87E6F3582756</gtr:id><gtr:title>Face Normals &amp;quot;In-the-Wild&amp;quot; Using Fully Convolutional Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1efa9a163072268cf3c2b3988f9e1d45"><gtr:id>1efa9a163072268cf3c2b3988f9e1d45</gtr:id><gtr:otherNames>Trigeorgis G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b6918e8724.98419466</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39ACE05F-BD19-4C34-BAAC-9A68D35D7A12</gtr:id><gtr:title>The Menpo Facial Landmark Localisation Challenge: A Step Towards the Solution</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b68faee871.59886449</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3A7FC77B-34E8-4446-B372-039D737A9180</gtr:id><gtr:title>Learning the Multilinear Structure of Visual Data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/737ffe0e530a693d7c0da01892f1528d"><gtr:id>737ffe0e530a693d7c0da01892f1528d</gtr:id><gtr:otherNames>Wang M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b691e4db84.68920001</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>48617226-11A3-4C8D-AA16-20E1CE8826F7</gtr:id><gtr:title>The 3D Menpo Facial Landmark Tracking Challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b691136bb6.54209358</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F22F3655-5E4C-4550-9B5F-7107E1643043</gtr:id><gtr:title>A Joint Discriminative Generative Model for Deformable Model Construction and Classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a8100cb550cfc8870c1dbf69c343419"><gtr:id>2a8100cb550cfc8870c1dbf69c343419</gtr:id><gtr:otherNames>Marras I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa9b690be3942.25956262</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E59140AF-5775-41C4-9CF9-604A6334B829</gtr:id><gtr:title>Efficient 3D morphable face model fitting</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/693f56a722a81938c30ffa263a2ad82a"><gtr:id>693f56a722a81938c30ffa263a2ad82a</gtr:id><gtr:otherNames>Hu G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9df832aed9b2.35555844</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B54763C-BA45-4B13-A769-20F411522085</gtr:id><gtr:title>Binary Online Learned Descriptors</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/931443eb7388e9495a76a89eb97c303e"><gtr:id>931443eb7388e9495a76a89eb97c303e</gtr:id><gtr:otherNames>V. Baltnas</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c957a1afe283.62667398</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>337E6D7D-E7C9-4312-B09B-BEC87FE94A6A</gtr:id><gtr:title>Estimating Correspondences of Deformable Objects &amp;quot;In-the-Wild&amp;quot;</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07c2a4af9f42a88efefb696767d62dd9"><gtr:id>07c2a4af9f42a88efefb696767d62dd9</gtr:id><gtr:otherNames>Zhou Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c69fee528880.38135270</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>66190744-F510-4F81-A7F2-3433F6C8F95B</gtr:id><gtr:title>An anomaly detection approach to face spoofing detection: A new formulation and evaluation protocol</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d92cf96059fcb25b802ebc091336e554"><gtr:id>d92cf96059fcb25b802ebc091336e554</gtr:id><gtr:otherNames>Arashloo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfc3622ca82.53052972</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N007743/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>