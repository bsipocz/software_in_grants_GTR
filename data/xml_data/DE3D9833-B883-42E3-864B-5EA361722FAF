<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/DD18D9A8-0FE2-4857-A405-7F41C183F65D"><gtr:id>DD18D9A8-0FE2-4857-A405-7F41C183F65D</gtr:id><gtr:name>Southampton Solent University</gtr:name><gtr:address><gtr:line1>East Park Terrace</gtr:line1><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO14 0YN</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0DD99372-D1BE-478B-B199-314F2729F479"><gtr:id>0DD99372-D1BE-478B-B199-314F2729F479</gtr:id><gtr:name>Centre for Maritime Society and Health</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/6DCA7B7A-0111-4A7C-95CC-DEAD58B89132"><gtr:id>6DCA7B7A-0111-4A7C-95CC-DEAD58B89132</gtr:id><gtr:name>University of Stockholm</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/6846E86E-057E-4F59-8C8C-E944E5C63C10"><gtr:id>6846E86E-057E-4F59-8C8C-E944E5C63C10</gtr:id><gtr:name>Dalian Maritime University</gtr:name><gtr:address><gtr:line1>1 Ling Hai Road</gtr:line1><gtr:postCode>116026</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/DD18D9A8-0FE2-4857-A405-7F41C183F65D"><gtr:id>DD18D9A8-0FE2-4857-A405-7F41C183F65D</gtr:id><gtr:name>Southampton Solent University</gtr:name><gtr:address><gtr:line1>East Park Terrace</gtr:line1><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO14 0YN</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0DD99372-D1BE-478B-B199-314F2729F479"><gtr:id>0DD99372-D1BE-478B-B199-314F2729F479</gtr:id><gtr:name>Centre for Maritime Society and Health</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6DCA7B7A-0111-4A7C-95CC-DEAD58B89132"><gtr:id>6DCA7B7A-0111-4A7C-95CC-DEAD58B89132</gtr:id><gtr:name>University of Stockholm</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6846E86E-057E-4F59-8C8C-E944E5C63C10"><gtr:id>6846E86E-057E-4F59-8C8C-E944E5C63C10</gtr:id><gtr:name>Dalian Maritime University</gtr:name><gtr:address><gtr:line1>1 Ling Hai Road</gtr:line1><gtr:postCode>116026</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B048C6BF-852E-46CE-834F-438500A6CA35"><gtr:id>B048C6BF-852E-46CE-834F-438500A6CA35</gtr:id><gtr:firstName>Kyle</gtr:firstName><gtr:surname>Cave</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/371B9849-1651-45DB-A06D-C74DE223DB26"><gtr:id>371B9849-1651-45DB-A06D-C74DE223DB26</gtr:id><gtr:firstName>Nicolas S</gtr:firstName><gtr:surname>Holliman</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0154872D-9237-444D-A0C5-D89105F335A5"><gtr:id>0154872D-9237-444D-A0C5-D89105F335A5</gtr:id><gtr:firstName>Nick</gtr:firstName><gtr:surname>Donnelly</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/9FD5E66C-DE3F-4DDA-A832-5B86042894FD"><gtr:id>9FD5E66C-DE3F-4DDA-A832-5B86042894FD</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:otherNames>Paul</gtr:otherNames><gtr:surname>Liversedge</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/48402650-D979-4F5A-9BB4-158FA49A4B26"><gtr:id>48402650-D979-4F5A-9BB4-158FA49A4B26</gtr:id><gtr:firstName>Hayward</gtr:firstName><gtr:otherNames>James</gtr:otherNames><gtr:surname>Godwin</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0A63EE67-E9AA-4703-9E89-8530DAB4505C"><gtr:id>0A63EE67-E9AA-4703-9E89-8530DAB4505C</gtr:id><gtr:firstName>Hazel</gtr:firstName><gtr:otherNames>Isobel</gtr:otherNames><gtr:surname>Blythe</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/1C94E7C2-3FD3-4535-B98D-9DD81F65A696"><gtr:id>1C94E7C2-3FD3-4535-B98D-9DD81F65A696</gtr:id><gtr:firstName>Tamaryn</gtr:firstName><gtr:otherNames>Stable</gtr:otherNames><gtr:surname>Menneer</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FI032398%2F1"><gtr:id>DE3D9833-B883-42E3-864B-5EA361722FAF</gtr:id><gtr:title>Teaching the visual system to segment and interpret images of overlapping transparent objects</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/I032398/1</gtr:grantReference><gtr:abstractText>&lt;p>A considerable amount of research has examined the problems associated with searching for multiple target objects, with a particular focus on X-ray baggage screening, where security personnel must search for a number of different objects simultaneously.&lt;/p>

&lt;p>The goal of this project is to extend the previous research that has been conducted by examining the complexities associated with X-ray images, such as transparency and overlap. Security search for threat items (weapons) within X-ray images of baggage presents a challenge for the visual system because X-ray images do not conform to the conventional rules of solidity, interposition and opacity. In X-ray images, transparency causes colour changes at points of overlap. The colours representing each object interact at areas of overlap in a way that is largely unfamiliar to the visual system.&lt;/p>

&lt;p>In this project, a series of experiments will be conducted using eye-tracking methodology to understand and explore the complexities involved in searching displays of this type. Furthermore, the displays will be presented in both 2D and 3D, to determine whether any difficulties associated with searching transparent overlapping images can be overcome when the objects are separated into different layers of depth.&lt;/p></gtr:abstractText><gtr:fund><gtr:end>2016-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2011-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>552196</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Stockholm University</gtr:collaboratingOrganisation><gtr:country>Sweden, Kingdom of</gtr:country><gtr:department>Stress Research Institute</gtr:department><gtr:description>MARTHA Project</gtr:description><gtr:id>C97BB18C-1116-466A-9AA8-35929E4DED74</gtr:id><gtr:impact>The project is influencing meetings at IMO to develop standards for Fatigue Risk Management Systems. IMO - the International Maritime Organization - is the United Nations specialized agency with responsibility for the safety and security of shipping. Some shipping companies are involved in the project - providing access to crews for the study - and others are paying attention to the results. Fatigue contributes to accidents. Accidents have an economic cost. 

The project is multidisciplinary, with researchers working from Psychology, Ship Science, Engineering, as well as the Maritime sector.

Publication:

Jepsen, J. R., Zhao, Z., &amp;amp; van Leeuwen, W. M. (2015). Seafarer fatigue: a review of risk factors, consequences for seafarers' health and safety and options for mitigation. International maritime health, 66(2), 106-117.</gtr:impact><gtr:outcomeId>56e060f7cf4af0.76278503-2</gtr:outcomeId><gtr:partnerContribution>The project is led by Southampton Solent University. Dalian University and the Centre for Maritime Society and Health ran the on-board study. The Stress Research Institute analysed weekly diary and survey results of the on-board study. Southampton Solent analysed actigraphy results of physiological measures of sleep. SMMI analysed the relationship of this to sea state.</gtr:partnerContribution><gtr:piContribution>The core of the project is about monitoring seafarer fatigue, well-being, and subjective performance during long sea-faring journeys (up to 14 weeks). Our group has contributed to design and analysis decisions for that research. Our larger contribution, however, is in conducting laboratory research about the effects of sleep loss on visual search and distraction. We, together with two postgraduate students recruited through the project, have been collecting data about that.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Dalian Maritime University</gtr:collaboratingOrganisation><gtr:country>China, People's Republic of</gtr:country><gtr:description>MARTHA Project</gtr:description><gtr:id>A223DF8D-EA7F-4902-8EFB-77C7BAE532AC</gtr:id><gtr:impact>The project is influencing meetings at IMO to develop standards for Fatigue Risk Management Systems. IMO - the International Maritime Organization - is the United Nations specialized agency with responsibility for the safety and security of shipping. Some shipping companies are involved in the project - providing access to crews for the study - and others are paying attention to the results. Fatigue contributes to accidents. Accidents have an economic cost. 

The project is multidisciplinary, with researchers working from Psychology, Ship Science, Engineering, as well as the Maritime sector.

Publication:

Jepsen, J. R., Zhao, Z., &amp;amp; van Leeuwen, W. M. (2015). Seafarer fatigue: a review of risk factors, consequences for seafarers' health and safety and options for mitigation. International maritime health, 66(2), 106-117.</gtr:impact><gtr:outcomeId>56e060f7cf4af0.76278503-4</gtr:outcomeId><gtr:partnerContribution>The project is led by Southampton Solent University. Dalian University and the Centre for Maritime Society and Health ran the on-board study. The Stress Research Institute analysed weekly diary and survey results of the on-board study. Southampton Solent analysed actigraphy results of physiological measures of sleep. SMMI analysed the relationship of this to sea state.</gtr:partnerContribution><gtr:piContribution>The core of the project is about monitoring seafarer fatigue, well-being, and subjective performance during long sea-faring journeys (up to 14 weeks). Our group has contributed to design and analysis decisions for that research. Our larger contribution, however, is in conducting laboratory research about the effects of sleep loss on visual search and distraction. We, together with two postgraduate students recruited through the project, have been collecting data about that.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Southampton Solent University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>MARTHA Project</gtr:description><gtr:id>467ABE78-C210-4E8E-9846-98B31E4023A0</gtr:id><gtr:impact>The project is influencing meetings at IMO to develop standards for Fatigue Risk Management Systems. IMO - the International Maritime Organization - is the United Nations specialized agency with responsibility for the safety and security of shipping. Some shipping companies are involved in the project - providing access to crews for the study - and others are paying attention to the results. Fatigue contributes to accidents. Accidents have an economic cost. 

The project is multidisciplinary, with researchers working from Psychology, Ship Science, Engineering, as well as the Maritime sector.

Publication:

Jepsen, J. R., Zhao, Z., &amp;amp; van Leeuwen, W. M. (2015). Seafarer fatigue: a review of risk factors, consequences for seafarers' health and safety and options for mitigation. International maritime health, 66(2), 106-117.</gtr:impact><gtr:outcomeId>56e060f7cf4af0.76278503-1</gtr:outcomeId><gtr:partnerContribution>The project is led by Southampton Solent University. Dalian University and the Centre for Maritime Society and Health ran the on-board study. The Stress Research Institute analysed weekly diary and survey results of the on-board study. Southampton Solent analysed actigraphy results of physiological measures of sleep. SMMI analysed the relationship of this to sea state.</gtr:partnerContribution><gtr:piContribution>The core of the project is about monitoring seafarer fatigue, well-being, and subjective performance during long sea-faring journeys (up to 14 weeks). Our group has contributed to design and analysis decisions for that research. Our larger contribution, however, is in conducting laboratory research about the effects of sleep loss on visual search and distraction. We, together with two postgraduate students recruited through the project, have been collecting data about that.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Centre for Maritime Society and Health</gtr:collaboratingOrganisation><gtr:country>Denmark, Kingdom of</gtr:country><gtr:description>MARTHA Project</gtr:description><gtr:id>D433CE2C-8902-4A1D-A86A-FBDBBF9FE6A0</gtr:id><gtr:impact>The project is influencing meetings at IMO to develop standards for Fatigue Risk Management Systems. IMO - the International Maritime Organization - is the United Nations specialized agency with responsibility for the safety and security of shipping. Some shipping companies are involved in the project - providing access to crews for the study - and others are paying attention to the results. Fatigue contributes to accidents. Accidents have an economic cost. 

The project is multidisciplinary, with researchers working from Psychology, Ship Science, Engineering, as well as the Maritime sector.

Publication:

Jepsen, J. R., Zhao, Z., &amp;amp; van Leeuwen, W. M. (2015). Seafarer fatigue: a review of risk factors, consequences for seafarers' health and safety and options for mitigation. International maritime health, 66(2), 106-117.</gtr:impact><gtr:outcomeId>56e060f7cf4af0.76278503-3</gtr:outcomeId><gtr:partnerContribution>The project is led by Southampton Solent University. Dalian University and the Centre for Maritime Society and Health ran the on-board study. The Stress Research Institute analysed weekly diary and survey results of the on-board study. Southampton Solent analysed actigraphy results of physiological measures of sleep. SMMI analysed the relationship of this to sea state.</gtr:partnerContribution><gtr:piContribution>The core of the project is about monitoring seafarer fatigue, well-being, and subjective performance during long sea-faring journeys (up to 14 weeks). Our group has contributed to design and analysis decisions for that research. Our larger contribution, however, is in conducting laboratory research about the effects of sleep loss on visual search and distraction. We, together with two postgraduate students recruited through the project, have been collecting data about that.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Workshop about Introducing Centralised Image Processing into Airports</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>525719AE-250F-4E62-BCBE-E8B3D6D2536A</gtr:id><gtr:impact>Approximately 100 security managers at airports considering adopting a new practice are being invited to attend this workshop. The workshop will include presentations about experiences of airports that have adopted it, and will also include presentations by us about our evaluation of the effect of the new practice on security effectiveness, business efficiency, and satisfaction of employees and airport passengers.</gtr:impact><gtr:outcomeId>56e05fdfbdcbb9.76012207</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Ground Handling Engagement Talk</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>1CC05BFB-840C-457D-94D4-6B8BAD6B9A94</gtr:id><gtr:impact>Reached out to industry stakeholders, practicioners and members of regulatory bodies, informing them of how our previous and ongoing work can help to make their working practices safer in the future. This was an invited presentation.

Engaged in a number of follow-up discussions with members of the audience, and are seeking to further engage with them in the near future, with a view to starting new projects and seeking further funding.</gtr:impact><gtr:outcomeId>546364a0273909.30649150</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.groundhandling.com/reducinggrounddamage/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Stereoscopic Displays and Applications Conference</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5E649B98-FE24-4217-9480-CD7974273311</gtr:id><gtr:impact>Presentation at Stereoscopic Display conference. Note that this was primarily aimed at computer scientists and industry, therefore broadening the engagement and reach beyond the main target audience of experimental psychology researchers. See also: https://www.youtube.com/watch?v=IOjBc6TTXbQ</gtr:impact><gtr:outcomeId>56df3f09a012d0.42553141</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://www.stereoscopic.org/2015/program.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Meeting with industry/government officials regarding the use of 3D in airport baggage screening</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>859FE5F0-81AB-47DA-990E-B9B442423DC4</gtr:id><gtr:impact>While engaged in an ongoing project in a major UK airport, we took the opportunity to have a detailed discussion with representatives from a number of government and industry groups from the travel and transport security sector relating to the potential benefits from using 3D imaging in modern displays systems. Representatives were from the UK Department for Transport, the International Air Transport Association, and Transec. The emerging results from the studies resulted in a a detailed discussion regarding implementing the use of 3D display systems in both a training context as well as across UK airports.</gtr:impact><gtr:outcomeId>56e09860c8f917.36433951</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Policymakers/politicians</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>109749</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DHCSTC Contract</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>5D0732F6-E963-4D79-BF5E-548561FE64A8</gtr:id><gtr:outcomeId>5461ff27364aa2.65199713</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Interdisciplinary Bursary</gtr:description><gtr:end>2015-08-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>DB632FF7-8E3F-4123-819B-36D7531C2C8B</gtr:id><gtr:outcomeId>56e0227d9bf5c3.41375241</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>35000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DHCSTC contract</gtr:description><gtr:end>2016-06-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>F54852BF-52FA-4C0A-841F-E5783A836188</gtr:id><gtr:outcomeId>56d82113216836.10437340</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>56529</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DHCSTC Contract</gtr:description><gtr:end>2012-03-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:fundingRef>DSTLX-1000051523</gtr:fundingRef><gtr:id>1FAA7FE3-4F96-4824-B3F9-C01E07D1C0B5</gtr:id><gtr:outcomeId>5461fddf6391f0.08446709</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>103813</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DHCSTC Contract</gtr:description><gtr:end>2012-03-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:fundingRef>STLX1000062076</gtr:fundingRef><gtr:id>CAA8CDF7-F5E9-4948-A832-77339FD66421</gtr:id><gtr:outcomeId>5461fe57034f53.60621008</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>85502</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DSTL National PhD Scheme</gtr:description><gtr:end>2016-07-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>B26DA068-7F1E-47A2-A7EB-7B6BB91087C7</gtr:id><gtr:outcomeId>5461ffe9bcf3b3.34613808</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>141000</gtr:amountPounds><gtr:country>Bahamas, Commonwealth of the</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Visual Search and Fatigue in Maritime Environments</gtr:description><gtr:end>2019-09-02</gtr:end><gtr:fundingOrg>Teekay Foundation</gtr:fundingOrg><gtr:id>8F47A06E-FE65-45C4-93A7-EED7AFB455CE</gtr:id><gtr:outcomeId>56e023ee14dd17.94031883</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Interdisciplinary Bursary</gtr:description><gtr:end>2013-08-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>9FDF560F-C38B-4C93-9107-9FE1DBB64993</gtr:id><gtr:outcomeId>56e024e68cb536.13062943</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>36033</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>FASS Checkpoint Big Data Analysis</gtr:description><gtr:end>2017-01-02</gtr:end><gtr:fundingOrg>Department of Transport</gtr:fundingOrg><gtr:id>875C2E50-A507-4F52-80F1-998F17A73661</gtr:id><gtr:outcomeId>588f390b5548f1.40935677</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>101517</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DSTL National PhD Scheme</gtr:description><gtr:end>2016-07-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>465EEDC1-C63E-42FE-9E7E-57E090AE32EE</gtr:id><gtr:outcomeId>5461ffb128a7c8.11547424</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>78525</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>DHCSTC Contract</gtr:description><gtr:end>2015-03-02</gtr:end><gtr:fundingOrg>Defence Science &amp; Technology Laboratory (DSTL)</gtr:fundingOrg><gtr:id>5F0B6050-FA5E-4486-BDAF-E31330DD2C1A</gtr:id><gtr:outcomeId>5462006ded55a7.32781922</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2014-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>9000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Studying How Individuals Search Text for Information</gtr:description><gtr:end>2015-12-02</gtr:end><gtr:fundingOrg>University of Southampton</gtr:fundingOrg><gtr:id>F0B3EC97-A776-4B13-AB46-07A2F307E971</gtr:id><gtr:outcomeId>56e02368295226.78044077</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Interdisciplinary Bursary</gtr:description><gtr:end>2014-08-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>AABE8890-B48B-4C2B-A99C-A6750D5289D3</gtr:id><gtr:outcomeId>56e0243803fed7.64381854</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>8000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Visual search for threats in real-world scenes</gtr:description><gtr:end>2016-10-02</gtr:end><gtr:fundingOrg>University of Southampton</gtr:fundingOrg><gtr:id>ECE2D8E0-2E1E-40D3-BCB7-5B2E08F8244F</gtr:id><gtr:outcomeId>56e024a186d4a6.57156266</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Narrative Impact: Teaching the Visual System to Segment and Interpret Images of Overlapping Transparent Objects

The project directly assessed the benefits to visual search performance that arise from presenting overlapping transparent objects on multiple depth planes (i.e., in 3D). Our main focus in terms of impact was in relation to airport X-ray baggage screening, since X-ray screening involves the presentation of overlapping transparent images that are highly cluttered. Indeed, the fact that the images in X-ray screening overlap one another presents a substantial problem to screeners: they must rapidly examine and identify multiple complex objects in order to determine whether a bag is safe to be allowed onto an aircraft. We found that presenting overlapping transparent images in 3D to searchers improved their search performance, particularly when there was a high level of overlap and clutter in the images. Although potential benefits from searching in 3D have been studied in various domains, our work is the first to study 3D search of overlapping transparent images in this manner. It therefore stands as an important foundational step in providing an evidence base to support the value of 3D imaging technologies in a range of real-world tasks.

Strand A: Maximising and Extending Impact with Government/Industry Groups

The main findings of the project have been communicated to both industry groups (International Air Transport Association [IATA: primarily via Guido Peetermans, Project Manager of Smart Security], Egremont Group) and government (UK Department for Transport [DfT: primarily via James McDonald, Head of Threat, Risk &amp;amp; Innovation Policy, Aviation Security] Defence Sciences Technology Laboratory [dstl, primarily via Dr Katherine Cornes, Capability Advisor and Principal Cognitive Psychologist]). Through presenting work at a meeting organised by IATA/DfT/Manchester Airport, we were also able to present our findings to 60+ airports from across Europe and beyond.

We had originally envisaged that our work might influence training regimes for screeners and this may still be the case as we have shown it could work in principle. As we have only just reported this data, we have not yet explored potential impacts for this instantiation of our work. It remains a possible future impact for this work, though we have been informed and have confirmed with contacts at DfT that using 3D in this manner would be a viable route for the future.

Reporting our current findings in relation to 3D search has had one major impact. During the course of this project, 3D visualisation technology has moved forward at a rapid pace. As a result we became aware that there is a drive towards using 3D visualisation that allow screeners to manipulate images. This is an advance from the technology in place when we began our studies. At present there is no behavioural data available to assess the impact of this technology on threat detection. We have been encouraged by contacts within DfT to submit a funding application to generate empirical evidence regarding the efficacy of such systems for enhancing threat detection. The bid takes as its starting point our demonstration of the conditions under which 3D can aid search before developing new lines of enquiry, and is currently under submission (as part of cross-governmental Future Aviation Security Solutions programme, funded by DfT, Centre for Defence Enterprise, dstl, and the Home Office).

In sum, the process of reporting and communicating our finding to stakeholders has shown new ways of evaluating search in the context of 3D screening. We are now considered by DfT as a key partner to evaluate this emerging 3D technology and consider it very likely that this work will form an impact case study in REF2020.

Strand B: Applying Fundamental Cognitive Psychology Research to Airport X-ray Baggage Screening

At the core of the project was the question of whether presenting transparent overlapping objects in 3D could benefit search performance. As noted above, this is a key component of why airport baggage screening is a difficult task to complete. But there are other, related issues that likely also reduce screening performance that we have examined during the course of this project. These additional experiments and published papers were directly inspired and conducted in parallel to the research conducted as part of this project. As such, they form a related and important body of work involving searching for multiple targets (as is the case in airport screening, where screeners search for guns, knives, and explosives), searching for targets that appear rarely (as is the case in airport screening: since rarely-appearing targets are often missed), as well as modelling the fundamental aspects of eye movement control during visual search. Put simply, understanding the 'bigger picture', we believe, is important in maximising performance gains in airport screening, and in order to understand that 'bigger picture', we must first construct an empirically-grounded understanding of human behaviour when searching in real-world tasks such as airport baggage screening. It is our perspective that any impact or recommendations made to government/industry groups must be based on a solid foundation of fundamental cognitive psychology research. For that reason, we have taken the opportunity to publish these findings, and present them at international conferences, and then distilled the key information that would be of value to government/industry groups when meeting with them.

By building and maintaining links with these government and industry groups, and in light of our acknowledged expertise on behavioural issues in screening, we have been able to extend our work and explore 'big data' approaches to analysing screening datasets that are routinely collected from millions of screening episodes at multiple airports. From data collected in situ and analysed with respect to issues we have shown to be important from our experimental work, we were able to obtain this data and analyse it to provide recommendations for changing current procedures and improving performance. This impact is ongoing we have now been awarded two projects funded by DfT (one funded by DfT and IATA) to develop these cognitive psychologically informed data analyses further, and we regularly update our funders, the regulators, and airport management with regards to the insights that our new approach in this manner has had.

The issues explored in this body of work have informed our analyses of the 'big data' sets garnered from multiple airports when conducting the studies outlined in Section A for IATA and DfT. With the help of an ESRC IAA award we have explored developing these principled analyses of 'big data' from airport screening into an analytical toolkit for monitoring and evaluating threat detection by screeners in the real world. 

With the advent of the new generation of 3D screening technologies noted in Section A, this toolkit will provide invaluable insights. Specifically, and in addition to other factors, these insights will include the ability to record the performance improvements that the new generation of imaging technologies may bring to threat detection by recording performance in situ. The impact case study for REF2020 noted in Section A will be strengthened by being able to conduct these analyses from data gathered from real screeners working in situ..

Strand C: Connecting with the 3D Visualisation Community

The use of stereoscopic 3D displays (as well as other technologies that present images in 3D) is not only developing rapidly, but also becoming more and more commonplace. We recognised when developing this project that any use of 3D, or recommendations to government/industry groups, would need to be grounded in the question of whether or not those recommendations could be put into practice. In other words, we not only wanted to make sure that 3D was of benefit to search performance, but also that those recommendations could be achieved with current hardware/software. Throughout the project we have ensured this was the case, while also directly engaging with the 3D visualisation community. This activity has primarily taken place via our expert in 3D visualisation and related technologies (Holliman). Indeed, at a recent international conference focusing on 3D visualisations and applications, the work conducted as part of this project was presented, thereby not only demonstrating the benefits of 3D to searching overlapping transparent displays, but also raising awareness in the visualisation community of the importance of connecting with cognitive psychology researchers in order to directly assess the benefits of any technology developments that they produce. We have maintained active links with the visualizations community in this regard, and indeed, as noted above, have recently applied for funding to further examine the benefits of examining modern, newly-developed forms of 3D visualisation that are likely to be used in airports in the next 5-10 years.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>861DD524-D536-4AEE-9239-7C8AD896073A</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Policy &amp; public services</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56e13b227aea34.59566165</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Government, Democracy and Justice,Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>1) Stereoscopic depth improves visual search through displays of transparent objects. 

When participants search displays of overlapping, transparent objects (transparent polygons and X-ray objects from airport baggage screening), the addition of stereoscopic depth profoundly improves visual search performance. In contrast, there is no effect of adding stereoscopic depth when searching displays of opaque objects (opaque polygons and household objects). Adding stereoscopic depth to displays of transparent object slows response times but improves search accuracy relative to when search is conducted on flat images. The increased accuracy and slowed response times are especially marked when objects overlap. The performance differences are underpinned by increased eye movement fixations on overlapping regions when displays are presented in stereoscopic depth as well as resulting in increased time to identify targets.


2) The benefit of stereoscopic depth to visual search performance in displays of transparent objects increases with training. 

The longevity of the benefit of stereoscopic depth to search in displays of overlapping, transparent objects was tested over multiple experimental sessions. To maximize ecological validity, participants searched for targets in images taken from airport baggage screening. Training increased the beneficial effect for search of adding stereoscopic depth (reported above). The addition of stereoscopic depth to displays of x-ray objects significantly enhances visual learning for cues relative to that for flat displays and this learning continues over extended training. Additionally, in a final session, participants searched flat displays to assess transfer effects from training with depth to search in 2D (current real-world practice). The critical finding is that the enhanced visual learning of those trained with stereoscopic displays transferred to 2D search resulting in improved search performance in flat displays.


3) Adding stereoscopic depth does not change the cost of searching for multiple targets. 

In all the experiments, search for single targets was compared with search for multiple targets. The cost of searching for more than one target relative to searching for single targets is pervasive and reflected in reduced accuracy and increased time to fixate and identify targets. These effects reflect the fact that multiple-target search occurs through increased load in the attention system. The increased times to fixate and identify targets found in multiple-target search are not moderated by stereoscopic depth. The important conclusion is that the beneficial effect of stereoscopic depth on search in displays of transparent objects does not happen by reducing load on the attention system. 


4) Methodological advances. 

The requirement to record the eye movement behaviour of participants as they searched stereoscopic images containing overlapping/transparent objects created very significant challenges (but ensured we obtained data reflecting moment-to-moment visual and cognitive processing). In order to expedite the processing and analysis of data, we developed a new open source software package (eyeTrackR) that facilitates the processing of eye movement data in the R statistical programming language. The code is freely available to all and we believe it will be beneficial to numerous researchers using eye movement methodology in visual search, serving to increase the efficiency and make more transparent data analyses.</gtr:description><gtr:exploitationPathways>Understanding Real-world Search Performance. 

We have identified several key factors that impede searching X-ray images of baggage, including searching displays containing overlapping objects, searching displays for multiple targets, searching through displays containing transparent overlapping objects, and searching for rare targets. 


Changes to Current Display Methods. 

We have assessed the role of stereoscopic depth as a tool for enhancing search performance in tasks such as airport baggage screening and found stereoscopic depth does benefit search. This benefit very probably holds in day-to-day practice. The conclusions also hold promise for improving understanding of urban data visualization, augmented medical visualization, visualization of complex datasets, etc.


Changes to Training Methods. 

Experience of searching 3D displays enhanced performance during training and during subsequent search with flat displays. This suggests that the use of stereoscopic depth during training could enhance performance of airport screening personnel even when their job is conducted on flat displays. 

Throughout we communicated our findings directly to stakeholders (e.g., IATA, Dstl, Egremont). These include Head of Threat, Risk and Innovation Policy at the Department for Transport. He is exploring introduction of 3D screening at UK airports. We are applying for an ESRC impact acceleration award to maximize the impact of our work.</gtr:exploitationPathways><gtr:id>8BB25FD4-D5AB-47BC-A073-713AFA30E305</gtr:id><gtr:outcomeId>56e13ab50f4d14.32501952</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Security and Diplomacy,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Dataset - behavioural and eye-movement - from the final experiment conducted as part of this project. Will be made available once a manuscript based upon this dataset is in press.</gtr:description><gtr:id>C9F34B58-C0F9-4DB7-A2D8-B0483C20E02F</gtr:id><gtr:impact>None as yet.</gtr:impact><gtr:outcomeId>56e062a9dd9d11.60036823</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Experiment 5 Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Model developed to predict eye-movement behavior during visual search.</gtr:description><gtr:id>CAB0ABEE-E0B0-449B-A104-0FEFB6373F88</gtr:id><gtr:impact>Article relating to the model is currently in press.</gtr:impact><gtr:outcomeId>56df39c74a1eb2.41214338</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>EVS</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>http://www.cognitivesciencesociety.org/</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>R scripts will be made available that enable researchers to re-process their eye-movement datasets. We developed these R scripts because the standard software that we use (SR Research DataViewer) produces incorrect output when using displays containing overlapping objects, as we did in the current project. To facilitate further research along these lines, we will make these scripts readily available to other researchers once manuscripts have been accepted for publication.</gtr:description><gtr:id>2820D878-D151-40D6-8DE8-E92245405DDA</gtr:id><gtr:impact>None as yet.</gtr:impact><gtr:outcomeId>56e0631b3144b5.12468203</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>R Scripts for Processing Eye Movement Data</gtr:title><gtr:type>Data handling &amp; control</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Datasets - behavioural and eye-movement for the first four experiments conducted as part of this project. Will be made openly available once manuscripts have been published.</gtr:description><gtr:id>176AB0DB-A80A-4C2B-A988-A2E9734DF2C1</gtr:id><gtr:impact>None as yet.</gtr:impact><gtr:outcomeId>56e0627400a1e2.86688553</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Experiments 1-4 Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>EyeTrackR consists of a set of R functions, bundled into a package, all geared towards analysing eye-tracking datasets.</gtr:description><gtr:id>635E8BAA-7A4B-4BC4-B5BB-E5249A0FC592</gtr:id><gtr:impact>Used by a growing number of researchers to process and analyse their eye-tracking datasets</gtr:impact><gtr:outcomeId>54636931946448.72192830</gtr:outcomeId><gtr:title>eyeTrackR</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/hjgodwin/eyeTrackR</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Software developed for the creation of stimuli for use in 3d visual search experiments.</gtr:description><gtr:id>FE656EF0-FFDE-4B4D-B14C-84541ED138A4</gtr:id><gtr:impact>Used to create stimuli for all experiments as part of this project.</gtr:impact><gtr:outcomeId>56df3ae16596e9.87797532</gtr:outcomeId><gtr:title>Search Commander</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>72AF3C24-0A4E-4B99-A78F-B279EDBC6294</gtr:id><gtr:title>Visual similarity is stronger than semantic similarity in guiding visual search for numbers.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>5461edb57c6230.32879997</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BCC61DFA-9D08-4020-BE5F-2FD392878A79</gtr:id><gtr:title>Using E-Z Reader to examine the concurrent development of eye-movement control and reading skill.</gtr:title><gtr:parentPublicationTitle>Developmental review : DR</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5130acf69a6f64bdc4d06ecf4c4886c5"><gtr:id>5130acf69a6f64bdc4d06ecf4c4886c5</gtr:id><gtr:otherNames>Reichle ED</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0273-2297</gtr:issn><gtr:outcomeId>585d52d28b6d97.59139106</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CE4E3662-C5BF-4E60-907A-9FD0C4DA1701</gtr:id><gtr:title>The influence of experience upon information-sampling and decision-making behaviour during risk assessment in military personnel</gtr:title><gtr:parentPublicationTitle>Visual Cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4890377ea091f326d4378900633570b4"><gtr:id>4890377ea091f326d4378900633570b4</gtr:id><gtr:otherNames>Godwin H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1350-6285</gtr:issn><gtr:outcomeId>56d81ef2e4dfa6.41528108</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A5081505-903D-483B-A4BE-636575C8160D</gtr:id><gtr:title>Search for two categories of target produces fewer fixations to target-color items.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Applied</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9b33353cf9b2d82732714d825d315da5"><gtr:id>9b33353cf9b2d82732714d825d315da5</gtr:id><gtr:otherNames>Menneer T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1076-898X</gtr:issn><gtr:outcomeId>pm_53cc01bc01b3c8de5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA4C8AE9-4A8D-440D-8A46-8A13AC0C21EF</gtr:id><gtr:title>Adding Depth to Overlapping Displays can Improve Visual Search Performance.</gtr:title><gtr:parentPublicationTitle>Journal of Experimental Psychology: Human Perception and Performance.</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1f00b22f93efa10b503f11a6485afbb"><gtr:id>d1f00b22f93efa10b503f11a6485afbb</gtr:id><gtr:otherNames>Godwin H J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>588e1ef9100b51.90232618</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>93406884-4BC8-4B0F-9640-121E9C0297EF</gtr:id><gtr:title>Categorical templates are more useful when features are consistent: Evidence from eye movements during search for societally important vehicles.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/789309457b2e24c7a0552bf51ff445c9"><gtr:id>789309457b2e24c7a0552bf51ff445c9</gtr:id><gtr:otherNames>Hout MC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>5aa7b4865c9597.14467965</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2C407F9-1D02-484C-8178-DB2725EC6E81</gtr:id><gtr:title>Individual Differences in Search and Monitoring for Color Targets in Dynamic Visual Displays.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Applied</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/474a4d496da9acb63d474b626c90d331"><gtr:id>474a4d496da9acb63d474b626c90d331</gtr:id><gtr:otherNames>Muhl-Richardson A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>1076-898X</gtr:issn><gtr:outcomeId>5aa7b4c3390be4.92921806</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>49204FD7-B029-4357-8923-A63FEBDFB408</gtr:id><gtr:title>Using multidimensional scaling to quantify similarity in visual search and beyond.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/789309457b2e24c7a0552bf51ff445c9"><gtr:id>789309457b2e24c7a0552bf51ff445c9</gtr:id><gtr:otherNames>Hout MC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>56d81f6f780df9.37332358</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19FC84E0-E1C4-45DA-870C-FA2CA31F4AD0</gtr:id><gtr:title>Modeling Lag-2 Revisits to Understand Trade-Offs in Mixed Control of Fixation Termination During Visual Search.</gtr:title><gtr:parentPublicationTitle>Cognitive science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0364-0213</gtr:issn><gtr:outcomeId>56df2750bb6403.10384121</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5FA02B41-7A2E-44D2-A775-C633B4B453EC</gtr:id><gtr:title>The effect of high- and low-frequency previews and sentential fit on word skipping during reading.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Learning, memory, and cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/265842e4c9f700bd4313c6ecdde91df0"><gtr:id>265842e4c9f700bd4313c6ecdde91df0</gtr:id><gtr:otherNames>Angele B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0278-7393</gtr:issn><gtr:outcomeId>585d53d722efa7.92255958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D327764-16AB-4ECA-88F6-11F55EA5815C</gtr:id><gtr:title>Adding depth to overlapping displays can improve visual search performance.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5a2fca88e1c099.39699993</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>965F651E-6C46-4A03-9EB5-F010761F4702</gtr:id><gtr:title>Cat and mouse search: the influence of scene and object analysis on eye movements when targets change locations during search.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42e224814e4a76ec9b1991172546e49f"><gtr:id>42e224814e4a76ec9b1991172546e49f</gtr:id><gtr:otherNames>Hillstrom AP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>588e1fe7ae0e70.70425124</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>29D49806-6D49-42CE-A12A-CA833643639C</gtr:id><gtr:title>Eye movements during visual search for emotional faces in individuals with chronic headache.</gtr:title><gtr:parentPublicationTitle>European journal of pain (London, England)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/708e0572dd6c0e7cb214831ab3ae24f7"><gtr:id>708e0572dd6c0e7cb214831ab3ae24f7</gtr:id><gtr:otherNames>Schoth DE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1090-3801</gtr:issn><gtr:outcomeId>56df26ea8707e1.91520826</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>18010078-912E-400F-B227-2C86537BBD75</gtr:id><gtr:title>Parafoveal preview benefit in unspaced and spaced Chinese reading.</gtr:title><gtr:parentPublicationTitle>Quarterly journal of experimental psychology (2006)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8dd06aba8ba734c9caa61f1ea65fe749"><gtr:id>8dd06aba8ba734c9caa61f1ea65fe749</gtr:id><gtr:otherNames>Cui L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1747-0218</gtr:issn><gtr:outcomeId>5675da42e1f82</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>45E2D29E-CB57-4EF0-AD2E-AD1315361CF3</gtr:id><gtr:title>Reading transposed text: effects of transposed letter distance and consonant-vowel status on eye movements.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c256d672ad87b6af4bd714352695daba"><gtr:id>c256d672ad87b6af4bd714352695daba</gtr:id><gtr:otherNames>Blythe HI</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>5675db0b3d84f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38BBE2BB-9164-4ADC-BB16-3E130489FFD1</gtr:id><gtr:title>Using interrupted visual displays to explore the capacity, time course, and format of fixation plans during visual search.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5461edb55533f0.26413235</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9C772498-AD1E-423A-BA64-33412E3B7D70</gtr:id><gtr:title>Understanding the contribution of target repetition and target expectation to the emergence of the prevalence effect in visual search.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>56df26ea671220.63452007</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BEA3F21A-A567-4C7E-A72C-C2D159F46A42</gtr:id><gtr:title>Using the dual-target cost to explore the nature of search target representations.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35795d2d18d5561668fc635dd56e6813"><gtr:id>35795d2d18d5561668fc635dd56e6813</gtr:id><gtr:otherNames>Stroud MJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>56d81f6fa7d4c6.60329671</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53C9CD50-30DB-45DE-9ED4-8E4B2718E9C1</gtr:id><gtr:title>Visual similarity is stronger than semantic similarity in guiding visual search for numbers</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9b33353cf9b2d82732714d825d315da5"><gtr:id>9b33353cf9b2d82732714d825d315da5</gtr:id><gtr:otherNames>Menneer T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546366d5df5e70.51578998</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3654225F-785D-49F9-AE47-231C488C7280</gtr:id><gtr:title>Evaluating the FVF framework and applying it to target prevalence effects</gtr:title><gtr:parentPublicationTitle>Behavioral and Brain Sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9b33353cf9b2d82732714d825d315da5"><gtr:id>9b33353cf9b2d82732714d825d315da5</gtr:id><gtr:otherNames>Menneer T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56df27b687bb84.01918468</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>65423652-2E8B-4D48-A47D-D20F5633B4FA</gtr:id><gtr:title>The importance of search strategy for finding targets in open terrain.</gtr:title><gtr:parentPublicationTitle>Cognitive Research: Principles and Implications.</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e903298e05821742728508cc6df6785"><gtr:id>2e903298e05821742728508cc6df6785</gtr:id><gtr:otherNames>Riggs C A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>588e1fc46fe0d9.47957596</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D79B72D2-5790-457A-8551-DAD8CF20CB30</gtr:id><gtr:title>Coarse-to-fine eye movement behavior during visual search.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>5461edb5a4b1b1.20598575</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>83994045-D04A-4619-8A5E-8EC7F76110D7</gtr:id><gtr:title>Dual-Target Cost in Visual Search for Multiple Unfamiliar Faces</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27c5a24f5909d345bf9856eb8250afb5"><gtr:id>27c5a24f5909d345bf9856eb8250afb5</gtr:id><gtr:otherNames>Mestry N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df30af87ac65.87026746</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C55DA577-2B15-4E2C-8742-DFF30B77EBD2</gtr:id><gtr:title>Faster than the speed of rejection: Object identification processes during visual search for multiple targets</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25b6d102df0d07e0d2178e356aed3cc2"><gtr:id>25b6d102df0d07e0d2178e356aed3cc2</gtr:id><gtr:otherNames>Hout M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56df30af5d8c80.05163984</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E699E9CA-6B2E-4336-80C3-534F38DF079B</gtr:id><gtr:title>Weakened Target Representations in Low Prevalence Visual Search</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4890377ea091f326d4378900633570b4"><gtr:id>4890377ea091f326d4378900633570b4</gtr:id><gtr:otherNames>Godwin H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546368ad68d2c2.91430546</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D885D273-54E0-4CBF-8A33-23EA7A956FB8</gtr:id><gtr:title>Low target prevalence weakens target templates during visual search.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/50156040481449c6639127b57727515e"><gtr:id>50156040481449c6639127b57727515e</gtr:id><gtr:otherNames>Godwin, H. J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463681cdbfee4.96939893</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EC96D450-2774-4914-80DE-D48E1368D902</gtr:id><gtr:title>Flexible Configural Learning of Non-Linear Discriminations and Detection of Stimulus Compounds.</gtr:title><gtr:parentPublicationTitle>Experimental psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/121463e819e0ccb4ce1deb646a4ce861"><gtr:id>121463e819e0ccb4ce1deb646a4ce861</gtr:id><gtr:otherNames>Glautier S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1618-3169</gtr:issn><gtr:outcomeId>588e201a9b3259.38255371</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E1ADEF7C-ABD4-438B-9122-C6CF119433FA</gtr:id><gtr:title>The effects of increasing target prevalence on information processing during visual search.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>5461edb52b8e27.71225360</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9C960D09-9229-44BA-80F0-05E09AC53AA6</gtr:id><gtr:title>Using eye movements to investigate selective attention in chronic daily headache.</gtr:title><gtr:parentPublicationTitle>Pain</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/864e881fd9627c39826037684a0bcbd4"><gtr:id>864e881fd9627c39826037684a0bcbd4</gtr:id><gtr:otherNames>Liossi C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0304-3959</gtr:issn><gtr:outcomeId>56df26eaa7b596.79681420</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>97E6BBFE-5933-44AE-9EB7-3CC48BD716AF</gtr:id><gtr:title>Modeling Lag-2 Revisits to Understand Trade-Offs in Mixed Control of Fixation Termination During Visual Search.</gtr:title><gtr:parentPublicationTitle>Cognitive science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0364-0213</gtr:issn><gtr:outcomeId>588e1f39026927.25379263</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0DE17259-2FE5-4198-888C-F99DC0F697B1</gtr:id><gtr:title>Searching for overlapping objects in depth: Depth speeds search, but does not improve response accuracy</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4890377ea091f326d4378900633570b4"><gtr:id>4890377ea091f326d4378900633570b4</gtr:id><gtr:otherNames>Godwin H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463663408e893.27901499</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7026896D-4B4C-48D5-A67A-D5F31125F6F2</gtr:id><gtr:title>Dual-target cost in visual search for multiple unfamiliar faces.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27c5a24f5909d345bf9856eb8250afb5"><gtr:id>27c5a24f5909d345bf9856eb8250afb5</gtr:id><gtr:otherNames>Mestry N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>588e1f790e9bc8.43966710</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0CC383B6-AF7F-4236-BF84-936FEB3D2D0A</gtr:id><gtr:title>Binocular coordination: reading stereoscopic sentences in depth.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/89ccce891e1f0db2662da120f60208a1"><gtr:id>89ccce891e1f0db2662da120f60208a1</gtr:id><gtr:otherNames>Schotter ER</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a350e338452e9.46147462</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>01265FAF-E82A-49BF-A5FD-3C41FB8D404A</gtr:id><gtr:title>Faster than the speed of rejection: Object identification processes during visual search for multiple targets.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>56d81ef2b3d273.00081358</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EC03E38C-9C7B-469E-9384-97FCECC3C049</gtr:id><gtr:title>Visual Search for Transparent Overlapping Objects in Depth: Overlap Impairs Performance, but Depth does not benefit Performance</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4890377ea091f326d4378900633570b4"><gtr:id>4890377ea091f326d4378900633570b4</gtr:id><gtr:otherNames>Godwin H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd81230557a2.62939425</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A43C5217-05FE-45C7-A7EC-E7AB8B47588A</gtr:id><gtr:title>Interword spacing and landing position effects during Chinese reading in children and adults.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/094e6a3d08f9cdb8cfc8964f05369d78"><gtr:id>094e6a3d08f9cdb8cfc8964f05369d78</gtr:id><gtr:otherNames>Zang C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5a350cfc7b1685.37052151</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>12087046-B0D2-4F54-9CB3-9A0586368D66</gtr:id><gtr:title>Perceptual failures in the selection and identification of low-prevalence targets in relative prevalence visual search.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d92d5aefef624a3892b9f1e912608dc"><gtr:id>2d92d5aefef624a3892b9f1e912608dc</gtr:id><gtr:otherNames>Godwin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>5461ed184b8a36.85949160</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/I032398/1</gtr:identifier><gtr:identifier type="RES">RES-062-23-3193</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>