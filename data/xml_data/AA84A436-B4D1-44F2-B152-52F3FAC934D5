<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A8025D32-2EFA-416A-B79C-BEFA6A379151"><gtr:id>A8025D32-2EFA-416A-B79C-BEFA6A379151</gtr:id><gtr:name>RWTH Aachen University</gtr:name><gtr:address><gtr:line1>Pallwelsstr. 30</gtr:line1><gtr:line4>Aachen</gtr:line4><gtr:line5>D-52056</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A8025D32-2EFA-416A-B79C-BEFA6A379151"><gtr:id>A8025D32-2EFA-416A-B79C-BEFA6A379151</gtr:id><gtr:name>RWTH Aachen University</gtr:name><gtr:address><gtr:line1>Pallwelsstr. 30</gtr:line1><gtr:line4>Aachen</gtr:line4><gtr:line5>D-52056</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/1CEC5A55-AE07-4A25-B172-EAAE8397BF64"><gtr:id>1CEC5A55-AE07-4A25-B172-EAAE8397BF64</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Bowden</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI011811%2F1"><gtr:id>AA84A436-B4D1-44F2-B152-52F3FAC934D5</gtr:id><gtr:title>Learning to Recognise Dynamic Visual Content from Broadcast Footage</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I011811/1</gtr:grantReference><gtr:abstractText>This research is in the area of computer vision - making computers which can understand what is happening in photographs and video. As humans we are fascinated by other humans, and capture endless images of their activities, for example home movies of our family on holiday, video of sports events or CCTV footage of people in a town center. A computer capable of understanding what people are doing in such images would be able to do many jobs for us, for example finding clips of our children waving, fast forwarding to a goal in a football game, or spotting when someone starts a fight in the street. For Deaf people, who use a language combining hand gestures with facial expression and body language, a computer which could visually understand their actions would allow them to communicate in their native language. While humans are very good at understanding what people are doing (and can learn to understand special actions such as sign language), this has proved extremely challenging for computers.Much work has tried to solve this problem, and works well in particular settings for example the computer can tell if a person is walking so long as they do it clearly and face to the side, or can understand a few sign language gestures as long as the signer cooperates and signs slowly. We will investigate better models for recognising activities by teaching the computer by showing it many example videos. To make sure our method works well for all kinds of setting we will use real world video from movies and TV. For each video we have to tell the computer what it represents, for example throwing a ball or a man hugging a woman . It would be expensive to collect and label lots of videos in this way, so instead we will extract approximate labels automatically from subtitle text and scripts which are available for TV. Our new methods will combine learning from lots of approximately labelled video (cheap because we get the labels automatically), use of contextual information such as which actions people do at the same time, or how one action leads to another ( he hits the man, who falls to the floor ), and computer vision methods for understanding the pose of a person (how they are standing), how they are moving, and the objects which they are using.By having lots of video to learn from, and methods for making use of approximate labels, we will be able to make stronger and more flexible models of human activities. This will lead to recognition methods which work better in the real world and contribute to applications such as interpreting sign language and automatically tagging video with its content.</gtr:abstractText><gtr:potentialImpactText>The proposed research has potential impact for three communities: 1. The computer vision and machine learning communities in academia 2. The potential end users of tools for automatic categorisation and searching of content such as organisations like the BBC 3. The Deaf community, by providing tools for automatic recognition of sign language The computer vision and machine learning communities will benefit from new knowledge and new techniques and the creation of new and challenging datasets for use by the wider research community. Dissemination to the research community will be via publishing in the major national and international conferences/journals and via a project website. The principal search and media companies have a significant presence at these conferences. Additionally the PIs will organise an international workshop in conjunction with one of the major international conferences to disseminate the outputs of the project and there is a possibility to run a technical meeting in association with the British Machine Vision Association (BMVA). There is potential to contribute to other scientific disciplines and one of our project partners, DCAL (see letter of support), have indicated that any tools for automatic recognition, annotation and search of sign language would be immensely beneficial to their research on Sign linguistics and the EPSC Corpus project. By automatically categorising, labelling and providing search facilities, the research has immense benefits to the broadcast and media industry. Much of the BBC's digital video archive material from the 1970s and 1908s has only the name of the programme and transmission data. Enabling search and annotation of such material will bring vast improvements in accessibility. BBC achieves are another project partner, specifically for this reason. However, with the ever growing quantities of digital media from personal devices such tools have far wider reaching applications to the general public. Finally the research could have a considerable impact on Deaf-hearing communication, providing tools to automatically translate sign into spoken English. Such tools could provide benefits to organisations that provide such services to the Deaf community such as Significan't, another project partner. They are also well placed to commercialise such technology and the wider application of tools that allow the categorisation and searching of sign have applications for online or web based resources for the Deaf community.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-02-29</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>489782</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>RWTH Aachen University</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>Aachen Uni</gtr:description><gtr:id>BCCB2C20-27FD-4339-AA78-7649AE20CB7E</gtr:id><gtr:impact>Koller O, Ney H, Bowden R, Deep Learning of Mouth Shapes for Sign Language, Accepted, to appear in Third Workshop on Assistive Computer Vision and Robotics (ACVR-15), ICCV 2015.

Koller O, Ney H, Bowden R, Read My Lips: Continuous Signer Independent Weakly Supervised Viseme Recognition. In Proc. Europen Conf Computer Vision, ECCV2014, LNCS 8690, pp281-296.

Ong E J, Koller O, Pugeault N, Bowden R, Sign Spotting using Hierarchical Sequential Patterns with Temporal Intervals. In Proc IEEE Conference on Computer Vision and Pattern Recognition (CVPR'2014), 2014, pp1931-1938.
DOI: 10.1109/CVPR.2014.248 

Koller O, Ney H, Bowden R, Weakly Supervised Automatic Transcription of Mouthings for Gloss-Based Sign Language Corpora. LREC Workshop on the Representation and Processing of Sign Languages: Beyond the Manual Channel. In LREC Proceedings 2014. pp94-98.

Koller O, Ney H, Bowden R, May the Force be with you: Force-Aligned SignWriting for Automatic Subunit Annotation of Corpora, 10th IEEE Int. Conf on Automatic Face and Gesture Recognition FG2013, Shanghai, China. 22-26 April 2013, pp1-6.
DOI:10.1109/FG.2013.6553777</gtr:impact><gtr:outcomeId>56e07ea26a5336.60544152-1</gtr:outcomeId><gtr:partnerContribution>Aachen have funded the student during their year long placement with us</gtr:partnerContribution><gtr:piContribution>PhD co-supervision with Aachen and hosting of visiting research from Aachen</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>405034</gtr:amountPounds><gtr:country>Switzerland, Swiss Confederation</gtr:country><gtr:currCode>CHF</gtr:currCode><gtr:currCountryCode>Switzerland</gtr:currCountryCode><gtr:currLang>it_CH</gtr:currLang><gtr:description>Sinergia</gtr:description><gtr:end>2019-02-02</gtr:end><gtr:fundingOrg>Swiss National Science Foundation</gtr:fundingOrg><gtr:fundingRef>crsii22_160811</gtr:fundingRef><gtr:id>BE4C1651-E81B-49BA-A746-49A8FB981FF3</gtr:id><gtr:outcomeId>56e07c99517db1.69626695</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We have had a high uptake for some of the datasets we have released and associated publications are gaining citations. Several keynote talks have also happened but it is still early days in terms of impact and citations to this work which is on-going. We are in discussions with a small SME about exploitation. We have now moved the code base to the company so they can market it and have a heads of terms to put a licence deal in place should the code generate revenue.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>02A0DDD5-948A-45EC-9563-61E87F968781</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5441437a35a5d5.88681726</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>we have developed new methods for recognition of sign and motion in video. Techniques for accurate long term tracking. Approaches to labelling image and video content without user supervision. Approaches to learning to categorise content automatically using linguistic annotation.</gtr:description><gtr:exploitationPathways>Automatic recognition and categorisation of images and video</gtr:exploitationPathways><gtr:id>F9D00F78-7DFF-4968-84D3-EA9AE2DF1B6B</gtr:id><gtr:outcomeId>544147757720a2.91263046</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Government, Democracy and Justice,Retail</gtr:sector></gtr:sectors><gtr:url>http://cvssp.org/projects/dynavis/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The data covers two languages GSL (Greek Sign Language) and DGS (German Sign Language). We provide the skeletal data extracted from the original (calibrated) OpenNI tracker with annotations at the sign level.</gtr:description><gtr:id>39A80798-74AF-4751-AAE1-8CBB58947E53</gtr:id><gtr:impact>.</gtr:impact><gtr:outcomeId>56de09d2e4b577.70579367</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Kinect Sign Data Sets</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://cvssp.org/data/KinectSign/webpages/index.html</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset contains two videosequences, NissanSkylineChase and LiverRun, which are a subset of the ytLongTrack dataset. These contain traffic scenes, with the camera mounted on a vehicle chasing another car. Both are of a low quality. They were chosen to test long-term tracking for their challenging properties such as length (LiverRun exceeds 29000 frames!), strong illumination and viewpoint changes, extreme scale changes and full occlusions. The following table summarises properties of the videosequences.</gtr:description><gtr:id>2BCD45D9-48D0-44DE-BB87-1E4FA9CC81CF</gtr:id><gtr:impact>.</gtr:impact><gtr:outcomeId>56de0905f07156.56185703</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>YouTube Long Term Tracking Sequences</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://cvssp.org/data/YTLongTrack/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>As part of the project, I have examined the exploitation of 3D information within natural action recognition, as a means to reduce the amount of variation within classes. To this end, a dataset of natural actions with 3D data was compiled, called Hollywood 3D from broadcast footage on 3D BluRay.</gtr:description><gtr:id>82F09067-124C-4A17-90E7-84B2C6EAAD40</gtr:id><gtr:impact>This benchmark is used by an increasing number of international research groups to asses the performance of algorithms.</gtr:impact><gtr:outcomeId>56de085f341896.81237017</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Hollywood3D</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://cvssp.org/data/Hollywood3D/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>46873F45-8B93-4FD7-9FB4-6F965108B352</gtr:id><gtr:title>Exploiting High Level Scene Cues in Stereo Reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfc39ced7e2.42537166</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13AF0CA5-BED8-4DE1-8054-C7BB99628914</gtr:id><gtr:title>Deep Learning of Mouth Shapes for Sign Language</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c778a0f47693eeee5d05646f80483e6b"><gtr:id>c778a0f47693eeee5d05646f80483e6b</gtr:id><gtr:otherNames>Koller O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de00de9bafb3.55194408</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1CC3C6BC-19F8-4ADA-A2BC-BF44CBB04593</gtr:id><gtr:title>Capturing relative motion and finding modes for action recognition in the wild</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d701c2482e5c8765aea6395150d25a1c"><gtr:id>d701c2482e5c8765aea6395150d25a1c</gtr:id><gtr:otherNames>Oshin O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>544122c7911e04.07157064</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>99B065C4-F04B-4F5A-A982-9451BF47A068</gtr:id><gtr:title>Accurate static pose estimation combining direct regression and geodesic extrema</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a35ecffee27823faf92db053b81a6d36"><gtr:id>a35ecffee27823faf92db053b81a6d36</gtr:id><gtr:otherNames>Holt B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>5441207dd8e485.30066840</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5A7D18BB-5524-4284-906E-9ADA72A91B67</gtr:id><gtr:title>Dense Rigid Reconstruction from Unstructured Discontinuous Video</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1402e41c7abbab1f68edf51e66639c7c"><gtr:id>1402e41c7abbab1f68edf51e66639c7c</gtr:id><gtr:otherNames>Lebeda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de00a27004a9.02857625</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>373B1863-3513-414C-995A-506A9FC1E7FD</gtr:id><gtr:title>Guided optimisation through classification and regression for hand pose estimation</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a8142992d277760fddf7459488e458"><gtr:id>a9a8142992d277760fddf7459488e458</gtr:id><gtr:otherNames>Krejov P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c860a934acc2.67812626</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C8CBD54-9B33-40D7-900C-46479922E65C</gtr:id><gtr:title>Texture-Independent Long-Term Tracking Using Virtual Corners.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1402e41c7abbab1f68edf51e66639c7c"><gtr:id>1402e41c7abbab1f68edf51e66639c7c</gtr:id><gtr:otherNames>Lebeda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>56ddfa2ba5d630.72145784</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A085E2E-59B7-4D38-8479-F8F6C8EEFEE3</gtr:id><gtr:title>2D and 3D tracking and modelling</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bfd0ecb026b5dd465ff18bf24ebebee"><gtr:id>4bfd0ecb026b5dd465ff18bf24ebebee</gtr:id><gtr:otherNames>Lebeda Karel</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c863c33fb7a0.54932310</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5942F89C-82CC-416A-A435-FFEECA5E8741</gtr:id><gtr:title>Direct-from-Video: Unsupervised NRSfM</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1402e41c7abbab1f68edf51e66639c7c"><gtr:id>1402e41c7abbab1f68edf51e66639c7c</gtr:id><gtr:otherNames>Lebeda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c86527195127.39713532</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94325FE1-2A5E-4E73-86E9-CE157800DB6C</gtr:id><gtr:title>Scene Flow Estimation using Intelligent Cost Functions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56de03812d73a6.01208443</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>953C7C46-140D-4057-BA75-76BAB8A2F86C</gtr:id><gtr:title>Scene particles: unregularized particle-based scene flow estimation.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>544122c7cc0467.81436832</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>07DF6AD9-967C-4BA0-AE62-887386EB0BCF</gtr:id><gtr:title>Exploring Causal Relationships in Visual Object Tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1402e41c7abbab1f68edf51e66639c7c"><gtr:id>1402e41c7abbab1f68edf51e66639c7c</gtr:id><gtr:otherNames>Lebeda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddfb99cce720.71237207</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B921F71E-6929-49BF-939D-5AC8AA9808AB</gtr:id><gtr:title>Combining discriminative and model based approaches for hand pose estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a8142992d277760fddf7459488e458"><gtr:id>a9a8142992d277760fddf7459488e458</gtr:id><gtr:otherNames>Krejov P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddff760cf748.71818103</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5262D0DA-EB37-480B-A267-B97676590B64</gtr:id><gtr:title>TMAGIC: A Model-Free 3D Tracker.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1402e41c7abbab1f68edf51e66639c7c"><gtr:id>1402e41c7abbab1f68edf51e66639c7c</gtr:id><gtr:otherNames>Lebeda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>58c88d2d238320.86293705</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3E1A423A-243A-4754-8026-2A4102681216</gtr:id><gtr:title>Improving recognition and identification of facial areas involved in Non-Verbal Communication by feature selection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c3a8ae006a1c52df3ef7c0e0f390538"><gtr:id>8c3a8ae006a1c52df3ef7c0e0f390538</gtr:id><gtr:otherNames>Sheerman-Chase T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>544120bfd3dee9.52711056</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>01F464BA-6143-422B-B32E-B02D6873AC9B</gtr:id><gtr:title>Hollywood 3D: What are the Best 3D Features for Action Recognition?</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c88df5b7f917.45878402</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61AFA464-FCAF-4D7D-99E4-8963F0FE7C4D</gtr:id><gtr:title>Sign Spotting Using Hierarchical Sequential Patterns with Temporal Intervals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f1d770686de08b20027dea3bcaef3cb1"><gtr:id>f1d770686de08b20027dea3bcaef3cb1</gtr:id><gtr:otherNames>Ong E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56de048d4fdb19.07718100</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>079EBE70-DF55-44C1-837B-539E53045F79</gtr:id><gtr:title>Hollywood 3D: Recognizing Actions in 3D Natural Scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>544122c75b7ce8.09165404</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F749BA6A-43A0-445E-8FAD-F1DE9085C94B</gtr:id><gtr:title>Image and video mining through online learning</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42a7336e4dc582e7aab8aaafd3d93d67"><gtr:id>42a7336e4dc582e7aab8aaafd3d93d67</gtr:id><gtr:otherNames>Gilbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c88d5a075c90.62265325</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>07903F35-F3CA-4B59-93C6-4A0A2A502B7E</gtr:id><gtr:title>Stereo reconstruction using top-down cues</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c88da7793f51.26379154</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>782B2B62-7EC5-4A7D-BCD7-C573B2B53AE3</gtr:id><gtr:title>Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data is Continuous and Weakly Labelled</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c778a0f47693eeee5d05646f80483e6b"><gtr:id>c778a0f47693eeee5d05646f80483e6b</gtr:id><gtr:otherNames>Koller O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c88e250aef56.14970043</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52E690EC-AAE0-4905-AE3C-482F7100FA4D</gtr:id><gtr:title>Non-linear predictors for facial feature tracking across pose and expression</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c3a8ae006a1c52df3ef7c0e0f390538"><gtr:id>8c3a8ae006a1c52df3ef7c0e0f390538</gtr:id><gtr:otherNames>Sheerman-Chase T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>5441209fbcba22.93294829</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>623C7FFB-A4A8-4320-B796-2CFCAB56FD2E</gtr:id><gtr:title>Sign Language Recognition using Sequential Pattern Trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/17637899d5da01dac259b36194e7fc61"><gtr:id>17637899d5da01dac259b36194e7fc61</gtr:id><gtr:otherNames>Eng-Jon Ong</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1226-4</gtr:isbn><gtr:outcomeId>54411ffea60393.93535677</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55B7C4EA-5F45-481A-BC6B-A2A830D3BC65</gtr:id><gtr:title>HARD-PnP: PnP Optimization Using a Hybrid Approximate Representation</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5faf76389b632fd1148dfb11c5d3ab8f"><gtr:id>5faf76389b632fd1148dfb11c5d3ab8f</gtr:id><gtr:otherNames>Hadfield S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a995b0b419551.32058846</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>723E95BB-7A4E-4E63-B2FA-17A3442A6EF2</gtr:id><gtr:title>May the force be with you: Force-aligned signwriting for automatic subunit annotation of corpora</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c778a0f47693eeee5d05646f80483e6b"><gtr:id>c778a0f47693eeee5d05646f80483e6b</gtr:id><gtr:otherNames>Koller O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>544122c6db2557.65445813</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B388A179-3174-4498-901F-EDCC98EBC42B</gtr:id><gtr:title>Geometric Mining: Scaling Geometric Hashing to Large Datasets</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42a7336e4dc582e7aab8aaafd3d93d67"><gtr:id>42a7336e4dc582e7aab8aaafd3d93d67</gtr:id><gtr:otherNames>Gilbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c88f7b6b70a5.01598709</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8E911F21-7B6D-465A-8CF6-36E71E0C1A04</gtr:id><gtr:title>Long-Term Tracking through Failure Cases</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1402e41c7abbab1f68edf51e66639c7c"><gtr:id>1402e41c7abbab1f68edf51e66639c7c</gtr:id><gtr:otherNames>Lebeda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>544122c721afa7.09016280</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I011811/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>