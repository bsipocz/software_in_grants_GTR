<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/AF3A7A6E-F17B-4E1E-9717-2752658C8E09"><gtr:id>AF3A7A6E-F17B-4E1E-9717-2752658C8E09</gtr:id><gtr:firstName>Ross</gtr:firstName><gtr:surname>Goutcher</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B54CCDA0-3F52-4995-B12B-10E4DCAE465B"><gtr:id>B54CCDA0-3F52-4995-B12B-10E4DCAE465B</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>Barry</gtr:otherNames><gtr:surname>Hibbard</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FC005260%2F1"><gtr:id>B9880CA6-D283-4F5C-B0A0-E13079142CD2</gtr:id><gtr:title>Binocular vision and the statistics of binocular disparity</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/C005260/1</gtr:grantReference><gtr:abstractText>Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.</gtr:abstractText><gtr:technicalSummary>By comparing the images formed in our two eyes, we are able to effortlessly perceived the three-dimensional world around us in vivid detail. This ability far surpasses that of any computer or robot based vision systems currently available. We will invesigate how this might be achieved by our brains. We will test the notion that our brain combines the information gathered by our eyes with its own prior assumptions or best-guesses about the most likely structure of the world around us. We will test the nature of any such biases, and whether they represent an accurate reflection of our environment.</gtr:technicalSummary><gtr:fund><gtr:end>2008-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2006-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>157745</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The main impact of the work has been within the academic community, as it was fundamental research that was not directly linked to a specific application. During and after the award we have taken many opportunities to engage with the public (mainly through science days and museum exhibits and public talks). However, the main focus of this has been the general areas of human vision, 3D vision. This more general approach to engaging with the public, rather than a constrained focus on the specific outcomes of the particular research, is more appropriate for the type of work, and we believe that the public will have benefit much more from this approach.</gtr:description><gtr:firstYearOfImpact>2006</gtr:firstYearOfImpact><gtr:id>C3FE506F-2679-41DB-AA05-FC12F652F120</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5464752b51cbd6.00557899</gtr:outcomeId><gtr:sector>Education,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Our two eyes view the world from slightly different vantage points. The resulting differences in the images formed on our two retinas are an important source of information about the three-dimensional shape and position of objects. The significance of this information is highlighted, for example, by the recent interest in 3D films at the cinema, and the promise of 3D television in our homes in the near future. The depth effects in these films depend on the presentation of different images to the two eyes.

For us to make sense of this information, and move from the two-dimensional retinal images to the vivid appearance of a three-dimensional world, is a difficult computational problem. We are beginning to understand how this is solved. The first stage in this process is to identify regions of the images in the two eyes that are most similar. However, this simple process does not result in anything like a good description of the three-dimensional shape of objects. The perception of depth therefore requires significantly more than this.

Our research addressed one simple idea about what this something more might be. Specifically, we reasoned that as our environment is relatively predicable, we might makes assumptions about the shape and layout of objects with which to guide the interpretation of the retinal images. As an example, as an object is moved further away from an observer, the images it will form in their eyes will get smaller and smaller. Equally, because things tend to be opaque, as we consider objects at greater distances, it becomes more likely that part or all of the object will not be visible because it will be behind a nearer object. We might therefore expect that most points in the world that we can see originate from relatively close objects. This is born out when we measure the distribution of distances in the real world. 

The idea addressed by our research was: are such statistical expectations useful when trying to understand how people make sense of the three-dimensional structure of their environment?

Specifically, we asked the following questions:

Are some matches between the eyes preferred over others?
Are some surfaces easier to see than others? 
Are we biased in the way we perceive shape? 

In all cases, we can ask whether the preferences shown reflect the kinds of surfaces we might expect in our environment.

The answer to the first question is yes, we prefer to match things so as to minimise the depth variation across the scene as a whole. This is more important than matching to produce a smooth surface. It is also inconsistent with simple matching models, and shows one way in which they can be improved.

The answer to the second question is also yes. Surfaces with neither too much nor too little depth are more readily detected. These results are again not consistent with simple matching models, which predict the best detection for surfaces with the smallest depth variation. The results are however consistent with the distributions of slant in the natural environment.

The answer to the final question is again yes. However, we can account for these biases by assuming that the observer is simply trying to estimate the differences between the two eyes' views as accurately as possible. Inclusion of information derived from analyses of the natural environment to our model of this process does not improve our understanding of these biases.

We have identified the kinds of surfaces that observers are best at seeing. We have highlighted ways in which these preferences are not compatible with a simple model of binocular vision, but consistent with the kinds of surfaces found in the environment. We have also built a mathematical model to account for the biases shown by observers. However, when we extend this to take account of measurements of the structure of the environment we do not shed additional light on the process.</gtr:description><gtr:exploitationPathways>These findings provided some of the first work on understanding the statistical structure of binocular images. This can be, and has been, built up by other scientists seeking to understand how our visual system (from individual neurons to the whole system) is optimised for processing this information. It can also be used to inspire computer vision algorithms, and in optimising 3D content in films, TV and other display technologies.</gtr:exploitationPathways><gtr:id>1654F684-4634-410B-97EC-9B7361DBBF1E</gtr:id><gtr:outcomeId>546472b6c9f108.85772829</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Collection of calibrated binocular images</gtr:description><gtr:id>1C2CB5DB-F3C1-4280-85FC-5470A7BE7602</gtr:id><gtr:impact>Scientific publications

Perceived direction of motion determined by adaptation to static binocular images KA May, L Zhaoping, PB Hibbard
Current Biology 22 (1), 28-32</gtr:impact><gtr:outcomeId>54646daa84f548.24706229</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>3D Image database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>7296A425-809F-485D-A9E8-DC3F54DF16F5</gtr:id><gtr:title>Contextual effects on perceived three-dimensional shape.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3ff441f05b1d18339cd7569c264cf0c5"><gtr:id>3ff441f05b1d18339cd7569c264cf0c5</gtr:id><gtr:otherNames>O'Kane LM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>544bbddf02b3c4.22051348</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55F391B7-9D31-4C03-85ED-3DCFEB24E896</gtr:id><gtr:title>Misperception of aspect ratio in binocularly viewed surfaces.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a663d40f64953891ab494a253453d12c"><gtr:id>a663d40f64953891ab494a253453d12c</gtr:id><gtr:otherNames>Hibbard PB</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>544bbddf6ac2b3.96866655</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C38825C-A9A1-40D4-8F9B-4682C4AB79F9</gtr:id><gtr:title>Evidence for relative disparity matching in the perception of an ambiguous stereogram.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6614cf1b81d78c6df678272d5d207077"><gtr:id>6614cf1b81d78c6df678272d5d207077</gtr:id><gtr:otherNames>Goutcher R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>544bbddf364535.23846923</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/C005260/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>