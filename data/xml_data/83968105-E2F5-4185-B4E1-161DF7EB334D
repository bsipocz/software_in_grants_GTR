<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0975468D-C7A0-4AD1-99E9-AC18841FBAF3"><gtr:id>0975468D-C7A0-4AD1-99E9-AC18841FBAF3</gtr:id><gtr:name>Justus-Liebig University Giessen</gtr:name><gtr:address><gtr:line1>Justus-Liebig Universitat Giessen</gtr:line1><gtr:line2>Ludwigstrasse 23</gtr:line2><gtr:line4>Giessen</gtr:line4><gtr:postCode>D-35390</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0932DA9B-4895-4669-848D-9FC883CDEDE0"><gtr:id>0932DA9B-4895-4669-848D-9FC883CDEDE0</gtr:id><gtr:name>University of California Santa Barbara</gtr:name><gtr:address><gtr:line1>UCSB</gtr:line1><gtr:line4>Santa Barbara</gtr:line4><gtr:line5>CA 93106</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9C10D78F-6430-4CA7-9528-B96B0762A4C6"><gtr:id>9C10D78F-6430-4CA7-9528-B96B0762A4C6</gtr:id><gtr:name>Cardiff University</gtr:name><gtr:address><gtr:line1>Research &amp; Consultancy</gtr:line1><gtr:line2>PO Box 923</gtr:line2><gtr:line4>Cardiff</gtr:line4><gtr:line5>South Glamorgan</gtr:line5><gtr:postCode>CF10 3TE</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Experimental Psychology</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0975468D-C7A0-4AD1-99E9-AC18841FBAF3"><gtr:id>0975468D-C7A0-4AD1-99E9-AC18841FBAF3</gtr:id><gtr:name>Justus-Liebig University Giessen</gtr:name><gtr:address><gtr:line1>Justus-Liebig Universitat Giessen</gtr:line1><gtr:line2>Ludwigstrasse 23</gtr:line2><gtr:line4>Giessen</gtr:line4><gtr:postCode>D-35390</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0932DA9B-4895-4669-848D-9FC883CDEDE0"><gtr:id>0932DA9B-4895-4669-848D-9FC883CDEDE0</gtr:id><gtr:name>University of California Santa Barbara</gtr:name><gtr:address><gtr:line1>UCSB</gtr:line1><gtr:line4>Santa Barbara</gtr:line4><gtr:line5>CA 93106</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9C10D78F-6430-4CA7-9528-B96B0762A4C6"><gtr:id>9C10D78F-6430-4CA7-9528-B96B0762A4C6</gtr:id><gtr:name>Cardiff University</gtr:name><gtr:address><gtr:line1>Research &amp; Consultancy</gtr:line1><gtr:line2>PO Box 923</gtr:line2><gtr:line4>Cardiff</gtr:line4><gtr:line5>South Glamorgan</gtr:line5><gtr:postCode>CF10 3TE</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4FCECA36-1A6F-4DC4-A713-FB052FBE1963"><gtr:id>4FCECA36-1A6F-4DC4-A713-FB052FBE1963</gtr:id><gtr:name>NASA</gtr:name><gtr:address><gtr:line1>Suite 5K39</gtr:line1><gtr:line2>2880 Broadway</gtr:line2><gtr:line4>Washington</gtr:line4><gtr:line5>DC 20546-0001</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/1B1F2323-F669-465D-A55F-643C58962339"><gtr:id>1B1F2323-F669-465D-A55F-643C58962339</gtr:id><gtr:firstName>Casimir</gtr:firstName><gtr:surname>Ludwig</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE054323%2F1"><gtr:id>83968105-E2F5-4185-B4E1-161DF7EB334D</gtr:id><gtr:title>Integrating 'when' and 'where' in models of saccade target selection</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/E054323/1</gtr:grantReference><gtr:abstractText>The human visual system is limited in its ability to resolve fine detail. In fact, only in the one or two degrees of central vision (an area of approximately the width of two thumbs at an arm's length) are we able to see with high acuity. Therefore, in order to explore the visual environment humans move their eyes very frequently. These eye movements are called saccades, and we make ~10,000 of these movements every hour of our waking lives.These eye movements are critical for our vision, and therefore for the successful interaction with the world around us. Eye movement researchers are interested in the properties of saccades, and the visual signals that are effective in triggering these movements. The critical question in this regard is: How do people decide where to look next and when to move the current point of fixation to the selected location?Theories of saccadic eye movement generation typically focus on one or the other of these two aspects. That is, sophisticated theories have been developed to explain which spatial locations in a visual scene are fixated. These theories do not explain the order in which the various locations are visited, nor do they account for how long the eyes remain stationary before moving on to the next location. Likewise, theories exist that account for the latency, or reaction time, of saccades in response to the appearance of a single visual target on a blank background. Such theories do not account for which locations are selected for detailed visual scrutiny. Therefore, current theories of saccade target selection are incomplete in that they only consider the spatial or the temporal aspects.The aim of this project is to develop a general theory of saccade target selection that incorporates both the spatial and temporal components. Specifically, the theory will be formulated in such a way that it can be implemented, or simulated, in a computer program. Detailed simulations of such kind are often helpful in that it enables one to make more specific, and sometimes counter-intuitive, predictions that can then be tested in experiments.Testing the theory in this way is an important and substantial part of the project. The experiments proposed generally involve having human observers perform some discrimination task on some visual image. For instance, we can present a number of patterns that contain oriented lines, and ask observers to find the one pattern that consists of vertically oriented lines. Using an eye tracker, we can then monitor which patterns are fixated, in what order, and how long it takes observers to look from one place to the next. These eye movement data are used to confirm or reject the theory's predictions.A unified theory of saccade target selection is intrinsically interesting to scientists who investigate the human eye movement and/or visual systems. In addition, its computational implementation may be relevant in more applied settings. For instance, a major problem in robotics is one of representation: a mobile robot simply cannot store all the information it might want to store about its environment (this amount would rapidly grow out of control). Instead, a solution might be to sample information from the environment if and when it is needed for some task, similar to the way humans sample the visual world using eye movements. Knowledge and theories about how humans do this, can be extremely useful in an engineering context.In addition, the empirical findings generated in the project are not only of theoretical relevance. Knowledge of the kinds of representations human observers can use to drive their eye movements can be important in a variety of settings in which people are required to quickly respond to some visual signals. Here you can think about a pilot flying an aircraft or the staff in an airtraffic control room. In these settings it may be very important to taylor the lay-out of the visual scene to the human who will be operating in that environment.</gtr:abstractText><gtr:fund><gtr:end>2013-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>668756</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Bristol</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration</gtr:description><gtr:id>95129A1D-BEE7-4B58-8792-FEBADA287B9E</gtr:id><gtr:outcomeId>b9cf9310b9cf932e-1</gtr:outcomeId><gtr:piContribution>Collaboration with computer scientists at the University of Bristol and University of Sheffield. Together with Drs Rafal Bogacz (Department of Computer Science, University of Bristol; now at University of Oxford) and James Marshall (Department of Computer Science, University of Sheffield), we developed an 'ideal observer' model for a particular class of decision problems and tested the model with experiments. We trained a computer science PhD student (modelling) and an experimental psychology PhD student (experiments). Some of the work has been published in PLoS ONE (Cassey et al., 2013).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Justus Liebig University Giessen</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>Collaboration</gtr:description><gtr:id>A2A3FDB3-DB81-4CC9-8035-C42E48197BFA</gtr:id><gtr:outcomeId>b9d05f66b9d05f7a-1</gtr:outcomeId><gtr:piContribution>Collaboration with Professor Karl Gegenfurtner, University of Giessen in Germany. A novel collaboration that started after being invited to give a departmental seminar at the University of Giessen, in June 2009. This collaboration led to a large body of experimental work on trans-saccadic perception, the results of which have been published in 2012 (Ludwig et al., 2012, Journal of Vision) and presented at two conferences (Vision Sciences Society 2011; European Conference on Eye Movements 2011).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Bristol</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Bristol Decision-Making Network</gtr:description><gtr:id>7C60B3F5-0EF6-4DE8-8C1A-E22D28A4999D</gtr:id><gtr:outcomeId>b9d4a8e6b9d4a904-1</gtr:outcomeId><gtr:piContribution>A consortium of 7 applicant from Bristol University was awarded &amp;pound;1.6m from the EPSRC Cross-Disciplinary Interfaces Programme, to build a network of scientists working on all aspects of human, biological and machine decision-making. The core group of 7 academics come from Psychology, Computer Science and Mathematics. A wider network involves academics from a host of other disciplines, including Biology and Economics.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Cardiff University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration</gtr:description><gtr:id>717BA86C-2923-4013-9B48-ADB697BCDFF9</gtr:id><gtr:outcomeId>b9ce44c4b9ce44d8-1</gtr:outcomeId><gtr:piContribution>Collaboration with Dr. Simon Rushton, University of Cardiff. Dr. Rushton is an internationally leading expert on optic flow and visual processing during ego-motion. We have begun the explore the allocation of visual attention during simulated self-movement. We have conducted a large series of experiments on this topic. Some preliminary data were reported at the 2008 European Conference on Visual Perception. A manuscript reporting the results of 5 experiments is currently in preparation.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2008-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Cardiff University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration</gtr:description><gtr:id>6DB667DF-7884-4BCB-A8DF-AFD65053A90F</gtr:id><gtr:outcomeId>b9d03c02b9d03c16-1</gtr:outcomeId><gtr:piContribution>Collaboration with Dr. Petroc Sumner and his group at the University of Cardiff. I am an external advisor on a recently awarded ESRC project, aiming to develop a modelling toolkit to estimate individual differences in inhibitory control. These estimates are linked to measures of impulsive behaviour.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Sheffield</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration</gtr:description><gtr:id>546F7E02-9CE9-4BB4-B751-2154E2A89863</gtr:id><gtr:outcomeId>b9cf945ab9cf946e-1</gtr:outcomeId><gtr:piContribution>Collaboration with computer scientists at the University of Bristol and University of Sheffield. Together with Drs Rafal Bogacz (Department of Computer Science, University of Bristol; now at University of Oxford) and James Marshall (Department of Computer Science, University of Sheffield), we developed an 'ideal observer' model for a particular class of decision problems and tested the model with experiments. We trained a computer science PhD student (modelling) and an experimental psychology PhD student (experiments). Some of the work has been published in PLoS ONE (Cassey et al., 2013).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of California, Santa Barbara</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Collaboration</gtr:description><gtr:id>72236B0C-92CA-48C4-A1C6-8CB792E65DF2</gtr:id><gtr:outcomeId>b9b14e5ab9b14e6e-1</gtr:outcomeId><gtr:piContribution>Existing collaboration with Professor Miguel Eckstein, University of California Santa Barbara. The fellowship involved strengthening of an existing collaboration with Prof Eckstein. In the final two years of the project, we have developed an innovative methodology to assess the uptake of visual information in eye movement control. An empirical paper describing the methodology and some initial results is close to being submitted. At least one other paper based on the current data set is to be expected. A further series of at least 6 experiments has been planned.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2005-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Public engagement at @Bristol</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>1D16FF24-EDA7-4943-A525-325590E3341A</gtr:id><gtr:impact>Together with my colleague Dr. Simon Rushton, we ran a public engagement activity in the @-Bristol science museum. Our demonstration involved a stereoscopic display with which we study people's sensitivity to &amp;quot;scene-relative&amp;quot; motion: object motion that is embedded within a global pattern of optic flow of the type that would result from self-movement of the observer.

I would like to think we engaged people's interest in psychological research generally, and human vision science specifically.</gtr:impact><gtr:outcomeId>r-7804820544.8359440b9b1a10</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bris.ac.uk/publicengagementstories/stories/2009/49.html</gtr:url><gtr:year>2009</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited seminars</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>683784AE-8569-4A9D-82F3-F019BA6DF1DC</gtr:id><gtr:impact>I have given a number of invited seminars about the work conducted as part of the fellowship: University of Potsdam, Germany, 2017; Paris Descartes University, France, 2017; University of Cardiff, UK, 2016; University of Bielefeld, Germany, 2016; Aston University, UK, 2015; Universidad Torcuato Di Tella, Buenos Aires, Argentina, 2015; University of Wales, Bangor, 2014; University of California Irvine, 2014; University of Southampton, UK, 2014; University of Potsdam, Germany, 2012; University of Giessen, Germany, 2009; University of Birmingham, UK, 2009; University of Strathclyde, UK, 2009; University of Geneva, Switzerland, 2008; University of Cardiff, UK, 2008; DSTL Knowledge Network Meeting, Winfrith Technology Centre, Winfrith, UK, 2008;.

The impact of this activity is primarily academic.</gtr:impact><gtr:outcomeId>r-9585108588.9945810b94b58a</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2008,2009,2012,2014,2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>806994</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>User Interaction with ICT</gtr:description><gtr:end>2020-02-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N013964/1</gtr:fundingRef><gtr:id>39FDFD1B-FF96-4D65-BE55-BBB63E045596</gtr:id><gtr:outcomeId>56dda3fd3d28f9.96678235</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1400000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Platform Grants</gtr:description><gtr:end>2020-01-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/M000885/1</gtr:fundingRef><gtr:id>13141E0E-35FE-4451-AD4F-244F7D993736</gtr:id><gtr:outcomeId>56dda282c19018.61871545</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>390439</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Wellcome Trust infrastructure funding</gtr:description><gtr:end>2014-07-02</gtr:end><gtr:fundingOrg>Wellcome Trust</gtr:fundingOrg><gtr:fundingRef>089367/Z/09/Z</gtr:fundingRef><gtr:id>FCC5C6C4-8CEB-4C44-81AA-DE72BABF1413</gtr:id><gtr:outcomeId>5ee160ba5ee160d8</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2010-08-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1646369</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC cross-disciplinary interfaces</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/I032622/1</gtr:fundingRef><gtr:id>1CFBF03B-839F-4E4E-8EEC-6130C660BDB4</gtr:id><gtr:outcomeId>5eda81c85eda81dc</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Humans pick up visual information from their environment by shifting their gaze a few times every second. This central aim of this project was to understand the decision processes involved in choosing where to look next and when to go there. In particular, this project was focused on saccadic eye movements - these are rapid jumps of the eyes and form the dominant mode of acquiring visual information in a wide range of visually driven activities (e.g. reading, driving).



The first three years of the fellowship were largely devoted to model development and testing with behavioural eye tracking experiments. The modelling is rooted in the idea that at any one point there are multiple possible targets to fixate next. Over the course of a fixation, humans accumulate &amp;quot;evidence&amp;quot; in favour of these targets in parallel. When the evidence for a particular target reaches a critical threshold, that target is fixated next. This type of model provides a coherent and integrated account of the 'when' and 'where' decisions in eye movement control.



The key findings from this phase of the project include:

1. Inclusion of physiologically realistic sensory response functions in the drift-diffusion model of decision-making (Ludwig, 2009, Vision Research).

2. An account for well-known sequential effects in eye movement control (Inhibition of Return; Ludwig et al., 2009, Cognitive Psychology).

3. Experimental evidence that shows that this sequential effect is flexibly adapted to the (learned) statistics of the local environment (Farrell et al., 2010, Proceedings of the National Academy of Sciences; Ludwig et al., 2012, Journal of Experimental Psychology: General).

4. Development of a &amp;quot;front-end&amp;quot; to the accumulator framework that tracks changing environmental statistics in a context-sensitive manner (Ludwig et al., 2012, Journal of Experimental Psychology: General).

5. Experimental and computational analyses of the sources of &amp;quot;internal noise&amp;quot; that need to be included in models in order to account for behavioural variability (Ludwig &amp;amp; Davies, 2011, Cognitive Psychology).



In the final two years of the project, I have considered more ecologically realistic demands faced by the visual and eye movement systems. I will highlight three major bodies of work that were conducted in this phase.



1. Consideration of visual information processing across saccadic eye movements. When you shift gaze, a different part of the visual scene projects to a same part of the retina. Given the frequency with which gaze shifts occur, the visual system is bombarded with such changes in visual input. We have shown experimentally that humans seem particularly sensitive to &amp;quot;trans-saccadic&amp;quot; changes in luminance. The key finding here is that a simple comparison of the luminance at the same retinal location before and after the saccade can act as a &amp;quot;dumb&amp;quot; edge detector (Ludwig, et al., 2012, Journal of Vision). Finding edges is important in scene segmentation and interpretation.



2. Consideration of visual processing in the fovea. You shift gaze in order to analyse an object or region of interest with foveal vision - the part of the retina where visual acuity is maximal. However, a great deal of work in this domain (including much of the work conducted in this project) examines gaze shifting mechanisms in the absence of any foveal processing requirement. We have developed a novel &amp;quot;noise classification&amp;quot; methodology that allows us to identify how visual information is used for the purpose of foveal object identification and selection of the next saccade target. Key findings here are that foveal object identification and target selection occur completely in parallel and independently from each other (Ludwig et al., 2014, Proceedings of the National Academy of Sciences). These findings have strong implications for theories of eye movement control in a range of visual-motor domains (e.g. reading, driving, scene perception).



3. Information foraging. During a fixation you extract information from the currently fixated region. Across multiple fixations, you have gathered information and you typically use that information for some purpose. For example, when you are about to cross the road, you look both ways in one or more cycles in order to estimate how safe it is to cross the road. One way to think of this behaviour is as a form of information foraging. We have studied such foraging behaviour empirically and developed computational models to account for this behaviour (Cassey et al., 2013, PLoS ONE; Ludwig &amp;amp; Evens, 2017, Journal of Experimental Psychology: Human Perception and Performance).</gtr:description><gtr:exploitationPathways>Knowledge of how humans decide what information to sample at what point in time is valuable in a number of domains, such as (i) Human-Computer Interaction (HCI): eye tracking interfaces such as those developed for quadriplegic patients; (ii) Robotics: overcome sensory bottlenecks through active sensors that focus the available, limited bandwidth selectively on information that is relevant for the task at hand; (iii) Ergonomics: promote the adaptive uptake of visual information through appropriate lay-out and alerting signals in the environment (e.g. cockpit design).



The transfer of knowledge to these more complex and naturalistic domains calls for techniques to study visually guided behaviour on a larger scale: that is, larger environments, richer visual inputs, and a richer repertoire of behaviour that also includes other motor systems (i.e. head and whole body movements). To this end, I have set up the Bristol Vision Institute Movement laboratory (funded by The Wellcome Trust), a facility that allows us to study complex behaviour in more naturalistic situations.</gtr:exploitationPathways><gtr:id>1EB2DD7B-F656-48A8-B55A-5C967F004777</gtr:id><gtr:outcomeId>r-3499071657.191097377485a28</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>https://sites.google.com/site/casimirludwig/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Models of evidence accumulation are widely used in cognitive and neuroscience. We have developed a toolbox that allows for straightforward simulation of a wide variety of these so-called &amp;quot;accumulator models&amp;quot;, with many different tunable parameters. The toolbox also includes code for quantifying the goodness-of-fit of a model with respect to empirically observed data.</gtr:description><gtr:id>937B7FA2-1D57-4E1B-950E-6362C039080B</gtr:id><gtr:impact>Informally, I know that the toolbox has been used by a variety of labs, typically as part of the stage of &amp;quot;trying out&amp;quot; a number of different model architectures, prior to the full-blown implementation and fitting of a specific model (the toolbox is meant for simulating data, not necessarily for fitting data---although for complex models, simulation is often the only way to fit the data and the toolbox may be used in that way).</gtr:impact><gtr:outcomeId>56dda78f771d39.31451257</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Competitive Accumulator Modelling toolbox</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>http://casiwiki.pbworks.com/w/page/15355615/Competitive%20accumulator</gtr:url><gtr:yearFirstProvided>2009</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>BAC551F0-C25D-4B40-8773-035A76633AF6</gtr:id><gtr:title>Foveal analysis and peripheral selection during active visual sampling.</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Academy of Sciences of the United States of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0027-8424</gtr:issn><gtr:outcomeId>doi_53d03a03ab7528b5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EEFAB345-BFAD-46BF-8E0E-A9E78221A7EE</gtr:id><gtr:title>The Oxford Handbook of Eye Movements</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3ab2a0699196b7c7d6bf0bc629fbd90c"><gtr:id>3ab2a0699196b7c7d6bf0bc629fbd90c</gtr:id><gtr:otherNames>Ludwig, C.J.H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-0-19953-978-9</gtr:isbn><gtr:outcomeId>r_977339927991e32110</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B278A591-724E-47A4-BBA7-28E074981012</gtr:id><gtr:title>A functional role for trans-saccadic luminance differences.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_55f976976e0f1864</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7EF9CF37-6EF6-4344-B44C-B49CB52F4C52</gtr:id><gtr:title>The mechanism underlying inhibition of saccadic return.</gtr:title><gtr:parentPublicationTitle>Cognitive psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0010-0285</gtr:issn><gtr:outcomeId>doi_53cfe9fe9d027cea</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CBD726C4-D90B-483C-A812-4B33AE174D56</gtr:id><gtr:title>Adaptive sampling of information in perceptual decision-making.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ca56abc9aecb0d6a239af4f38dfeb25"><gtr:id>4ca56abc9aecb0d6a239af4f38dfeb25</gtr:id><gtr:otherNames>Cassey TC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>doi_53d081081e1ab898</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E81BE57-C351-4C3E-A421-9E578DBF590A</gtr:id><gtr:title>Testing a simplified method for measuring velocity integration in saccades using a manipulation of target contrast.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a92286b22b9831452d82e5466ca1c58c"><gtr:id>a92286b22b9831452d82e5466ca1c58c</gtr:id><gtr:otherNames>Etchells PJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>doi_53d087087ca2fcae</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>981CAABD-5ACA-4F05-8F0F-C9BC8976CE5A</gtr:id><gtr:title>Temporal integration of sensory evidence for saccade target selection.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00f00f02702d2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0014A47B-30D1-4777-AFCB-92273BB5C98F</gtr:id><gtr:title>Context-gated statistical learning and its role in visual-saccadic decisions.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. General</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0022-1015</gtr:issn><gtr:outcomeId>doi_53d024024dc19cbf</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8843A47A-8E84-49F0-BADE-EC5C9628B297</gtr:id><gtr:title>Influence of environmental statistics on inhibition of saccadic return.</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Academy of Sciences of the United States of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a85cc6c7f7486c140a4f07a5c55cbc25"><gtr:id>a85cc6c7f7486c140a4f07a5c55cbc25</gtr:id><gtr:otherNames>Farrell S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0027-8424</gtr:issn><gtr:outcomeId>doi_53d03a03a055ed67</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>551615C0-29E9-4BA9-8140-DB630EA149A9</gtr:id><gtr:title>Dual-task costs and benefits in anti-saccade performance.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/20ffc1b2263f0c8f81391922c835ecc0"><gtr:id>20ffc1b2263f0c8f81391922c835ecc0</gtr:id><gtr:otherNames>Evens DR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>doi_53cfd2fd280cc4c9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>389C5966-0F40-475F-8209-04A5CCFCAF54</gtr:id><gtr:title>Information foraging for perceptual decisions.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>585d4af9999f38.32341406</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0D277DAA-167A-4205-AFB2-594A44300687</gtr:id><gtr:title>Estimating the growth of internal evidence guiding perceptual decisions.</gtr:title><gtr:parentPublicationTitle>Cognitive psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0010-0285</gtr:issn><gtr:outcomeId>doi_53cfe9fe9d0bea19</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9EF7425-2E19-4367-A997-19ACC5E7B9C4</gtr:id><gtr:title>The target velocity integration function for saccades.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a92286b22b9831452d82e5466ca1c58c"><gtr:id>a92286b22b9831452d82e5466ca1c58c</gtr:id><gtr:otherNames>Etchells PJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d07707737edc2c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BA0CB20-9776-476D-952D-D26398B72EB7</gtr:id><gtr:title>Bayesian and maximum likelihood estimation of hierarchical response time models.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a85cc6c7f7486c140a4f07a5c55cbc25"><gtr:id>a85cc6c7f7486c140a4f07a5c55cbc25</gtr:id><gtr:otherNames>Farrell S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>doi_53d08808828962da</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA4EF206-4439-43FE-ACE8-A933F5E392F5</gtr:id><gtr:title>Modelling contralesional movement slowing after unilateral brain damage.</gtr:title><gtr:parentPublicationTitle>Neuroscience letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd52801c4872da74045af1148c522ee"><gtr:id>afd52801c4872da74045af1148c522ee</gtr:id><gtr:otherNames>Ludwig CJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0304-3940</gtr:issn><gtr:outcomeId>doi_53d00200234aba03</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E054323/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>