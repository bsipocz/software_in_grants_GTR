<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/8736CDCC-BE36-403A-8FBC-F9D9D26A0F9E"><gtr:id>8736CDCC-BE36-403A-8FBC-F9D9D26A0F9E</gtr:id><gtr:name>Synthesia Limited</gtr:name><gtr:address><gtr:line1>101, Miro Studios, 94 Hanbury Street</gtr:line1><gtr:city>London</gtr:city><gtr:postCode>E1 5JL</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:typeInd>P</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8736CDCC-BE36-403A-8FBC-F9D9D26A0F9E" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="gtr:organisationParticipantRole"><gtr:id>8736CDCC-BE36-403A-8FBC-F9D9D26A0F9E</gtr:id><gtr:name>Synthesia Limited</gtr:name><gtr:address><gtr:line1>101, Miro Studios, 94 Hanbury Street</gtr:line1><gtr:city>London</gtr:city><gtr:postCode>E1 5JL</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_PARTICIPANT</gtr:name></gtr:role><gtr:role><gtr:name>PARTICIPANT</gtr:name></gtr:role></gtr:roles><gtr:offerGrant>69384.0</gtr:offerGrant><gtr:projectCost>99120.0</gtr:projectCost></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A0846F5D-1676-4331-ADE5-3A52CA0E943A"><gtr:id>A0846F5D-1676-4331-ADE5-3A52CA0E943A</gtr:id><gtr:firstName>Steffen</gtr:firstName><gtr:surname>Tjerrild-Hansen</gtr:surname><gtr:roles><gtr:role><gtr:name>PROJECT_MANAGER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=133525"><gtr:id>E4018D7B-27ED-4B37-B232-91B6AF36AB88</gtr:id><gtr:title>Visual understanding of faces in motion</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Feasibility Studies</gtr:grantCategory><gtr:grantReference>133525</gtr:grantReference><gtr:abstractText>Recent days have seen an explosion of markerless facial performance capture methods that can reconstruct the dense geometry of the face from multi-view or even a single video. Unfortunately, even the best of these methods fail to capture the extensive range of shapes and deformations displayed by lips in motion, which results in a lack of expression in video synthesis of faces in speech. Lips exhibit agile and highly deformable motions which are extremely hard to capture -- at the same time, subtle lip motions are often the main vehicle for humans to convey emotions. **Capturing 3D lip motion with high accuracy** is therefore crucial if we are to design systems that can **synthesise realistic human speech** to convey human-like emotions.

**The key goal and main innovation in VISIM will be to construct a low-cost, class-leading, three-dimensional (3D) morphable model (3DMM) of the face that focuses on representing the detailed dynamics of the mouth in speech to achieve unprecedented levels of realism in video synthesis of human speech.**

We will achieve this by constructing a lightweight, multi-view, high frame-rate, video capture system to reconstruct the motion of the mouth and perform further lip shape refinement using photometric consistency and exploiting the redundancy in multiple views. We will use this to capture a range of subjects and train a new, richer 3DMM that encodes the detailed 3D dynamics of different people in speech.

The **business value for Synthesia** is clear: this new, highly expressive 3DMM model will be instrumental in allowing us to **automate** the accurate capture of detailed 3D lip motion just from **monocular video** using an analysis-by-synthesis approach. We will then apply this to high-end facial video synthesis for media and entertainment industries. VISIM will create impact by bringing the high-levels of realism at the affordable price of a low-cost, video-based, automated capture system. This will pave the way to democratising high-end facial performance capture, allowing it to be adopted in new markets such as language dubbing for the TV, movie and advertising industries, where photorealistic results are critical but production budgets have so far restricted access to high-end content creation. This technology will enable Synthesia, a newly formed UK SME, to establish a new market in 'natural' language dubbing with photorealistic synthesis of the face.

Keywords: Facial Performance Capture, Video-Based Tracking, Digital Content Creation.</gtr:abstractText><gtr:fund><gtr:end>2019-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/E18E2F0F-AC7D-4E02-9559-669F7C8FEC74"><gtr:id>E18E2F0F-AC7D-4E02-9559-669F7C8FEC74</gtr:id><gtr:name>Innovate UK</gtr:name></gtr:funder><gtr:start>2018-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>69384</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">133525</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>