<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2ADE8077-D275-4FCF-9D3B-9B08C2CFDBBD"><gtr:id>2ADE8077-D275-4FCF-9D3B-9B08C2CFDBBD</gtr:id><gtr:firstName>Phil</gtr:firstName><gtr:surname>Green</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/461244CD-E991-44D9-A2F7-318530121EC1"><gtr:id>461244CD-E991-44D9-A2F7-318530121EC1</gtr:id><gtr:firstName>Jon</gtr:firstName><gtr:surname>Barker</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG039046%2F1"><gtr:id>97DE34A5-C18A-452D-A379-F179517909DB</gtr:id><gtr:title>CHIME: Computational Hearing in Multisource Environments</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G039046/1</gtr:grantReference><gtr:abstractText>In everyday environments it is the norm for there to exist multiple sound sources competing for the listener's attention. Understanding any one of the jumble of sounds arriving at our ears requires being able to hear it separately from the other sounds arriving at the same time. For example, understand what someone is saying when there is a television on in the same room requires separating their voice from the television audio. The lack of an adequate computational solution to this problem prevents hearing technologies from working reliably in typical noisy human environments -- often the situations where they could be most useful. Computational hearing algorithms designed to operate in multisource environments would enable a whole range of listening applications: robust speech interfaces, intelligent hearing aids, audio-based monitoring and surveillance systems.The CHIME project will develop a framework for computational hearing in multisource environments. Our approach operates by exploiting two levels of processing that combine to simultaneously separate and interpret sound sources. The first processing level exploits the continuity of sound source properties, such as location, pitch, and spectral profile, to clump the acoustic mixture into pieces (`fragments') belonging to individual sources. Such properties are largely common to all sounds and can be modelled without having to first identify the sound source. The second processing level uses statistical models of specific sound sources expected to be in the environment. These models are used to separate fragments belonging to the acoustic foreground (i.e. the `attended' source) from fragments belonging to the background. For example, in a speech recognition context, this second stage will recruit sound fragments which string together to form a valid utterance. This second stage both separates foreground from background and provides an interpretation of the foreground.The CHIME project aims to investigate and develop key aspects of the proposed two-level hearing framework: we will develop statistical models that use multiple signal properties to represent sound source continuity; we will develop approaches for combining statistical models of the attended `foreground' and the unattended `background' sound sources; we will investigate approximate search techniques that allow acoustic scenes containing complex sources such as speech to be processed in real-time; we will investigate strategies for trying to learn about individual sound sources directly from noisy audio data. The results of this research will be built into a single demonstration system simulating a home-automation application with a speech-driven interface that will operate reliably in a noisy domestic environment.</gtr:abstractText><gtr:fund><gtr:end>2012-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>326244</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The CHiME project has inspired a number of workshops and speech recognition evaluation events based around the distant microphone research challenges that lie at the project's heart.

- The 1st CHiME International Workshop on Machine Listening in Multisource Environments, Florence, 2011
- The 2nd CHiME International Workshop on Machine Listening in Multisource Environments, Vancouver, 2013
- The 3rd CHiME Challenge Evaluation Campaign, ASRU, Scottsdale, Arizona, 2015
- The 4th CHiME Challenge Workshop, Google, Mountain View, CA, 2016

These events have exclusively employed data that has been collected with the funding provided by the CHiME project. Each event has reached beyond academia to attract significant industrial input. For example, the workshops and challenges have been guided by an industrial advisory committee. In particular, the most recent completed evaluation has had participation from NTT, Hitachi, Mitsubishi, A-Star and was hosted by Google. These events have stimulated engagement between academia and industry and are influencing the development of speech technology in major speech technology companies in Europe, the US and Asia

A 5th CHiME Challenge event is now being planned for 2018. This edition is being informed by the findings of the previous challenges but will involve a much larger scale data collection (approximately 100 hours of conversational speech). The collection and preparation of this data is a large undertaking and will require considerable funding. We are currently in negotiation with Google who have expressed an interest in providing financial sponsorship and who view the challenge as an opportunity to compare their latest algorithms with those of their competitors. We hope to start data recording during the Spring and Summer of 2017 for a launch by the end of the year.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>2C72D9BB-F532-4A1F-BCD8-0746DA3EEBEC</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56db0ad4a96650.85151231</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The CHiME project was concerned with building speech recognisers that can operate reliably in everyday `acoustically cluttered' environments. For example, imagine attempting to communicate with a home automation system by speaking across a room while the television is on, children are playing and traffic noise is coming through an open window. Current speech recognition technology performs extremely poorly in such conditions.

The research project built on an existing framework know as `speech fragment decoding'. This intuitively simple approach, inspired by the `scene analysis' account of auditory perception, operates in two stages: first, signal processing techniques are used to split the acoustic mixture into local time-frequency `fragments' of individual sound sources; second, statistical models of speech and other sources are employed to match and select fragments belonging to the target speech source while rejecting fragments coming from distracting sound sources.

The project had several important objectives that were met in the three year running period. First, a collection of over 50 hours of audio data was made using recordings from real domestic living spaces. Using this data a noisy speech recognition challenge was designed that has since been used as the basis of an international robust speech recognition evaluation campaign. Second, novel approaches were developed that combine sound-direction and speech-pitch cues to better locate the individual sound source fragments. Third, new ways of combining models of speech and models of the noise background were developed that are better able to distinguish between speech and masking noise. Finally, a new improved version of our software framework has been generated and is publicly available on request for use by other researchers. 

The project has been successful in raising the profile of the `multisource environment' hearing problem. Beyond the usual dissemination via journal and conference publication, in 2011 a CHiME workshop and ASR evaluation was organised with multisource environments as its central theme. The workshop has inspired a special issue of the journal, Computer Speech and Language that will be published next year. A 2nd CHiME evaluation and workshop is planned for 2013 this time with industrial support and financial sponsorship (http://spandh.dcs.shef.ac.uk/chime_workshop/).</gtr:description><gtr:exploitationPathways>The CHiME project's research objectives have influence the wider speech recognition and signal processing community. There is now a well-established series of speech recognition evaluations, called the CHiME challenges, that targets the problem of distant microphone speech recognition in everyday environments. 

http://spandh.dcs.shef.ac.uk/chime_challenge/

These Challenges have attracted significant industrial interest both in the form of sponsorship and participation.</gtr:exploitationPathways><gtr:id>0CCB04A6-2AE1-4142-9DAB-911A5B480F74</gtr:id><gtr:outcomeId>56db0a868521e1.47259010</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://spandh.dcs.shef.ac.uk/projects/chime/index.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2A6D73C1-06A0-487B-8A37-80521D70F03C</gtr:id><gtr:title>An analysis of environment, microphone and data simulation mismatches in robust speech recognition</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76932d4d499b7c53fc66a3b063bfd255"><gtr:id>76932d4d499b7c53fc66a3b063bfd255</gtr:id><gtr:otherNames>Vincent E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c0021ca099e1.53390581</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>318E2B9F-32A4-482B-AD1C-9D2D30763CEA</gtr:id><gtr:title>Combining Speech Fragment Decoding and Adaptive Noise Floor Modeling</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1b2866c216d4e82149d595671349d946"><gtr:id>1b2866c216d4e82149d595671349d946</gtr:id><gtr:otherNames>Ma N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>56db0436abecd5.65862585</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6A3E2F2-9A08-476D-9B32-96771EF94915</gtr:id><gtr:title>A hearing-inspired approach for distant-microphone speech recognition in the presence of multiple sources</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1b2866c216d4e82149d595671349d946"><gtr:id>1b2866c216d4e82149d595671349d946</gtr:id><gtr:otherNames>Ma N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56db04362fba71.52511026</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AF30809-A86D-4F91-9CE7-E35201C1DB24</gtr:id><gtr:title>MMSE-Based Missing-Feature Reconstruction With Temporal Modeling for Robust Speech Recognition</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0a2d0d03afd56e14e57c60320a94897e"><gtr:id>0a2d0d03afd56e14e57c60320a94897e</gtr:id><gtr:otherNames>Gonzalez J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56db043667ed02.98245461</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43657261-D22C-4FD9-98D8-5C0C470559F9</gtr:id><gtr:title>Speech fragment decoding techniques for simultaneous speaker identification and speech recognition</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ebfae3358ad60a5be3c295c3af021f6b"><gtr:id>ebfae3358ad60a5be3c295c3af021f6b</gtr:id><gtr:otherNames>Barker J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>56db043728ae56.85588407</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F843CA46-F803-48D1-8761-7FE49519237A</gtr:id><gtr:title>The PASCAL CHiME speech separation and recognition challenge</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ebfae3358ad60a5be3c295c3af021f6b"><gtr:id>ebfae3358ad60a5be3c295c3af021f6b</gtr:id><gtr:otherNames>Barker J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56db04364c6d98.24116228</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB37848F-C79C-4E98-811A-198E7A4C8341</gtr:id><gtr:title>The third 'CHiME' speech separation and recognition challenge: Analysis and outcomes</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ebfae3358ad60a5be3c295c3af021f6b"><gtr:id>ebfae3358ad60a5be3c295c3af021f6b</gtr:id><gtr:otherNames>Barker J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c0021cd115b3.28444665</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G039046/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>