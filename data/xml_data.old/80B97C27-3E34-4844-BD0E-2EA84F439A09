<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FD66C955-5923-42A3-A9A8-8D66CA2E5942"><gtr:id>FD66C955-5923-42A3-A9A8-8D66CA2E5942</gtr:id><gtr:name>Home Office Sci Dev't Branch</gtr:name><gtr:address><gtr:line1>Home Office Sci Dev't Branch Langhurst</gtr:line1><gtr:line2>Langhurst House</gtr:line2><gtr:line3>Langhurstwood Road</gtr:line3><gtr:line4>Horsham</gtr:line4><gtr:line5>West Sussex</gtr:line5><gtr:postCode>RH12 4WX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0"><gtr:id>09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0</gtr:id><gtr:firstName>Josef</gtr:firstName><gtr:surname>Kittler</gtr:surname><gtr:orcidId>0000-0002-8110-9205</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/1CEC5A55-AE07-4A25-B172-EAAE8397BF64"><gtr:id>1CEC5A55-AE07-4A25-B172-EAAE8397BF64</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Bowden</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE027946%2F1"><gtr:id>80B97C27-3E34-4844-BD0E-2EA84F439A09</gtr:id><gtr:title>LILiR2 - Language Independent Lip Reading</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E027946/1</gtr:grantReference><gtr:abstractText>It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading, particularly in non-laboratory environments. This project will collect data for lip-reading and use it to build automatic lip-reading systems: machines that convert videos of lip-motions into text. To be effective such systems must accurately track the head over a variety of poses; extract numbers, or features, that describe the lips and then learn what features correspond to what text. To tackle the problem we will need to use information collected from audio speech. So this project will also investigate how to use the extensive information known about audio speech to recognise visual speech.The project is a collaboration between the University of East Anglia who have previously developed state-of-the-art speech reading systems; the University of Surrey who built accurate and reliable face and lip-trackers and the Home Office Scientific Branch who wish to investigate the feasibility of this approach for crime fighting.</gtr:abstractText><gtr:fund><gtr:end>2010-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>350596</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of East Anglia</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>UEA</gtr:description><gtr:id>FEF5E891-C685-40C8-90FD-635ACF48A31F</gtr:id><gtr:impact>We have published many joint papers. We have also received 5 years work of funding from UK gov to continue the work.</gtr:impact><gtr:outcomeId>54415ae29320f5.88744351-1</gtr:outcomeId><gtr:partnerContribution>This was a joint research project between the two institutions.</gtr:partnerContribution><gtr:piContribution>This was a joint research project between the two institutions. However, I only report outcomes from our half of the project</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>100090</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>MoD Phase 3</gtr:description><gtr:end>2013-03-02</gtr:end><gtr:fundingOrg>Ministry of Defence (MOD)</gtr:fundingOrg><gtr:id>8AAB358F-FE3A-497E-B530-EFDEF017E71B</gtr:id><gtr:outcomeId>5ec810105ec81024</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2012-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>140522</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>MoD Phase 2</gtr:description><gtr:end>2012-03-02</gtr:end><gtr:fundingOrg>Ministry of Defence (MOD)</gtr:fundingOrg><gtr:id>874777E1-2343-43AB-907B-0BEFA0E5EB2A</gtr:id><gtr:outcomeId>5eba969c5eba96b0</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>100000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>MoD Phase 4</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>Ministry of Defence (MOD)</gtr:fundingOrg><gtr:id>5C920C2F-C55F-484A-BA7C-93CBBF612D4B</gtr:id><gtr:outcomeId>r-4426022595.19139606a2acce</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>114453</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>MoD Phase 1</gtr:description><gtr:end>2011-03-02</gtr:end><gtr:fundingOrg>Ministry of Defence (MOD)</gtr:fundingOrg><gtr:id>3FFF57BE-11B7-425A-9E20-B7C9A98EA879</gtr:id><gtr:outcomeId>r-5539613920.7665068449b4</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2010-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We are in the process of liscensing IPR generated from this project to a UK SME</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>6FCB69F4-9474-4F7A-9CD3-2B3E529E49AB</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>544144e3808344.40518787</gtr:outcomeId><gtr:sector>Creative Economy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Under a KTA we extended the linear tracker code developed in this project into a tool for the post production industry</gtr:description><gtr:grantRef>EP/E027946/1</gtr:grantRef><gtr:id>EAF5C1A4-B9CB-4FED-862B-70D90673908B</gtr:id><gtr:impact>.</gtr:impact><gtr:licensed>Commercial In Confidence</gtr:licensed><gtr:outcomeId>5441602b5d10a7.80789974</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>A new approach to rotoscoping using linear predictor tracking</gtr:title></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>We developed methods for language identification. recognition of non verbal cues, accurate facial feature tracking and person dependant lip reading.</gtr:description><gtr:exploitationPathways>We have already applied this work in a number of demonstration systems for both government and commercial clients. The tracking technology is currently in the process of being licenced.</gtr:exploitationPathways><gtr:id>DDCDDB61-4D09-43E9-99C9-9EA1CBD1C99B</gtr:id><gtr:outcomeId>5441463936ded0.31243154</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Government, Democracy and Justice,Retail</gtr:sector></gtr:sectors><gtr:url>http://www.ee.surrey.ac.uk/Projects/LILiR/index.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Several audio visual datasets were captured and released</gtr:description><gtr:id>039CDCF0-32E1-442F-9BE4-C14A853B2632</gtr:id><gtr:impact>These datasets were and still are fundamental in our work in terms of benchmarking techniques. They have also been used by the wider research community.</gtr:impact><gtr:outcomeId>54415fa9ef6f49.43647835</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>LILIR Datasets</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.ee.surrey.ac.uk/Projects/LILiR/datasets.html</gtr:url><gtr:yearFirstProvided>2010</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>42878B90-0F50-4265-BCDC-97EDBB9E589C</gtr:id><gtr:title>Robust Facial Feature Tracking Using Shape-Constrained Multiresolution-Selected Linear Predictors.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52cfbc204d5d4847a7daecdb9c192275"><gtr:id>52cfbc204d5d4847a7daecdb9c192275</gtr:id><gtr:otherNames>Ong EJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05ef8a2d88</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E1F2021A-F122-45FB-BC90-19245219C211</gtr:id><gtr:title>Learning temporal signatures for Lip Reading</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f1d770686de08b20027dea3bcaef3cb1"><gtr:id>f1d770686de08b20027dea3bcaef3cb1</gtr:id><gtr:otherNames>Ong E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4673-0062-9</gtr:isbn><gtr:outcomeId>544157915a81b2.32982857</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A64578F-78DB-4022-AEE4-E7C1422AA029</gtr:id><gtr:title>Comparing visual features for lipreading</gtr:title><gtr:parentPublicationTitle>AVSP-2009</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>54415925002976.05362605</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CBF939F2-C789-4920-AFD2-03530A9C7871</gtr:id><gtr:title>MIMiC: Multimodal Interactive Motion Controller</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ca5c90966ab1031cda5a1a8a75f81ee"><gtr:id>4ca5c90966ab1031cda5a1a8a75f81ee</gtr:id><gtr:otherNames>Okwechime D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d05e05ea07faba</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61A80DAE-2F84-4799-8E8B-68A860017BBA</gtr:id><gtr:title>Feature selection of facial displays for detection of non verbal communication in natural conversation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c3a8ae006a1c52df3ef7c0e0f390538"><gtr:id>8c3a8ae006a1c52df3ef7c0e0f390538</gtr:id><gtr:otherNames>Sheerman-Chase T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4442-7</gtr:isbn><gtr:outcomeId>54415790ad29c9.56199631</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4D2EF262-55DE-4906-A8AB-8C2918BE0094</gtr:id><gtr:title>Robust facial feature tracking using selected multi-resolution linear predictors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f1d770686de08b20027dea3bcaef3cb1"><gtr:id>f1d770686de08b20027dea3bcaef3cb1</gtr:id><gtr:otherNames>Ong E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4420-5</gtr:isbn><gtr:outcomeId>doi_53d05805866a8302</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8513BB7D-A95D-4B5C-9D1B-8499110F68B4</gtr:id><gtr:title>Visualisation and prediction of conversation interest through mined social signals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ca5c90966ab1031cda5a1a8a75f81ee"><gtr:id>4ca5c90966ab1031cda5a1a8a75f81ee</gtr:id><gtr:otherNames>Okwechime D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4244-9140-7</gtr:isbn><gtr:outcomeId>544157918ef4f4.92147429</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BB7A1D18-8012-4A0B-9D4F-FC754F09F686</gtr:id><gtr:title>Online learning of robust facial feature trackers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c3a8ae006a1c52df3ef7c0e0f390538"><gtr:id>8c3a8ae006a1c52df3ef7c0e0f390538</gtr:id><gtr:otherNames>Sheerman-Chase T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4442-7</gtr:isbn><gtr:outcomeId>54415790e0c429.46612857</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D3F2B06E-B45F-40DF-805B-B768DFF7C399</gtr:id><gtr:title>Analysis and Modeling of Faces and Gestures</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96d5647ba90b3cbbf1d24092c7e36fbc"><gtr:id>96d5647ba90b3cbbf1d24092c7e36fbc</gtr:id><gtr:otherNames>Moore S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:isbn>978-3-540-75689-7</gtr:isbn><gtr:outcomeId>544157903b25d6.87123533</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>54A7735F-AE08-491D-84F6-BCEEAEBFB353</gtr:id><gtr:title>Real-time motion control using pose space probability density estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ca5c90966ab1031cda5a1a8a75f81ee"><gtr:id>4ca5c90966ab1031cda5a1a8a75f81ee</gtr:id><gtr:otherNames>Okwechime D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4442-7</gtr:isbn><gtr:outcomeId>544157900194e8.32368757</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8FBFBA31-E44B-4F3B-AD06-BAA148614E27</gtr:id><gtr:title>Estimating the joint statistics of images using nonparametric windows with application to registration using mutual information.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4d8584b5eea0ce3ea2eb7929238a157d"><gtr:id>4d8584b5eea0ce3ea2eb7929238a157d</gtr:id><gtr:otherNames>Dowson N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05ef115bd6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7024B63C-C042-4CFA-B41F-422FD7C3DB48</gtr:id><gtr:title>Cultural factors in the regression of non-verbal communication perception</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c3a8ae006a1c52df3ef7c0e0f390538"><gtr:id>8c3a8ae006a1c52df3ef7c0e0f390538</gtr:id><gtr:otherNames>Sheerman-Chase T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4673-0062-9</gtr:isbn><gtr:outcomeId>54415790739c31.98572506</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F29D1BCC-6676-420E-82BA-92AF4309824C</gtr:id><gtr:title>Local binary patterns for multi-view facial expression recognition</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96d5647ba90b3cbbf1d24092c7e36fbc"><gtr:id>96d5647ba90b3cbbf1d24092c7e36fbc</gtr:id><gtr:otherNames>Moore S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53cfecfec48c6a48</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E027946/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>