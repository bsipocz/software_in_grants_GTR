<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/679E3226-7E46-4F18-90B6-0B4FE734C099"><gtr:id>679E3226-7E46-4F18-90B6-0B4FE734C099</gtr:id><gtr:name>Glasgow Caledonian University</gtr:name><gtr:department>Sch of Engineering &amp; Built Environment</gtr:department><gtr:address><gtr:line1>70 Cowcaddens Road</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G4 0BA</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/679E3226-7E46-4F18-90B6-0B4FE734C099"><gtr:id>679E3226-7E46-4F18-90B6-0B4FE734C099</gtr:id><gtr:name>Glasgow Caledonian University</gtr:name><gtr:address><gtr:line1>70 Cowcaddens Road</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G4 0BA</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/ED33366A-646B-4D42-96D8-F4423BA37045"><gtr:id>ED33366A-646B-4D42-96D8-F4423BA37045</gtr:id><gtr:firstName>Raymond</gtr:firstName><gtr:surname>MacDonald</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/71F1B679-2A54-4A6F-BB57-C96EE18E2697"><gtr:id>71F1B679-2A54-4A6F-BB57-C96EE18E2697</gtr:id><gtr:firstName>Donald</gtr:firstName><gtr:surname>Knox</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF00558X%2F1"><gtr:id>FE28B9C9-C2A2-4F43-8E63-BA7977D44EFD</gtr:id><gtr:title>Emotion Classification in Contempory Music</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F00558X/1</gtr:grantReference><gtr:abstractText>The aim of the project is to develop software which can automatically classify the emotional content of any piece of music. The recent increase in popularity of large personal digital music collections and online music retailers has led to a need for new, interactive access methods. Current interfaces to large music databases only allow users to search for music by genre, artist, or similarity to other music items.There is very little research in the area of automatically extracting the emotional content of music. Existing methods have been tested only on classical music pieces, and these algorithms have tended to rely on a fairly limited set of musical features. There is a variety of music features such as mode (a given series of musical intervals), tonality (the relationship between pitches in the music), and pitch (perceived frequency) which are key to expressing emotion in the composition of music. The project aim is to develop software that uses these features to more accurately identify the emotional content of any piece of musicAllowing the user to browse for music by its emotional effect represents a step toward addressing the needs and preferences of the user in music information retrieval. This technology will allow the user to easily select a subset of their own personal music database to suit their current mood. Software like this is a good example of how technology can seamlessly merge into our daily activities.</gtr:abstractText><gtr:fund><gtr:end>2010-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>82245</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Edinburgh</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Scottish Music and Health Network</gtr:description><gtr:id>BD717457-8D37-4F0B-BD7C-FABBB545842B</gtr:id><gtr:impact>One major one-day event hosted at Univ. Edinburgh June 2014.
One Leverhulme grant application (awaiting decision).</gtr:impact><gtr:outcomeId>54466222aaa511.44474750-1</gtr:outcomeId><gtr:partnerContribution>Psychology, music therapy, music for health and wellbeing.</gtr:partnerContribution><gtr:piContribution>Music technology expertise in a cross-disciplinary setting (Psychology and Health)</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The findings from my research have been cited in the academic research of others. This has crossed disciplines, has been predominantly in the are of music psychology, and has influenced research in the area of health and wellbeing.</gtr:description><gtr:id>348B013B-C65B-4DD8-A0D4-D67A4E4772E2</gtr:id><gtr:impactTypes/><gtr:outcomeId>54465ff489d0b1.79491995</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project aimed to develop audio signal analysis algorithms which automatically classify music in terms of the emotion it expresses. The project focused on the analysis of digital audio files with the aim of extracting a large number of acoustical and structural parameters. This stage of analysis is followed by statistical classification schemes used to group these parameters in terms of their importance to the expression of musical emotion. In particular, this project concentrated upon western contemporary music - this being both the most popular with listeners, and difficult to classify by automatic means.

A main output from the project is the development of emotion classification algorithms, which more accurately label popular music for emotion. Another output is identification of an audio analysis feature set most salient to the expression of emotion in western contemporary music. A full description of the operation of these algorithms is presented in published outputs, both complete and in preparation. 

An key route this research has taken recently is the analysis and mood classification of music chosen by participants in psychological studies of music listening for health and wellbeing. For example the use of preferred music listening in studies of pain perception and tolerance. The algorithms allow analysis of music used in these experiments in unprecedented detail, and allow examination of the role of particular acoustical parameters and expressed emotion in music chosen to be particularly beneficial as regards reduction of anxiety and tolerance of pain. The findings are discussed in the Journal of the Acoustical Society of America (Knox et al, JASA Vol. 130, No. 3. Sept 2011).</gtr:description><gtr:exploitationPathways>Music browsing and recommendation technology is a key area where the techniques developed in this project may have potential commercial benefits. A key means by which listeners access music is through large online music databases such as Spotify and Last.fm. These systems host millions of songs, and are dependent upon music recommendation algorithms which seek to recommend music to the listener. This recommendation might be made on the basis of the content of the music, the music choices of peers, and so-on. A key reason given for music listening is 'how it makes me feel', and emotion expressed by the music plays a role in influencing the music choices made by the listener.</gtr:exploitationPathways><gtr:id>1F4CAEE3-1C9D-43D2-89F7-86D230DD9D37</gtr:id><gtr:outcomeId>r-6542921575.2919477751c70</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>https://www.researchgate.net/profile/Don_Knox</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9F43650E-E6F6-4879-8E32-4902F308CC96</gtr:id><gtr:title>Acoustic analysis and mood classification of pain-relieving music.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b04eeb5b42d9d8fec8b190dad2c0265"><gtr:id>7b04eeb5b42d9d8fec8b190dad2c0265</gtr:id><gtr:otherNames>Knox D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>doi_53d06e06eaad6f14</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>881E9771-C00D-449E-B733-4F23AC13E522</gtr:id><gtr:title>Acoustic variables in the communication of composer emotional intent</gtr:title><gtr:parentPublicationTitle>ICMPC-ESCOM 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8e893477d6ffecb77d8cccdc6881e529"><gtr:id>8e893477d6ffecb77d8cccdc6881e529</gtr:id><gtr:otherNames>Don Knox</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>543eae137d4016.48923485</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>239E7F94-10B0-4D13-94ED-A3253832A0CF</gtr:id><gtr:title>Music emotion classification by audio signal analysis: Analysis of self-selected music during game play</gtr:title><gtr:parentPublicationTitle>10th International Conference on Music Perception and Cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fcb0316f4df96ba740540a9e633d645c"><gtr:id>fcb0316f4df96ba740540a9e633d645c</gtr:id><gtr:otherNames> Don Knox</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>r_7276455488cac5fad2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39734F65-7E06-42D6-9A0B-6F23F5C490EA</gtr:id><gtr:title>An Exploration of the Effect of Structural and Acoustical Features on Perceived Musical Emotion</gtr:title><gtr:parentPublicationTitle>Proceedings of Audio Mostly 2009, 4th Conference on Interaction with Sound</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f67a2743806c6eed63b08c9e51d13762"><gtr:id>f67a2743806c6eed63b08c9e51d13762</gtr:id><gtr:otherNames> Scott Beveridge (co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>m_5281780734140646cc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8394B498-AE47-40DF-A553-DA789B13F2FA</gtr:id><gtr:title>Emotion classification of Western contemporary Music: Identifying a representative feature set</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/189ee8bc25985bd0d6f09fa8eeb6225a"><gtr:id>189ee8bc25985bd0d6f09fa8eeb6225a</gtr:id><gtr:otherNames>Scott Beveridge (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_2688485454140645b4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>501EA6C6-E97B-4ED6-8FFA-6D1AA2E9BC73</gtr:id><gtr:title>An analysis of structural components of music chosen for pain relieving purposes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/86ff9850c13f042405c21515a499f91b"><gtr:id>86ff9850c13f042405c21515a499f91b</gtr:id><gtr:otherNames>Raymond MacDonald (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_63424207251403370c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D5811C40-B1DA-4EFA-9826-DEB6F258A3DC</gtr:id><gtr:title>Emotion Recognition in Western Popular Music: The Role of Melodic Structure</gtr:title><gtr:parentPublicationTitle>ICMPC-ESCOM 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2af2b5d2a7995a3ff1da35974ce834ab"><gtr:id>2af2b5d2a7995a3ff1da35974ce834ab</gtr:id><gtr:otherNames>Scot Beveridge (co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>543ead5e36cdc4.13466761</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95D1FC8A-4E48-4E4D-94EC-D9F20524A88A</gtr:id><gtr:title>A Feature Survey for Emotion Classification of Western Popular Music</gtr:title><gtr:parentPublicationTitle>The 9th International Symposium on Computer Music Modeling and Retrieval (CMMR): Music and Emotions</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c756c8424dc1e5a468cdab79b38b5f4b"><gtr:id>c756c8424dc1e5a468cdab79b38b5f4b</gtr:id><gtr:otherNames>Scott Beveridge (co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>543eac1f2be670.24062403</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F00558X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>