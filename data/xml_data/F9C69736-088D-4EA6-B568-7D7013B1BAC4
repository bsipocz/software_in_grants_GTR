<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/EAAD4D43-BD15-432B-9385-2DBD0C65958D"><gtr:id>EAAD4D43-BD15-432B-9385-2DBD0C65958D</gtr:id><gtr:name>University of Bath</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>University of Bath</gtr:line1><gtr:line2>Claverton Down</gtr:line2><gtr:line4>Bath</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EAAD4D43-BD15-432B-9385-2DBD0C65958D"><gtr:id>EAAD4D43-BD15-432B-9385-2DBD0C65958D</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>University of Bath</gtr:line1><gtr:line2>Claverton Down</gtr:line2><gtr:line4>Bath</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/7B3C0C16-DEA3-4C5B-8405-10907BB3B6C8"><gtr:id>7B3C0C16-DEA3-4C5B-8405-10907BB3B6C8</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:otherNames>Maxwell</gtr:otherNames><gtr:surname>Hall</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK015966%2F1"><gtr:id>F9C69736-088D-4EA6-B568-7D7013B1BAC4</gtr:id><gtr:title>Classifying Images Regardless of Depictive Style</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K015966/1</gtr:grantReference><gtr:abstractText>Computer's today can recognise objects in photographs. This ability has formed the basis of many familiar applications such as Facebook tagging, Google Image Search, Google Goggles, and automated passport checking at UK borders.

Yet a significant restriction remains: computers can only recognise objects in photographs. At least, their ability to recognise objects in
drawings and paintings - in artwork of any kind - is strictly limited. If this limitation can be overcome then many more applications will
become possible.

One is a new way to search the web for images in which a drawing (say) is dragged from the desktop into a search bar,
and paintings and photographs are given back to the user (at the moment a user gets the same sort of image back as was dragged into the
search bar).

Another is the automated production of catalogues for taxonomy - which is important to scientists faces with tens of thousands of
microscopic creatures; species catalogues are hand-drawn right now so automation would be a significant advance for them.

The output of the programme would also allow ordinary photographs to be converted to icons. This is not as dry as it sounds, but could help the visually impaired to gain access to photographic content. If photographs and drawings can be linked in the way this project has in mind, then
objects in photographs could be turned into icons rendered by a set of raised pins. So there would be a symbol for car, say, not unlike that which might be drawn by a child - and in fact this is very close to the icons blind artists draw. This would allow the visually impaired to read photographs in newspapers, or in text books, and allow them to share the holiday snaps of family and friends.

This proposal is about building the basic technology that underpins these applications, and quite possibly others too. Key to it is lifting the barrier that computers of today face - allowing them to recognise objects no matter how they are depicted.</gtr:abstractText><gtr:potentialImpactText>The impact of this proposal will show in the near, medium, and longer terms.
It will also show in the academic sector, the industrial sector, in society, and is media friendly.
We plan to pursue impact in all of these.

The academic beneficiaries are detailed in another section of this form and so will not be detailed here,
expect to say that medium and longer impact requires further input from the academic communities.

Impact in the industrial sector has an obvious route via sophisticated search engines, as built by companies such as Google and Microsoft.
Internet search will benefit because the total of all images includes not just photographs but also artwork of all kinds.
The ability to pose queries as sketches and retrieve photographs is a current research topic; this proposal would not contribute directly
but we do plan to build a prototype as our application (objective 4). We note that Google offer PhD studentships, a route we intend to take advantage of as this proposal progresses, with a view to constructing a more substantive web-search application, and so we regard it as a medium term aim.

In terms of wider society, we find the possibility of allowing the visually impaired access to photographic content particularity appealing. In fact, because our philosophy is to not discriminate between depictive styles, our approach should allow the visually impaired access to a mush wider variety of images than at present. Access is possible if we can summarise visual content so as to remove unwanted clutter; recognising objects and synthesising icons that can be felt allows people to read newspapers, share photos, and generally raises their standard of living.
This would a medium to long-term aim.

The proposal is media friendly: the ability to drag a photograph of the Queen onto a search bar and have all of her portraits returned is something that would be of general interest. It is to the benefit of the research community as a whole and to research councils in particular to receive favourable publicity.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-06-23</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-06-24</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>241836</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>workshop organisation</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9085DB45-2377-4B88-91B2-E3980BED100D</gtr:id><gtr:impact>We have organised and helped to organise the &amp;quot;Expressive&amp;quot; triad of workshops, and the VisArt workshop. Our work in the area of cross depiction and NPR is therefore helping bring together computer graphics, computer vision, cultural history, and have impact with the British Librayr, the RNIB, as well ad CAD companies and others.</gtr:impact><gtr:outcomeId>56df07e2bf9ea5.45467071</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2014,2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Interested new industrial partners include The Foundry, CAD/CAM companies, and automotive companies.

But the most interesting developments are with the British Library and RNIB. If we work together we may make BL's content easier to access for all, and some it accessible to blind people for the first time ever.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>A4651CFD-AE3F-412A-B4F4-D66AAEA88481</gtr:id><gtr:impactTypes/><gtr:outcomeId>546a19224084f9.05038590</gtr:outcomeId><gtr:sector>Creative Economy,Other</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>All current methods for object recognition, including ours, are able to recognise objects in photographs very well. However, no current method except ours is able to recognise objects in both photographs and art .

Our key findings
+ Released two cross-depiction datasets. Currently no such dataset exists for cross-depiction problem.
+ Conducted extensive experiments to benchmark classification, domain adaptation, detection and deep learning methods on the cross-depiction task. It greatly helps the computer vision community to understand the performance of leading techniques on this new field and gives insights for potential solutions.
+ Developed a multi-labeled graph model with learned discriminative weights which is able to model object classes over a broad range of depictive styles.
+ Adapted DPM with cross-depiction expansion to bridge the gap between photo and art domains, leading to a significant rise in performance.
+ Adapted the state-of-the-art deep learning method -- fast-RCNN (Regional Convolutional Neural Networks) to detect people in all kinds of art images. 
+ Designed dual convolutional neural networks to simultaneously minimise the classification error and the domain discrepancy.</gtr:description><gtr:exploitationPathways>This is a growing area in Computer Vision. Web search engines, commercial companies requiring advanced indexing, even converting photos to icons for (eg) blind people. We plane also the extend the work to 3D objects and to video.

The work is of broad value, with expressions of interest from the RNIB, British Library, car component manufacturers, CAD companies, and the creative sector.

We plan to meet BL and RNIB to discuss joint ways forward.</gtr:exploitationPathways><gtr:id>4793B922-7181-4149-916C-977688F7AE01</gtr:id><gtr:outcomeId>546a19b479a759.38629976</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Culture, Heritage, Museums and Collections,Other</gtr:sector></gtr:sectors><gtr:url>http://opus.bath.ac.uk/41062/2/main.pdf</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A collection of 4000+ images, each showing at least one human figure. The images come in a broad range of artistic styles.</gtr:description><gtr:id>25A61414-C31F-42D8-9773-4FDC5D272DC6</gtr:id><gtr:impact>The database shows the failure of all contemporary computer vision methods to detect people in artwork. It has motivated our current research direction.</gtr:impact><gtr:outcomeId>56c2fe2a6d3839.09248049</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>People-Art</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>A collection of images; artwork of all kinds. This augments the famous CalTech-256 dataset with artwork it was previously lacking.</gtr:description><gtr:id>E54C4F8D-E7F8-4A56-B98A-B816B9C0C1DB</gtr:id><gtr:impact>The data was used to show a failure mode for all contemporary methods in computer vision for recognition; to explain the failure empirically; and the to address it. A URL will be published shortly.</gtr:impact><gtr:outcomeId>56c2fd61b6e1e7.57914020</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Photo-Art-50</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>E74940F1-B191-4CC6-9E23-BF5E0E7297E1</gtr:id><gtr:title>Beyond Photo Domain Object Recognition: Benchmarks for the Cross Depiction Problem</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b4d099021b017a4a40fe01dcb55d9ec0"><gtr:id>b4d099021b017a4a40fe01dcb55d9ec0</gtr:id><gtr:otherNames>Cai H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56c2f7fe975f01.11329270</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1BEFD769-1F7A-451E-8914-E21E5E4EBE4D</gtr:id><gtr:title>Cross-depiction problem: Recognition and synthesis of photographs and artwork</gtr:title><gtr:parentPublicationTitle>Computational Visual Media</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eedf2b367c7e98ff88bd11fb621f949a"><gtr:id>eedf2b367c7e98ff88bd11fb621f949a</gtr:id><gtr:otherNames>Hall P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56c2f171efe717.03288628</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K015966/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>