<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Mechanical Engineering</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F47B5EEA-B32F-40B5-84BA-681344D7422B"><gtr:id>F47B5EEA-B32F-40B5-84BA-681344D7422B</gtr:id><gtr:firstName>J</gtr:firstName><gtr:surname>Burn</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0A73F6B3-EE27-4627-B3A4-6F6BFA7772AA"><gtr:id>0A73F6B3-EE27-4627-B3A4-6F6BFA7772AA</gtr:id><gtr:firstName>Walterio</gtr:firstName><gtr:otherNames>Wolfgang</gtr:otherNames><gtr:surname>Mayol-Cuevas</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/60D5ACA5-CD46-4D4F-B79D-0F574E16F291"><gtr:id>60D5ACA5-CD46-4D4F-B79D-0F574E16F291</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Bull</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3F5FF400-3C1E-434C-A2E6-73F6536F2ECA"><gtr:id>3F5FF400-3C1E-434C-A2E6-73F6536F2ECA</gtr:id><gtr:firstName>Iain</gtr:firstName><gtr:otherNames>Donald</gtr:otherNames><gtr:surname>Gilchrist</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ012025%2F1"><gtr:id>CA7EEEC5-5D23-4D5B-A7EC-FC86CFD95567</gtr:id><gtr:title>Bioinspired vision processing for autonomous terrestrial locomotion</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J012025/1</gtr:grantReference><gtr:abstractText>Land vehicles have been designed almost exclusively to use wheels whereas terrestrial animals almost exclusively use legs for locomotion. Wheeled systems can be fast and efficient on hard flat ground; leg based systems are more versatile and efficient on natural terrain. As we move towards a future of autonomous systems operating beyond the extent of the road network and on other planets it is likely that development of robust artificial leg-based locomotion will become increasingly important.
At present, several limits of technology prevent the emergence of autonomous legged systems with biocomparable performance. Even if a system was to emerge that could walk, run, leap, and turn without falling over, the technology does not exist safely to guide it through complex terrain using vision. Typically research into using vision for autonomous locomotion is undertaken using available vehicle technology - suggesting that the emergence of high-performance, vision-guided legged systems might occur at some time following the emergence of a basic high performance legged vehicle platform. In a novel approach we will expedite the development of a vision control architecture for locomotion over complex terrain by using human subjects as high performance vehicle platforms.
The visual scene captured using a head mounted camera will be processed to identify terrain characteristics known to be important for control of locomotion. A map of the terrain synthesised in 3D virtual space and updated in real-time is presented to the human using a virtual reality headset. The overall outcome measure will be the locomotion performance achieved by the humans using the system compared to that with no vision information available and with normal vision.
There are many benefits of this approach: it will allow us to investigate how humans modulate gait paramters and limb mechanics to compensate for partial or unreliable inforamtion about the environment. It will provide insight into the integration of feedforward and feedback control of locomotion. It will allow us to determine the locomotion performance that is possible from a given amount and quality of visually derived information given a highly developed locomotor platform and thus to understand how these two components of a high performance locomotor sytem combine to determine overall performance.
The basic principles and technologies establilsed during this project will be applicable to any land vehicle whether based on wheels or legs. Additionally, the processing of visual information for locomotion control is a special case of the more generalised task to search the ground for an object or visual feature. The technology developed in this project may be translated to other applications in which visually-guided autonomous function is required.</gtr:abstractText><gtr:potentialImpactText>(See Academic Beneficiaries for impact in the Academic community)

Systems for autonomous land locomotion have potential application accross most major industry sectors (see case for support). The nature of the work in this project is to develop fundamental key enabling technology for autonomous locomotion, which will in turn enable a wide range of autonomous systems. The full extent of the impact therefore will be wide and over the long term. Two types of company may derive short term benefit from our research (1) Solutions Providers such as BAE Systems and SCISYS who are already engaged in programmes of R&amp;amp;D of autonomous systems (2) Industries with specific and well defined applications for autonomous systems, these include the partner companies Sellafield Limited, Network Rail, National Nuclear Laboratory.

Core technology will be developed using advanced signal processing techniques to map features in the enviroment from fused video and kinematic data. Short term beneficiares include industries engaged in visual mapping, autonomous systems and robotics, and those with 'foraging' type applications where a large geographical location is to be searched for certain localised visual characteristics.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-08-23</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-08-24</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>548721</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>As of November 2015 we have developed a prototype system that presents a human with a synthesised version of the visual environment in realtime using a single head-mounted camera to collect visual information and a VR headset. The video stream from the camera is split into two processing pathways which construct estimates of the shape of the environemnt and the materials from which it is made. We have shown that the performance of vision systems used controlling terrestrial locomotion can be optimised by satisfying certain relationships between basic system paramters such as camera height above ground, resolution, angle of view etc. We have shown experimentally that guided and stable locomotion is possible using a very small amount of key visual information and there is a relationship between the information rate and speed of progression.
Through 2016 we have focussed on developing hardware and software technology that will allow us to design rigorous experiments to understand how humans use vision to control locomotion when the visual information is of poor quality or incomplete.</gtr:description><gtr:exploitationPathways>the principle areas of application for our findings will be:
1) in medical research to develop tools to assist the partialy sighted
2) by autonomous systems engineers to control vehicles
3) by vision scientists to research the human vision systems</gtr:exploitationPathways><gtr:id>67539C07-A566-4234-85B8-DF366A6CF452</gtr:id><gtr:outcomeId>5464b4086e0aa8.71350011</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Construction,Digital/Communication/Information Technologies (including Software),Healthcare,Culture, Heritage, Museums and Collections,Pharmaceuticals and Medical Biotechnology,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>A synthesised visual environment containing a subset of visual information available using human vision is presented to a human using VR. The human wears a VR headset and a head-mounted camera. Software translates the camera view into a 3D geometric model of the environment which is projected onto a viewing plane and shown in the VR headset. The method allows the study of locomotor performance in reduced visual environments</gtr:description><gtr:id>FAC85543-7D20-48A1-84E9-EDEDE2FEB9F0</gtr:id><gtr:impact>none yet</gtr:impact><gtr:outcomeId>56e121082ebe65.24872869</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Realtime VR scene synthesis</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8F07419A-66AC-4CA8-9F21-D8DA680358EA</gtr:id><gtr:title>Visual salience and priority estimation for locomotion using a deep convolutional neural network</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15224880</gtr:issn><gtr:outcomeId>58c95d10a2f506.78343774</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>35CEE74B-62E5-421D-8761-7AE955012C4E</gtr:id><gtr:title>Terrain Classification From Body-Mounted Cameras During Human Locomotion.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>5675dd0a817e7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>07F3AE52-EE0E-4176-B722-988B8DC04353</gtr:id><gtr:title>Parameter optimisation for vision guided terrestrial locomotion: Multi-frame</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0f4df6ca336d729be3f2a2f0be23f204"><gtr:id>0f4df6ca336d729be3f2a2f0be23f204</gtr:id><gtr:otherNames>Daniels G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e12b89187891.85211744</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>33692A6D-99AF-4BD9-9FBC-0BEEBE903E5D</gtr:id><gtr:title>Projective image restoration using sparsity regularization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464bbc42bb264.57110716</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D1E22C1F-EA2F-46CA-94EA-6ADA81E15858</gtr:id><gtr:title>Robust texture features based on undecimated dual-tree complex wavelets and local magnitude binary patterns</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e12721ec2413.67241269</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E3EF42D-3D02-4788-B9AC-318F5B09F06F</gtr:id><gtr:title>ORIENTATION ESTIMATION FOR PLANAR TEXTURED SURFACES BASED ON COMPLEX WAVELETS</gtr:title><gtr:parentPublicationTitle>Proceedings of the 21st IEEE International Conference on Image Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>5464ba246f2b59.75512871</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5695B75C-BDF2-4279-83E8-087A4B32B24C</gtr:id><gtr:title>ROBUST TEXTURE FEATURES FOR BLURRED IMAGES USING UNDECIMATED DUAL-TREE COMPLEX WAVELETS</gtr:title><gtr:parentPublicationTitle>Proceedings of the 21st IEEE International Conference on Image Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>5464baedf31675.82896960</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BF3339F-ECD3-43FC-B1F3-D27A668F61E4</gtr:id><gtr:title>Fixation Prediction and Visual Priority Maps for Biped Locomotion.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>5a2fea3fdc7960.42917855</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>47B897E3-8491-42A5-AE16-4B5616BF83DE</gtr:id><gtr:title>Fixation identification for low-sample-rate mobile eye trackers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c974e28a0d58.98177491</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J012025/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>