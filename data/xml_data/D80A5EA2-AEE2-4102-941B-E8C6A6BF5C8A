<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FA6E1CBA-992B-4060-8F8F-513D55A737C6"><gtr:id>FA6E1CBA-992B-4060-8F8F-513D55A737C6</gtr:id><gtr:firstName>Krystian</gtr:firstName><gtr:surname>Mikolajczyk</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF003420%2F1"><gtr:id>D80A5EA2-AEE2-4102-941B-E8C6A6BF5C8A</gtr:id><gtr:title>Recognition of Object Categories and Scenes</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F003420/1</gtr:grantReference><gtr:abstractText>This research project proposes to advance state-of-the-art image recognition techniques to be able to recognize a large number ofscenes and object categories in real and unconstrained indoor andoutdoor environments i.e. traffic scenes (cars, bicycle vehicles,pedestrians, human faces, street signs etc.), urban and naturalscenes (buildings, landscapes etc.) with various rigid andarticulated objects as well as textures. Nowadaysalmost everybody carries a digital camera and taking a photo or ashort video has never been easier. Broadcasting companies receivethousands of pictures from the general public after every majorevent and the annotation of those documents is done manually. Crime investigators collect large amounts ofvisual evidence and its classification is also done manually. The UKhas the largest number of security cameras in Europe but the dataprovided by the cameras is very little explored. Furthermore,recognition and interpretation of visual information is one of themajor requirements for autonomous intelligent robots. There is therefore a dire need for a reliable recognition system capable of automatic classification and annotation of large amounts of visual documents. Any success towards achieving that goal i.e., automatic prioritizing of document browsing for experts, will be seen as a clear benefit in improvingthe efficiency of work.To fulfil the objectives of this project major progress has to bemade in the domain of features extraction, category representationand efficient search. Recent interest point based approachesdemonstrate the capability of dealing with large numbers ofcategories in the context of visual recognition. These methods showpromising directions towards successful scene and objectrecognition. Based on these results we propose to develop noveltechniques for extracting image features robust to backgroundclutter and viewpoint change, which are currently great challengesin image recognition domain. Those features will be suitable forsimultaneous representation of scenes and objects at variousappearance and structure levels as well as for segmentation ofobjects. Mid-level image segmentation methods have a potential toprovide such features and can bridge the gap between interest pointdetectors and semantic segmentation in the context of categoryrecognition. There has been little overlap between recognition andsegmentation domains although the goal is to solve both problemssimultaneously.We also propose to introduce novel hierarchical representationswhich will exploit the properties of new features and allow to dealefficiently with large number of image categories. Therepresentation will model the categories in multiple hierarchies ofvarious image attributes i.e., intensity, color and texture as wellas relations between different object parts and views. The multiplehierarchies will allow for coarse-to-fine classification based onimage cues relevant to the query. Very little work has been done inthis area and the proposed research can shed new light on imagerepresentation problems. Finally, efficient tree structures andnearest neighbor search techniques will be employed to handle largeamounts of data in multi-category learning.Developing novel, efficient and robust techniques which may providesuccessful solutions to fundamental recognition problems and advancethe state-of-the-art in feature extraction, categoryrepresentation and data exploration, make this project verychallenging and adventurous. The project is expected to achieve theobjectives within 36 months and it will involve a research student,a research assistant and the principal investigator.</gtr:abstractText><gtr:fund><gtr:end>2011-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>236179</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>BBC R &amp;amp; D</gtr:description><gtr:id>4EE51274-45DF-4E32-87FB-EC0929553BE3</gtr:id><gtr:impact>The collaboration defined several benchmark dataset for evaluating image classification and retrieval systems. Software packages were produced for feature extraction, image classification, and tracking. 
Several publications were output during this project which are listed in the publications section.</gtr:impact><gtr:outcomeId>545cee5aacda53.53915965-1</gtr:outcomeId><gtr:partnerContribution>BBC has provided image and video data for analysis, defined user requirements, participated in the data annotation and evaluation of the developed system.</gtr:partnerContribution><gtr:piContribution>Development of image retrieval and classification for exploring video archives of the BBC and user generated content for the BBC website. The system was presented at the BBC festival of research in 2008.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Image retrieval and classification as well as feature detectors developed in this project were used in various industrial applications such as video archive exploration by the BBC.
Matching of user generated content in the BBC web pages.
These state-of-the-art recognition systems were evaluated in international competitions in image classification. The recently developed recognition systems are submitted to independent evaluations organized by National Institute of Standards and Technology (NIST, USA), or Pascal Network of Excellence (Europe). Our image classification system with novel multi-kernel KDA classifiers won 3 different competitions (TrecVid, Pascal, ImageClef) in 2008, 2009, and 2010. 
The system was extensively tested as a part of a UAV robot designed for MoD Grand Challenge 2008. For design, construction and control of an autonomous reconnaissance robot the Swarm Systems Team received Best Innovative Idea Award from the Ministry of Defence (UK) in 2008. 


TLD technology is exploited in various professional fields all over the world.
Photron is the world's leading manufacture of high speed digital imaging systems. Head Office is located in Japan. TLD is used for motion analysis for video images in high speed camera software.
Virginia Tech is a public land-grant university with a main campus in Blacksburg, Virginia. TLD is used for building a face and object (vehicle) tracker.
Movcam is a company that develops and manufactures camera stabilizing devices. TLD technology is used in development autonomous camera and lens control system.
Indra Sistemas, is an information technology and defense systems company. TLD technology is used in development of EWE-8000 family emulators for antenna alignment.
Cladoop is a startup company focusing on computer vision, surveillance and big data. TLD technology is used for development of their core product.
MAGnet Systems is a software and hardware development company based in Canada. G2Associates is the authorized representatives of their products. TLD is used for devleopment of advanced object tracking capabilities for UAV.</gtr:description><gtr:firstYearOfImpact>2008</gtr:firstYearOfImpact><gtr:id>2E26567C-08B6-482D-A943-A8D87B912863</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545bc4415b08e4.38906183</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections,Retail,Transport</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>TLD is an award-winning, real-time algorithm for tracking of unknown objects in video streams. The object of interest is defined by a bounding box in a single frame. TLD simultaneously Tracks the object, Learns its appearance and Detects it whenever it appears in the video. The result is a real-time tracking that typically improves over time.</gtr:description><gtr:grantRef>EP/F003420/1</gtr:grantRef><gtr:id>BA016D16-DBF5-413D-9F38-51F37FE56FF6</gtr:id><gtr:impact>The tracking code has been licensed to a large international company by the University of Surrey under an NDA for exclusive commercial use in the entertainment business.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>545cf4946d8e41.09429643</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Tracking Learning Detection</gtr:title><gtr:yearProtectionGranted>2010</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>A Smart Camera that Learns from Experience' is addressing challenges in visual tracking i.e. following an object moving through video (well demonstrated in computer games). Instead of a programme that will inevitably make mistakes, Predator accepts the mistakes, stores them and is then able to make better decisions in the future - resulting in a unique, real-time visual tracking system that improves its performance over time with the end result resembling the performance of human vision.

A generic visual recognition system capable of dealing with large numbers of scene and object categories in unconstrained indoor and outdoor environments i.e. traffic scenes (vehicles, pedestrians, human faces, street signs etc.), urban (buildings, rooms) and natural scenes with various rigid and articulated objects as well as textures (landscapes, animals, vegetation). It addresses extremely challenging problems in visual recognition, which are simultaneous recognition, localization and segmentation of various objects and scenes independently of viewing conditions, with background clutter and occlusion. The human eye and brain has an outstanding ability to deal with these problems. Unfortunately, existing recognition systems are still far from this level of performance. One of the main limiting factors is the unlimited and unpredictable variability of the appearance of objects even for the same semantic meaning. This implies large amounts of training data, compact image representations and efficient search techniques.

 The main achievements of this project was to advance the state of-the-art in visual recognition, to classify large numbers of scenes as well as detect and segment object categories in still images or video frames. The project developed:
Novel image representations suitable for simultaneous modeling of scenes and object categories.
New methods for extracting local features robust to viewpoint change and background clutter.
Data structures, clustering and search techniques for efficient recognition.
Generic recognition system capable of dealing with hundreds of scene and object categories.</gtr:description><gtr:exploitationPathways>The potential applications span from new human computer interfaces, eHealth, animal behaviour analysis, surveillance, to robot navigation, assisted driving etc.</gtr:exploitationPathways><gtr:id>31FD8124-3D32-45BD-8C87-A5BC41328EC3</gtr:id><gtr:outcomeId>545bc2b9f1ef73.47848829</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections,Retail,Transport</gtr:sector></gtr:sectors><gtr:url>http://kahlan.eps.surrey.ac.uk/featurespace</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>We proposed an approach for action recognition based on a
vocabulary of local motion-appearance features and fast approximate search
in a large number of trees. Large numbers of features with associated motion vectors are extracted from video data and are represented by many
trees. Multiple interest point detectors are used to provide features for every
frame. The motion vectors for the features are estimated using optical flow
and a descriptor based matching. The features are combined with image
segmentation to estimate dominant homographies, and then separated into
static and moving ones despite the camera motion. Features from a query
sequence are matched to the trees and vote for action categories and their
locations. The locations are then validated with an SVM classifier. Large
number of trees make the process efficient and robust. The system is capable of 
simultaneous categorization and localization of actions using only
a few frames per sequence. The approach obtains excellent performance on
standard action recognition sequences. We perform large scale experiments
on 17 challenging real action categories from various sport disciplines. We
demonstrated the robustness of our method to appearance variations, camera
motion, scale change, asymmetric actions, background clutter and occlusion.</gtr:description><gtr:id>F76E08C2-D2BE-4D4C-8CD0-F8E6884A0849</gtr:id><gtr:impact>This approach had scientific impact on various computer vision projects including Vidi-video, IDASH, as well as EPSRC project Visen etc, where the image classification code developed here was used to analyse the data. 
The system was extensively tested in various application scenarios and was a part of a robot designed for MoD Grand Challenge 2008. For design, construction and control of an autonomous reconnaissance robot the Swarm Systems Team received Best Innovative Idea Award from the Ministry of Defence (UK) in 2008. 
 The system was also evaluated in international competitions in image classification. These are prestigious events receiving a lot of attention from the research community and the industry. The recently published recognition systems that claim state-of-the art performance are submitted to independent evaluations organized by National Institute of Standards and Technology (NIST, USA), or Pascal Network of Excellence (Europe). The participants are the top research institutions. Our image classification system with novel multi-kernel KDA classifiers won 3 different competitions (TrecVid, Pascal, ImageClef) in 2008, 2009, and 2010.</gtr:impact><gtr:outcomeId>545cf3a49a3437.62999097</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Actions Clasification</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>http://kahlan.eps.surrey.ac.uk/featurespace</gtr:url><gtr:yearFirstProvided>2010</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>TLD is an award-winning, real-time algorithm for tracking of unknown objects in video streams. The object of interest is defined by a bounding box in a single frame. TLD simultaneously Tracks the object, Learns its appearance and Detects it whenever it appears in the video. The result is a real-time tracking that typically improves over time.</gtr:description><gtr:id>8A11F518-974F-41A4-A025-E19131A879A3</gtr:id><gtr:impact>This technology is a significant step forward in the direction of reliable long term tracking with learning capabilities. The potential applications span from new human computer interfaces, eHealth, animal behaviour analysis, surveillance, to robot navigation, assisted driving etc. In 2011 this work has won ICT Pioneer Price, in a new national scientific competition organized by the EPSRC. Our smart camera (called Predator) was featured worldwide in The Engineer, Engadget, New Electronics, Surrey Advertiser, Time, New Scientist, Gottabemobile, Laptopmag, Hacker News etc. It received a lot of attention from the industry with direct inquires well known companies including NASA - Johnson Space Center, Google, Microsoft, Sony, Nokia and many others. A number of invited seminars on this research in various institutions including Google Tech Talk. A licence has been created at the University of Surrey in September 2011 (PENN-DMS.FID1994352]).</gtr:impact><gtr:outcomeId>545cf58a4327f0.33486761</gtr:outcomeId><gtr:title>Tracking Learning Detection</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://kahlan.eps.surrey.ac.uk/featurespace/tld/</gtr:url><gtr:yearFirstProvided>2010</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F38B2737-09EA-4792-8672-0F9A9F509422</gtr:id><gtr:title>Action recognition with appearance-motion features and fast search trees</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7dede21775de26f98ebf1ab075895012"><gtr:id>7dede21775de26f98ebf1ab075895012</gtr:id><gtr:otherNames>Mikolajczyk K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55fa9fa9f1780cfb</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6CA9570-9234-47CD-886C-D1F46A96FFD9</gtr:id><gtr:title>Learning linear discriminant projections for dimensionality reduction of image descriptors.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b4d099021b017a4a40fe01dcb55d9ec0"><gtr:id>b4d099021b017a4a40fe01dcb55d9ec0</gtr:id><gtr:otherNames>Cai H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05ef9b8abe</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C8ED37AF-57F1-46E1-9C17-3E373FEF1EEB</gtr:id><gtr:title>The MediaMill TRECVID 2009 semantic video search engine</gtr:title><gtr:parentPublicationTitle>2009 TREC Video Retrieval Evaluation Notebook Papers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e01513adc3a5dc0ead77b2623ff56d5"><gtr:id>2e01513adc3a5dc0ead77b2623ff56d5</gtr:id><gtr:otherNames>Snoek C.G.M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>545ceb9b297395.60281074</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26A8A297-7E3E-4BC6-BBE0-95B36437B328</gtr:id><gtr:title>The MediaMill TRECVID 2008 semantic video search engine</gtr:title><gtr:parentPublicationTitle>2008 TREC Video Retrieval Evaluation Notebook Papers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e01513adc3a5dc0ead77b2623ff56d5"><gtr:id>2e01513adc3a5dc0ead77b2623ff56d5</gtr:id><gtr:otherNames>Snoek C.G.M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>545ceb9b527f92.43581643</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>70FE2B80-03E1-42A8-9F4D-EA8BBD3D6567</gtr:id><gtr:title>On a Quest for Image Descriptors Based on Unsupervised Segmentation Maps</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/496b72b2cb5bb2cb524e10db471777b2"><gtr:id>496b72b2cb5bb2cb524e10db471777b2</gtr:id><gtr:otherNames>Koniusz P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7542-1</gtr:isbn><gtr:outcomeId>545ceb2ab46a34.86729372</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>22FDD968-F01F-450E-B55A-45D973A5CE70</gtr:id><gtr:title>Forward-Backward Error: Automatic Detection of Tracking Failures</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c7e213cb9aa4892db8703da08a217fe9"><gtr:id>c7e213cb9aa4892db8703da08a217fe9</gtr:id><gtr:otherNames>Kalal Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7542-1</gtr:isbn><gtr:outcomeId>545ceb2b55f629.19788838</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>97AC1B65-06CF-46E4-A595-D8321DDCC468</gtr:id><gtr:title>Multiple kernel learning and feature space denoising</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8bb95557325dc92e149eaa32cb7a250"><gtr:id>a8bb95557325dc92e149eaa32cb7a250</gtr:id><gtr:otherNames>Yan F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-6526-2</gtr:isbn><gtr:outcomeId>545ceb2b2e9294.94493114</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A8671916-31EF-4F96-BDD1-306D67595C1A</gtr:id><gtr:title>Multi-frame, multi-modal, and multi-kernel concept detection in video</gtr:title><gtr:parentPublicationTitle>2009 TREC Video Retrieval Evaluation Notebook Papers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e01513adc3a5dc0ead77b2623ff56d5"><gtr:id>2e01513adc3a5dc0ead77b2623ff56d5</gtr:id><gtr:otherNames>Snoek C.G.M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>545ceb9b9f0509.08499716</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>45253282-F49B-48BF-B6A7-1D3706A73713</gtr:id><gtr:title>Concept learning for image and video retrieval: The inverse random under sampling approach</gtr:title><gtr:parentPublicationTitle>European Signal Processing Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ffdef4e94e129be0de85992165dbb605"><gtr:id>ffdef4e94e129be0de85992165dbb605</gtr:id><gtr:otherNames>Tahir M.A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>22195491</gtr:issn><gtr:outcomeId>545ceb2b9e8055.91385800</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>076AE478-7D0A-4AB9-BFD4-DB230FF23235</gtr:id><gtr:title>Learning from video browse behavior</gtr:title><gtr:parentPublicationTitle>2009 TREC Video Retrieval Evaluation Notebook Papers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e01513adc3a5dc0ead77b2623ff56d5"><gtr:id>2e01513adc3a5dc0ead77b2623ff56d5</gtr:id><gtr:otherNames>Snoek C.G.M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>545ceb9bc31692.88141869</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E1B6A30B-F70E-46BA-9E21-B674045C5E39</gtr:id><gtr:title>Face-TLD: Tracking-Learning-Detection applied to faces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c7e213cb9aa4892db8703da08a217fe9"><gtr:id>c7e213cb9aa4892db8703da08a217fe9</gtr:id><gtr:otherNames>Kalal Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7992-4</gtr:isbn><gtr:outcomeId>545ceb2b79da99.99511138</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EC323FB5-4A7A-4089-9214-D6796F58E9EE</gtr:id><gtr:title>Spatial Coordinate Coding to reduce histogram representations, Dominant Angle and Colour Pyramid Match</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/496b72b2cb5bb2cb524e10db471777b2"><gtr:id>496b72b2cb5bb2cb524e10db471777b2</gtr:id><gtr:otherNames>Koniusz P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1304-0</gtr:isbn><gtr:outcomeId>545ceb28e781c9.84744796</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C2E67560-4EF8-40C9-840D-07013A1734E1</gtr:id><gtr:title>A hands-on approach to high-dynamic-range and superresolution fusion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9e04307631976a48a46831ab21ee3004"><gtr:id>9e04307631976a48a46831ab21ee3004</gtr:id><gtr:otherNames>Schubert F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-5497-6</gtr:isbn><gtr:outcomeId>545cea6ad00b45.20994158</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>23187F5D-73D7-4D5A-A7F8-76E7DCC792D9</gtr:id><gtr:title>Soft assignment of visual words as Linear Coordinate Coding and optimisation of its reconstruction error</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/496b72b2cb5bb2cb524e10db471777b2"><gtr:id>496b72b2cb5bb2cb524e10db471777b2</gtr:id><gtr:otherNames>Koniusz P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1304-0</gtr:isbn><gtr:outcomeId>545ceb291daf20.53712498</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B6654CC7-6D60-4EDF-8DDA-01D09AC1E5AF</gtr:id><gtr:title>The University of Surrey Visual Concept Detection System at ImageCLEF@ICPR: Working Notes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fce242c6484754ca439b336688f80b07"><gtr:id>fce242c6484754ca439b336688f80b07</gtr:id><gtr:otherNames>Tahir M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7542-1</gtr:isbn><gtr:outcomeId>545ceb28b7ec45.97122174</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F003420/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>