<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/8319F78A-DCBD-49F6-BE00-78E1CD75CDA9"><gtr:id>8319F78A-DCBD-49F6-BE00-78E1CD75CDA9</gtr:id><gtr:name>University of York</gtr:name><gtr:department>Theatre Film and TV</gtr:department><gtr:address><gtr:line1>Heslington</gtr:line1><gtr:line4>York</gtr:line4><gtr:line5>North Yorkshire</gtr:line5><gtr:postCode>YO10 5DD</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8319F78A-DCBD-49F6-BE00-78E1CD75CDA9"><gtr:id>8319F78A-DCBD-49F6-BE00-78E1CD75CDA9</gtr:id><gtr:name>University of York</gtr:name><gtr:address><gtr:line1>Heslington</gtr:line1><gtr:line4>York</gtr:line4><gtr:line5>North Yorkshire</gtr:line5><gtr:postCode>YO10 5DD</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3DCA2D94-DE67-42C4-A793-ADE5440D59D9"><gtr:id>3DCA2D94-DE67-42C4-A793-ADE5440D59D9</gtr:id><gtr:name>Trilabyrinth</gtr:name><gtr:address><gtr:line1>River House</gtr:line1><gtr:line2>East Wall Road</gtr:line2><gtr:region>Northern Ireland</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9D7F5608-A2F8-40F1-8F90-FDE3B482AE44"><gtr:id>9D7F5608-A2F8-40F1-8F90-FDE3B482AE44</gtr:id><gtr:name>IOSONO GmbH</gtr:name><gtr:address><gtr:line1>Erich-Kaestner-Str. 1</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/91DE80AC-A6FA-4301-BB63-1E601D6427F6"><gtr:id>91DE80AC-A6FA-4301-BB63-1E601D6427F6</gtr:id><gtr:firstName>Gavin</gtr:firstName><gtr:surname>Kearney</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM001210%2F1"><gtr:id>DE9AC1F5-E5BB-4F1B-AAC8-EDFE52408599</gtr:id><gtr:title>SADIE: Spatial Audio for Domestic Interactive Entertainment</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M001210/1</gtr:grantReference><gtr:abstractText>Interactive media systems such as games consoles have become commonplace in UK domestic environments offering increased connectivity and media integration. Whilst many systems support 3-D visuals and Ultra-High Definition video, most do not address the importance of 3-D sound for immersive experiences. 5.1 surround is the most common format supported by such systems, but its immersive capability is limited to a two-dimensional plane. New formats incorporating height channels have been targeted at cinema but could also naturally complement the virtual reality experience of gaming by extending the soundfield to three dimensions. The benefits not only include new audio features for enhanced gameplay, but also enable the design of immersive sound-centric games with significant social, cultural, educational and healthcare gains. However, sound immersion is difficult to achieve in the home as it is impractical to have dozens of transducers placed around the living room. SADIE pioneers new approaches for rendering interactive spatial sound in the home that includes sound source rendering with height.

For gaming, there is also the complex issue of listener position. For example, in games based on kinetic motion tracking the listener moves in reaction to the game play and the formation of a stable soundfields becomes difficult as they are no longer located at the acoustic sweet-spot. Real-time soundfield compensation through tracking of the listener can be used to counteract this, but the listener movement also makes digital equalisation of the room acoustics a considerable challenge. The result is a flawed virtual environment in which visual and auditory cues are not spatially coincident. 

SADIE will address the improvement of spatial audio quality for immersive interactive media experiences in the home. It will undertake novel science to improve soundfield immersion and the formation of 3-D sound sources beyond the horizontal plane, lifting the constraints of loudspeaker placement and dynamic source-listener movements whilst conserving good sound reproduction quality. The research will pioneer new methods for soundfield rendering formed through characterisation of the cues required for perception of sources with height in dynamic listening. The work is poised to have significant transformative impact on sound reproduction in homes in the UK, whilst also addressing wider questions on auditory perception and acoustic signal processing that serve other research disciplines outside of interactive media.</gtr:abstractText><gtr:potentialImpactText>The SADIE programme of research will impact upon:

The Interactive Media Industry:
The scientific outcomes have the potential for commercialization and knowledge transfer with (i) Hardware and Software audio systems producers (E.g. Headphone/Loudspeaker producers, Virtual Studio Technology producers) and Games Console manufacturers, (ii) Middleware game audio producers, and (iii) Games studios. Adoption of the research outcomes by the gaming industry will transform immersive sound capability in games. Whilst this project focuses on sound reproduction in the home, it also facilitates the development of new games which ultilise full 3-D surround sound for tracked players. This enables new sound features which can be exploited for market potential such as improved situational awareness, use of 3-D audio game cues to reduce user interface 'clutter' and improved inter-player dialogue via spatial positioning in networked games. The financial cost of creating a start-up company or employing knowledge transfer of the research into games companies is low in comparison to the revenue generated by successful games enterprises.

The Broadcast Industry:
Due to the logic of media convergence, games consoles offer increased media integration allowing viewers to not only play interactive games with local and remote players, but also to watch movies and television and use internet services. Consequently the broadcast industry is undergoing a significant change from video-based content delivery to multi-platform content. This means that immersive technology which benefits the games industry can also be used to enhance broadcast facilities in the home. SADIE will enable new 3-D sound formats aimed at cinematic reproduction which include height channels, such as 22.2 surround sound or Dolby Atmos, to now be presented to domestic viewers. The broadcast and cinematic industries are actively researching new methodologies for creating immersive media content for both theatres and the living room. Consequently, the British Broadcasting Corporation are a partner on this project, in an effort to advance spatial sound reproduction to support broadcast of immersive content to UK homes. 

Game Players and Society:
Improved soundfield immersion and the discrimination of 3-D sound source locations beyond the horizontal plane will lead to enriched gaming experiences and creates the potential for new sound reproduction cues to enhance gameplay, not just for entertainment value, but also to support serious games with learning outcomes. The average child up to 21 years of age spends over 10,000 hours playing games meaning there is a 'cognitive surplus' which can be tapped into to increase the learning potential of digital games. Indeed, digital games have the capability to go beyond mere entertainment value and touch on deeper societal issues. Immersive 3-D audio can therefore help in the development of games aimed at providing deep cultural (e.g. interactive concert experiences), social (e.g. e-Learning games between local and remote classrooms) and even therapeutic learning outcomes (e.g. 3-D auditory stimulation for listening and learning disabilities). Immersive 3-D sound also enables the creation of enhanced audio-only games designed for the vision impaired. 

Academics in other disciplines:
As outlined in the Academic Impact section, the proposed psychoacoustic and signal processing research will also have significant impact in other academic disciplines that utilise interactive audio technologies. These include psychology, computer science, music, and biology amongst many others. The psychoacoustic research fits into the EPSRC area Vision, Hearing and Other Senses and the scientific outcomes are well placed to make a larger impact to the wider ICT research portfolio.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-11-30</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98784</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Interactive Auralisation for Live Theatrical Performance, BBC Sound: Now and Next, 19th/20th May 2015.</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A300A31B-4D75-4FBB-8446-61A2EA18EC4B</gtr:id><gtr:impact>Interactive Auralisation for Live Theatrical Performance,
Helena Daffern, Gavin Kearney, Alex Southern, Will Smith and Damian Murphy
BBC Sound: Now and Next, 19-20 May, 2015.

On 19th and 20th May, 2015 BBC's Broadcast house was home to a two day event on the future of broadcast audio technology. 'Sound: Now &amp;amp; Next' featured talks from audio producers, artists and engineers as well as a technology fair demonstrating the latest developments in audio technology. The University of York's Audiolab were present as part of the BBC Audio Research Partnership. The team demonstrated recent work on a virtual version of York Theatre Royal. The project allows viewers to experience 360 degree visuals and auralisations based on acoustic measurements and multicamera recordings of performances in the theatre and includes interactive binaural rendering developed for the SADIE project.

Created new contacts for project partners for the AHRC 'Enhancing Audio Description' project.</gtr:impact><gtr:outcomeId>569664578f2556.16497461</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/rd/events/sound2015</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>WISHED: Well-being - Investigating Singing Health in Ensembles through Digital Technologies, Research showcase, AHRC Commons York</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>91FCCFAE-ED9A-47BD-857D-7FEA2BD42E31</gtr:id><gtr:impact>Daffern, H. and Kearney, G., Priming New Research in Creativity, WISHED: Well-being - Investigating Singing Health in Ensembles through Digital Technologies, AHRC Commons, York, UK, June 2016.

Demonstration of a prototype interactive audio system aimed at improving health and well being through singing in a VR environment. The demo was housed at a stand in the main exhibition part of the AHRC commons event. Participants put on a VR headset as well as a mic and headphones and were virtually transported to a performance space that they could fully acoustically interact with. The demo utilised binaural technology developed as part of the SADIE project. Participants were able to discuss the technical details as well as the potential of the technology with the investigators. This opened up avenues of collaboration that the team are looking to capitalise on.</gtr:impact><gtr:outcomeId>58c7c981ca94d8.98480136</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.ahrc.ac.uk/about/ahrc-commons/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Immersive VR: When will audio catch up?, Article in IDGCONNECT, September 13, 2016</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5E1A2F5E-A9FC-44A9-935C-AC324F2AB2B1</gtr:id><gtr:impact>Article in IDGconnect looking at spatial audio in virtual reality.
Kearney was interviewed based on expertise in spatial audio and involvement in SADIE project.
Article resulted in numerous queries about research and made public aware of research challenges facing SADIE project.</gtr:impact><gtr:outcomeId>58b843e82b17c7.74302028</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.idgconnect.com/abstract/20024/immersive-vr-when-audio-catch</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Spatial Audio for Domestic Interactive Entertainment: Possiblilities and Limitations. January 4th 2015, Sonic Arts Research Centre, Queens University Belfast</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B30E001D-89A5-4587-9931-47D69D009420</gtr:id><gtr:impact>Spatial Audio for Domestic Interactive Entertainment: Possiblilities and Limitations.
Dr. Gavin Kearney, January 4th 2015, Sonic Arts Research Centre, Queens University Belfast.

Recent advances in immersive video technology, such as ultra-high definition video or 3-D visual headsets like the Oculus rift, are paving the way for immersive 360-degree virtual reality media systems in the home. Surround sound systems will naturally accompany such video technologies but reproduction in domestic media 'caves' or on headsets is not without significant challenges. This talk discussed the role spatial audio will play in the development of personal and domestic VR systems and looked at the technological and content creation possibilities and limitations for immersive audio-visual media.</gtr:impact><gtr:outcomeId>56dfdf9a4bb8c6.39108234</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>You are surrounded: Surround Sound, Past, Present and Future. York Festival of Ideas, June 16th, 2016</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>16583786-5C29-4C26-BD63-F347FFEAF7F3</gtr:id><gtr:impact>Abstract:
You are Surrounded! - Surround Sound Past, Present and Future, June 16th 2016, University of York

Surround sound is a way to create immersive audio experiences through placing multiple loudspeakers around an audience, in the cinema, theatre or at home. The development of surround sound as we know it has had a rich and varied history but has largely focused on increasing the number of loudspeakers around the audience for a more immersive experience. But can this strategy realistically be employed in the home? Join Gavin Kearney of the University of York to discuss - and listen to - how surround sound experiences have evolved from 16th Century Venetian choral performances to the large scale cinematic installations we hear today. Find out what recent research in surround sound technology means for the future of surround sound in the home.

Date: Thursday 16 June 2016, 6.30pm to 7.30pm
Venue: Holbeck Cinema, Department of Theatre, Film and Television, University of York
Admission is free, but booking is required. 

Impact summary: Attendees were mostly from the general public. Awareness was made of recent developments in surround sound technology and how this could impact on everyday life through binaural listening on headphones and loudspeaker based surround sound.</gtr:impact><gtr:outcomeId>58b83807c6c6c2.99300054</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://yorkfestivalofideas.com/2016/talks/surround-sound-past-present-future/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshop on Virtual Reality Spatial Audio, AES Audio for Games Conference, London, 11th February 2016.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B52D1AE1-031A-4D5F-A8A6-9C847F494664</gtr:id><gtr:impact>Spatial Audio for VR
Gavin Kearney (Chair), AudioLab, Department of Electronics, University of York
Marcin Gorzel and Alper Gungormusler, Google Inc.
Pedro Corvo, Playstation VR, Sony Computer Entertainment Europe Ltd.
Jelle Van Mourik, Playstation VR, Sony Computer Entertainment Europe Ltd.
Varun Nair, Two Big Ears Ltd.

This workshop focused on rendering truly dynamic and spatially coherent mixes for Virtual Reality. The panel presented practical workflows for mixing and rendering 3-D sound for VR and explored the challenges in delivering dynamic spatial audio over a variety of VR technologies and applications. The AES Audio for Games conference took place on the 10th to 12th of February, 2016 at the Royal Society of Chemistry in the centre of London. The workshop was followed by an open panel discussion that sparked much debate and conversation amongst the patrons of the event about the future of audio for virtual reality.</gtr:impact><gtr:outcomeId>56dfda87b18f25.04087637</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.audioforgames.net/2016/timetable/#VRSpatial</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>YouTube live-streams in virtual reality and adds 3D sound - BBC News online, 18th April 2016</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E280ED04-F8CC-4E65-9A1D-3A70C1E74794</gtr:id><gtr:impact>Online article describing the development of spatial audio in the Youtube pipeline. 
Kearney interviewed based on expertise on Spatial Audio and involvement in SADIE project. 
Resulted in strong international interest in SADIE project and interest from other potential funders.</gtr:impact><gtr:outcomeId>58b842bd0ceb91.57480491</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/news/technology-36073009</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Multi-modal experience of a virtual concert hall: A tool for exploring the influence of the visual modality in interactive real-time room acoustic simulation, ICMEM, Sheffield 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9DF137CA-FB68-4E00-838E-73C55EFB17A8</gtr:id><gtr:impact>Multi-modal experience of a virtual concert hall: A tool for exploring the influence of the visual modality in interactive real-time room acoustic simulation, 
Jude Brereton, Alex Southern and Gavin Kearney.
3rd International Conference on the Multimodal Experience (ICMEM) of Music, 23rd-25th March. 

ICMEM brought together researchers from various disciplines who investigate the multimodality of musical experiences from different perspectives. Dr. Jude Brereton and Dr. Gavin Kearney from the University of York presented a demonstration of the Virtual Singing Studio which is a loudspeaker based room acoustics simulation for real-time musical performance. Participants at the conference were able to transport themselves to an immersive interactive simulation of the National Centre for Early Music (York) and were able to experience the same visual and acoustic cues as they would in the real venue. Users could also acoustically interact with the virtual space, demonstrating the ways in which immersive VR can be used as a rehearsal tool for musicians in preparation for performances in real (or remote) spaces. The demonstrated version of the Virtual Singing Studio utilised an Ambisonic engine developed for the SADIE project. 
Main outcomes:
- Encouraged discussion around the technology
- Made practitioners and academics aware of the technologies capabilities.
- Practitioners stated that they found the technology exciting and inspiring and could see many applications for it in their own work.

- Invited to submit a paper to a special issue of Psychomusicology on multi-modal experiences in music.</gtr:impact><gtr:outcomeId>5696621ce6bd88.38708560</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>https://www.sheffield.ac.uk/polopoly_fs/1.448256!/file/ICMEMFinalProgram.pdf</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Binaural Processing of Ambisonics, University of Huddersfield, February 3rd, 2016.</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>818D74F1-8907-4259-878B-C89D086ABB6A</gtr:id><gtr:impact>Binaural Processing for Ambisonics: Analysis, Measurement and Reproduction
Dr. Gavin Kearney, February 3rd, 2016, University of Huddersfield.

This was an invited talk by the Applied Psychoacoustics Group, led by Dr. Hyunkook Lee at the University of Huddersfield. The seminar showcased techniques for measuring and rendering binaural sound as well as the current work on the SADIE project.</gtr:impact><gtr:outcomeId>56dfde640ae2b1.81702654</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>3D Auralisation And Virtual Reality Technology in Digital Heritage, Moesgaard Museum and Aarhus University, Denmark, 11th June 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FCC7CA28-6FB2-424C-A8C1-A6AE55812D70</gtr:id><gtr:impact>'3D Auralisation And Virtual Reality Technology in Digital Heritage'
Damian Murphy, Jude Brereton, Andrew Chadwick, Helena Daffern, Gavin Kearney, William Smith &amp;amp; Alex Southern
Digital Heritage 2015, Moesgaard Museum and Aarhus University, Denmark, May 2015</gtr:impact><gtr:outcomeId>56dfe1a555be07.06433727</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Third sector organisations</gtr:primaryAudience><gtr:url>http://conferences.au.dk/fileadmin/conferences/2015/Digital_Heritage/Program_2015_-_FINAL.pdf</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Interactive Audio Systems Symposium,  September 23rd, 2016, University of York, United Kingdom.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>781DC56C-BD8C-46F1-927F-40EF1E18EBBE</gtr:id><gtr:impact>Interactive Audio Systems Symposium

September 23rd, 2016, University of York, United Kingdom.

Symposium chair: Gavin Kearney

This was a one-day symposium dedicated to the topic of interactive audio systems. The symposium explored the perceptual, signal processing and creative challenges and opportunities arising from audio systems affected through enhanced human-computer interaction.

The symposium consisted of keynote talks, papers, posters and demonstrations and was sold out to an international audience. As a major output of the SADIE project, it demonstrated key technological developments from the project to an international audience. The proceedings are also online to contribute to the wider field of research in interactive audio.</gtr:impact><gtr:outcomeId>58b83a43072d99.26718489</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.york.ac.uk/sadie-project/IASS2016.html</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Immersive Audio for Virtual Reality, AES 140th Convention, Paris, 4th June 2016.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>783A2027-76B3-4499-BD1C-E91040719CDF</gtr:id><gtr:impact>Audio Engineering Society Convention, Paris 2016
Workshop W2

Saturday, June 4, 10:30 - 12:00 (Room 352A)
W2 - Immersive Audio for Virtual Reality
Chair:
Gavin Kearney, University of York - York, UK
Panelists:
Jamieson Brettle, Google - Mountain View, California
Pedro Corvo, Sony Computer Entertainment - London, UK
Marcin Gorzel, Google - Dublin, Ireland
Jelle van Mourik, Sony Computer Entertainment - London, UK; University of York - York, UK

Abstract:
In recent years, major advances in gaming technologies, such as cost-effective head-tracking and immersive visual headsets have paved the way for commercially viable virtual reality to be delivered to the individual. Now the consumer finally has the opportunity to experience new gaming, cinematic and social media experiences with truly immersive and interactive 3-D audio and video content.

For many sound designers, rendering a truly dynamic and spatially coherent mix for VR presents a new learning curve in soundtrack production. What spatial audio techniques should we be using to create engaging and interactive 3-D mixes? What audio workflows should we employ for similar immersive experiences on headphones, 5.1 loudspeakers and beyond? Are new VR production methods backwards compatible with existing game audio pipelines? Can binaural reproduction work for everyone?

In this workshop our panel of experts will present practical workflows for mixing and rendering 3-D sound for VR. The workshop will explore different production techniques for creating immersive mixes such as Ambisonics processing and Head-Related Transfer Function rendering. It will also explore the importance of environmental rendering for VR as well as outlining workflow challenges and pipelines for dynamic spatial audio over a variety of VR technologies and applications.


Approximately 200 people attended the event, which showcased live binaural demos from Google, Sony, Facebook (Two Big Ears) and the SADIE project. There were short presentations and demos from each of the panel members followed by a Q and A session after.</gtr:impact><gtr:outcomeId>58b8309ea605f2.16323986</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://www.aes.org/events/140/workshops/?ID=4887</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Digital Creativity Labs Launch Event, University of York, 6th April, 2016.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>AF9AFB58-6FE2-41AC-9429-96779E41BF5D</gtr:id><gtr:impact>Wednesday 6th April saw the launch of the Digital Creativity Labs (DC Labs), a major (&amp;pound;18 million) investment by three UK research councils, four universities, and over 80 collaborative partner organisations to create a world centre of excellence for impact-driven research, focusing on digital games, interactive media and the rich space where they converge. The main DC Labs site is York, with &amp;quot;spoke&amp;quot; sites at the Cass Business School, Goldsmiths (University of London) and Falmouth University. Participants at the launch event had a chance to learn more about the exciting activities of the Digital Creativity Labs and its partner institutions. Amongst the demonstrations and 'buzz talks' at the event was the SADIE project, demonstrating immersive motion tracked transaural sound for interactive gaming. Attendees were very interested in the developed technology and how it could be used in their business, research and creative practice.</gtr:impact><gtr:outcomeId>58b8320a5c6a36.30265990</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.york.ac.uk/news-and-events/news/2016/campus/digital-creativity-labs/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Immersed in the UK, Digital Catapult, London, Sept. 20, 2016.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>34FF89C9-9A78-4B3A-BC2E-09B5BF9C79CB</gtr:id><gtr:impact>Immersed in the UK
September 20, 2016 @ 5:00 pm - 9:00 pm, Digital Catapult London.

Abstract:
By February 2016 investment in virtual reality and other immersive technologies had already exceeded $1.1bn, and this is an industry that looks unlikely to slow down anytime soon. A fair share of investment in immersive technologies has been down to entertainment and marketing, but recent advances such as Dr Shafi Ahmed, surgeon and Co-founder of Medical Realities broadcasting the first ever VR surgery prove that the technology is broadening its horizons quickly.

SMEs, academics and investors working in immersive technologies are welcome to attend this event. It's a fantastic opportunity to find out what the UK's leading academic and research organisations and businesses are doing in this space. By attending, you'll also gain exclusive insights into market opportunities, challenges and exciting advances.

 David Swapp, Immersive VR Lab Manager, University College London
 Mark Sandler, Director of Centre for Digital Music, Queen Mary University of London
 Gavin Kearney, Lecturer in Audio &amp;amp; Music Technology, University of York
 Eammon O'Neill, Co-Director of CAMERA and Head of the Department of Computer Science at the University of Bath
 Mark Skilton, Professor of Practice, Warwick Business School
 Dave Haynes, Investor, Seedcamp
 Humphrey Hardwicke, Creative Director, Luminous Group
 Steve Dann, Co-founder of Medical Realities
 Phil Channock, Marketing Manager, Draw and Code

In collaboration with Digital Catapult Centre Brighton, Digital Catapult Centre Northern Ireland and VRTGO Labs

This event was part of ColLab Fest (19-23 September), a week of events sourced from Digital Catapult's network of five centres, nine associates and numerous partners. It was an exciting programme of events exploring the strengths and nuances of the UK's digital economy.</gtr:impact><gtr:outcomeId>58b833439fb2f8.06627284</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>https://www.digitalcatapultcentre.org.uk/event/immersed-uk/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>121136</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Arts and Humanities Research Council Grant, Research Grants (Early Career)</gtr:description><gtr:end>2017-09-02</gtr:end><gtr:fundingOrg>Arts &amp; Humanities Research Council (AHRC)</gtr:fundingOrg><gtr:fundingRef>AH/N003713/1</gtr:fundingRef><gtr:id>5ECBCB08-BF32-45A7-87AB-2430FC9ECCB4</gtr:id><gtr:outcomeId>56dfd8a21a2dd9.69358882</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>94500</gtr:amountPounds><gtr:country>China, People's Republic of</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Huawei Innovation Research Partnership</gtr:description><gtr:end>2017-12-02</gtr:end><gtr:fundingOrg>Huawei Technologies</gtr:fundingOrg><gtr:fundingRef>H02016050002B2</gtr:fundingRef><gtr:id>B97ACDB0-CBEF-4F1E-8EAE-35851C98F8AF</gtr:id><gtr:outcomeId>58b814b8050f48.61443316</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>29550</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Google Faculty Research Award</gtr:description><gtr:end>2017-04-02</gtr:end><gtr:fundingOrg>Google</gtr:fundingOrg><gtr:id>9F7F5073-CCE7-4AAC-9556-E5CD44C70748</gtr:id><gtr:outcomeId>56dfd732442422.71171846</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2016-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The datasets of binaural filters and Ambisonic decoders measured for this research are made publically available under an Apache license on the project website (http://www.sadie-project.co.uk). HRTFs from 18 human subjects and two binaural heads were measured using a custom built motion-tracked laser-guide measurement rig. 

As a result these datasets are currently being adopted in a wide range of fields that utilise VR technologies. Most noticeably they are now being adopted by Google as part of their immersive VR pipeline. This includes YouTube 360 as well as applications developed for Google cardboard and the newly released Google Daydream VR headset. Millions of users worldwide who utilise Google's VR platform will therefore use the York SADIE binaural filters, making them the new standard for spatial audio quality in VR productions. 
Consequently the work has major impact on improved audio quality in VR productions worldwide and any developed VR application that has cultural, societal or economic impact.</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>195F97F6-6C26-4E8C-A3E4-FFFB1454E686</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56e00a55452ac1.56021398</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The main findings from the SADIE project are
- Fully immersive motion tracked 3-D audio is possible using a 4-channel Ambisonic/transaural hybrid system.
- Effective height reproduction for binaural processing in transaural systems can be achieved using low-order IIR approximation of HRTF (for real-time rendering) with a filter order as low as 11.
- It is possible to achieve motion tracked transaural reproduction with the same level of localisation accuracy as found in motion tracked binaural reproduction when individualised head-related transfer functions are used.
- There is no change in the perception of auditory depth with real sound source presentations at different angles to the head. 
- 1st Order Ambisonics is sufficient for perception of auditory depth at all angles to the head.
- There is no significant difference in the perception of moving sources for binaural presentations between 1st, 3rd and 5th order Ambisonics
- There are significant timbral differences in Ambisonic mixes presented over different loudspeaker configurations
- The perception of auditory height is poor in 1st and 3rd order Ambisonics in comparison to 5th order.</gtr:description><gtr:exploitationPathways>Enhanced immersive audio experiences will have a transformative impact on gameplay, not only improving naturalness and integration with UHD displays but also enabling new features including improved situational awareness, use of 3-D audio game cues to reduce visual interface 'clutter', improved inter-player dialogue via spatial positioning in networked games and enriched games design for the visually impaired and cognitive therapy applications. The findings from this project can be utilised in all such areas by technology manufacturers as well as sound designers of VR games, serious games and gamification services that can exploit the scientific outputs. The research findings are also of interest to other working in the wider field of spatial audio.</gtr:exploitationPathways><gtr:id>44420357-E39F-4A92-B551-19920978CFD2</gtr:id><gtr:outcomeId>56e00aaee7f028.73054466</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Communities and Social Services/Policy,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Electronics,Environment,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://www.sadie-project.co.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>High-resolution head-related transfer function (HRTF) databases have become more prevalent of late, with new datasets offering increased angular accuracy and spatial resolution of measurements. However, for optimal virtual loudspeaker decoding for Ambisonics binaural research, quite often interpolation of HRTF measurements is required in order to achieve loudspeaker positions on the sphere optimal for decoder design, which is undesirable. We present a new online database of HRTFs derived for 1st to 5th order full sphere Ambisonic reproduction. We also implement factorisation of the datasets such that source material intended for binaural reproduction can be rendered using directionally independent pre-filtering, leaving shorter run-time filters for computational ease.</gtr:description><gtr:id>01567BCA-73FA-42F3-95F4-4AE28702CE23</gtr:id><gtr:impact>The binaural database can be used with compatible VST plugins for audio production. This makes immersive 3-D binaural sound more accessible from a content creation perspective for Games developers, filmmakers, VR developers, musicians and artists.

The database can also be used by the academic community to study spherical acoustics for multiple listeners.</gtr:impact><gtr:outcomeId>56dfd60484ee88.72668565</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>SADIE Binaural database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.york.ac.uk/sadie-project/binaural.html</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The SADIE project uses an Ambisonic framework for its signal processing. As part of this, we have compiled a set of decoders for various spherical loudspeaker configurations that would be useful to the spatial audio research community, and also can be utilised with our binaural database. Ambisonic decoders up to 5th Order for different loudspeaker configurations are given. The config files can be used with compatible VST plugins that use the AMBIX format. The decode matrices are presented in ACN/SN3D format.</gtr:description><gtr:id>3609A795-EDE5-42F6-BD14-455AE26CB637</gtr:id><gtr:impact>The decode config files can be used with compatible VST plugins for audio production. This makes immersive 3-D binaural sound more accessible from a content creation perspective for Games developers, filmmakers, VR developers, musicians and artists.</gtr:impact><gtr:outcomeId>56dfd5176f1413.01793733</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Ambisonic Decoders for multiple loudspeaker systems</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.york.ac.uk/sadie-project/ambidec.html</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>A website has been created for the SADIE project to document research outcomes, house databases arising from the project and to advertise public engagement events, workshops and seminars.</gtr:description><gtr:id>7D216121-A3DE-48CC-9613-C030B8F9BF81</gtr:id><gtr:impact>The website has enabled industry and academics to engage with the outputs of the project.</gtr:impact><gtr:outcomeId>56dfe5047f7c07.40438515</gtr:outcomeId><gtr:title>Spatial Audio for Domestic Interactive Entertainment (SADIE) website</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>http://www.sadie-project.co.uk</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>7B0338A3-937B-4357-B4BC-DC2900D69582</gtr:id><gtr:title>Moving Virtual Source Perception in 2D Space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/69df773c643fd4394faef6039c278245"><gtr:id>69df773c643fd4394faef6039c278245</gtr:id><gtr:otherNames>Hughes S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b82e0d695089.60151652</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F45F3F0B-8656-4267-99A1-AC73E82E775E</gtr:id><gtr:title>Design of an Interactive Virtual Reality System for Ensemble Singing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/024ca54e16d3faf64a1708c6b01cf922"><gtr:id>024ca54e16d3faf64a1708c6b01cf922</gtr:id><gtr:otherNames>Kearney G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b82addcd9192.57117204</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A8AAD4B0-3362-48F6-8C35-EF167B21B3B0</gtr:id><gtr:title>Preliminary Investigations Into Binaural Cue Enhancement for Transaural Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6311498e6858b84116886fc3c0999a4"><gtr:id>e6311498e6858b84116886fc3c0999a4</gtr:id><gtr:otherNames>McKenzie, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5696baae690fb0.53081966</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F129EAB9-C170-454C-A1B3-7F492E0038A0</gtr:id><gtr:title>Auditory height perception in cross-talk cancellation using low order HRTF approximation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2afd47f153bf32bb0108b55ecc147cf1"><gtr:id>2afd47f153bf32bb0108b55ecc147cf1</gtr:id><gtr:otherNames>Kearney G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b82eecc93e27.12200441</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>97455B3A-89DD-48AD-AB48-B0DE0EADF0F1</gtr:id><gtr:title>A Virtual Loudspeaker Database For Ambisonics Research</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d42b8582c8f26d3249ff818e00d80743"><gtr:id>d42b8582c8f26d3249ff818e00d80743</gtr:id><gtr:otherNames>Kearney, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>9812830</gtr:issn><gtr:outcomeId>569643083c4a61.93576971</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>99DE7F41-9A5D-49E8-AA6D-AEC67425AF86</gtr:id><gtr:title>A HRTF Database for Virtual Loudspeaker Rendering</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d42b8582c8f26d3249ff818e00d80743"><gtr:id>d42b8582c8f26d3249ff818e00d80743</gtr:id><gtr:otherNames>Kearney, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56963fa84320e1.40125488</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CE0EF2E7-4767-440F-AB62-D9EA18D4E859</gtr:id><gtr:title>Auditory Distance Perception with Static and Dynamic Binaural Rendering</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d42b8582c8f26d3249ff818e00d80743"><gtr:id>d42b8582c8f26d3249ff818e00d80743</gtr:id><gtr:otherNames>Kearney, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56963e985e8a19.87730002</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>247BA7CD-D229-47EF-B355-1AB9FE458B9B</gtr:id><gtr:title>Boundary element modelling of KEMAR for binaural rendering: Mesh production and validation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6ff3076383d77eea8406d718e3266d0e"><gtr:id>6ff3076383d77eea8406d718e3266d0e</gtr:id><gtr:otherNames>Young K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b82d75daaa98.47899456</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C74549E-B6E4-4E31-AC3E-A10D6D19DDCA</gtr:id><gtr:title>Height Perception in Ambisonic Based Binaural Decoding</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d42b8582c8f26d3249ff818e00d80743"><gtr:id>d42b8582c8f26d3249ff818e00d80743</gtr:id><gtr:otherNames>Kearney, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56964032630144.17286053</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C80470D1-04F3-41D6-8724-05C1E52D169A</gtr:id><gtr:title>Motion Tracking and Ambisonics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d42b8582c8f26d3249ff818e00d80743"><gtr:id>d42b8582c8f26d3249ff818e00d80743</gtr:id><gtr:otherNames>Kearney, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56963cd2a9c919.06320431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B672FF53-2B9E-40A6-A02F-9C13F64307A4</gtr:id><gtr:title>On Prediction of Auditory Height in Ambisonics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d42b8582c8f26d3249ff818e00d80743"><gtr:id>d42b8582c8f26d3249ff818e00d80743</gtr:id><gtr:otherNames>Kearney, G.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>9812830</gtr:issn><gtr:outcomeId>5696416da72671.16167105</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3354ED9E-778D-4D29-B67B-84572065E38A</gtr:id><gtr:title>The Perception of Auditory Height in Individualised and Non-Individualized Dynamic Cross-Talk Cancellation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2afd47f153bf32bb0108b55ecc147cf1"><gtr:id>2afd47f153bf32bb0108b55ecc147cf1</gtr:id><gtr:otherNames>Kearney G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b829eb536e15.38820978</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>28308143-9CC5-422B-832C-579AB018F6EE</gtr:id><gtr:title>Fear and Localisation: Emotional Fine-Tuning Utlising Multiple Source Directions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3aa4e849d00fefae9cc5424efaa7c913"><gtr:id>3aa4e849d00fefae9cc5424efaa7c913</gtr:id><gtr:otherNames>Hughes, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56963d9f1e8385.31762993</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E3F9D306-A2B2-4CC6-8A14-8C60EFB7A15C</gtr:id><gtr:title>Echolocation in Virtual Reality</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/83b75befe4d3b087e46dfabc61e3224f"><gtr:id>83b75befe4d3b087e46dfabc61e3224f</gtr:id><gtr:otherNames>Robinson, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b82b57f07989.90873854</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M001210/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>