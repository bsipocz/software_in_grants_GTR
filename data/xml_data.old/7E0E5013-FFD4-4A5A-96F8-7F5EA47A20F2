<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A89F7E56-4599-4BB3-B4F5-6F71A367E980"><gtr:id>A89F7E56-4599-4BB3-B4F5-6F71A367E980</gtr:id><gtr:name>iniLabs Ltd</gtr:name><gtr:address><gtr:line1>Laternengasse 4</gtr:line1><gtr:postCode>8001</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Engineering Mathematics</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A89F7E56-4599-4BB3-B4F5-6F71A367E980"><gtr:id>A89F7E56-4599-4BB3-B4F5-6F71A367E980</gtr:id><gtr:name>iniLabs Ltd</gtr:name><gtr:address><gtr:line1>Laternengasse 4</gtr:line1><gtr:postCode>8001</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/12F8C476-29C9-4A2E-BFC1-C3A9FD0A0A00"><gtr:id>12F8C476-29C9-4A2E-BFC1-C3A9FD0A0A00</gtr:id><gtr:name>Italian Institute of Technology</gtr:name><gtr:address><gtr:line1>Via Morego 30</gtr:line1><gtr:postCode>16163</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Italy</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/CF47CA4C-7559-4229-8824-E5A85A41BE65"><gtr:id>CF47CA4C-7559-4229-8824-E5A85A41BE65</gtr:id><gtr:firstName>Nathan</gtr:firstName><gtr:otherNames>Francis</gtr:otherNames><gtr:surname>Lepora</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM02993X%2F1"><gtr:id>7E0E5013-FFD4-4A5A-96F8-7F5EA47A20F2</gtr:id><gtr:title>Tactile superresolution sensing</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M02993X/1</gtr:grantReference><gtr:abstractText>Superresolution encompasses a range of techniques for transcending the resolution limit of a sensor and earned the 2014 Nobel Prize in Chemistry (for superresolved fluorescence microscopy). Superresolution is analogous to biological hyperacuity of vision and touch where the discrimination is finer than the spacing between sensory receptors. Superresolution research in visual imaging has impacted science from cell biology to medical scanning 'in ways unthinkable in the mid-90s' (Editorial, Nature 2009). 

The success of this proposal will enable the widespread uptake of superresolution techniques in the domain of artificial tactile sensing, potentially impacting multiple application areas across robotics from autonomous quality control in manufacturing to sensorized grippers for autonomous manipulation to sensorized prosthetic hands and medical probes in healthcare.

Proposed research

More specifically, the development of robust and accurate artificial touch is required for autonomous robotic systems to interact physically with complex environments, underlying the future robotization of broad areas of manufacturing, food production, healthcare and assisted living that presently rely on human labour. Currently, there are many designs for tactile sensors and various methodologies for perception, from which general principles are emerging, such as taking inspiration from human touch (Dahiya et al 2012), using statistical approaches to capture sensor and environment uncertainty and combining tactile sensor control and perception (Prescott et al 2012).

All application areas of robot touch are currently limited by the capabilities of tactile sensors. This first grant proposal aims to demonstrate that tactile superresolution can radically improve tactile sensor performance and thus potentially impact all areas of robotics involving physical interaction with complex environments. Visual superresolution has revolutionised the life sciences by enabling the imaging of nanoscale features within cells. Tactile superresolution has the potential to drive a step-change in tactile robotics, with applications from quality control and autonomous manipulators in manufacturing (Yousef et al 2011) to sensorized prosthetics and probes in healthcare.

Proposed initial application domain

Currently, across the entire automobile industry, gap and flush quality controls are made manually by human operators using their hands to check the alignment between vehicle parts. Experts in the industry have informed me that human hands are used because modern vision-based measuring technologies (such as laser scanners) do not robustly detect sub-millimetre misalignments between parts of differing reflectivity and refractivity. An automated system using robot touch would be more reliable, enable traceability of defects, and move production towards a fully automated paradigm. 

The proposed research will culminate in a pilot study demonstrating that tactile superresolution will enable readily available tactile sensors to make gap and flush measurements of the requisite sub-millimetre tolerance and how the sensors should be controlled during the tactile perception task. This will constitute a first step towards building a consortium between academic and industrial partners to develop a fully working prototype for test installation on a production line.</gtr:abstractText><gtr:potentialImpactText>Tactile superresolution is a key enabling technology for existing tactile sensors to operate sensitivities sufficient for deployment in application areas such as healthcare technologies and manufacturing the future. The success of this proposal would have wide impact across Engineering, by raising awareness that superresolution can have many diverse applications, rather than being confined to conventional imaging applications such as microscopy in the life sciences. 

Industry: One of the main beneficiaries of the proposed tactile superresolution research will be the automotive manufacturing sector where quality control of the finished product is of primary concern. In the UK, car production presently accounts for 11% of all UK exports, and over 1.5 million vehicles per year. I will utilize my contacts with Peugeot-Citroen PSA (who are supporting this proposal) and Centro Ricerche Fiat to disseminate the research findings and also submit a proposal for EU funding to develop the concept into a prototype for test installation on a production line. All results will be published to maximally disseminate the implications for industry. I will also utilize the University of Bristol's strong links with the UK transport manufacturing sector including Airbus (Filton site) and Rolls Royce, who are both based in Bristol, to build a complementary UK-based consortium.

Society: The success of this proposal will enable the widespread uptake of superresolution techniques in the domain of artificial tactile sensing applying to multiple application domains with significant societal impact. According to an influential recent report: 'robotics is the fastest growing industry in the world, and poised to become the largest industry in the next decade.' I will contribute to this growth by enabling robust and accurate artificial touch that is needed for autonomous robotic systems to interact physically with complex environments, underlying the future robotization of broad areas of manufacturing, food production, healthcare and assisted living that presently rely on human labour.

People: Output of trained people will also be an important outcome of the project. In addition to furthering the career of the named RA (Dr Martinez-Hernandez), I will involve BEng, MEng, MSc and PhD students in the proposed research, allowing them to develop skills in utilizing advanced robotics with application to industry. I am well-placed to enable the student participation as Program Director for the Bristol MSc in Robotics and as a member of the management panel for the Bristol CDT in Robotics. This training will help address the shortage of trained people in the fields of Robotics and Engineering in the UK. The equipment for demonstrating autonomous tactile quality control can also be used in University Open Days to stimulate school students' interest in Robotics and Engineering

Academic Career Impact

Given that the proposal is a First Grant Application, a key aspect of its impact is its ability to provide me with a framework for developing an independent research career. The success of the proposal would allow me to build up my research team (currently consisting of PhD and project students) to critical mass with the addition of an experienced postdoctoral researcher with whom I have already had a highly successful working relationship. A strong research team is needed to translate my research program into impactful outcomes. 

The proposal research will impact the career of the Dr Martinez-Hernandez, who I supervised before moving to the University of Bristol. We could continue our successful teamwork that resulted in many publications in highly-ranked conferences and journals and laid the foundations for this proposal. I will also ensure that Dr Martinez-Hernandez makes full use of the training opportunities offered by the University of Bristol's Staff Development team, enabling him to become an independent researcher.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-06-15</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98357</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>iniLabs Ltd</gtr:collaboratingOrganisation><gtr:country>Switzerland, Swiss Confederation</gtr:country><gtr:description>Commercialisation - Neuromorphic tactile sensing (Inilabs)</gtr:description><gtr:id>70CE3CBA-6891-442F-920A-CF9085B25A88</gtr:id><gtr:impact>Partnership still underway and covered by a NDA.</gtr:impact><gtr:outcomeId>58c29c6de3fc96.57814595-1</gtr:outcomeId><gtr:partnerContribution>Expertise on event-based vision sensors and help with commercialisation.</gtr:partnerContribution><gtr:piContribution>Development of a neuromorphic tactile sensor, using event-based coding for the tactile output based on the dynamic vision sensor developed by inilabs.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>donation to ROBOTS exhibiton Science Museum, London</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C563A1FF-C908-4E99-9189-141B7EF7EAAA</gtr:id><gtr:impact>Donated output of research (3D-printed tactile sensor and robot hand) to the ROBOTS exhibition at the Science Museum, London</gtr:impact><gtr:outcomeId>58c2aeec58df71.58686869</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://beta.sciencemuseum.org.uk/robots/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Press release on winning Harvard softrobotics competition</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BA437D0C-8125-4797-ABC5-77300C49827E</gtr:id><gtr:impact>Press release on the Tactile Robotics Team won first prize in the research category in the 2016 Soft Robotics Competition, from over 80 entries internationally. Our entry was on an open source 3d-printed tactile fingertip.
Led to a few interviews and several media articles internationally.</gtr:impact><gtr:outcomeId>58c2b4e70e7530.39317265</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bristol.ac.uk/news/2017/january/tactip.html</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>5000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Pump priming</gtr:description><gtr:end>2015-06-02</gtr:end><gtr:fundingOrg>University of Bristol</gtr:fundingOrg><gtr:id>484E7628-4328-43BA-B950-B12A5B5356E2</gtr:id><gtr:outcomeId>56aa8df6584c18.06999799</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1000000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Leverhulme Leadership Award</gtr:description><gtr:end>2022-03-02</gtr:end><gtr:fundingOrg>The Leverhulme Trust</gtr:fundingOrg><gtr:fundingRef>RL-2016-039</gtr:fundingRef><gtr:id>9D80D4A7-B5BC-46BC-8ABB-F7A7228397C3</gtr:id><gtr:outcomeId>58c29da02abbc2.16069494</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>20000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Impact Acceleration Award</gtr:description><gtr:end>2017-03-02</gtr:end><gtr:fundingOrg>University of Bristol</gtr:fundingOrg><gtr:id>08F7AF6B-4226-4DD3-8B48-705069C0461A</gtr:id><gtr:outcomeId>58c29d227a6ec0.65178435</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>22000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Engineering Capital Equipment / Refurbishment/ Investment</gtr:description><gtr:end>2016-05-02</gtr:end><gtr:fundingOrg>University of Bristol</gtr:fundingOrg><gtr:id>7C4CDDCB-097C-42E5-96F5-9CA540E37267</gtr:id><gtr:outcomeId>56aa8da59d9af3.85722676</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-12-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The Science Museum (London) is displaying a 3d printed tactile robot hand developed under this award as part of a robotics exhibition and in their permanent collection.
There have been several media articles relating to this award, in particular we won the Harvard SoftRobotics competition and this was covered in the press.
We are currently exploring commercialisation of the academic findings (3d-printed tactile sensors).</gtr:description><gtr:firstYearOfImpact>2017</gtr:firstYearOfImpact><gtr:id>72AB6B1B-CEF3-48C1-8705-313998D21F0F</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56dd426d19f314.71724602</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>1) We demonstrate that an artificial tactile system offers a solution to autonomous quality inspection, using a biomimetic tactile fingertip mounted as end-effector on an industrial arm. The study considers a task of gap width inspection suitable for judging parts alignment, although the methods apply generally. We conclude that an artificial tactile system of the type here offers an ideal solution to automated quality control on the production line.
Reference: N. Lepora, B. Ward-Cherrier. Tactile quality control with biomimetic active touch. IEEE Robotics and Automation Letters, 2016. 
2) Although superresolution has been studied to huge impact in visual imaging, it is relatively unexplored in tactile robotics. Here we demonstrate a novel optical sensor design (the TacTip) capable of achieving 40-fold localization superresolution to 0.1mm accuracy compared with a 4mm resolution between tactile elements. The attained superresolution is comparable to the best perceptual hyperacuity in humans.
Reference: N. Lepora and B. Ward-Cherrier. Superresolution with an optical tactile sensor In: Intelligent Robots and Systems (IROS): 2686-2691, IEEE/RSJ, 2015. 
3) Tactile manipulation will be essential for automating industrial and service tasks currently done by humans. Here we present the TacThumb (Tactile Thumb): a cheap, robust, 3d-printed optical tactile sensor integrated on the Yale GrabLab model M2 gripper. This model-free approach gives a demonstration of basic tactile manipulation without the need for a kinematic model of the hand, in a manner that should generalize to other tactile manipulation tasks.
Reference: B. Ward-Cherrier, L. Cramphorn, N. Lepora. Tactile manipulation with a TacThumb integrated on the Open-Hand M2 Gripper. IEEE Robotics and Automation Letters, 2016 
4) Tactile perception methods rely on creating mappings from tactile data to percepts. We introduce here a method to generalize tactile features. The generalization method leads to a strong reduction in the time needed to gather training data and only a moderate increase in classification error, and is particularly suited for multidimensional tactile data sampling in complex tasks such as tactile manipulation. 
Reference: B. Ward-Cherrier, L. Cramphorn, N. Lepora. Exploiting sensor symmetry for generalized tactile perception in biomimetic touch. IEEE Robotics and Automation Letters, 2017 
5) A key unsolved problem in tactile robotics is how to combine tactile perception and control to interact robustly and intelligently with the surroundings. Our approach brings together active perception and haptic exploration as instantiations of a common active touch algorithm, and has potential to generalize to more complex tasks requiring the flexibility and robustness of human touch.
Reference: N. Lepora, K. Aquilina, L. Cramphorn,. Exploratory Tactile Servoing With Active Touch. IEEE Robotics and Automation Letters, 2017 
6) The fingerprint is a morphological aspect of the human fingertip that has interesting implications for our sense of touch. Here, we endow a biomimetic sensor with both papillary ridges (fingerprint) and a dermal stiffness contrast (stiffer intermediate ridges), and assess the impact on localization perception accuracy. This supports the theories that the fingerprint aids the classification of edges and smaller spatial scales, and demonstrates that the addition of a fingerprint to an artificial tactile sensors improves its acuity.
Reference: L. Cramphorn, B. Ward-Cherrier, N. Lepora. Addition of a Biomimetic Fingerprint on an Artificial Fingertip Enhances Tactile Spatial Acuity. IEEE Robotics and Automation Letters, 2017 
7) In this paper, we present a novel and robust Bayesian approach for autonomous active exploration of unknown objects using tactile perception and sensorimotor control. Our work demonstrates that active perception has the potential to enable robots to perform robust autonomous tactile exploration in natural environments.
Reference: U. Martinez-Hernandez, T. Dodd, M. Evans, T., N. Lepora. Active sensorimotor control for tactile exploration. Robotics and Autonomous Systems 2017
8) This study provides a synthetic viewpoint that compares, contrasts, and draws commonalities for biomimetic perception over a range of tactile sensors and tactile stimuli. The active perception consistently reaches superresolved accuracy (hyperacuity) finer than the spacing between tactile elements. Biomimetic active touch thus offers a common approach for biomimetic tactile sensors to accurately and robustly characterize and explore non-trivial, uncertain environments.
Reference: N. Lepora. Biomimetic Active Touch with Fingertips and Whiskers. IEEE Transactions on Haptics 2016
9) Tactile manipulation is the ability to control objects in real-time using the sense of touch. Here we examine tactile manipulation from the perspective of active touch with a biomimetic tactile sensor, which combines tactile perception with control of sensor location. In general, it appears there is a trade-off between the responsiveness to unknown change and manipulation accuracy, which must be set appropriately for each task.
Reference: L. Cramphorn, B. Ward-Cherrier, N. Lepora. Tactile manipulation with biomimetic active touch. IEEE ICRA, 2016</gtr:description><gtr:exploitationPathways>Superrresolution methods are relevant to any application of tactile sensing, as they give a method to improve the accuracy of tactile sensors. Applications range from measuring devices with touch, to application in grippers for picking up objects or manipulating objects in-hand. We demonstrated specifically in this grant how tactile superresolution applies directly to the automotive industry.</gtr:exploitationPathways><gtr:id>F73C53C3-7A6A-4F59-A28D-3FB84BEB9A44</gtr:id><gtr:outcomeId>56dd4589952b14.48370490</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Manufacturing, including Industrial Biotechology</gtr:sector></gtr:sectors><gtr:url>http://www.lepora.com</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The following data was obtained from the TacThumb, an optical tactile sensor integrated on a robotic gripper. The hand rolled a 25mm cylinder along the sensor in 0.5mm increments over a 20mm range. The data was used to demonstrate tactile manipulation in the open-hand M2 gripper</gtr:description><gtr:id>050EFD08-1F75-48EB-B903-E45E39F498A7</gtr:id><gtr:impact>This dataset was collected as part of the research for the paper: &amp;quot;Tactile manipulation with a TacThumb integrated on the Open-Hand M2 gripper&amp;quot;.</gtr:impact><gtr:outcomeId>58b45887d2d761.31041284</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Tactile manipulation with a TacThumb integrated on the Open-Hand M2 gripper</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://data.bris.ac.uk/data/dataset/14tradpvkkd681344nss1lxvbz</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The following data was obtained from the TacTip, an optical tactile sensor while rolling a 25mm cylinder horizontally at 800 locations separated horizontally by 0.1mm. The data was used in a study demonstrating tactile manipulation with the TacTip mounted on an ABB industrial robot arm.</gtr:description><gtr:id>9FC1DC0D-E715-47A1-B780-162E4305C712</gtr:id><gtr:impact>This dataset was collected as part of the research for the paper: &amp;quot;Tactile manipulation with biomimetic active touch&amp;quot;.</gtr:impact><gtr:outcomeId>58b46087343136.58088201</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Tactile manipulation with biomimetic active touch</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://data.bris.ac.uk/data/dataset/x1xiof2r2xa315nlad33ms7ow</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The following data was obtained from the TacTip, an optical tactile sensor while tapping vertically downwards on a 40mm diameter hemicylinder, at 4000 locations separated horizontally by 0.01mm. The data was used in a study exploring superresolution with the TacTip sensor.</gtr:description><gtr:id>2916E0EC-977F-4197-9894-9F2C59E6EA71</gtr:id><gtr:impact>This dataset was collected as part of the research for the paper: &amp;quot;Superresolution with an optical tactile sensor&amp;quot;.</gtr:impact><gtr:outcomeId>58b460ef9c1318.30703917</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Superresolution with an optical tactile sensor</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://data.bris.ac.uk/data/dataset/1dusr7ea0y2hf11zq8cfbehd7r</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The following data was obtained from the TacTip, an optical tactile sensor while tapping vertically downwards on a series of gaps cut in a horizontal perspex sheet. Gaps go from 0.25mm to 5mm in 0.25mm steps and are each tapped 110 times at depths ranging over 5.5mm. The data was used in a study investigating tactile quality control with the TacTip sensor.</gtr:description><gtr:id>DA68C678-3A66-4D2B-A6BB-667247E2A45C</gtr:id><gtr:impact>This dataset was collected as part of the research for the paper: &amp;quot;Tactile quality control with biomimetic active touch&amp;quot;.</gtr:impact><gtr:outcomeId>58b46048926e03.34883999</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Tactile quality control with biomimetic active perception</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://data.bris.ac.uk/data/dataset/ejsgfgl8qrtq1gn8g43m65onq</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The following data was obtained from 3 variants of the TacTip (with fingerprint, with fingerprint and cores, without fingerprint), an optical tactile sensor integrated on a 6-dof robotic arm The sensor performed localization on 9 stimuli with varying spatial frequency over a 30 mm range. The data was used to demonstrate the effect of fingerprints on tactile spatial perception.</gtr:description><gtr:id>5B7A9C05-0F25-482F-9410-673C81D1D8FA</gtr:id><gtr:impact>This dataset was collected as part of the research for the paper: &amp;quot;Addition of a biomimetic fingerprint on an artificial fingertip enhances tactile spatial acuity&amp;quot;.</gtr:impact><gtr:outcomeId>58b46129ac5a12.04594211</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Addition of a biomimetic fingerprint on an artificial fingertip enhances tactile spatial acuity</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://data.bris.ac.uk/data/dataset/2nky5v0esi6mu2pza3kncbb3eq</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The following data was obtained from the TacTip, an optical tactile sensor integrated on a 6-dof robotic arm. Data was collected for edge orientation in 1 degree increments over a 360 degree range, and for contour following experiments location and identity data was collected over 360 deg (5 degree increments) and a 20 mm range (1 mm increments) across the edge. Data was then generalized using sensor symmetry from the first 30 degrees of collected data, and used to demonstrate contour following with the TacTip using generalized data.</gtr:description><gtr:id>C8F4B88A-AC17-4125-90AB-80D7CC5B7289</gtr:id><gtr:impact>This dataset was collected as part of the research for the paper: &amp;quot;Exploiting sensor symmetry for generalized tactile perception in biomimetic touch&amp;quot;.</gtr:impact><gtr:outcomeId>58b459111dcef9.59456523</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Exploiting sensor symmetry for generalized tactile perception in biomimetic touch</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://data.bris.ac.uk/data/dataset/vq6mpgxfyb8h2x0s6k85yu62r</gtr:url></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>DA4D61DF-9628-4C5D-8E0D-946D39BC4B48</gtr:id><gtr:title>Tactile Manipulation With a TacThumb Integrated on the Open-Hand M2 Gripper</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64af6184d5390af1ffefc3fba7dcd707"><gtr:id>64af6184d5390af1ffefc3fba7dcd707</gtr:id><gtr:otherNames>Ward-Cherrier B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56aa8c71b9ff41.10170221</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DE11EAE8-1DEF-4087-9005-8497F07D6953</gtr:id><gtr:title>Tactile manipulation with biomimetic active touch</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cf662bdcdbc091885882db1a01bba324"><gtr:id>cf662bdcdbc091885882db1a01bba324</gtr:id><gtr:otherNames>Cramphorn L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c298bb48bef5.84007599</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2361C35C-6AC2-421F-A442-039B59434860</gtr:id><gtr:title>The TacTip Family: Soft Optical Tactile Sensors with 3D-Printed Biomimetic Morphologies.</gtr:title><gtr:parentPublicationTitle>Soft robotics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64af6184d5390af1ffefc3fba7dcd707"><gtr:id>64af6184d5390af1ffefc3fba7dcd707</gtr:id><gtr:otherNames>Ward-Cherrier B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>2169-5172</gtr:issn><gtr:outcomeId>5a95d7e85077a4.18150441</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ADD66239-90C2-4ED1-8672-A5B19172ABCF</gtr:id><gtr:title>Exploratory Tactile Servoing With Active Touch</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6f6866c1754a536fd50275bda3e3e7b"><gtr:id>e6f6866c1754a536fd50275bda3e3e7b</gtr:id><gtr:otherNames>Lepora N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c29848bab7a0.73692695</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C7177686-4D03-411D-B9AA-7D0E67ABEFC2</gtr:id><gtr:title>Addition of a Biomimetic Fingerprint on an Artificial Fingertip Enhances Tactile Spatial Acuity</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cf662bdcdbc091885882db1a01bba324"><gtr:id>cf662bdcdbc091885882db1a01bba324</gtr:id><gtr:otherNames>Cramphorn L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c29849660dc8.07008239</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8CAD4F9C-8514-4788-8106-1434614D8AC3</gtr:id><gtr:title>Superresolution with an optical tactile sensor</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6f6866c1754a536fd50275bda3e3e7b"><gtr:id>e6f6866c1754a536fd50275bda3e3e7b</gtr:id><gtr:otherNames>Lepora N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56aa8b92299d13.61614877</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>453A1150-C283-45E8-9281-182297B12061</gtr:id><gtr:title>Model-Free Precise in-Hand Manipulation with a 3D-Printed Tactile Gripper</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64af6184d5390af1ffefc3fba7dcd707"><gtr:id>64af6184d5390af1ffefc3fba7dcd707</gtr:id><gtr:otherNames>Ward-Cherrier B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe2731ef781.31680707</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>027FD91C-B82A-4745-82D0-A725B3524FB0</gtr:id><gtr:title>Object exploration using vision and active touch</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/851a472230d2cd69a9cab22daa89e275"><gtr:id>851a472230d2cd69a9cab22daa89e275</gtr:id><gtr:otherNames>Yang C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a95d7e88ad5b8.02458765</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E6FE933D-4CA1-4787-B777-EF18B4B20933</gtr:id><gtr:title>Tactile Quality Control With Biomimetic Active Touch</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6f6866c1754a536fd50275bda3e3e7b"><gtr:id>e6f6866c1754a536fd50275bda3e3e7b</gtr:id><gtr:otherNames>Lepora N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd435d670bd0.54244930</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>246392A6-0EF0-4B5D-A366-F3CEA0987A89</gtr:id><gtr:title>Exploiting Sensor Symmetry for Generalized Tactile Perception in Biomimetic Touch</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64af6184d5390af1ffefc3fba7dcd707"><gtr:id>64af6184d5390af1ffefc3fba7dcd707</gtr:id><gtr:otherNames>Ward-Cherrier B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c298492bfeb6.96366599</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9FDB8CC6-854C-479A-8049-EEF18A582EB3</gtr:id><gtr:title>Dual-Modal Tactile Perception and Exploration</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8258d7cc3bbeae6beeba48b7b155e60c"><gtr:id>8258d7cc3bbeae6beeba48b7b155e60c</gtr:id><gtr:otherNames>Pestell N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a95d76a580966.19073253</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>49EE2606-1E91-4BFF-9C9E-4B5F86E646E5</gtr:id><gtr:title>Active sensorimotor control for tactile exploration</gtr:title><gtr:parentPublicationTitle>Robotics and Autonomous Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3ddee02b6b8102edc0a726d60a7efe7f"><gtr:id>3ddee02b6b8102edc0a726d60a7efe7f</gtr:id><gtr:otherNames>Martinez-Hernandez U</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c2984902a393.80104894</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A55DABD6-F058-45CC-A4D7-DFC6BEE641BB</gtr:id><gtr:title>Biomimetic Active Touch with Tactile Fingertips and Whiskers.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on haptics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ff8958cc1d50ec1c8897e55bc1322b8d"><gtr:id>ff8958cc1d50ec1c8897e55bc1322b8d</gtr:id><gtr:otherNames>Lepora NF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1939-1412</gtr:issn><gtr:outcomeId>585d6237e2d722.44388856</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M02993X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>FA25733D-A456-4A1B-9D5C-7119577368D9</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Manufacturing</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>85</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>DA992CBF-8F56-4B55-AF51-A6876E825318</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Design &amp; Testing Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F78E4567-DD59-4364-9D1F-0A778996E941</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Instrumentation Eng. &amp; Dev.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>65</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>