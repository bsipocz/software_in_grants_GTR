<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/44CE5F15-F313-48D2-AE30-1FCAF862B8DD"><gtr:id>44CE5F15-F313-48D2-AE30-1FCAF862B8DD</gtr:id><gtr:firstName>Mauricio</gtr:firstName><gtr:otherNames>Alex?nder</gtr:otherNames><gtr:surname>?lvarez L?pez</gtr:surname><gtr:orcidId>0000-0002-8980-4472</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FR034303%2F1"><gtr:id>3FE36537-F994-44BD-9A05-B33E918F65A8</gtr:id><gtr:title>Learning to move as a human: one-shot learning of human motion</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/R034303/1</gtr:grantReference><gtr:abstractText>Computational models for human motion analysis and synthesis have applications in fields as diverse as healthcare, computer graphics, and robotics. In healthcare, analysis of human movements can be used, for example, for tracking motor decline in the elderly. In computer graphics, human motion analysis can be used for human pose tracking from a single camera when measurements might be noisy or missing due to occlusion. In robotics, human motion analysis and synthesis can be used for teaching robots new skills by imitating demonstrations of a human, reducing the effort required to program an industrial robot or a service robot.

One approach to understand how humans move consists of collecting examples of a particular human activity and designing a machine learning model that extracts patterns from those examples. The more examples we collect, the more likely it is for the model to find common features in the data that can be exploited for solving predictive tasks. However, in different applications that require human motion analysis and synthesis, particularly in robot programming by demonstration, collecting many examples is expensive and time-consuming. I.e. we would like a robot to learn a new skill with as few demonstrations as possible, more like a human does. Indeed, humans learn efficiently by imitation with just one or few examples, which is further validated by their ability to generate new examples or creating abstract motions that were not previously seen in the examples that were used to imitate.

In this project, our objective is to develop a data-efficient machine learning model for human motion using the cognitive science concept of one-shot learning.

In cognitive science, one-shot learning (OL) refers to the idea of building intelligent agents using one or few examples. Successful illustrations of the use of this concept for building data efficient models include OL models for generating speech concepts and handwritten characters with human-like appearance. Recent research in cognitive science suggests that humans achieve OL through the combination of three core principles applied to primitive concepts: causality, compositionality, and &amp;quot;learning to learn&amp;quot;. It also claims that these ingredients could play an active role in producing machine learning models that replicate human intelligence.

We will achieve our objective through the two key novelties of this proposal: (i) a generic methodology that simultaneously combines causality, compositionality and learning to learn of motor primitives and (ii) a particular instantiation that uses physics-inspired Gaussian process (GP) representations of such motor primitives.

With respect to (i), although there are machine learning models that incorporate some of the ingredients of OL, their simultaneous combination to build data-efficient models for human motion analysis and synthesis has not been proposed yet. With respect to (ii), our GP representation of a motor primitive uses a physics-inspired covariance function with two features: the efficient use of data due to its non-parametric nature; and the inclusion of the principle of causality of OL, providing a generative mechanism for trajectory data. Compositionality of these GP motor primitives will be approached using ideas from formal language theory, in particular, hidden Markov models with explicit state durations. Learning to learn will be accomplished by providing hierarchies of such hidden Markov models.

In order to use the model in practice, we will provide a statistical inference framework for fitting the parameters of the OL model to given data, and for computing probability distributions for prediction. We will test the performance of the OL model for different tasks related to motion capture data, and for imitation learning using kinesthetic demonstrations from anthropomorphic robots. Our results will be fully reproducible and our software to be released as open source.</gtr:abstractText><gtr:potentialImpactText>Developing data-efficient machine learning models for human motion can potentially have a positive impact in the following areas.

-- Economy. The Fourth Industrial revolution is taking place at the time this proposal is being written. It has been coined with the term Industry 4.0 (or Digital Manufacturing) and its aim is to use advances in information and communication (IC) technologies to promote the computerisation of manufacturing, consequently increasing productivity. Adopting IC technologies in the industry has the potential to increase the UK manufacturing revenue by 12.5%, equivalent to &amp;pound;20bn (based on the same experience in Germany) by 2020. A key driver force of this Fourth Industrial revolution is Industrial Robotics, a market that is expected to reach $41bn by 2020. These new industrial robots are expected to be more intelligent and flexible enough to develop different tasks in the factory. Data-efficient machine learning models for imitation learning of human motion can impact positively on the advancement of these capabilities for industrial robotics in the UK. Likewise, the UK spent &amp;pound;8.34 billion on social care for the elderly in 2015/2016 and it is estimated that today, one in eight older people live without the proper level of care. The methods that we will develop in this project will potentially provide technologies to build assistive robots with the ability to easily being programmed by imitation for an older person to carry out a daily living task, alleviating the pressure on the social care system.

-- Society. Computational models for human motion have a range of applications in healthcare. For example, they can be used for measuring the progress of a rehabilitation therapy for a previously injured limb, or for early diagnosis of motor-related diseases, e.g. Parkinson's. Developing more faithful models of human locomotion can potentially be used to build more accurate biomarkers in these applications. In addition, analysis of human motion inealthcare is progressively moving from a laboratory-based analysis to daily life monitoring. Therefore, developing probabilistic models for human motion that can cope with noisy measurements obtained from wearable sensors can help to establish them as reliable tools for assessing a medical condition correlated to human locomotion.

-- Knowledge. The generative models that we will develop in this project will have an impact on machine learning, and on imitation learning for robotics. Within machine learning, the probabilistic models that we will develop are examples of semi-parametric models, where the parametric part of the model is modular and given by different types of hidden Markov models, and the non-parametric part is given by powerful representations of observed data in the form of Gaussian processes distributions. Further research can exploit these architectures for designing machine learning models in different applications like computational biology and geostatistics. Within imitation learning for robotics, we claim that using the principles of human-like intelligence for building machine learning models for human motion paves the road for data-efficient robot learning. Further research projects can be developed by increasing the complexity of the models that take these principles into account.

-- People. The project will have a positive impact on the careers of the PI and the RA, who will both gain additional experience in the formulation of novel probabilistic machine learning models and hands-on experience with real robotic systems. An EPSRC New Investigator Award is the ideal opportunity for the PI to start building himself as a leading researcher in human-like computing in the UK.

To achieve these potential impacts, in the shorter and longer term, we will develop Pathways to Impact as described in the corresponding Pathways to Impact section.</gtr:potentialImpactText><gtr:fund><gtr:end>2020-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2018-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>219757</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/R034303/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>