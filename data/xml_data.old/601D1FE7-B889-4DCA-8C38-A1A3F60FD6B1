<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Experimental Psychology</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BD6BE61E-B930-47A1-822C-74FDF95F3115"><gtr:id>BD6BE61E-B930-47A1-822C-74FDF95F3115</gtr:id><gtr:name>Wicab, Inc</gtr:name><gtr:address><gtr:line1>8313 Greenway Blvd</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/44B3B477-75B9-44EB-8916-318136B765B2"><gtr:id>44B3B477-75B9-44EB-8916-318136B765B2</gtr:id><gtr:firstName>Alan</gtr:firstName><gtr:surname>Johnston</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7A7D1301-C125-4222-8F40-0F2AC9E6E165"><gtr:id>7A7D1301-C125-4222-8F40-0F2AC9E6E165</gtr:id><gtr:firstName>Martin</gtr:firstName><gtr:surname>Sereno</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DD5F1EA8-2867-4E11-AD32-1B5EE1688869"><gtr:id>DD5F1EA8-2867-4E11-AD32-1B5EE1688869</gtr:id><gtr:firstName>Jeremy</gtr:firstName><gtr:otherNames>I</gtr:otherNames><gtr:surname>Skipper</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM026965%2F1"><gtr:id>601D1FE7-B889-4DCA-8C38-A1A3F60FD6B1</gtr:id><gtr:title>New pathways to hearing: A multisensory noise reducing and palate based sensory substitution device for speech perception</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M026965/1</gtr:grantReference><gtr:abstractText>Disabling hearing loss is a global problem that affects nearly half a billion people. Furthermore, it is a problem that is growing with an aging population and has clear negative functional, social, emotional, and economic impacts. In the United Kingdom, adult onset hearing loss is predicted to be one of the top ten disease burdens by 2030 (WHO). Commercially available correction for hearing loss is mostly limited to hearing aids and cochlear implants. These devices suffer from signal processing and sensory transduction limitations. On the signal processing side, they struggle with the separation of speech from noise, often from other voices in social situations - the cocktail party phenomenon. On the transduction side, devices continue to rely on the damaged cochlea as the channel of communication. The aim of this proposal is to address these limitations through multisensory remapping at both the signal processing and transduction stages. 
We will address signal processing limitations by introducing a new multisensory algorithm that will aim to recover the auditory signal from talking faces. The moving face can provide a source of information that is independent of environmental noise. Facial movement can also be used to enhance the signal to noise ratio of audio-only based speech. The new method can also recover facial movement from auditory signals alone so that speech perception might continually benefit from known improvements associated with being able to see the face, even when it is not present, as when the recovered face is presented on a device carried by the listener (e.g. a smart phone or Google Glass). 
We will address signal transduction limitations by building on recent successes in supplementing vision through high-density tactile stimulation of the tongue and previous work demonstrating promise for supplementing word learning through tactile stimulation. In particular, we will build a novel non-invasive conformable high-density electrode array that provides electrotactile stimulation of the hard palate. This is the first device with high enough channel density to realistically provide, in tactile form, the spatial information about sound frequency available along the healthy cochlea. By putting it on the hard palate, this device will be the first sensory supplementation device to have direct access to sensorimotor brain circuitry important for speech learning and perception through the trigeminal nerve. 
Finally, we will use behavioural and brain imaging methods to experimentally test the combined use of these signal processing and transduction innovations for hearing supplementation. From past experience with more primitive devices (e.g. The Tickle Talker), we expect people will be able to rapidly learn words and transfer training to novel words in new contexts. We expect training to be enhanced by combined presentation of audio to the hard palate and the face to a portable display device, so learning can occur in natural contexts. We will test the ability of participants to use the device for speech perception behaviourally and use functional imaging to look for indications of activation or modification of speech circuits in the brain after training. This work will contribute to our understanding of multisensory signal processing algorithms for hearing devices, auditory-to-tactile hearing supplementation, and multisensory brain plasticity. Success with this experimental device would warrant clinical trials to supplement hearing in individuals with hearing loss and bring these innovations to market as a new device to help with the social and economic challenges posed by disabling hearing loss.</gtr:abstractText><gtr:potentialImpactText>Disabling hearing loss is a global problem that affects nearly half a billion people. More than 16% of the UK population overall and 70% of those over 70 has some form of hearing loss (Action on Hearing Loss Statistics, 2014). The problem is particularly acute given the ageing population and the ubiquity of age-related hearing loss. Age-related hearing loss is one of the most significant contributors to social isolation and lack of wellbeing in the elderly. Associated problems include increased social and cognitive difficulties, social isolation, depression, dementia, and mortality when uncorrected (Arlinger, S., Negative consequences of uncorrected hearing loss-a review. International Journal of Audiology, 2003). Commercially available correction for hearing loss is mostly limited to hearing aids that only amplify sounds and cochlear implants that are invasive, expensive, and limited in the number of channels carrying acoustic information. Health related quality of life and cost effectiveness results associated with these devices are quite variable with only small to medium typical effect sizes (e.g., Chisolm et al., 2007). The technologies on which these devices are built are already mature. Our strategy is to take a different approach that exploits recent advances in computation power, particularly in handheld devices, and new developments in the fabrication of conformable electrode arrays. Our project makes two primary advances that will have an impact on those with hearing loss. First, we propose a new multisensory signal processing algorithm for increasing the signal to noise ratio of speech through facial speech reading that can be adopted by both existing hearing devices and a new generation of devices. Second, we propose a new signal transduction method for supplementing speech hearing through a high-density conformable electrode array worn on the hard palate. We have identified a project partner Wicab Inc. who already market an electrotactile stimulator for the blind which is placed on the tongue. They have expressed a strong interest in their letter of support in developing new products for the deaf and deaf-blind and are keen to support the project with their existing expertise and to develop any new devices to a point at which they could be brought to market.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-12-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-12-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>700743</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>60D17791-EE41-499E-B4C8-DB7E47081D57</gtr:id><gtr:title>The hearing ear is always found close to the speaking tongue: Review of the role of the motor system in speech perception.</gtr:title><gtr:parentPublicationTitle>Brain and language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/758ee884992c19edf20d322c116df520"><gtr:id>758ee884992c19edf20d322c116df520</gtr:id><gtr:otherNames>Skipper JI</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0093-934X</gtr:issn><gtr:outcomeId>585d622cac1ff5.36598823</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M026965/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>