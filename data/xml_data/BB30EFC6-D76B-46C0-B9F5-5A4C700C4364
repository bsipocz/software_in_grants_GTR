<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/58E3A7F8-5B1E-48D2-BDC5-0E835B3FA948"><gtr:id>58E3A7F8-5B1E-48D2-BDC5-0E835B3FA948</gtr:id><gtr:name>Xerox Corporation</gtr:name><gtr:address><gtr:line1>800 Phillips Road</gtr:line1><gtr:line2>Webster</gtr:line2><gtr:line4>New York 14580</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/58E3A7F8-5B1E-48D2-BDC5-0E835B3FA948"><gtr:id>58E3A7F8-5B1E-48D2-BDC5-0E835B3FA948</gtr:id><gtr:name>Xerox Corporation</gtr:name><gtr:address><gtr:line1>800 Phillips Road</gtr:line1><gtr:line2>Webster</gtr:line2><gtr:line4>New York 14580</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/27812A1D-D032-4A2A-8CC1-4EEE59C345C4"><gtr:id>27812A1D-D032-4A2A-8CC1-4EEE59C345C4</gtr:id><gtr:name>Thales Research and Technology UK Ltd</gtr:name><gtr:address><gtr:line1>Worton Drive</gtr:line1><gtr:line2>Worton Grange Business Park</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG2 0SB</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/36778D94-49F1-470C-BCF6-E9486B29E44D"><gtr:id>36778D94-49F1-470C-BCF6-E9486B29E44D</gtr:id><gtr:name>Xerox Research Centre Europe</gtr:name><gtr:address><gtr:line1>6 Chemin de Maupertuis</gtr:line1><gtr:line2>38240 Meylan</gtr:line2><gtr:line4>Grenoble</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>France</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E4F53ABD-8ECF-4261-A666-0358CD9BCCBD"><gtr:id>E4F53ABD-8ECF-4261-A666-0358CD9BCCBD</gtr:id><gtr:name>Qinetiq Ltd</gtr:name><gtr:address><gtr:line1>St Andrews Road</gtr:line1><gtr:line4>Malvern</gtr:line4><gtr:line5>Worcestershire</gtr:line5><gtr:postCode>WR14 3PS</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/8AFF7C0F-ECFC-4E6A-8CE6-6E1570F288BD"><gtr:id>8AFF7C0F-ECFC-4E6A-8CE6-6E1570F288BD</gtr:id><gtr:firstName>Dima</gtr:firstName><gtr:surname>Damen</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN033779%2F1"><gtr:id>BB30EFC6-D76B-46C0-B9F5-5A4C700C4364</gtr:id><gtr:title>LOCATE: LOcation adaptive Constrained Activity recognition using Transfer learning</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N033779/1</gtr:grantReference><gtr:abstractText>It is estimated that there are six million surveillance cameras in the UK, with only 17% of them publicly operated. Increasingly, people are installing CCTV cameras in their homes for security or remote monitoring of elderly, infants or pets. Despite this increase, the use of the overwhelming majority of these cameras is limited to evidence gathering or live viewing. These sensors are currently incapable of providing smart monitoring - identifying an infant in danger or a dehydrated elderly. Similarly, CCTV in public places is mostly used for evidence gathering. 

Following years of research, methods capable of automatically recognising activities of interest, such as a person departing a service station without making a payment for refueling the car, or one tampering with a fuel dispenser, are now available, achieving acceptable levels of success and low false alarms. Though automatic after installation, the installation process not only requires putting the hardware in place but also involves an expert studying the footage and designing a model suitable for the monitored location. At each new location, e.g. each new service station, a new model is needed, requiring the effort and time of an expert. This is expensive, difficult to scale and at times implausible such as for home monitoring for example. This requirement to build location-specific models is currently limiting the adoption of automatic recognition of activities, despite the potential benefits.

This project, LOCATE, proposes an algorithmic solution that is capable of using a pre-built model in a different location and adapting it by simply observing the new scene for a few days. The solution is inspired by the human ability to intelligently apply previously-acquired knowledge to solve new challenges. The researchers will work with senior scientists from two leading UK video analytics industrial partners; QinetiQ and Thales. Using these partners' expertise, the project will provide practical and valuable insight that can further boost the strong UK industry of video analytics. The United Kingdom is currently a global player in the video analytics market, and the leading country in the Europe, Middle East and Africa (EMEA) region. 

The method will be applicable to various domains, including for home monitoring and CCTV in public places. To evaluate the proposed approach for home monitoring, LOCATE will work alongside the EPSRC-funded project SPHERE, which aims to develop and deploy a sensor-based platform for residential healthcare in and around Bristol. The findings of LOCATE will be integrated within the SPHERE platform, towards automatic monitoring of activities of daily living in a new home, such as preparing a meal, eating or taking medication. 

The targeted plug-and-play approach will enable a non-expert user to setup a camera and automatically detect whether an elderly in the home had had their meal and medication, for example. A shop owner can similarly detect pickpocketing attempts in their store. The community can thus make better use of the already in place network of visual sensors.</gtr:abstractText><gtr:potentialImpactText>A) Economic Impact:
The LOCATE framework attempts to enable plug-and-play automatic activity recognition using visual sensors. This is central to the already strong and growing UK industry in video analytics. Current approaches require hand-crafted location-specific activity representation models, increasing costs and at times limiting the applicability of automatic monitoring. A location-adaptive solution would (i) decrease installation costs and enable wider adoption of automatic activity recognition, (ii) extend solutions to domains where location-specific models are difficult to obtain such as homes and highly-sensitive security environments, and (iii) encourage other established companies and start-ups to enter the video analytics market, as a result of the decrease in cost and increase in applicability. 

Recently, a number of UK and international SMEs have focused on developing mobile applications that are capable of achieving basic image processing such as motion detection towards alarms for intrusions in residential environments. The LOCATE framework could encourage these SMEs to expand their approaches to more advanced computer vision methods that are capable of detecting activities such as an infant in danger or a pet unable to access food or water. This could result in further boosting the customer base of these SMEs. During the follow-up phase of the project, links will be established to these SMEs and start-ups.

LOCATE primarily aims to make the most of the already installed and functioning network of wired and wireless cameras in the UK. Empowering this infrastructure to detect and prevent, rather than to be used for evidence gathering or only scarcely for live viewing, is a better utilisation of available resources.

B) Societal Impact:
Following from the economic impact, wider adoption of automatic monitoring and its extension to novel domains is one step closer to a healthier and safer society. 
When applied to healthcare monitoring, Activities of Daily Living (ADL) have been established as a measure of one's functional status and quality of life. Automatic monitoring of ADLs would allow better assessment of one's health as well as intervention when needed. 
When used for surveillance, automatic detection of activities of interest will enable intervention towards saving belongings as well as lives.
Automatic understanding of a person's activities can also encourage developing approaches to human computer interaction as well as robot computer interaction that are smarter with agile responses.

C) Academic Impact:
The project contributes to two research areas: visual activity recognition and relational-knowledge transfer learning, establishing a novel area of research in relational-knowledge transductive transfer learning for visual activity recognition. A challenge will be released to encourage other researchers to pose solutions to this problem.
The LOCATE project aims to establish the PI as a leading researcher in this novel area, continuing what is already a successful career in video analysis and activity recognition. The project will establish working collaborations between the PI and the current project partners as well as new extended collaborations.
At least one postdoctoral researcher and PhD candidate will become proficient in transfer learning approaches - a skill in high demand in research laboratories world wide and of profound effects for applications in Computer Vision and beyond.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-05-03</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-07-04</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98100</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Xerox Corporation</gtr:collaboratingOrganisation><gtr:country>France, French Republic</gtr:country><gtr:department>Xerox Research Centre Europe - XRCE</gtr:department><gtr:description>Xerox Research Center Europe (XRCE)</gtr:description><gtr:id>548FAEEE-73C4-41D0-8877-BF4231F96ADA</gtr:id><gtr:impact>Agreement signed, internship details finalised.</gtr:impact><gtr:outcomeId>58b83aa53558f0.26278144-1</gtr:outcomeId><gtr:partnerContribution>Work is curried on to finalise the details of a PhD student visit to XRCE over the summer working on semantic embedding for action recognition</gtr:partnerContribution><gtr:piContribution>Research internship secured for PhD student over summer 2017</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Chair - BMVA Symposium on Transfer Learning in Computer Vision</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>CB7635AD-9989-4A4D-8E67-42693C9F9A01</gtr:id><gtr:impact>The Computer Vision community is in need of moving beyond dataset or task-specific methods towards those that can efficiently adapt to new tasks or domains in a supervised, semi-supervised or unsupervised manner. We aim in this technical meeting to bring together leading researchers, at various levels in their career, with expertise or strong interest in TL for Computer Vision problems, in order to discuss current challenges and propose future directions including potentially establishing a continuous forum or a workshop series.

The symposium invited keynote speakers and researchers to present short talks and posters that address the motivation, methodologies, challenges and applications of using TL in Computer Vision. The day concluded with an hour of discussions by key researchers, with conclusions to be published in a report by BMVA</gtr:impact><gtr:outcomeId>58b83c779b7ad4.64111051</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://www.cs.bris.ac.uk/~damen/TLCV/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited Talk - BMVA symposium on Analysis and Processing of RGBD Data</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>B6DCF93B-1AB4-4EF7-B4ED-084DFC11DF2A</gtr:id><gtr:impact>Dr Dima Damen gave an invited talk at the BMVA symposium on Analysis and Processing of RGBD Data in London. The talk focused on challenges and opportunities for Action and Activity Recognition using RGBD Data alongside two prominent professors in the UK (Prof Ling Shao, University of East Anglia and Prof Adrian Hilton, University of Surrey). The day was well-attended by graduate students, academics and representatives of the industry.</gtr:impact><gtr:outcomeId>58b83ba0b9cec5.20333481</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>https://www.eventbrite.co.uk/e/bmva-technical-meeting-analysis-and-processing-of-rgbd-data-registration-28455212306#</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The Transfer Learning in Computer Vision day, held in Jan 2017 in London concluded with a panel discussion on the need to study task relatedness prior to attempting transfer learning. This panel discussion was very informative and influential for researchers in this team as well as all academic attendees.</gtr:description><gtr:exploitationPathways>The finding will influence future research directions within this project and elsewhere</gtr:exploitationPathways><gtr:id>EC7F30A8-1C26-4ECE-BC82-D9239E851267</gtr:id><gtr:outcomeId>58b83dc11f0ba2.72691879</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>DDF07F1A-803E-4B81-B71F-F5DE7A13C125</gtr:id><gtr:title>Recurrent Assistance: Cross-Dataset Training of LSTMs on Kitchen Tasks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8883c17044b3694371708fb224174519"><gtr:id>8883c17044b3694371708fb224174519</gtr:id><gtr:otherNames>Perrett T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a89a75e2d9b95.50973148</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N033779/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>