<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/ED6A6B32-663C-4A62-A33B-2C6A68E2E102"><gtr:id>ED6A6B32-663C-4A62-A33B-2C6A68E2E102</gtr:id><gtr:name>University of Essex</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>Wivenhoe Park</gtr:line1><gtr:line4>Colchester</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CO4 3SQ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/ED6A6B32-663C-4A62-A33B-2C6A68E2E102"><gtr:id>ED6A6B32-663C-4A62-A33B-2C6A68E2E102</gtr:id><gtr:name>University of Essex</gtr:name><gtr:address><gtr:line1>Wivenhoe Park</gtr:line1><gtr:line4>Colchester</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CO4 3SQ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name><gtr:address><gtr:line1>Polaris House</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:line4>Swindon</gtr:line4><gtr:line5>Wiltshire</gtr:line5><gtr:postCode>SN2 1ET</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/70BC4A3B-E948-42A8-A217-B16B53D3F4EC"><gtr:id>70BC4A3B-E948-42A8-A217-B16B53D3F4EC</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>Hunter</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B54CCDA0-3F52-4995-B12B-10E4DCAE465B"><gtr:id>B54CCDA0-3F52-4995-B12B-10E4DCAE465B</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>Barry</gtr:otherNames><gtr:surname>Hibbard</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FK018973%2F1"><gtr:id>9ED502D4-566A-4E4E-82DA-7F17C838714E</gtr:id><gtr:title>Learning to see in depth: neural models of binocular stereopsis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/K018973/1</gtr:grantReference><gtr:abstractText>When you go to see a 3D movie, you are provided with a pair of glasses to be worn while in the cinema. You then experience a vivid awareness of the three-dimensional shape of the objects and people in the movie, which appear to leave the screen and occupy space within the room. This occurs because the glasses allow the cinema to present two slightly different versions of the movie to your left and right eye.

The purpose of this proposal is to determine how your brain is able to interpret these differences between what is seen by your two eyes. We know that this is achieved by neurons in the brain that respond to these differences. We will establish how it is able to interpret these, to provide you with a clear appreciation of the three-dimensional shape of objects.

This research is important in providing a good theoretical understanding of binocular vision, so that we may develop successful therapies for conditions such as amblyopia and strabismus (&amp;quot;lazy eye&amp;quot; and &amp;quot;squint&amp;quot;) in which binocular depth perception may be impaired or absent. 

It is also important in enabling designers of movies and virtual reality systems to make the results as &amp;quot;real&amp;quot; as possible. This research will help us to understand the binocular image differences that our brains respond to, and how it uses these to determine three-dimensional shape. This can then be used to directly inform the design of virtual reality systems.
Finally, it is important in allowing engineers to develop robots and vehicles that can &amp;quot;see&amp;quot;; what we learn about how this is achieved by our brains is very valuable in developing artificial, computer vision.</gtr:abstractText><gtr:technicalSummary>Binocular vision provides important information allowing us to see in depth. This information is encoded by binocular neurons in the primary visual cortex. We know, however, that depth perception does not arise in any simple manner from the activity in these disparity-tuned neurons. Rather, the perception of depth involves complex exitatory and inhibitory interactions between these neurons, and activity in extrastriate visual areas. 

This project will develop models of this processing, based on Independent Components Analysis and the spectral properties of binocular images. We will create a novel set of binocular images with ground-truth data with which to develop and test these models. Our objectives in developing these models are twofold. First, they will provide a theoretical framework with which to understand binocular visual processing in the brain, beyond the initial encoding stage. Second, they will be used to develop novel, biologically-inspired algorithms for binocular stereopsis.</gtr:technicalSummary><gtr:potentialImpactText>The research has significant potential for impact within and beyond academia:

Academia

Vision Science: The results will be of significance to those studying binocular vision from behavioural and physiological perspectives. 

Computer Vision: The development of artificial vision systems has great potential to benefit from our understanding of how our (highly successful) biological visual system operates.

Optometry: A thorough understanding of binocular vision provides the necessary science underpinning the development of therapeutic interventions in cases in which this is impaired.

Business/Industry
The development of biologically inspired stereo algorithms will benefit technicians working in computer vision. Basing our algorithms on state-of-the-art knowledge of neural processing of binocular information will provide new ways of thinking to this field.

The General Public

The recent resurgence of 3D movies means that there is significant public interest in binocular depth perception. There is therefore a demand from the general public to understand the science of binocular depth perception, and an opportunity to use this as an engaging way to present neuroscience.

Pathways to Impact

Hibbard, together with colleagues at other Scottish unviersities, is currently developing the Scottish Vision Group website. This will provide information, and a forum for discussion for a variety of audiences. The first is the general public. We will provide accessible summaries of the state of knowledge of vision, in the context of (i) fundamental science (ii) understanding of disease and sensory impairment and (iii) technology. The second audience is industry and the third-sector; by bringing together the expertise available throughout Scotland in one place, we will make is easy to establish contacts with companies and charities would could benefit from out work. This will allow us to develop our current links (e.g. with the Fife Society for the Blind, and NCR). The third audience is each other, and academics more generally; the website hosts blogs and discussion groups to facilitate networking across the community. 
A prototype of the website has been built, and is hosted on servers at St Andrews; the site is scheduled to be made publically available at the end of 2012.

Academic work will be presented at a variety of conferences in neuroscience and machine vision, to ensure impact in both of these fields. Interactive presentations to the general public will be made at the annual science fair hosted by the University of St Andrews.

People

The researcher employed on the project will develop considerable skills that are of value beyond the immediate project. This includes the application of advanced programming skills to a difficult problem in biology, and the analytical, writing and presentation skills required to present the results of the research at conferences, in journal papers, and to the general public.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-03-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2013-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>300051</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Hibbard Website</gtr:description><gtr:form>Engagement focused website, blog or social media channel</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8B5AE10F-E9F0-4163-A282-B0D4CAF782E0</gtr:id><gtr:impact>Website developed to explain research in an accessible way so as to engage with the general public</gtr:impact><gtr:outcomeId>56dd9719ea14f3.57723450</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.paulhibbard.org</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Public Lecture</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>D2EDFE79-49F9-42B8-9EC5-30F7C263BB6A</gtr:id><gtr:impact>A public lecture in which I presented my research findings. This was followed by question and answer session. We also gave out free virtual reality headsets to encourage people to think about the importance of their 3D vision, and the importance of this emerging technology.</gtr:impact><gtr:outcomeId>56dd94ac9402a5.74855174</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.essex.ac.uk/events/event.aspx?e_id=9769</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Colchester Ultrafast Broadband launch</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>DED31D3A-6699-48A9-97B7-2102590AEC55</gtr:id><gtr:impact>The event was to launch new initiatives around ultrafast broadband in Colchester. The activities were to showcase digital business and research in the local area. We used this to develop links with a number of local businesses, and are currently actively pursuing potential collaborations, with the Univeristy of Essex Research and Entrerprise Office. We have also been invited back for additional meetings, with a view to helping to direct local funding in this area.</gtr:impact><gtr:outcomeId>58c7b7f81e0704.21316983</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.colchesterultraready.com</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Workshop to establish links between researchers working in biological, clinical and engineering aspects of binocular vision</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>9E65164B-F681-464F-8981-30DCF91C5E88</gtr:id><gtr:impact>30 researchers (undergraduate, postgraduate, staff) from universities across the UK came together in a 2 day workshop to discuss the challenges of working in binocular vision from the perspectives of biology, engineering and clinical practice. We discussed practical ways in which individuals could work together more effectively across traditional discipline barriers.</gtr:impact><gtr:outcomeId>56dd98341425c1.34018962</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>71714</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Research Project Grant</gtr:description><gtr:end>2020-09-02</gtr:end><gtr:fundingOrg>The Leverhulme Trust</gtr:fundingOrg><gtr:fundingRef>RPG-2016-361</gtr:fundingRef><gtr:id>8F24EDA4-D315-40C6-99EA-877D0272B415</gtr:id><gtr:outcomeId>58c7b584028862.56767047</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The research findings have been presented in an public lecture and discussion. The aim of this was to educate, inform and engage with the general public. We also have an ongoing programme of engaging with local business around 3D creative industries, and are currently exploring future collaboration around this. The work on complex 3D scenes, and the development of a 3D database, and methods to present these on 3D displays and in virtual reality, has opened a novel research area, at a time when consumer VR is rapidly developing. This has allowed us to secure funding from Facebook Oculus, one of the leading manufacturers of consumer virtual reality, in order to work directly with them in understanding how perception science can be used to optimise the VR experience. This work has receive TV coverage. 

I have developed a website whose primary aim is to explain the research area in an accessible and engaging way.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>F078B65E-DEA5-4D64-BB0F-35FEB6860AF7</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56dd9a91556b55.70342217</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Education,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>(i) Image database. We have devised an improved methodology in which, in additional to capturing 3D objects, we have produced software for creating scenes. The advantage of this approach is that it removes many of the difficulties faced in calibration, and allows great flexibility in generating images. For example, it allows for the parametric variation of viewing parameters (e.g vergence) while viewing the same scene - something that would have been beyond our original approach. We have a growing database and the results have been used in our own research. The final version was presented at the Vision Sciences Society meeting in May 2016.

2. Independent Component Analysis. The first part of this has been completed and published in Journal of Vision. An extension was presented at the Vision Sciences Society in May 2014 (to be published in Journal of Vision), and has now been completed for submission.
 
3. In addition to performing Independent Component Analysis, we have also applied Independent Subspace Analysis. This works towards our goal of understanding binocular encoding beyond the initial stage of individual filters, with a view to establish the necessary invariances (e.g. to luminance phase) to create reliable disparity sensitivity. This has been published in PLOS ONE. 

4. We have published additional work, modelling the responses of cells higher in the processing pathway (compared with the modelling included in the papers above) in Vision Research.

5. We have developed a computational model which extracts relevant depth structure from binocular information. This neural model of binocular depth perception directly links the outputs of early stages of processing in the brain to the perception of depth. In doing so, it provides a theoretically-driven (rather than data fitting) approach to understanding the activity of cells higher in the visual brain. This work was presented at the European Conference on Visual Perception in late 2016; the manuscript is in preparation. We have also performed novel experiments with human observers to test this model.

6. We have developed a model of the early stages of binocular vision, based on the idea that information should be encoded as efficiently as possible. This model provides a theoretical explanation of the properties of neurons in the early visual cortex. We have performed novel experiments with human observers to test this model.

7. Analysis of complex 3D models with ground truth. Having developed a library of 3D scans of natural objects, we have performed statistical analysis of the 3D structure of these objects. This allows us to understand the complex 3D structure that needs to be encoded by the visual system. In the same way that the simple distributions of depth and binocular disparity have allowed us to understand the early encoding of binocular information, this analysis helps us to understand the way in which more complex shape properties are also encoded at higher stages in the brain.</gtr:description><gtr:exploitationPathways>We have made code and images available, and these have the potential for impact in computer vision and the creative industries.</gtr:exploitationPathways><gtr:id>1425183C-6939-468E-8890-354E4B2A3523</gtr:id><gtr:outcomeId>54647730321700.35827407</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Source code and data of the analysis performed in the following publication:

Hunter, D.W. &amp;amp; Hibbard, P.B. (2015) Distribution of independent components of binocular natural images, Journal of Vision, 15(13):6, 1-31, doi:10.1167/15.13.6</gtr:description><gtr:id>8F271DFE-A14C-466A-A989-85CF200878FF</gtr:id><gtr:impact>Too early to say - only completed recently.</gtr:impact><gtr:outcomeId>56dd95eb169ea0.14482593</gtr:outcomeId><gtr:title>Source code for Binocular Independent Component Analsyis</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>https://github.com/DavidWilliamHunter/Bivis</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>DEC7BBC6-CBF0-43DA-AAFD-E4B241E0FBE1</gtr:id><gtr:title>Is there a cost to binocular vision: A link between the statistics of binocular images and the effect of eccentricity on visual performance</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d21ba6e99510245211038a19343fb5b8"><gtr:id>d21ba6e99510245211038a19343fb5b8</gtr:id><gtr:otherNames>Hunter David W.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>58c7b07d4c9f95.83710891</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DCEA2633-7597-4061-BD21-B84EED851E5E</gtr:id><gtr:title>Learning Disparity tuned complex-cell like models using Independent Subspace Analysis</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ba93db8c481a50dc8ea8893dc6040754"><gtr:id>ba93db8c481a50dc8ea8893dc6040754</gtr:id><gtr:otherNames>Hunter David</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>56dd7932360d83.17370071</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B209BFF9-EA90-4A71-A0C4-3AA599F55A7B</gtr:id><gtr:title>The influence of viewing distance, depth range and inter-camera distance on depth perception and preference judgments in complex natural scenes</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4914245a8b63f2df9a8c34b1e95b2280"><gtr:id>4914245a8b63f2df9a8c34b1e95b2280</gtr:id><gtr:otherNames>Hornsey R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7b4d1645ac3.52549303</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E30BC0E-B31F-404B-AEC4-26F42C8C9C74</gtr:id><gtr:title>The effect of image position on the Independent Components of natural binocular images.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c59b9276691b7263650c0ad14abc81e"><gtr:id>8c59b9276691b7263650c0ad14abc81e</gtr:id><gtr:otherNames>Hunter DW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn><gtr:outcomeId>5a917dc527e701.55255163</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>11BCA170-2F4F-44AB-BE48-B6D54594738F</gtr:id><gtr:title>Towards a biologically plausible model of binocular disparity encoding</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/781493e77a1d94b1fd6b7e9454cd5982"><gtr:id>781493e77a1d94b1fd6b7e9454cd5982</gtr:id><gtr:otherNames>Hunter D. W.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>56dd7931f172a6.17171870</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D9378CD3-6C94-4CBA-856C-8D383723D118</gtr:id><gtr:title>Masking Effects in Cyclopean Surface Perception</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6614cf1b81d78c6df678272d5d207077"><gtr:id>6614cf1b81d78c6df678272d5d207077</gtr:id><gtr:otherNames>Goutcher R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7b4d1c10719.00288879</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FDA5D48A-EE54-4256-B0B5-C49FA9EDAA56</gtr:id><gtr:title>Ideal Binocular Disparity Detectors Learned Using Independent Subspace Analysis on Binocular Natural Image Pairs.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c59b9276691b7263650c0ad14abc81e"><gtr:id>8c59b9276691b7263650c0ad14abc81e</gtr:id><gtr:otherNames>Hunter DW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>585d710a2e1ab7.48811497</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>212DF312-A6D0-40FA-A59E-A0E5EDA2F45E</gtr:id><gtr:title>Ordinal judgments of depth in monocularly- and stereoscopically-viewed photographs of complex natural scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4914245a8b63f2df9a8c34b1e95b2280"><gtr:id>4914245a8b63f2df9a8c34b1e95b2280</gtr:id><gtr:otherNames>Hornsey R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c7aff7511387.69822045</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D99AE26-888B-4EF4-AF26-A1D685A21022</gtr:id><gtr:title>The threshold of binocularity: natural image statistics explain the reduction of visual acuity in peripheral vision.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b5dad253f425e5dbacf146568c67b0d"><gtr:id>8b5dad253f425e5dbacf146568c67b0d</gtr:id><gtr:otherNames>Hunter D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a917d761335d7.70143244</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9EED76D1-3A67-4816-8B45-03C4F9A12F6D</gtr:id><gtr:title>When does an encoding become a percept?</gtr:title><gtr:parentPublicationTitle>I-PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/781493e77a1d94b1fd6b7e9454cd5982"><gtr:id>781493e77a1d94b1fd6b7e9454cd5982</gtr:id><gtr:otherNames>Hunter D. W.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2041-6695</gtr:issn><gtr:outcomeId>58c7b04b432882.18872812</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D6A3420-4158-4B7C-9CFA-E31F5DC8B6A9</gtr:id><gtr:title>A 3D database of everyday objects for vision research</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a493aa4b768f8d485f798254b9d7020"><gtr:id>9a493aa4b768f8d485f798254b9d7020</gtr:id><gtr:otherNames>Hibbard P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7b4d19bd3b5.12155063</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0316F939-8B35-45C3-9C8C-F202442EB27C</gtr:id><gtr:title>Distribution of independent components of binocular natural images.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8c59b9276691b7263650c0ad14abc81e"><gtr:id>8c59b9276691b7263650c0ad14abc81e</gtr:id><gtr:otherNames>Hunter DW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>56dd77969157a3.06033643</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E7011D2-C55B-4318-ACDA-917963F595F8</gtr:id><gtr:title>Tuned Inhibitory Responses in Binocular Natural Images</gtr:title><gtr:parentPublicationTitle>I-PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23067e9887d1f4426fe30a54d9f0cc24"><gtr:id>23067e9887d1f4426fe30a54d9f0cc24</gtr:id><gtr:otherNames>Goutcher R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>2041-6695</gtr:issn><gtr:outcomeId>56dd793218ffd4.30113545</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A92844A-ABEE-4336-806E-4C405C4B6DE3</gtr:id><gtr:title>Optimal encoding of binocular images by cortical neurons</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/81cbf23f57cd0c36be330592644d3211"><gtr:id>81cbf23f57cd0c36be330592644d3211</gtr:id><gtr:otherNames>Hibbard P. B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>56dd7931d560a4.01144701</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F8EC632E-7938-40B7-B4A0-F01721C117DE</gtr:id><gtr:title>Encoding and estimation of first- and second-order binocular disparity in natural images.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a663d40f64953891ab494a253453d12c"><gtr:id>a663d40f64953891ab494a253453d12c</gtr:id><gtr:otherNames>Hibbard PB</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>56dd7796ad4934.14131967</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98E97E58-C4B6-4EE7-931E-B677145F61BF</gtr:id><gtr:title>The independent components of binocular images reflect the spatial distribution of horizontal and vertical disparities</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b5dad253f425e5dbacf146568c67b0d"><gtr:id>8b5dad253f425e5dbacf146568c67b0d</gtr:id><gtr:otherNames>Hunter D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c7b4d1e92072.49808388</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AA73CE5B-DE5D-4C22-A387-BC0751CAECCF</gtr:id><gtr:title>The Contribution of Binocular and Motion Cues to Depth Quality in Complex Naturalistic Scenes</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b93763ccb0faa7e0f5cebe041e7f7cc5"><gtr:id>b93763ccb0faa7e0f5cebe041e7f7cc5</gtr:id><gtr:otherNames>Hibbard Paul B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>5a917e4dd65371.69947742</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7200901B-1901-4639-99AD-56A2B1966510</gtr:id><gtr:title>The Extraction of Disparity- Defined Structure Using Cyclopean Filtering</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b93763ccb0faa7e0f5cebe041e7f7cc5"><gtr:id>b93763ccb0faa7e0f5cebe041e7f7cc5</gtr:id><gtr:otherNames>Hibbard Paul B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>58c7b04b060b08.51653697</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/K018973/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E1AC33C6-9927-41AC-B23B-2EED8F593588</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Experimental Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>