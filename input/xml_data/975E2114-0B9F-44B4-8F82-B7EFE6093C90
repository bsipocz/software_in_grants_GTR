<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A1FA5E2C-705B-4A74-9596-1A1CAC2A8375"><gtr:id>A1FA5E2C-705B-4A74-9596-1A1CAC2A8375</gtr:id><gtr:name>IBM</gtr:name><gtr:address><gtr:line1>IBM Corporation</gtr:line1><gtr:line2>1 New Orchard Road</gtr:line2><gtr:line4>Armonk</gtr:line4><gtr:line5>NY 10504-1722</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A1FA5E2C-705B-4A74-9596-1A1CAC2A8375"><gtr:id>A1FA5E2C-705B-4A74-9596-1A1CAC2A8375</gtr:id><gtr:name>IBM</gtr:name><gtr:address><gtr:line1>IBM Corporation</gtr:line1><gtr:line2>1 New Orchard Road</gtr:line2><gtr:line4>Armonk</gtr:line4><gtr:line5>NY 10504-1722</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F905DDE4-25A7-459F-BBFB-01B95F49AE69"><gtr:id>F905DDE4-25A7-459F-BBFB-01B95F49AE69</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>HARPER</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE062954%2F1"><gtr:id>975E2114-0B9F-44B4-8F82-B7EFE6093C90</gtr:id><gtr:title>SASWAT: Structured Accessibility Stream for Web 2.0 Access Technologies</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E062954/1</gtr:grantReference><gtr:abstractText>We are witnessing a profound change in the interaction model of the World Wide Web (Web). Documents, once created from a single source and delivering static client-side content, have now evolved into composite documents created from multiple third party sources delivering dynamically changing information streams. There are few interaction problems when delivering these parallel streams visually. The real problems arise due to the underlying incoherent nature of this 'new' Web model and the composite documents it creates. Changes in context and multiple dynamic updates all compete for the user's attention, producing an incoherent cacophony if the delivery is serial and in audio. Consequently, naive one--shot sensory translation can no longer support the user.This shift in the way the Web works comes with a corresponding increase in the cognitive load required for audio interaction. Without a full understanding of this evolving interaction model, along with its extent and context, the Web will rapidly become unable to support the interaction of visually disabled people.Our objective is to investigate, design, and build a homogeneous mapping framework to support the relating of competing visual streams into a single coherent and mediated accessibility stream such that when automatically applied to a Web document a mapping from parallel visual to serial audio can be achieved. Indeed, because serial mappings are cognitively simpler to understand we would also expect to see side-benefits in cognitive impairment, ageing, and the mobile Web (Whose users share a number of cognitive similarities with visually disabled users -- RIAM EP/E002218/1).To achieve this objective we propose to undertake fundamental research in the areas of: (a) the cognition and perception of dynamic Web based information; (b) the nature of the new Web interaction / infrastructure model as it evolves; and (c) new Web technologies when applied to visually disabled and sighted users. Thus, SASWAT is multidisciplinary with an industrial route to exploitation and has five major aims: 1) Carry out a fundamental investigation of the visual experiences of sighted individuals interacting with competing dynamic information streams in order to better understand the nature of their interaction; 2) Develop a profound understanding of the nature and evolution of the underlying Web infrastructure as it moves from a traditional stateless paradigm to one focused on composite / compound documents and `push' information streams;3) Build a model of Web interaction, based on this investigation, and a mapping of perceptual and cognitive interactivity from sighted to visually disabled users;4) Design and develop an experimental framework to mediate between the competing demands of compound Web pages and multiple information streams;5) Use our corpus of knowledge and experimental tools to perform a systematic and replicable evaluation of the utility of our approaches.</gtr:abstractText><gtr:fund><gtr:end>2010-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>351836</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>IBM</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>IBM T. J. Watson Research Center, Yorktown Heights</gtr:department><gtr:description>IBM Watson Research Centre</gtr:description><gtr:id>06E81D42-0D18-4C63-8B85-44B91478C308</gtr:id><gtr:outcomeId>b9c83020b9c83034-1</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>IBM</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>IBM T. J. Watson Research Center, Yorktown Heights</gtr:department><gtr:description>IBM TJ Watson</gtr:description><gtr:id>19063B50-80AF-4D13-A146-783404F616BC</gtr:id><gtr:outcomeId>b973e402b973e420-1</gtr:outcomeId><gtr:piContribution>Our collaborations with IBM were, in some ways, muted because our main collaborator became a Professor and moved to Dundee. In addition, after this move the group she led at 'Watson' became a casualty of the economic downturn.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Industrial Training Courses</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>2E75890E-5F79-4C5A-B142-D8B28079EDA6</gtr:id><gtr:impact>As part of the SASWAT project, and to disseminate its findings directly, we ran one tutorial / training course: Simon Harper and Yeliz Yesilada. Practical Accessibility: A Web Accessibility Primer. Invited Tutorial - 9th International Conference on Web Engineering. Further, we have provided hands on SASWAT demonstrator evaluations and training to: Macclesfield Eye Society and Walthew House (an independent local charity supporting people in Stockport who are blind, visually impaired, Deaf or hard of hearing or who have dual sensory loss).

Dissemination to the public such that a disadvantaged groups needs are better serviced and peoples needs are listened to.</gtr:impact><gtr:outcomeId>r-3887288685.7830812b8c27d6</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2009</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>International Standards &amp; Public Engagement</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E76349C2-1614-4053-AB4C-2785BCBD7935</gtr:id><gtr:impact>The SASWAT project (and its investigators) is a full-member of the World Wide Web Consortium (W3C), the premier body for the formulation of Web Standards. In this way SASWAT directly influenced the evolution of specific areas of the Web, particularly, the work on accessibility and user agent guidelines. Indeed, the Principle Investigator is an active member in the User Agent Working Group. This contribution is resulting in the creation of a new primary standards document co-authored by the Principle Investigator and directly influenced by SASWAT. To enhance engagement we have instigated the creation of Web space, data repositories, and ePrint facilities where all SASWAT experimental raw data, prototypes, demonstrators and other relevant material are made freely available. The lab and SASWAT Weblogs net an average of 30 unique visitors per day. Both the PI and the RA's have also attended the University's Media Skills training course (recommended by the EPSRC). This has enabled us to make a number of press releases via the central University machinery, netting 46 news stories in the international, national, and local press for SASWAT (EP/E062954/1), including the Manchester Evening News, PhysOrg, and Access IT magazine as well as appearances by 'Jay' on BBC 1 Regional Television News and 'Brown' talking about the success of the project on RNIB Insight Radio. Further, SASWAT has enabled the creation of ongoing links with leading charities and community groups of all ages across the North of England. These currently include: (1) Henshaws Society for Blind People (HSBP); (2) Deaf Blind Services (Walthew House); (3) Macclesfield Eye Society; (4) Access SUMMIT; (5) the UoM Disability Support Office; (6) Age Concern (Trafford); and the (7) Christopher Grange Rehabilitation Centre.

46 news stories in the international, national, and local press for SASWAT (EP/E062954/1), including the Manchester Evening News, PhysOrg, and Access IT magazine as well as appearances by 'Jay' on BBC 1 Regional Television News and 'Brown' talking about t</gtr:impact><gtr:outcomeId>r-6900643396.0987652b8c38ac</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2010</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>52000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>ACup: Accessibility Catch-Up - Techniques for Disseminating Accessibility Research</gtr:description><gtr:end>2015-12-02</gtr:end><gtr:fundingOrg>Google</gtr:fundingOrg><gtr:id>F89710A8-6F7D-41DF-B100-459C78560888</gtr:id><gtr:outcomeId>5ec8b92a5ec8b93e</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2010-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>37000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>MSO: Multiple Screen Orchestration</gtr:description><gtr:end>2012-07-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/H500154/1?R114060</gtr:fundingRef><gtr:id>1FBF2FC8-1A02-4401-86B9-224325E84FFB</gtr:id><gtr:outcomeId>5ed9d2be5ed9d2d2</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>110000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>SAG (Aloo): Synchronised Attention Grant</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/K503782/1</gtr:fundingRef><gtr:id>40BB0A30-26F3-41BD-852C-30FBF5214BD6</gtr:id><gtr:outcomeId>545a49ab412349.22064092</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>This knowledge enables a better translation form visual to auditory input and removes the threat of the expected auditory cacophony from occurring at anywhere near the levels expected. The work has now received a follow on funding grant from Google who wish to see this work brought to a practical resolution in their 'Chrome' Web Browser. By using UoM technology in partnership with Google technology we hope to to bring real practical benefits to visually disabled Web surfers while contributing back to the research community which initially sponsored this fundamental work.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>4BD50F2E-BD66-48F8-85C4-5FC2471DCA90</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545a4671283781.97860656</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Chrome Auditory Extensions.
MobileOK Extensions</gtr:description><gtr:grantRef>EP/E062954/1</gtr:grantRef><gtr:id>3B291E70-68EC-47D8-88C1-87AD418B17E3</gtr:id><gtr:impact>All Software is released on GNU licence.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>545a4d94602f77.51413162</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>All Software</gtr:title><gtr:yearProtectionGranted>2009</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>This research work has transformed our understanding of the interaction requirements of visually disabled users. It was originally thought that sighted users would attend to different visual updates in parallel, however, this is not the case. Sighted users prefer to focus on one user initiated task and complete it before moving to the next. This knowledge enables a better translation form visual to auditory input and removes the threat of the expected auditory cacophony from occurring at anywhere near the levels expected. The work has now received a follow on funding grant from Google who wish to see this work brought to a practical resolution in their 'Chrome' Web Browser. By using UoM technology in partnership with Google technology we hope to to bring real practical benefits to visually disabled Web surfers while contributing back to the research community which initially sponsored this fundamental work.</gtr:description><gtr:exploitationPathways>We already have uses in the non-academic space via our BBC collaborations. Our original question was 'how can we orchestrate to best effect competing updates occurring over multiple viewports on a single device?' Our new collaborative project wishes to recast this question by asking 'how can we orchestrate to best effect competing updates occurring over a single viewport on multiple devices?' Simply we will apply our models, firstly into the DigitalTV space and then into the dual device presentation space, such that multiple independent devices take part in a single orchestrated presentation. In this way, presentations can be tailored to the needs of the individual on their personal device while still communally interacting on the DigitalTV. The work was primarily focused on blind and mobile users and translated multiple parallel visual updates to a structured serial auditory presentation. In turn, the BBC have become interested in how users experience information presented on dual screens (e.g. a TV and mobile phone or tablet computer) simultaneously - and in particular how synchronised content forms an overall experience across the devices - with a long term aim of developing perceptual and cognitive models of dual screen experience to aid development of future technology. Both organisations see a synergy between these pieces of work and wish to answer the question 'How would we orchestrate to best effect multiple information streams over individual devices, which are all participating in a single unified presentation?' In this case we see a route to knowledge transfer from niche into mainstream technology, while the BBC sees a business case for including our knowledge as a base for their dual screen technology development.</gtr:exploitationPathways><gtr:id>1ABE06CA-A101-4F6E-A220-02CD2FD3642F</gtr:id><gtr:outcomeId>r-8808559215.449318777ee502</gtr:outcomeId><gtr:sectors><gtr:sector>Communities and Social Services/Policy,Digital/Communication/Information Technologies (including Software),Transport</gtr:sector></gtr:sectors><gtr:url>http://wel.cs.manchester.ac.uk/research/saswat/project-status/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>World Wide Web Consortium, Research and Development Working Group</gtr:description><gtr:geographicReach>Multiple continents/international</gtr:geographicReach><gtr:id>0E4C60D7-036F-4DD5-B0F9-46F0B1D51729</gtr:id><gtr:impact>W3C standards body work in globally influencing the Web Accessibility Agenda.</gtr:impact><gtr:outcomeId>545a4bf67131a1.63873004</gtr:outcomeId><gtr:type>Influenced training of practitioners or researchers</gtr:type></gtr:policyInfluenceOutput></gtr:policyInfluenceOutputs><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>ACTF is a framework that serves as an extensible infrastructure upon which developers can build a variety of utilities that help to evaluate and enhance the accessibility of applications and content for people with disabilities. A collection of example utilities will also be provided which were created on top of the framework such as compliance validation tools, assistive technology simulation applications, usability visualization tools, unit-testing utilities, and alternative accessible interfaces for applications. The ACTF componentry and the utilities will be integrated into a single tooling environment on top of the Eclipse framework. The framework components will function cooperatively with each other and with other Eclipse projects to provide a comprehensive development environment for creating accessible applications and content.</gtr:description><gtr:id>8D897801-7774-4230-A6DE-8C3E2E723BF7</gtr:id><gtr:impact>Now used by the Japanese government; among many others.</gtr:impact><gtr:outcomeId>545a4d19bd5f43.36888704</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Accessibility Tools Framework (Eclipse Technology Platform)</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>http://www.eclipse.org/actf/team.php</gtr:url><gtr:yearFirstProvided>2009</gtr:yearFirstProvided></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>78D9E2B4-C2CF-4D36-A4B6-48D3C9C00809</gtr:id><gtr:title>Tailored presentation of dynamic web content for audio browsers</gtr:title><gtr:parentPublicationTitle>International Journal of Human-Computer Studies</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7ff5c71c07b14727cc3d86a39f46a326"><gtr:id>7ff5c71c07b14727cc3d86a39f46a326</gtr:id><gtr:otherNames>Brown A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53cff6ff672ca8c9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9E6FFA53-5783-490B-A4EB-A37BD13B0299</gtr:id><gtr:title>A 'visual-centred' mapping approach for improving access to Web 2.0 for people with visual impairments.</gtr:title><gtr:parentPublicationTitle>Disability and rehabilitation. Assistive technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ea9fdb02218c20b4ab8d55fd149baadf"><gtr:id>ea9fdb02218c20b4ab8d55fd149baadf</gtr:id><gtr:otherNames>Jay C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1748-3107</gtr:issn><gtr:outcomeId>doi_53d085085ce4168c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6DD9570A-CC8E-414F-849E-E2CC6D6924A4</gtr:id><gtr:title>Web Accessibility: A Foundation for Research</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/41b550405d7870c54d5e83eaa0d4c91a"><gtr:id>41b550405d7870c54d5e83eaa0d4c91a</gtr:id><gtr:otherNames>Harper, Simon; Yesilada, Yeliz</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-1-84800-049-0</gtr:isbn><gtr:outcomeId>i_52954151253c07de38</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ADFE3129-E78D-4985-92E0-AB82BD5E19EF</gtr:id><gtr:title>Using qualitative eye-tracking data to inform audio presentation of dynamic Web content</gtr:title><gtr:parentPublicationTitle>New Review of Hypermedia and Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7ff5c71c07b14727cc3d86a39f46a326"><gtr:id>7ff5c71c07b14727cc3d86a39f46a326</gtr:id><gtr:otherNames>Brown A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d03d03de7ea959</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98E313D7-8FE9-4A63-B54F-3889A38F0283</gtr:id><gtr:title>Web accessibility guidelines</gtr:title><gtr:parentPublicationTitle>World Wide Web</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1af133ab2b4fc03b92374262655b27b0"><gtr:id>1af133ab2b4fc03b92374262655b27b0</gtr:id><gtr:otherNames>Harper S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55f95b95b80a6367</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6351D46C-5954-4CAA-AB4F-694391AD15EA</gtr:id><gtr:title>Widget Identification: A High-Level Approach to Accessibility</gtr:title><gtr:parentPublicationTitle>World Wide Web</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/11e591749d9e03d48c9eff836f040718"><gtr:id>11e591749d9e03d48c9eff836f040718</gtr:id><gtr:otherNames>Chen A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55fa9fa9f0e4aa14</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D6D625A-B529-4669-A4F4-5AEDDA772697</gtr:id><gtr:title>Learning from sighted user behaviour to present dynamic Web content in audio</gtr:title><gtr:parentPublicationTitle>New Review of Hypermedia and Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c323851e9db921152edd0a5beecd1aea"><gtr:id>c323851e9db921152edd0a5beecd1aea</gtr:id><gtr:otherNames>Brown A (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_88102045171391f63c</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E062954/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>