<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name><gtr:address><gtr:line1>Polaris House</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:line4>Swindon</gtr:line4><gtr:postCode>SN2 1FL</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/98842265-278F-4528-A476-8AD3E8D988FF"><gtr:id>98842265-278F-4528-A476-8AD3E8D988FF</gtr:id><gtr:firstName>Tillman</gtr:firstName><gtr:otherNames>Erik</gtr:otherNames><gtr:surname>Weyde</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FR004005%2F1"><gtr:id>54A05947-D384-4B81-B865-ED648A3B0212</gtr:id><gtr:title>Dig that lick: Analysing large-scale data for melodic patterns in jazz performances</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/R004005/1</gtr:grantReference><gtr:abstractText>The recorded legacy of jazz spans a century and provides a vast corpus of data documenting
its development. Recent advances in digital signal processing and data analysis technologies
enable automatic recognition of musical structures and their linkage through metadata to
historical and social context. Automatic metadata extraction and aggregation give unprecedented
access to large collections, fostering new interdisciplinary research opportunities.

This project aims to develop innovative technological and music-analytical methods to gain
fresh insight into jazz history by bringing together renowned scholars and results from several
high-profile projects. Musicologists and computer scientists will together create a deeper and
more comprehensive understanding of jazz in its social and cultural context. We exemplify our
methods via a full cycle of analysis of melodic patterns, or &amp;quot;licks&amp;quot;, from audio recordings to an
aesthetically contextualised and historically situated understanding.</gtr:abstractText><gtr:potentialImpactText>The study of jazz requires insights from, and feeds knowledge back into, African American Studies,
Anthropology, Art History, Literary Studies, Music, Philosophy, Political Science, and Sociology.
A thorough analysis of a century's worth of jazz recordings, and the practices the music entails,
is now possible thanks to recent advances in the computational analysis of audio content, or
Music Information Retrieval (MIR), and to progress in processing large datasets and information
management with Semantic Web technologies. The former enables the automatic description
of audio recordings in terms of high-level or structural musical aspects, and the latter allows
such analyses to be linked to discographic metadata, distributed over multiple sites, describing
performers and composers, listeners, performance venues, and production and consumption
factors, and general historic, cultural and geographic information from external resources. These
technologies can now facilitate access to large collections by researchers from the many disciplines
interested in the evolution of musical expression.

The Dig that Lick project will: enhance infrastructures for semantic audio analyses
of large collections; facilitate access to large collections of audio and associated metadata via
interfaces for content selection, semantic analysis, and aggregation of results that humanities
researchers can easily use; develop this infrastructure to analyse melodic patterns across large
corpora of jazz audio; and relate the results to metadata and background knowledge in order
to trace and interpret musical influence across time and space as well as cultures and societies.
We will develop tool sets and resources that allow researchers to perform studies over wide
time-spans and geographic locations, for example to trace the evolution or spread of certain
musical phenomena. This will enable cross-historical or comparative geographical music research
with direct reference to data and metadata on music performance and creation, an approach
rarely attempted in the musicology of jazz, or of non-notated music.

Our target audiences include: academic communities in jazz studies, whether in music, cultural
studies, social sciences, or business management; MIR practitioners in engineering or library and
information sciences as they relate to music; the J-DISC user community; that is, researchers
and educators who require structured, comprehensive search capabilities in investigating the
cultural background and social networks of jazz performance, accessed via the recorded legacy
of jazz; the Jazzomat community of musicologists and engineers interested in musical and cognitive
questions derived from jazz solo analysis; jazz musicians interested in a topic they wish to document
or explore for professional reasons; and jazz fans or students wishing to know more about an artist they follow.

We will engage with our target audiences via academic publications and presentations, spe-
cial events, software and data releases, and communication to the public via non-academic
channels, including the Dig that Lick web site, blogs, outreach events, social media and press
releases, as appropriate.

Our audiences will benefit in the following ways: jazz researchers - our tools and resources
will provide a powerful new paradigm for evidence-based research; jazz musicians - an unusually
analytical community, we expect many of them to be interested in the software developed by
this project to investigate and reflect on their own performances, helping to develop a better
understanding of the process of transmission and assimilation of patterns that are sometimes
conscious, but often opaque to the artists themselves; jazz aficionados - our software tools and
resources will be available through the web platform operated by the Center for Jazz Studies,
giving deeper understanding of their favorite artists.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2017-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>160073</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">ES/R004005/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>55773495-BB0B-43EB-B99D-D5C15272A52F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musical Performance</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>54D637A1-400F-4B2B-8FB4-48DF799C853C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Popular Music</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>