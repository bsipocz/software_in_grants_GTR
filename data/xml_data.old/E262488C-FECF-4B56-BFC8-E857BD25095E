<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9DE31489-7E3B-4E34-BA75-DD367FF833FB"><gtr:id>9DE31489-7E3B-4E34-BA75-DD367FF833FB</gtr:id><gtr:name>The Foundry Visionmongers Ltd</gtr:name><gtr:address><gtr:line1>The Foundry</gtr:line1><gtr:line2>5 Golden Square</gtr:line2><gtr:postCode>W1F 9HT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/456959E8-6263-4AD9-913A-4D8FD0F9985E"><gtr:id>456959E8-6263-4AD9-913A-4D8FD0F9985E</gtr:id><gtr:name>Double Negative</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9DE31489-7E3B-4E34-BA75-DD367FF833FB"><gtr:id>9DE31489-7E3B-4E34-BA75-DD367FF833FB</gtr:id><gtr:name>The Foundry Visionmongers Ltd</gtr:name><gtr:address><gtr:line1>The Foundry</gtr:line1><gtr:line2>5 Golden Square</gtr:line2><gtr:postCode>W1F 9HT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/456959E8-6263-4AD9-913A-4D8FD0F9985E"><gtr:id>456959E8-6263-4AD9-913A-4D8FD0F9985E</gtr:id><gtr:name>Double Negative</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0954F2A0-CC4B-4798-9401-D96389885987"><gtr:id>0954F2A0-CC4B-4798-9401-D96389885987</gtr:id><gtr:name>Double Negative Ltd</gtr:name><gtr:address><gtr:line1>Double Negative Ltd</gtr:line1><gtr:line2>77 Shaftesbury Avenue</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>W1D 5DU</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6A9E5617-D8D4-4AF7-B9A3-68360A2C0BDA"><gtr:id>6A9E5617-D8D4-4AF7-B9A3-68360A2C0BDA</gtr:id><gtr:firstName>Jean-Yves</gtr:firstName><gtr:surname>Guillemaut</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM021793%2F1"><gtr:id>E262488C-FECF-4B56-BFC8-E857BD25095E</gtr:id><gtr:title>Shape and Reflectance Acquisition of Complex Dynamic Scenes</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M021793/1</gtr:grantReference><gtr:abstractText>Scene modelling is central to many applications in our society including quality control in manufacturing, robotics, medical imaging, visual effects production, cultural heritage and computer games. It requires accurate estimation of the scene's shape (its 3D surface geometry) and reflectance (how its surface reflects light). However, there is currently no method capable of capturing the shape and reflectance of dynamic scenes with complex surface reflectance (e.g. glossy surfaces). This lack of generic methods is problematic as it limits the applicability of existing techniques to scene categories which are not representative of the complexity of natural scenes and materials. This project will introduce a general framework to enable the capture of shape and reflectance of complex dynamic scenes thereby addressing an important gap in the field.

Current image or video-based shape estimation techniques rely on the assumption that the scene's surface reflectance is diffuse (it reflects light uniformly in all directions) or assume it is known a priori thus limiting the applicatibility to simple scenes. Reflectance estimation requires estimation of a 6-dimensional function (the BRDF) which describes how light is reflected at each surface point as a function of incident light direction and viewpoint direction. Due to high dimensionality, reflectance estimation remains limited to static scenes or requires use of expensive specialist equipment. At present, there is no method capable of accurately capturing both shape and reflectance of general dynamic scenes, yet scenes with complex unknown reflectance properties are omnipresent in our daily lives.

The proposed research will address this gap by introducing a novel framework which enables estimation of shape and reflectance for arbitrary dynamic scenes. The approach is based on two key scientific advances which tackle the high dimensionality issue of shape and reflectance estimation. First, a general methodology for decoupling shape estimation from reflectance estimation will be proposed; this will allow decomposition of the original high dimensional problem, which is ill-posed, into smaller sub-problems that are tractable. Second, a space-time formulation of reflectance estimation will be introduced; this will utilise dense surface tracking techniques to extend reflectance estimation to the temporal domain and thereby increase the number of observations available to overcome the inherently low number of observations at a single time instant. This will build on the PI's pioneering research in 3D reconstruction of scenes with arbitrary unknown reflectance properties and his expertise in dynamic scene reconstruction, surface tracking/animation and reflectance estimation.

This research represents a radical shift in scene modelling which will result in several major technical contributions: 1) a reflectance independent shape estimation methodology for dynamic scenes, 2) a non-rigid surface tracking method suitable for general scenes with complex and unknown reflectance and 3) a general and scalable reflectance estimation method for dynamic scenes. This will benefit all areas requiring accurate acquisition of shape and reflectance for real-world scenes with complex dynamic shape and reflectance without the requirement for complex and restrictive hardware setups; such scenes are a common occurrence in natural environments, manufacturing (metallic surfaces) and medical imaging (human tissue) but accurate capture of shape is not possible with existing approaches which assume diffuse reflectance and fail dramatically for such cases. This will achieve for the first time accurate modelling of dynamic scenes with arbitrary surface reflectance properties thus opening up novel avenues in scene modelling. The application of this technology will be demonstrated in digital cinema in collaboration with industrial partners to support the development of the next generation of visual effects.</gtr:abstractText><gtr:potentialImpactText>The proposed research will have significant impact in the creative industry, particularly the visual media production community (film, television, game) where accurate shape and reflectance estimation are critical to produce photorealistic visual effects or digital characters as well as other communities where shape and reflectance estimation of scenes with complex surface properties are beneficial (cultural heritage, retail, manufacturing, robotics and medicine).

The creative industry is a rapidly evolving sector with a constant demand for novel technologies to improve the quality of content and reduce production times and costs for competitiveness. Currently the lack of suitable techniques for capture of scenes with complex surface reflectance properties is problematic as it places a tremendous burden on CG artists who need to manually generate photorealistic models. This increases post production times and costs and prevents novel applications due to budget constraints (e.g. limited photorealism in games or online fashion retail). This proposal is focused on achieving the fundamental scientific advances required to enable practical technologies for capture of dynamic shape with arbitrary surface reflectance. This will enable the generation of more sophisticated visual effects by removing current limitations on captured scenes with potential to significantly cut down production times and costs via reduction of the amount of manual interaction needed. For example, using current technology it is not possible to capture the performance of a fashion model or actress wearing a garment with complex reflectance properties such as silk to create a digital double. The project will combine CVSSP's expertise in computer vision and reconstruction with the expertise of world-leading companies in film VFX production (Double Negative) and film post-production tools (The Foundry). This will provide the platform to both deploy the technology for evaluation in production and enable technology transfer within the leading post-production tools (Nuke) for evaluation by production professionals. This will allow early prototyping and trialling of the technology to identify commercialisation opportunities.

The project will have a wide impact beyond the creative industries including the following communities (see further details in Pathways to Impact):
- Cultural heritage: the research will result in photorealistic digitisation techniques allowing preservation of cultural assets via creation of digital copies and supporting cultural enrichment of society by facilitating dissemination and access to cultural assets via the internet. Cultural assets considered will include artefacts held in museums and dynamic content such as traditional dances or performances.
- Online retail: the technology may be used to digitise objects or scenes so that they can be virtually experienced by a customer (e.g. photorealistic online fashion retail).
- Manufacturing: reflectance independent shape acquisition methods will enable contactless measurement on objects with complex reflectance properties (e.g. metallic surfaces) which are common in this industry. This will inspire novel technology for metrology or visual inspection.
- Robotics: the research will lead to advances in sensing technology useful in robot vision to improve the reliability and safety of mobile robot applications by providing richer representations of the environment (including for example information on material properties).
- Medicine: Human tissues have complex reflectance properties which prevent use of traditional image-based modelling techniques. The proposed research has the potential to result in novel non invasive tools for clinical diagnosis or surgical assistance.
Exploitation in these areas will be explored in the context of parallel or subsequent projects using the expertise in CVSSP and the University and links to the industry to engage with the relevant communities.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>99139</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Double Negative</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Foundry/Double Negative</gtr:description><gtr:id>AAC99FF0-F8AF-4A09-879F-30BC577AD527</gtr:id><gtr:impact>The collaboration has resulted in the development of a prototype acquisition system at the University of Surrey and testing on a range of static and dynamic scenes.</gtr:impact><gtr:outcomeId>58c2dca9a26940.05409926-2</gtr:outcomeId><gtr:partnerContribution>In kind contribution. Provided feedback on the research, advised on the suitability for use in creative industries and provided licenses to software.</gtr:partnerContribution><gtr:piContribution>Developed novel approaches for the digitisation of scenes with complex surface reflectance.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>The Foundry Visionmongers Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Foundry/Double Negative</gtr:description><gtr:id>1C12854F-3F20-4548-BCA9-28E9D5999BE1</gtr:id><gtr:impact>The collaboration has resulted in the development of a prototype acquisition system at the University of Surrey and testing on a range of static and dynamic scenes.</gtr:impact><gtr:outcomeId>58c2dca9a26940.05409926-1</gtr:outcomeId><gtr:partnerContribution>In kind contribution. Provided feedback on the research, advised on the suitability for use in creative industries and provided licenses to software.</gtr:partnerContribution><gtr:piContribution>Developed novel approaches for the digitisation of scenes with complex surface reflectance.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>14911</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Society Research Grant</gtr:description><gtr:end>2017-03-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>RG150625</gtr:fundingRef><gtr:id>A5249505-EA78-42A2-8E57-C5FB7F946E0C</gtr:id><gtr:outcomeId>56e0722430e3b1.76451301</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project is pioneering techniques for modelling dynamic scenes with an emphasis on scenes with complex material properties (e.g. glossy materials) which are notoriously difficult to model.

Key findings include:
- A new methodology for modelling dynamic scenes with arbitrary material properties. The approach developed is agnostic to the scene's surface reflectance properties and is therefore applicable to a significantly broader class of scenes than traditional methods. The approach estimates both the shape (depth and surface normals) and the surface material properties (BRDF).
- A novel optimisation approach to enforce the coherence of depth and surface normals during reconstruction. The approach was found to significantly improve modelling accuracy and robustness.
- A flexible acquisition setup requiring only three cameras and three light sources. The design uniquely combines collocation of cameras and light sources with wavelength multiplexing to enable digitisation of dynamic scenes from a minimal number of cameras and light sources.
- An evaluation of the technology on a range of static and dynamic scenes captured during the project. The evaluation considers the modelling accuracy and the application to the production of visual effects such as scene relighting. This resulted in the release of a new dataset to the scientific community.</gtr:description><gtr:exploitationPathways>The research findings open up new capabilities in terms of modelling natural scenes. This is of direct relevance to researchers in computer vision and graphics who are concerned with accurate scene modelling. Additionally, the research may impact other fields concerned with scene analysis and understanding such as robotics or machine learning. The proposed optimisation approach is of high relevance to researchers working on photometric reconstruction methods (shape from shading, photometric stereo) where it may be used as an alternative to error-prone normal integration.

The research findings will also benefit non-academic practitioners with the creative industry being the primary beneficiary. In the creative industry, this technology could be used to digitise assets for applications such as film post-production, design and gaming. The technology may find applications in other sectors where modelling scenes with complex reflectance properties is required such as in medicine, cultural heritage, retail, manufacturing, etc.</gtr:exploitationPathways><gtr:id>6C980EF6-9A92-41A4-9030-EC96F7ED45AB</gtr:id><gtr:outcomeId>56d580e12d79e2.53595301</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset contains a range of static and dynamic scenes, each captured simultaneously by 3 Viper cameras under multi-spectral (RGB) illumination and featuring objects with reflectance properties of varying complexity. The intended use of the data is for geometric 3D reconstruction by Colour Helmholtz Stereopsis. Additional data essential for geometric and photometric calibration procedures as well as the pre-computed calibration files are also included.</gtr:description><gtr:id>2FE06035-35BE-42F2-BB27-8DFB1C03C316</gtr:id><gtr:impact>This is the first dataset dedicated to the reconstruction of scenes using colour Helmholtz stereopsis. The dataset is made publicly available to the research community for non-commercial use. It is anticipated that the public release of this dataset will facilitate the development and evaluation of new algorithms aimed at reconstructing scenes with complex surface reflectance.</gtr:impact><gtr:outcomeId>58c277cece3849.63455473</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Colour Helmholtz Stereopsis Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://epubs.surrey.ac.uk/811986/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>0D8DABD1-4C9D-42A8-BB4B-C0E5DAB6683B</gtr:id><gtr:title>Colour Helmholtz Stereopsis for Reconstruction of Dynamic Scenes with Arbitrary Unknown Reflectance</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce40fd68075517e1aeb0af87c9a59a6c"><gtr:id>ce40fd68075517e1aeb0af87c9a59a6c</gtr:id><gtr:otherNames>Roubtsova N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bfc37595ef53.38302334</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>374C614C-B44F-4FF4-9EB1-153C570E1E1B</gtr:id><gtr:title>Decoupled Shape and Appearance Acquisition for Photometrically Complex Scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/18a5c2b2d94c7b87b2c05a7910f4c09f"><gtr:id>18a5c2b2d94c7b87b2c05a7910f4c09f</gtr:id><gtr:otherNames>Roubtsova N.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bfc4645ddcf1.19681103</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>938E106F-CEC4-4ED5-8487-6722F6AE90DD</gtr:id><gtr:title>Bayesian Helmholtz Stereopsis with Integrability Prior.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce40fd68075517e1aeb0af87c9a59a6c"><gtr:id>ce40fd68075517e1aeb0af87c9a59a6c</gtr:id><gtr:otherNames>Roubtsova N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5a2fe8771c5a22.68247209</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3E32348A-37AF-40F8-89AB-A818F1918C9E</gtr:id><gtr:title>Accurate 3D reconstruction of dynamic scenes with complex reflectance properties</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f2d7c4baedddc8c2ed1d38add18b791a"><gtr:id>f2d7c4baedddc8c2ed1d38add18b791a</gtr:id><gtr:otherNames>Roubtsova Nadejda S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c27677479c43.79228629</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M021793/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>