<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/33952E5D-60ED-4D33-8638-669D9CFF6674"><gtr:id>33952E5D-60ED-4D33-8638-669D9CFF6674</gtr:id><gtr:name>Swiss Federal Institute of Technology (ETH), Zurich</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/33952E5D-60ED-4D33-8638-669D9CFF6674"><gtr:id>33952E5D-60ED-4D33-8638-669D9CFF6674</gtr:id><gtr:name>Swiss Federal Institute of Technology (ETH), Zurich</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5FC2511-03F1-4AC0-9C83-DE14AEA0FFDE"><gtr:id>D5FC2511-03F1-4AC0-9C83-DE14AEA0FFDE</gtr:id><gtr:name>Swiss Federal Inst of Technology (EPFL)</gtr:name><gtr:address><gtr:line1>Station 15</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>Switzerland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/ABF1C165-0FD4-4743-BE25-371D4CF5C22F"><gtr:id>ABF1C165-0FD4-4743-BE25-371D4CF5C22F</gtr:id><gtr:firstName>Dimitris</gtr:firstName><gtr:surname>Agrafiotis</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE065538%2F1"><gtr:id>252AF9B7-A004-4915-B40B-58E3A812EE3E</gtr:id><gtr:title>Multiview Distributed Video Coding For Wireless Multicamera Networks</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E065538/1</gtr:grantReference><gtr:abstractText>Continuing advances in hardware technology have led to the emergence of small, low-power devices with limited onboard processing and wireless communication capabilities typically referred to as sensors. A large number of such spatially distributed signal processing devices, each with finite battery lifetime and thus limited computing and communication capabilities, is known as a wireless sensor network. Sensor networks are ideal for situation awareness applications such as environmental monitoring, healthcare monitoring, battlefield and public space surveillance, etc. Video sensor networks are made up of multiple cameras with varying degrees of spatially and temporally overlapping coverage.To fully exploit the potential of such sensor networks, it is essential to develop energy and bandwidth efficient compression algorithms at the individual sensor nodes. Additionally the information loss that is associated with error-prone, wireless environments calls for robust coding methods. All the current video coding standards rely on the hybrid block-based motion compensation/discrete cosine transform architecture which is primarily driven by the one-to-many model of a single complex encoder and multiple light (cheap) decoders, as is the case in the broadcasting scenario. The presence of limited and generally irreplaceable power sources in wireless sensor networks makes the complexity / compression trade off more relevant than pure rate-distortion performance, the latter being normally the goal of these power hungry motion estimation based hybrid codecs. The error prone wireless environment makes robustness a necessary as opposed to a desired characteristic, something that the error propagation prone, temporal prediction based, hybrid codecs cannot easily claim to possess.These new requirements have recently motivated the video coding community to re-visit the information theory principles of Slepian-Wolf and Wyner-Ziv and examine distributed video coding (DVC) systems. Wyner-Ziv coding shifts the complexity - and consequently the power consumption and cost - from the encoder(s) to the decoder(s). DVC systems posses complexity and robustness properties that make then particularly promising for applications with multiple wireless cameras. This project will develop a novel distributed video coding system for multi-camera and multimedia sensor networks that will offer efficient, error resilient and adaptive performance with low complexity / low power encoders under varying content, user and channel conditions. The project will consider all aspects of distributed video coding and will suggest enhanced algorithms with the aim of optimising the performance particularly for the multi-source case. The coding performance and error resilience of the proposed system will be investigated and optimised for wireless networks with the specific application in mind..</gtr:abstractText><gtr:fund><gtr:end>2010-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-08-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>255729</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>ETH Zurich</gtr:collaboratingOrganisation><gtr:country>Switzerland, Swiss Confederation</gtr:country><gtr:description>Swiss Federal Institute of Technology</gtr:description><gtr:id>7BE15510-AC4E-478F-B77E-0D5C39891065</gtr:id><gtr:outcomeId>b9c86fe0b9c86ff4-1</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Findings from this research have been (and still are) referenced by other researchers</gtr:description><gtr:id>B2F1E939-5673-4860-93A5-3EF4A208FB21</gtr:id><gtr:impactTypes/><gtr:outcomeId>545ce0cc77f029.19921110</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>New compression methods for power limited devices/scenarios</gtr:description><gtr:exploitationPathways>Researchers can built on the contributions made by this grant to further improve and deploy the investigated compression approach</gtr:exploitationPathways><gtr:id>468B252D-4B0D-4B7F-BC78-816C8CF50155</gtr:id><gtr:outcomeId>545ce2b064b0a6.71380334</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>79B15113-5190-4B1E-8F4D-741152B588E6</gtr:id><gtr:title>Performance evaluation of two state of the art DVC codecs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d0580587f0b0f3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7BAA0162-EAD7-4545-9649-478F2462EB2E</gtr:id><gtr:title>A concealment based approach to distributed video coding</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-1-4244-1765-0</gtr:isbn><gtr:outcomeId>doi_53d058058b26ca02</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>241EF5D3-3C09-4552-BE03-59E1FABF8E55</gtr:id><gtr:title>Enhanced spatially interleaved DVC using diversity and selective feedback</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b1afafde87d9e82d1503debeb12a4b4c"><gtr:id>b1afafde87d9e82d1503debeb12a4b4c</gtr:id><gtr:otherNames>N Anantrasirichai</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_478670687113f11fc2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A48456C-C833-4205-8C22-F5711CC16641</gtr:id><gtr:title>Distributed video coding for wireless multi-camera networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99863c5f2246ab50a67b3095024c85b"><gtr:id>c99863c5f2246ab50a67b3095024c85b</gtr:id><gtr:otherNames>Anantrasirichai N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978 0 86341 914 0</gtr:isbn><gtr:outcomeId>doi_53d0310311f1e547</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E065538/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>