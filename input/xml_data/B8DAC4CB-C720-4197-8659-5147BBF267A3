<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/0F1A72EA-5229-4C1F-9AA7-6327A7D09052"><gtr:id>0F1A72EA-5229-4C1F-9AA7-6327A7D09052</gtr:id><gtr:firstName>Harriet</gtr:firstName><gtr:otherNames>Ann</gtr:otherNames><gtr:surname>Allen</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B7063807-C02B-41A6-BA8A-6A3498EC70F4"><gtr:id>B7063807-C02B-41A6-BA8A-6A3498EC70F4</gtr:id><gtr:firstName>Jason</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Braithwaite</gtr:surname><gtr:orcidId>0000-0003-1263-9410</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A536DD0C-05E7-4DCE-899C-25B41BFA1DBB"><gtr:id>A536DD0C-05E7-4DCE-899C-25B41BFA1DBB</gtr:id><gtr:firstName>Glyn</gtr:firstName><gtr:surname>Humphreys</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FG010919%2F1"><gtr:id>B8DAC4CB-C720-4197-8659-5147BBF267A3</gtr:id><gtr:title>Segmentation in 4D visual search</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/G010919/1</gtr:grantReference><gtr:abstractText>The ability to select objects that are relevant to our behavioural goals is critical for guiding action in complex environments - as when we want to find a friend in a crowd or reach to an object across a cluttered table. This ability depends on processes that can segment visual scenes rapidly into appropriate figure and ground regions, defined in 4D space and time. Prior work has used visual search to explore this issue, with the efficiency of search used as a measure of the ease with which segmentation takes place. This work has shown that segmentation operates efficiently using depth, motion and temporal cues, but we still know little about (i) how some of these forms of segmentation operate, and (ii) how these different cues interact to guide attention to relevant stimuli. This project will combine behavioural and functional imaging methods to provide detailed information on how the different forms of segmentation operate and interact. A first set of behavioural studies will evaluate the evidence for excitatory processes, that guide search to targets, and inhibitory processes that suppress distractors, when displays segment by depth, motion and time. Subsequently we will detail the relations between these process, varying the expectations that participants have about the depth and motion properties of targets. These behavioural experiments will be complemented by functional imaging studies which examine the neural basis of the different forms of segmentation and whether there are common neural mechanisms that are invariant to cue-type. Finally, the necessary role of the neural regions in processing will be assessed by altering activity in targeted areas using trans-cranial magnetic stimulation (TMS). Through the TMS work, and by constructing explicit models of the interactions between neural regions, we will gain new information about how networks of brain regions co-operate to facilitate the selection of visual targets. The work provides the first detailed analysis of the relations between the different forms of segmentation, and their neural basis, using visual search.</gtr:abstractText><gtr:technicalSummary>Human attention operates efficiently in time and space to select targets that are relevant to our behavioural goals. Currently we have sparse knowledge about how some critical properties of scenes are used to guide selection (notably how depth information is used) and we also know little about how different cues combine to optimise behaviour. We will provide detailed information about how depth, motion and temporal cues are used to segment displays and to select visual targets. Behavioural experiments will assess evidence for the roles of excitatory processes that guide search to targets and inhibitory processes that suppress distractors, when depth, motion and temporal cues are present in isolation or combination. The nature of these different processes, and how they interact, will then be characterised. The behavioural studies will be complemented by brain imaging experiments that assess the neural basis of spatial and temporal segmentation in search. We will isolate brain regions that respond in a cue-specific manner from areas that are cue-invariant. We will also test the necessary role of these regions by altering activity through trans-cranial magnetic stimulation (TMS). The TMS procedures will be used alongside explicit modelling of neural connectivity (through Dynamic Causal Modelling) to provide novel information on how different brain regions interact to guide search through space and time.</gtr:technicalSummary><gtr:fund><gtr:end>2012-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2009-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>521355</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>To inform scientific models of visual selective attention.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>0BED4A3D-E740-4B40-8012-79EC5913E6AF</gtr:id><gtr:impactTypes/><gtr:outcomeId>5444c3ef344cb7.43652267</gtr:outcomeId><gtr:sector>Other</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project has generated several key findings:



1. It has shown for the first time the necessary role of parietal cortex in efficient search with moving stimuli

2. It has revealed common brain regions for efficient search by time and motion, which are linked to spatially parallel segmentation of targets and distractors,

3. It has reveal a specific role for distractor suppression in efficient visual search.</gtr:description><gtr:exploitationPathways>Potential use in computer vision and in neuropsychological assessment The work is basic science but it provides guidance on the way that visual segmentation could be implemented in computer vision and how tests of temporal and motion-based search could be developed to understand deficits in these processes.</gtr:exploitationPathways><gtr:id>DF9B5E36-2FBF-476F-8299-0919C043845E</gtr:id><gtr:outcomeId>r-4433725014.732894776fe9c6</gtr:outcomeId><gtr:sectors><gtr:sector>Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B2A09147-3BC7-4752-8094-F6E5FD8701DC</gtr:id><gtr:title>Surface-based constraints on target selection and distractor rejection: evidence from preview search.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>5444b877f31461.92083190</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E74AD0B6-146F-4DC3-B32A-3F58645C18D3</gtr:id><gtr:title>Comparing segmentation by time and by motion in visual search: an FMRI investigation.</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>doi_53d076076e55e630</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2988CCFF-4813-47C4-BFAD-843625F5EC20</gtr:id><gtr:title>Deficits in visual search for conjunctions of motion and form after parietal damage but with spared hMT+/V5.</gtr:title><gtr:parentPublicationTitle>Cognitive neuropsychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0264-3294</gtr:issn><gtr:outcomeId>pm_5435ae95ae99894b8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BB63C5E7-FE7D-4398-9F4B-90E617B00061</gtr:id><gtr:title>Neuropsychological evidence for a competitive bias against contracting stimuli.</gtr:title><gtr:parentPublicationTitle>Neurocase</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1355-4794</gtr:issn><gtr:outcomeId>doi_53d03d03dd79e830</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D3C30543-E6CB-49B0-BE6E-71EB21E74766</gtr:id><gtr:title>Visual search in depth: The neural correlates of segmenting a display into relevant and irrelevant three-dimensional regions.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35c34ed729fbda00ed41955e94795c37"><gtr:id>35c34ed729fbda00ed41955e94795c37</gtr:id><gtr:otherNames>Roberts KL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>585d75ada9edf8.44692668</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D31626F1-2E63-4114-9011-D85BE6FC223D</gtr:id><gtr:title>Integrating space and time in visual search: how the preview benefit is modulated by stereoscopic depth.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>5444b75745b666.55704403</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A8C4AA5-0A2F-4E64-91B2-2FDD15BC7F69</gtr:id><gtr:title>Spreading suppression and the guidance of search by movement: evidence from negative color carry-over effects.</gtr:title><gtr:parentPublicationTitle>Psychonomic bulletin &amp; review</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1069-9384</gtr:issn><gtr:outcomeId>5444b581f26960.62183149</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2D53CD5-521C-48E3-92CC-66C37B84F8FB</gtr:id><gtr:title>Parallel distractor rejection as a binding mechanism in search.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>5444b7fcb1da15.63796888</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D388AE3C-6215-40EC-912E-4F7C64AFB609</gtr:id><gtr:title>Spatial and temporal attention deficits following brain injury: a neuroanatomical decomposition of the temporal order judgement task.</gtr:title><gtr:parentPublicationTitle>Cognitive neuropsychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/35c34ed729fbda00ed41955e94795c37"><gtr:id>35c34ed729fbda00ed41955e94795c37</gtr:id><gtr:otherNames>Roberts KL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0264-3294</gtr:issn><gtr:outcomeId>pm_5435ae95ae9ba5f3b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0F0F78ED-D9ED-4093-8F5B-EA72246BA8B2</gtr:id><gtr:title>Inhibitory guidance in visual search: the case of movement-form conjunctions.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/359517badc40f9bb4a0ef6c3c202fa12"><gtr:id>359517badc40f9bb4a0ef6c3c202fa12</gtr:id><gtr:otherNames>Dent K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>5444b6336c6801.90872672</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/G010919/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>