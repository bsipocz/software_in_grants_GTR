<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/284D1FDA-CEEF-4FD6-9C6B-3AC8A07EB521"><gtr:id>284D1FDA-CEEF-4FD6-9C6B-3AC8A07EB521</gtr:id><gtr:firstName>Nial</gtr:firstName><gtr:surname>Friel</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/BA1958FA-5D03-43A8-8CB2-469390168B8A"><gtr:id>BA1958FA-5D03-43A8-8CB2-469390168B8A</gtr:id><gtr:firstName>Philippe</gtr:firstName><gtr:otherNames>Georges</gtr:otherNames><gtr:surname>Schyns</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7AA796B9-D5FC-4467-B120-CF14B3D23683"><gtr:id>7AA796B9-D5FC-4467-B120-CF14B3D23683</gtr:id><gtr:firstName>Marie</gtr:firstName><gtr:otherNames>Louise</gtr:otherNames><gtr:surname>Smith</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/94AEDD89-1F40-4296-A617-6064C7A226C8"><gtr:id>94AEDD89-1F40-4296-A617-6064C7A226C8</gtr:id><gtr:firstName>Nema</gtr:firstName><gtr:surname>Dean</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE038492%2F1"><gtr:id>F04A6B79-D1CC-4985-90BD-9D79E8AA733B</gtr:id><gtr:title>Adaptive Sampling Algorithms for Cognitive Neuroscience Applications Using Bubbles</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E038492/1</gtr:grantReference><gtr:abstractText>A powerful methodology is required to resolve what is still one of the greatest methodological challenges in cognitive neuroscience: When dealing with complex visual stimuli, how can a brain response be attributed to a specific object category (e.g. a face), a specific feature (e.g. the eye) or a specific function? In the absence of a principled method, the specificity of response (e.g., to the face) is determined by contrast with responses from other categories (e.g., cars, furniture, hands and so forth), and informal hypotheses tested. Unfortunately, a dense correlative structure exists in the low-level visual properties of category members (e.g., luminance energy, main directions of orientation, spatial frequency composition and many others), only a very small subset of which can be controlled with a finite number of contrast categories, though the potential incidence of this obvious point on data interpretation is often underestimated. Consequently, the specificity of the brain response might be due to incidental input statistics, not to the category per se.Furthermore, if the ultimate goal of cognitive science is to frame the functions of the brain (e.g. visual recognition, language, memory and so forth) as the operations of mechanisms of the brain, we know from automata theory that experimental methods should be sufficiently powerful to isolate the elements of a machine. Specifically, experimental methods should enable the extraction of a number of distinct states of the brain corresponding to distinct states of information processing, and then characterise with state transitions the information processing steps taking place between stimulus onset and response.Bubbles is a method that has been designed to address such issues of credit assignment (Gosselin &amp;amp; Schyns, 2001). It works by using the stimulus (not other contrast categories) as its own control for measures of brain response (e.g. EEG, MEG or fMRI amplitudes, or the activity in a network reconstructed from sources). Bubbles samples an input space to present sparse versions of the stimuli (e.g. faces). Subjects categorize the sparse stimuli and Bubbles keeps track of the information samples that lead to either correct or incorrect recognition (when the goal is to understand the informational determinant of behavior), or the amplitude distribution of a given brain signal (e.g. the N170 or the P300 when the goal is to understand the determinants of ERPs; amplitudes of oscillatory MEGS, or magnitudes of voxel fMRI signals). Performance information is then used to establish how each region of the input space selectively contributes to behavior or brain signal.Bubbles has great potential, but it is currently hindered by the exhaustive search of the input space required to find the subspace correlated with performance/typically, several thousands of trials. The potential scope of applications would be widened (and there is an increasing demand from users, particularly in brain imaging and in cognitive neuropsychology, where brain-damaged patients simply cannot be submitted to such an extensive procedure), if statistical methods were designed to optimize sampling and thereby reduce the number of trials required for convergence. Designing such rigorous and efficient statistical methods is the object of this proposal.</gtr:abstractText><gtr:fund><gtr:end>2011-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>352178</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Bubbles is a classification image technique that randomly samples visual information from input stimuli to derive the diagnostic features that observers use in visual categorization tasks. To reach statistical significance, Bubbles performs an exhaustive and repetitive search in the stimulus space. To reduce the search trials, we developed an adaptive method that uses reinforcement learning techniques to optimize sampling by exploiting the observer's history of categorization. We compared the performance of the original and the adaptive Bubbles algorithms in a model observer and eight human adults who all resolved the same visual categorization task (i.e., five facial expressions of emotion). We demonstrate the feasibility of a substantial reduction (by a factor of ~2) in the number of search trials required to locate the same diagnostic features with the adaptive method, but only when the observer reaches a performance threshold of 50% correct for each expression category. When this threshold is not reached, both the original and adaptive algorithms converge in the same number of trials.</gtr:description><gtr:exploitationPathways>Users of Bubbles, or more generally of information sampling algorithms (e.g. to test the projections of Deep Learning Networks) might use the method we developed here to optimize the search they perform.</gtr:exploitationPathways><gtr:id>D9720123-8CE9-417E-828D-839BD1809B82</gtr:id><gtr:outcomeId>56dda74db97cf9.36173457</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>466CAF30-3F06-4D8A-BDE0-C7B53C0F95A2</gtr:id><gtr:title>Efficient bubbles for visual categorization tasks.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd65977ffea77479bfc548634cbd4a7c"><gtr:id>fd65977ffea77479bfc548634cbd4a7c</gtr:id><gtr:otherNames>Wang HF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00f00f0724209</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E038492/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EFFEC6B1-6BC8-4C9D-9D77-02CEF5E4E301</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Biomedical neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>