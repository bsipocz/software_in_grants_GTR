<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Computer Science</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/ABA3AE56-6367-4175-B0FF-CDA79DEF6184"><gtr:id>ABA3AE56-6367-4175-B0FF-CDA79DEF6184</gtr:id><gtr:firstName>Nick</gtr:firstName><gtr:surname>Hawes</gtr:surname><gtr:orcidId>0000-0002-7556-6098</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK014293%2F1"><gtr:id>668FD25F-C9E2-477C-82AD-FD3C73B26F36</gtr:id><gtr:title>Learning the structure and dynamics of human environments to support intelligent mobile robot behaviour</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K014293/1</gtr:grantReference><gtr:abstractText>Advances in mobile robot technology over the last 10 years has made the use of service robots (i.e. robots performing tasks for, or with, humans) in the workplace increasingly possible. The problem of building maps of environments that do not change over time has been solved for many application environments, enabling robots to move around in these places for ever-increasing durations. However, real environments do change over time as their inhabitants move around and move furniture and objects as they do so. The resulting changes to a robot's world make it difficult for it to run reliably, and thus there is a danger that we will not be able to create service robots that are able to perform useful tasks in realistic situations. 

The proposed research treats the fact that a robot's environment changes regularly as an opportunity rather than a challenge. We will develop systems that are able to extract reliable, significant patterns from the changes observed by an existing intelligent robot (the Dora the Explorer robot from the CogX project). In particular we will develop two approaches. The first will capture how easy or difficult it is for a robot to move through particular parts of its map at particular times of day (e.g. due to humans getting in the way). The second approach will capture how the positions of objects in rooms change over time (e.g. the desk in my office never moves, but my chair tends to move around in front of the desk, but never near the door). Taken together these approaches will allow a robot to improve its performance on typical service robot tasks such as searching for an object in a building, whilst avoiding certain areas at certain times of day (e.g the corridor by a canteen during lunchtime), all in dynamic environments.

Our research will be informed by an advisory board of experts in reasoning about space and robot learning, and also by the security company G4S who are interested in using mobile robots to assist security guards. Our results could help them by allowing robots to choose better patrol routes through buildings, and to learn how the objects in a room are typically arranged (allowing them to spot when things change due to a burglary or other incidents). As well as presenting our results through the usual scientific channels, we will also demonstrate our finished robot system at the Thinktank science museum in Birmingham, giving the public a chance to learn more about state-of-the-art robots and AI, whilst also testing our systems in a challenging environment.</gtr:abstractText><gtr:potentialImpactText>This project will develop innovative approaches to making intelligent mobile robots adapt to human-populated environments. There are a wide range of applications where such abilities are necessary to ensure a robotic solution provides appropriate service, including security, healthcare (portering and monitoring), and logistics (robots finding and delivering items). As such, the results of this project will create impact by enabling companies interested in robotic solutions to take a further step towards fully autonomous, adaptive service robots.

An example of such a company is G4S, the UK and Ireland's largest security solutions group. We have been working with them to understand their requirements for mobile robots, and their needs have led in part to this research. For example, our results could be used to allow security robots to detect (statistically) unusual variations in object positions in rooms (potentially caused during a security violation). G4S's interest is evidenced by a letter of support for this project, and we will build on this by meeting with David Ella, CTO of G4S Technology, during the project to present him our results, and also to discuss potential follow-on activities such as a Knowledge Transfer Partnership. 

As the demographics of our society change, in particular the ratio between the number of people surviving into old age and the number of people training to care for them, more roles in our lives will have to be filled by autonomous systems such as robots, or will require their assistance. The proposed research will have impact by enabling robots to perform more efficiently and effectively in human-populated environments (i.e. the environments where they will be required), thus benefitting society by enabling better service robots.

In order to ensure this impact can be made, it is necessary for the public in general to be more aware of the potential of service robotics, and also have a better understanding of what they can, and cannot offer. By engaging the public on the topic of autonomous robots now, we can start discussing the technological, societal and ethical issues surrounding their wider adoption. We will do this by running a week-long installation of our robot at Thinktank, Birmingham's science museum. Thinktank's interest in this installation is described in a letter of support for this project.

We will increase the impact of our research by making all of our results -- papers, software and data -- available online in a free and open manner. This will include open-source implementations of the new approaches we develop, and an open-source release of the full robot software system we will implement these approaches within. This will create impact by allowing industrial and academic groups developing intelligent mobile robots to build directly on our work, speeding up the process of creating and deploying such systems in real-world environments.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-04-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-04-29</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>94315</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Talk at Science Museum Lates.</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>C023CE0D-6AA7-4B6B-8DC7-D915B7D9A940</gtr:id><gtr:impact>Lively discussions and many questions.

Nothing notable.</gtr:impact><gtr:outcomeId>54612a42a31e93.33928902</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Lord Kelvin Award Lecture at the 2013 British Science Festival</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>A90B05F8-BDC4-4310-81A9-7F32AF9E708B</gtr:id><gtr:impact>Many interested people talked to me afterwards

Further opportunities to speak at similar events.</gtr:impact><gtr:outcomeId>54612b2edfe1b1.84577196</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.britishscienceassociation.org/british-science-festival/news/robots-and-humans-when-two-societies-meet</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPSRC's Autonomous Robots show at Cheltenham Science Festival.</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>322F207F-39ED-4F2D-9D9C-0FFB89894529</gtr:id><gtr:impact>Many questions from an engaged audience.

None.</gtr:impact><gtr:outcomeId>54612a92136079.11102516</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.cheltenhamfestivals.com/science/whats-on/2013/autonomous-robots/</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In this grant we developed a method to combine methods for planning robot behaviours under uncertainty with more qualitative goal descriptions. This allows robots to be given goals in formats which are readily provided by human input but can be addressed in a way which more naturally captures the underlying dynamics of the operating environment.</gtr:description><gtr:exploitationPathways>We are feeding our findings into our work on long-term autonomy in everyday environments, where the developed method underlies our service robot's planning approach for long-term behaviour.</gtr:exploitationPathways><gtr:id>90FED84A-FD5C-44D5-A19C-906C06A85A44</gtr:id><gtr:outcomeId>56d036a9325827.25805502</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Manufacturing, including Industrial Biotechology,Retail,Security and Diplomacy,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>229C71C4-F614-4A30-8DD6-C2FDB3F99DF0</gtr:id><gtr:title>Combining top-down spatial reasoning and bottom-up object class recognition for scene understanding</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d94c29d52db092109e34cbd4bd808125"><gtr:id>d94c29d52db092109e34cbd4bd808125</gtr:id><gtr:otherNames>Kunze L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54612858d2f335.16571682</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EF53869A-DF9B-4435-A31A-B3294362A9F2</gtr:id><gtr:title>On the Notion of Uncontrollable Marking in Supervisory Control of Petri Nets</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Automatic Control</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7cecbac4133fd7d244d7a4439272b09"><gtr:id>a7cecbac4133fd7d244d7a4439272b09</gtr:id><gtr:otherNames>Lacerda B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f9759756cee01d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>78565FD0-E741-48AF-80E5-B57F4486D81B</gtr:id><gtr:title>Bootstrapping Probabilistic Models of Qualitative Spatial Relations for Active Visual Object Search</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d94c29d52db092109e34cbd4bd808125"><gtr:id>d94c29d52db092109e34cbd4bd808125</gtr:id><gtr:otherNames>Kunze L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54612620e66c77.79396287</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39769783-0CE7-4C28-AD3D-47350CC6282F</gtr:id><gtr:title>Effects of Training Data Variation and Temporal Representation in a QSR-Based Action Prediction System</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd9dc19f58376ec7ea99cac38db7a810"><gtr:id>dd9dc19f58376ec7ea99cac38db7a810</gtr:id><gtr:otherNames>Young J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546126a09b43b2.59365892</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BC65CE43-69C6-45EA-9C88-707974633DC4</gtr:id><gtr:title>The Role of Context in Spatial Region Identification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3a660237ce90dcf62d8fde7671f604bc"><gtr:id>3a660237ce90dcf62d8fde7671f604bc</gtr:id><gtr:otherNames>Lockwood K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5461273ac000f6.97683856</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B81AED5B-6279-4BEA-A2C0-96BAE347F228</gtr:id><gtr:title>Optimal and dynamic planning for Markov decision processes with co-safe LTL specifications</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a7cecbac4133fd7d244d7a4439272b09"><gtr:id>a7cecbac4133fd7d244d7a4439272b09</gtr:id><gtr:otherNames>Lacerda B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546125000274c9.70952640</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K014293/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>