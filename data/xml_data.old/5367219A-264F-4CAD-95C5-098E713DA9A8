<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/68D0E3C9-9246-4CFC-B5E9-48584CF82993"><gtr:id>68D0E3C9-9246-4CFC-B5E9-48584CF82993</gtr:id><gtr:name>University of Manchester</gtr:name><gtr:address><gtr:line1>Oxford Road</gtr:line1><gtr:city>Manchester</gtr:city><gtr:postCode>M13 9PL</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2558DFA4-CF1D-47F7-A378-A9682D76EBCE"><gtr:id>2558DFA4-CF1D-47F7-A378-A9682D76EBCE</gtr:id><gtr:name>TRW Conekt</gtr:name><gtr:address><gtr:line1>TRW Conekt</gtr:line1><gtr:line4>Stratford Road</gtr:line4><gtr:line5>Solihull</gtr:line5><gtr:postCode>B90 4GW</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F5FD1D97-8BF0-48DA-84D8-A13D5D494E7A"><gtr:id>F5FD1D97-8BF0-48DA-84D8-A13D5D494E7A</gtr:id><gtr:name>Blue Bear Systems Research Ltd</gtr:name><gtr:address><gtr:line1>Building 32</gtr:line1><gtr:line2>Twinwoods Business Park</gtr:line2><gtr:postCode>MK41 6JE</gtr:postCode><gtr:region>South East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF81E303-6E50-4348-9599-52B81DED7BC3"><gtr:id>EF81E303-6E50-4348-9599-52B81DED7BC3</gtr:id><gtr:name>BAE Systems</gtr:name><gtr:address><gtr:line1>Chelmsford Office and Technology Park</gtr:line1><gtr:line2>West Hanningfield Road</gtr:line2><gtr:line3>Great Baddow</gtr:line3><gtr:line4>Chelmsford</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CM2 8HN</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6DFEF97C-3034-4830-B95F-8319F848D129"><gtr:id>6DFEF97C-3034-4830-B95F-8319F848D129</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Carey</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/9EFDC7CD-A1C6-4236-8125-C91600AAC291"><gtr:id>9EFDC7CD-A1C6-4236-8125-C91600AAC291</gtr:id><gtr:firstName>Piotr</gtr:firstName><gtr:surname>Dudek</gtr:surname><gtr:orcidId>0000-0002-6511-6165</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM019284%2F1"><gtr:id>5367219A-264F-4CAD-95C5-098E713DA9A8</gtr:id><gtr:title>An Integrated Vision and Control Architecture for Agile Robotic Exploration</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M019284/1</gtr:grantReference><gtr:abstractText>Autonomous robots, capable of independent and intelligent navigation through unknown environments, have the potential to significantly increase human safety and security. They could replace people in potentially hazardous tasks, for instance search and rescue operations in disaster zones, or surveys of nuclear/chemical installations. Vision is one of the primary senses that can enable this capability, however, visual information processing is notoriously difficult, especially at speeds required for fast moving robots, and in particular where low weight, power dissipation and cost of the system are of concern. Conventional hardware and algorithms are not up to the task. The proposal here is to tightly integrate novel sensing and processing hardware, together with vision, navigation and control algorithms, to enable the next generation of autonomous robots.

At the heart of the system will be a device known as a 'vision chip'. This bespoke integrated circuit differs from a conventional image sensor, including a processor with each pixel. This will offer unprecedented performance. The massively parallel processor array will be programmed to pre-process images, passing higher-level feature information upstream to vision tracking algorithms and the control system. Feature extraction at pixel level results in an extremely efficient and high speed throughput of information. Another feature of the new vision chip will be the measurement of 'time of flight' data in each pixel. This will allow the distance to a feature to be extracted and combined with the image plane data for vision tracking, simplifying and speeding up the real-time state estimation and mapping capabilities. Vision algorithms will be developed to make the most optimal use of this novel hardware technology.

This project will not only develop a unique vision processing system, but will also tightly integrate the control system design. Vision and control systems have been traditionally developed independently, with the downstream flow of information from sensor through to motor control. In our system, information flow will be bidirectional. Control system parameters will be passed to the image sensor itself, guiding computational effort and reducing processing overheads. For example a rotational demand passed into the control system, will not only result in control actuation for vehicle movement, but will also result in optic tracking along the same path. A key component of the project will therefore be the management and control of information across all three layers: sensing, visual perception and control. Information share will occur at multiple rates and may either be scheduled or requested. Shared information and distributed computation will provide a breakthrough in control capabilities for highly agile robotic systems.

Whilst applicable to a very wide range of disciplines, our system will be tested in the demanding field of autonomous aerial robotics. We will integrate the new vision sensors onboard an unmanned air vehicle (UAV), developing a control system that will fully exploit the new tracking capabilities. This will serve as a demonstration platform for the complete vision system, incorporating nonlinear algorithms to control the vehicle through agile manoeuvres and rapidly changing trajectories. Although specific vision tracking and control algorithms will be used for the project, the hardware itself and system architecture will be applicable to a very wide range of tasks. Any application that is currently limited by tracking capabilities, in particular when combined with a rapid, demanding control challenge would benefit from this work. We will demonstrate a step change in agile, vision-based control of UAVs for exploration, and in doing so develop an architecture which will have benefits in fields as diverse as medical robotics and industrial production.</gtr:abstractText><gtr:potentialImpactText>High technology industries have been identified as a key sector in the UK economy. Robotic and autonomous systems have in turn been singled out as technologies of particular importance to maintain economic competitiveness in high technology applications. We intend to develop the next generation of active vision sensors for autonomous robots, focusing on delivering systems with real-time high-speed functionality at low-power, low-weight, and low-cost. These will have potential applications across a range of industries, including security, transportation, manufacturing, agriculture, nuclear and healthcare. Just some of these applications are highlighted below.

Agile micro air-vehicles, and more generally, advanced vision-based navigation systems for autonomous robots will find both civilian and military applications in reconnaissance and search and rescue operations. The system being developed will be applicable in unmanned vehicles (air, land and water based), either as a primary navigation and control system, or alternatively to enhance safety and provide additional features. Further applications include inspection (e.g. nuclear decommissioning, inspection of overhead power lines, monitoring of oil rigs or power sub-stations, agricultural surveys, etc) and space exploration. In all these fields, robots are currently used, but our research will offer significant performance benefits, expanding the scope of applications. Applications of autonomous robots, from self-driving automobiles, to drone-based goods delivery and robotic companions, have been recently attracting both large amount of public interest and significant industry investment. While some of these still remain long-term aspirations, the companies we collaborate with on this project see a more immediate (on a 5-10 year horizon) use of our technologies in their application domains.

There is widespread expectation of autonomous robots entering everyday life - but for many applications not just the performance but also the system cost, size and power consumption are currently prohibitive. The combination of performance and small-size/low-cost of our proposed system will make it suitable for cost-sensitive applications, from consumer robotics to toys. Similar properties are also needed for automated video surveillance (especially distributed smart cameras, e.g. crowd and traffic monitoring, early forest-fire detection, or fall monitoring for the elderly) and in vehicular applications (e.g. parking assistance, collision avoidance and driver alertness). Low-power intelligent sensing is a prerequisite for the plethora of much talked-about &amp;quot;internet-of-things&amp;quot;, &amp;quot;ambient intelligence&amp;quot; and &amp;quot;cyber-physical systems&amp;quot; applications, and in portable and wearable systems.

The unparalleled high-speed potential of our near-sensor vision processing approach will be a key advantage in the field of manufacturing process control, where an advanced machine vision system can provide the opportunity to respond to and to control high-speed events. These might include component manufacture and assembly, laser welding, control of industrial robots, and high-speed metrology for sorting or visual inspection for quality assurance. 

Further afield, and on longer time-scales, potential applications of the developed technologies can be identified in fields ranging from nuclear research to healthcare. For example, high-speed sensor-level tracking systems, based on the technologies we will develop could be used for beam control in particle accelerators, or to reduce the highly-redundant data typically collected by these systems, or to locate and track radiation instability within nuclear fusion research facilities. In healthcare, ultra high-speed vision systems could be used to improve the accuracy and reduce time-to-diagnosis of cancer screening - identifying circulating tumour cells from blood samples of several million cells.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>858323</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>CA960FB4-DE4F-4397-AA78-233F780E2C8C</gtr:id><gtr:title>Parallel HDR tone mapping and auto-focus on a cellular processor array vision chip</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0ba1f50313fc2d9c117bfca5878f797"><gtr:id>d0ba1f50313fc2d9c117bfca5878f797</gtr:id><gtr:otherNames>Martel J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>02714310</gtr:issn><gtr:outcomeId>58c6c9d2896cf9.07035342</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D438DF38-77E1-4425-BF99-9C93B7C12F4F</gtr:id><gtr:title>Real-Time Depth From Focus on a Programmable Focal Plane Processor</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems I: Regular Papers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0ba1f50313fc2d9c117bfca5878f797"><gtr:id>d0ba1f50313fc2d9c117bfca5878f797</gtr:id><gtr:otherNames>Martel J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2fe7bda0a4c2.53943090</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C66CD4D-EC2B-4BDB-90B2-15E30BEB9EB8</gtr:id><gtr:title>Toward joint approximate inference of visual quantities on cellular processor arrays</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0ba1f50313fc2d9c117bfca5878f797"><gtr:id>d0ba1f50313fc2d9c117bfca5878f797</gtr:id><gtr:otherNames>Martel J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>02714310</gtr:issn><gtr:outcomeId>56deeec0247e50.31884185</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>575D6855-6FAA-4B63-A5AE-D010CCF483FC</gtr:id><gtr:title>Pixel interlacing to trade off the resolution of a cellular processor array against more registers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0ba1f50313fc2d9c117bfca5878f797"><gtr:id>d0ba1f50313fc2d9c117bfca5878f797</gtr:id><gtr:otherNames>Martel J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c6c9d2ae59a3.68094147</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M019284/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>35</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>53F27348-198B-4AEF-A34B-8307067F507C</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Systems engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>21CE2EA6-E7A2-4406-A045-0DA7CA19B695</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Control Engineering</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>