<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/EAAD4D43-BD15-432B-9385-2DBD0C65958D"><gtr:id>EAAD4D43-BD15-432B-9385-2DBD0C65958D</gtr:id><gtr:name>University of Bath</gtr:name><gtr:department>Mathematical Sciences</gtr:department><gtr:address><gtr:line1>University of Bath</gtr:line1><gtr:line2>Claverton Down</gtr:line2><gtr:line4>Bath</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EAAD4D43-BD15-432B-9385-2DBD0C65958D"><gtr:id>EAAD4D43-BD15-432B-9385-2DBD0C65958D</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>University of Bath</gtr:line1><gtr:line2>Claverton Down</gtr:line2><gtr:line4>Bath</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FBA1E067-ED1E-4752-9BD3-D8C0FDA46F3B"><gtr:id>FBA1E067-ED1E-4752-9BD3-D8C0FDA46F3B</gtr:id><gtr:name>Lawrence Livermore National Laboratory</gtr:name><gtr:address><gtr:line1>Lawrence Livermore National Laboratory</gtr:line1><gtr:line2>7000 East Avenue</gtr:line2><gtr:line3>PO Box 5508</gtr:line3><gtr:line4>Livermore</gtr:line4><gtr:line5>California</gtr:line5><gtr:postCode>CA 94550 9</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4E67597F-C9AB-4ED2-BB87-FB1D1FC6FB6D"><gtr:id>4E67597F-C9AB-4ED2-BB87-FB1D1FC6FB6D</gtr:id><gtr:name>University of Heidelberg</gtr:name><gtr:address><gtr:line1>University of Heidelberg</gtr:line1><gtr:line4>Heidelberg</gtr:line4><gtr:postCode>D-69120</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C67659FE-0455-48EF-89E8-B4E256441597"><gtr:id>C67659FE-0455-48EF-89E8-B4E256441597</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>Murray</gtr:otherNames><gtr:surname>Pickles</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6E8A5982-B3A2-4FA1-8E6A-49315FC84DA2"><gtr:id>6E8A5982-B3A2-4FA1-8E6A-49315FC84DA2</gtr:id><gtr:firstName>Robert</gtr:firstName><gtr:surname>Scheichl</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=NE%2FJ005576%2F1"><gtr:id>EF8CC41B-C521-4828-8E8B-F04CBDF45320</gtr:id><gtr:title>Parallel Scalability of Elliptic Solvers in Weather and Climate Prediction</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>NE/J005576/1</gtr:grantReference><gtr:abstractText>The UK Met Office is one of the world leaders in weather and climate prediction, and the Met Office's global forecast model is used by many other centres worldwide to drive their individual local area models. However, many short scale phenomena as well as important longterm dynamics are still difficult to predict accurately due to the limited spatial resolution of global models and the additional errors introduced by local area models. Novel computing architectures with more than 10^5 cores provide a chance to push these boundaries and to keep the UK Met Office at the forefront of developments. 

Decades of experience with numerical weather and climate prediction have produced a good understanding of the core dynamics inherent in atmospheric flow and of their stable and accurate numerical approximations. As outlined in the call, the Met Office's Unified Model uses lattitude-longitude grids and achieves high efficiency on parallel computers with up to 1000 cores. However, (artificial) grid clustering at the poles renders these grids impractical for large-scale computations, and so one of the core tasks in this NERC Programme is the search for suitable alternative grids. Several separate proposals address this issue. However, the equations governing atmospheric flow form a time-dependent system of differential equations which strongly couple the solution everywhere on the globe (the famous &amp;quot;butterfly effect&amp;quot;). Most current atmospheric dynamics models use semi-implicit time discretisation schemes which provide some global coupling of the equations at each time step. This prevents the system from becoming unstable and as a consequence it allows for larger time steps than fully explicit schemes, which include no global coupling. Since the cost of the forecast is proportional to the number of time steps, a scheme that allows for larger time steps (with satisfactory accuracy) seems preferable. But these benefits come at a price, especially in the context of large-scale problems and on massively parallel architectures. An elliptic system for the pressure has to be solved in each time step, leading to a very large, ill-conditioned algebraic system, the solution of which is difficult to parallelise efficiently. There are two main factors that make the scaling of this elliptic solve to large problem sizes and to large processor numbers difficult: algorithmic scalability and parallel scalability. Since the solution operator for the elliptic equation couples the pressures globally, only multilevel iterative solvers which use a hierarchy of discretisations on grids of varying resolution allow optimal, linear growth in cost (algorithmic scalability). But in a massively parallel computing environment, where global communication is costly, it is necessary to implement these solvers well, keeping most of the communication local, to ensure that the computational cost continues to scale optimally to 100K or more processors (parallel scalability).

This proposal addresses this problem and will thus facilitate the best possible decisions on the design of the Met Office's future dynamical core, thus guaranteeing the UK's competitiveness in this key societal/technological challenge. An optimal scalability of semi-implicit schemes has not been achieved in atmospheric flow up to now, but success of the Project Partners, IWR Heidelberg and Lawrence Livermore National Lab, on simpler model elliptic problems shows that it is possible. The PIs experience over the years in obtaining optimal scalability of elliptic solvers on the most current architectures in various application areas, most notably for elliptic problems from atmospheric flow discretised on latitude-longitude grids up to 256 cores, as well as his status as one of the world's leading theoretical analysts of multilevel iterative elliptic solvers and his links to other world leading groups in this field, mean that that he is ideally equipped to achieve this goal.</gtr:abstractText><gtr:potentialImpactText>The proposed research will have enormous impact on all those fields where semi-implicit schemes are needed to treat the diffusive parts of the system and where solving elliptic systems is the most costly computational component of the analysis.

Who? The focus of the research is on applying existing and on developing new highly efficient parallel techniques for solving elliptic systems that arise when using a semi-implicit approach to global modelling of atmospheric flow, ensuring optimal scalability on modern multicore architectures. A number of very important areas would benefit directly from the outcomes:
1. The meteorological and climatological forecasting community, in particular the UK Met Office and other users of the UM and related tools . 
2. The oceanographic community who deal with similar models that require scalable elliptic solvers.
3. The water resources management sector, where weather and climate prediction has always played a major role.
4. The military, where accurate weather predictions are crucial to the safety of the troops and to strategic decisions.
5. Other environmental sectors, in particular the subsurface waste management or the carbon capture and storage sectors.
Large elliptic systems are at the heart of many environmental models. They usually constitute the bottleneck in large scale computations, particularly on massively parallel computers, and thus all the areas above would benefit directly from the proposed research.

How? In computational approaches to model physical systems, semi-implicit treatment of diffusive processes is used to ensure the the stability of the numerical scheme. This approach leads to large elliptic systems that often constitute the most expensive part in the simulation and classical solvers have been observed not to scale well beyond 1000s of processors on massively parallel architectures. Future simulation and forecasting tools, such as the Met Office's dynamical core, will have to address this issue in a more satisfactory manner. Similar comments could be made regarding many of the other areas mentioned above. Thus, when our methods are applicable we expect them to reduce computation times significantly and bring many important problems within reach of analysis. The benefit will be in removing the computational bottleneck of parallel elliptic solvers which are at the heart of many parallel forecasting and simulation tools in engineering and environmental applications.

What? The primary beneficiary of this research will be the weather and climate prediction sector. Transfer of the benefits of the research will be achieved by working closely with the Met Office and with other similar institutions such as the ECMWF in Reading. Nils Wedi from the ECMWF will act as one of the advisers of the project. The University of Bath and the PI in particular have an excellent and effective established relationship with the Met Office. The entire project will be carried out in close collaboration with the other project teams on this programme and with the Met Office. The PDRA and the PI will visit the Met Office regularly, thus keeping them informed constantly about the progress. Regular progress reports will also be posted on the programme Wiki and given at regular programme meetings. In Phase 2 of the programme the methods developed and tested in this project will be considered for inclusion in the next generation dynamical core at the Met Office and thus become also available to the weather and climate forecasting community as a whole through the many links and clients the Met Office has worldwide. In addition we will also disseminate our results at appropriate international meetings.

Measures of success. The key measure will be adoption of semi-implicit schemes in the new dynamical core and their implementation in Phase 2 of the programme. But through our links with the project partners we hope to also see a wider use of the methods which we develop.</gtr:potentialImpactText><gtr:fund><gtr:end>2013-11-06</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/8A03ED41-E67D-4F4A-B5DD-AAFB272B6471"><gtr:id>8A03ED41-E67D-4F4A-B5DD-AAFB272B6471</gtr:id><gtr:name>NERC</gtr:name></gtr:funder><gtr:start>2011-09-07</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>180062</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>229800</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Programme Grant</gtr:description><gtr:end>2015-11-02</gtr:end><gtr:fundingOrg>Natural Environment Research Council</gtr:fundingOrg><gtr:fundingRef>NE/K006754/1</gtr:fundingRef><gtr:id>0F78A288-EA6D-4ABD-8E11-FAEEB25D28B5</gtr:id><gtr:outcomeId>5463e22e563730.18418373</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings in Phase 1 of this project have centrally informed decisions about the next generation dynamical core at the Met Office. The solvers we tested and in particular the geometric multigrid solver that we designed and that performed best, will now form the solver in the code developed in Phase 2 and beyond. It has also been implemented and tested within the current Met Office Dynamical Core, EndGAME, and will potentially also be used operationally in the very near future. A secondment of Eike Mueller is planned to ensure this impact can be achieved. It has also had a huge impact on developers of other atmospheric flow solvers and even ocean modellers.</gtr:description><gtr:id>73CAED55-117B-46DC-8A13-294528ADAA20</gtr:id><gtr:impactTypes><gtr:impactType>Policy &amp; public services</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5463dc6d852613.10436594</gtr:outcomeId><gtr:sector>Environment</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>As part of the NERC programme on a Next Generation Dynamical Core for the Met Office, Gung-Ho, we carried out a comprehensive study of the parallel scalability of elliptic solvers which are central in semi-implicit time stepping methods for atmospheric flow. We found that both Krylov iterative methods as well as multigrid methods scale almost optimally up to the largest numbers of processors currently available. In terms of absolute performance, a matrix-free, geometric multigrid solver, tailored to the anisotropic nature of atmospheric flow, performed best both in terms of robustness and efficiency. In this first phase of the project we mainly focussed on simplified model problems which were sufficiently complex to genuinely establish whether elliptic solvers were massively scalable, but at the same time simple enough to test a wide and representative range of solvers in the two years of the project. Towards the end of Phase 1 we added more and more of the physical features in the eventual dynamical core.</gtr:description><gtr:exploitationPathways>We have published all our results in international journals and presented them in several relevant fora. Through some key events, such as the Newton Institute Programme in 2013, the GungHo programme is known world-wide in all major meteorological centres and people are following our progress. As part of this, our results on the massive scalability of elliptic solvers - doubted prior to this project by many in the area - have lead to quite some interest also from other groups. Triggered by this, other teams are now also pursuing similar approaches. More directly, all people involved in the GungHo project are taking it forward also in their own projects and users of the final software that we are developing will be able to benefit directly from it.</gtr:exploitationPathways><gtr:id>2AA454DE-9EC0-40D5-BD23-B365AA028F8A</gtr:id><gtr:outcomeId>5463dbf5f3b3f8.97092206</gtr:outcomeId><gtr:sectors><gtr:sector>Environment</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This multi-GPU implementation of a tensor-product multigrid solver was used to solve a simplified pressure correction equation and test the performance of the solver on multi-GPU clusters.</gtr:description><gtr:id>5FF459FC-FD89-4499-A77A-AF26A3BAD354</gtr:id><gtr:impact>Since the Met Office is considering using chip architectures similar to GPUs for their next generation forecast model, the results obtained with this code will have an impact on the ultimate choice of solver algorithm. The code was used to produce results for the following two publications:
M&amp;uuml;ller, E., Guo, X., Scheichl, R. and Shi, S., 2013. &amp;quot;Matrix-free GPU implementation of a preconditioned conjugate gradient solver for anisotropic elliptic PDEs&amp;quot;. Computing and Visualization in Science, 16(2), pp.41-58.
M&amp;uuml;ller, E.H., Scheichl, R. and Vainikko, E., 2015. &amp;quot;Petascale solvers for anisotropic PDEs in atmospheric modelling on GPU clusters&amp;quot;. Parallel Computing, 50, pp.53-69.</gtr:impact><gtr:outcomeId>58c96343380458.63738899</gtr:outcomeId><gtr:title>Multi-GPU implementation of tensor-product multigrid algorithm</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://bitbucket.org/em459/ellipticsolvergpu</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Fortran 90 implementation of a matrix-free tensor-product multigrid solver for the pressure correction equation on structured grids. For first tests of the feasibility of the multigrid method a simplified model equation was solved. This equation reproduces key characteristics of the full equation encountered in atmospheric models. The code also has the option to run a standard, single-level method which is similar to what is currently used by the Met Office, thus allowing the comparison of the two methods.</gtr:description><gtr:id>E5BC2EE6-A608-4DF1-9928-06058BC8D508</gtr:id><gtr:impact>This code was used to compare matrix-free geometric implementations to matrix-based AMG solvers. Those tests were used to inform the choice of solver algorithm in the project, and this will have an impact on the choice of solver for the future LFRic model, which is currently implemented at the Met Office. Similar solvers were implemented in the Met Office ENDGame Dynamical core and it is planned for those new multigrid solvers to be used in operational applications in the future. The software was also crucial to obtain results in the following paper: M&amp;uuml;ller, E.H. and Scheichl, R., 2014. Massively parallel solvers for elliptic partial differential equations in numerical weather and climate prediction. Quarterly Journal of the Royal Meteorological Society, 140(685), pp.2608-2624.</gtr:impact><gtr:outcomeId>58c95e88ad6119.93724615</gtr:outcomeId><gtr:title>Tensor product multigrid for atmospheric equations</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://bitbucket.org/em459/tensorproductmultigrid</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Implementation of a tensor-product multigrid solver in the Met Office ENDGame code base. The multigrid algorithm is used to solve the pressure equation in implicit time stepping methods, this step is one of the computational bottlenecks in the forecast model. The code was tested in operational configurations of the Unified Model.</gtr:description><gtr:id>69CA24DE-A0EF-4246-94D5-EEC250FDF354</gtr:id><gtr:impact>It is planned to use this solver in operational runs of the Met Office Unified Model in the future. As already demonstrated in several test runs, this can reduce the time spent in the implicit solver by a factor of at least two. Ultimately this has the potential of increasing the Met Office's forecast capabilities since it will allow the production of more accurate forecasts in a shorter time.</gtr:impact><gtr:outcomeId>58c961df4c3504.69744676</gtr:outcomeId><gtr:title>Multigrid solver in ENDGame</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Implementation of a geometric tensor-product multigrid solver in the DUNE C++ library for grid based applications. The code generalises other implementations of the algorithm since it supports more generals grids and more realistic pressure equations encountered in atmospheric modelling.</gtr:description><gtr:id>70056FAE-A6B0-4A68-A48C-D47FE3672C16</gtr:id><gtr:impact>The code was used to obtain results in the following paper:
Dedner, A., M&amp;uuml;ller, E. and Scheichl, R., 2016. Efficient multigrid preconditioners for atmospheric flow simulations at high aspect ratio. International Journal for Numerical Methods in Fluids, 80(1), pp.76-102</gtr:impact><gtr:outcomeId>58c96400781278.25478198</gtr:outcomeId><gtr:title>DUNE implementation of tensor-product multigrid solver</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://bitbucket.org/em459/tensorproductmultigrid</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2E661EB1-28E1-4E6A-88B6-C930E60339E9</gtr:id><gtr:title>Efficient multigrid preconditioners for atmospheric flow simulations at high aspect ratio</gtr:title><gtr:parentPublicationTitle>International Journal for Numerical Methods in Fluids</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/001bab1f58d6ca8fcc672c0a794f703c"><gtr:id>001bab1f58d6ca8fcc672c0a794f703c</gtr:id><gtr:otherNames>Dedner A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5675eef7757c8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>877A4439-0378-4DC2-BD0F-C7CD391D425C</gtr:id><gtr:title>High level implementation of geometric multigrid solvers for finite element problems: Applications in atmospheric modelling</gtr:title><gtr:parentPublicationTitle>Journal of Computational Physics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c0e9261caa69db81a8ffade4e3be326e"><gtr:id>c0e9261caa69db81a8ffade4e3be326e</gtr:id><gtr:otherNames>Mitchell L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d45d2e87d45.72089941</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4647DEEC-115E-4F58-9C45-7FD9CE0AA163</gtr:id><gtr:title>Large Scale Inverse Problems: Computational Methods and Applications in the Earth Sciences</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fa37af926653cc5560d585f96c93b40f"><gtr:id>fa37af926653cc5560d585f96c93b40f</gtr:id><gtr:otherNames>Cullen, Mike, Freitag, Melina A., Kindermann, Stefan, Scheichl, Robert</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978223110282221</gtr:isbn><gtr:outcomeId>5460e94bb674f2.74667111</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1E09F0F2-9A10-4277-AEB3-3DC19F5D7963</gtr:id><gtr:title>Massively parallel solvers for elliptic partial differential equations in numerical weather and climate prediction</gtr:title><gtr:parentPublicationTitle>Quarterly Journal of the Royal Meteorological Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba4518441fed59aae8b568164dd6676"><gtr:id>dba4518441fed59aae8b568164dd6676</gtr:id><gtr:otherNames>M?ller E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e765d496c3.04897366</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E4D770E4-9DE0-4BDE-9691-AF9FD7C81F8F</gtr:id><gtr:title>A robust numerical method for the potential vorticity based control variable transform in variational data assimilation</gtr:title><gtr:parentPublicationTitle>Quarterly Journal of the Royal Meteorological Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1b316aaa308f72c630355e9aa20bfe43"><gtr:id>1b316aaa308f72c630355e9aa20bfe43</gtr:id><gtr:otherNames>Buckeridge S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53cfc9fc9ac63390</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B93A799A-351F-4799-B943-4FCF4F12855D</gtr:id><gtr:title>Algebraic multigrid for discontinuous Galerkin discretizations of heterogeneous elliptic problems</gtr:title><gtr:parentPublicationTitle>Numerical Linear Algebra with Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/504919c462ebe1d3de969a8a24ce7b42"><gtr:id>504919c462ebe1d3de969a8a24ce7b42</gtr:id><gtr:otherNames>Bastian P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53cfc7fc7f32e13f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>412973F5-FA31-44DD-8296-6F3F8040CB09</gtr:id><gtr:title>Matrix-free GPU implementation of a preconditioned conjugate gradient solver for anisotropic elliptic PDEs</gtr:title><gtr:parentPublicationTitle>Computing and Visualization in Science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba4518441fed59aae8b568164dd6676"><gtr:id>dba4518441fed59aae8b568164dd6676</gtr:id><gtr:otherNames>M?ller E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460e91d1a8876.59092316</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>77039C1E-DEED-40AE-A4E9-F79BD90EE740</gtr:id><gtr:title>Petascale solvers for anisotropic PDEs in atmospheric modelling on GPU clusters</gtr:title><gtr:parentPublicationTitle>Parallel Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dba4518441fed59aae8b568164dd6676"><gtr:id>dba4518441fed59aae8b568164dd6676</gtr:id><gtr:otherNames>M?ller E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9b80626a102.50351054</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">NE/J005576/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>E4C03353-6311-43F9-9204-CFC2536D2017</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Atmospheric phys. &amp; chemistry</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EE4457DB-92A3-44EA-8D5F-77013CC107E0</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Climate &amp; Climate Change</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>DEA11FBC-BEED-4EDD-890B-97D728462D26</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Mathematical sciences</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>62E0966C-A067-4075-B244-33F1F4DD4B1E</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Atmospheric Kinetics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>EE4457DB-92A3-44EA-8D5F-77013CC107E0</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Climate &amp; Climate Change</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>9EAAD5EA-2E54-4986-942F-2E204958FE29</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>High Performance Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>CFBD6626-50CB-4AA6-8B49-9A76519BA91F</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Numerical Analysis</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>01AC56BC-45B4-434D-B015-7C879327F09F</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Parallel Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>