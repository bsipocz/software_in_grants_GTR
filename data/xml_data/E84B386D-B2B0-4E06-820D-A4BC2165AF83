<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C754D612-4D8B-42CB-922C-07E7F5BE4B49"><gtr:id>C754D612-4D8B-42CB-922C-07E7F5BE4B49</gtr:id><gtr:name>Washington University in St. Louis</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0CF6353C-24C9-4866-99CF-FB1DB633AF6F"><gtr:id>0CF6353C-24C9-4866-99CF-FB1DB633AF6F</gtr:id><gtr:name>University of North Carolina at Chapel Hill</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Physiology Anatomy and Genetics</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C754D612-4D8B-42CB-922C-07E7F5BE4B49"><gtr:id>C754D612-4D8B-42CB-922C-07E7F5BE4B49</gtr:id><gtr:name>Washington University in St. Louis</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0CF6353C-24C9-4866-99CF-FB1DB633AF6F"><gtr:id>0CF6353C-24C9-4866-99CF-FB1DB633AF6F</gtr:id><gtr:name>University of North Carolina at Chapel Hill</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/1318ADD8-85A3-4FDE-816A-99386CE74212"><gtr:id>1318ADD8-85A3-4FDE-816A-99386CE74212</gtr:id><gtr:firstName>Jennifer</gtr:firstName><gtr:otherNames>Kim</gtr:otherNames><gtr:surname>Bizley</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FH016813%2F1"><gtr:id>E84B386D-B2B0-4E06-820D-A4BC2165AF83</gtr:id><gtr:title>Identifying the signal in the noise: a systems approach for examining invariance in auditory cortex</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/H016813/1</gtr:grantReference><gtr:abstractText>We are able to recognize and understand speech across many different speakers, voice pitches and listening conditions. However, the acoustic waveform of a sound (e.g. the vowel 'ae') will vary considerably depending on the individual speaker, and the 'ae' may be embedded in a cacophony of other, background sounds in our often noisy environments. Despite this, we have no difficulty recognizing an 'ae' as an 'ae', suggesting that the brain is capable of forming a representation of the vowel sound which is invariant to these 'nuisance' variables. For vowel sounds, the timbre, or vowel identity, is determined by the spectral envelope. Filtering by the mouth, lips and tongue results in energy peaks, or 'formants' in the spectrum, and it is the location of these formants which differentiates vowel sounds from one another. Thus, the fact that we are able to discriminate 'ae' from 'ih' irrespective of the gender, age or accent of a speaker suggests that we are able to form an invariant representation of the formant relations independently of the fundamental frequency, room reverberations, or spatial location in both quiet and noisy conditions. The aim of this research program is to discover where and how such invariant representations arise in the central auditory system and how these representations are maintained in noisy environments. Forming invariant representations is one of the greatest challenges for sensory systems, and understanding where and how such representations are read out is crucial for the design of any neuroprosthetic device. Our research uses ferrets as their hearing range spans a very similar range of frequencies to ours. Moreover, ferret vocalizations share many similarities with human vowel sounds. Ferrets rapidly learn to discriminate vowel sounds and we are able to record the activity of their nerve cells whilst they perform such listening tasks. By probing the circumstances under which the ferret is able to discriminate vowel sounds, and measuring the neural activity, we can look for where in the auditory brain invariant vowel representation might occur. The second part of this project involves reversibly silencing individual brain areas by cooling them. The principle of this technique is much the same was as using an ice pack to cool pain neurons in a bruised piece of skin. Small 'cryoloops' are implanted above auditory cortex in trained animals.This technique allows us to test whether particular brain areas are causally involved in vowel discrimination. The final part of this project investigates the role of visual information in auditory perception. It is well known that seeing a persons mouth movements while they talk to you enhances your ability to understand them - especially if you are listening in a very noisy room. When trying to pick out a quiet sound in a noisy background knowing when the sound is likely to occur also enhances your ability to correctly identify it. It has recently been shown that visual information is integrated into the very earliest auditory cortical areas. However, quite how this visual information shapes our auditory perception is unknown. The work in this proposal seeks to examine how visual information helps a trained animal to identify vowel sounds more accurately, whilst simultaneously examining how the visual stimulus influences the behaviour of neurons in auditory cortex. Inappropriate integration of auditory and visual information is postulated to underlie schizophrenic symptoms and understanding how informative visual stimuli influence auditory cortical activity will provide valuable insight into how sensory integration occurs in the healthy brain.Hearing impaired individuals most frequently suffer from an inability to effectively identify speech in noisy environments. Understanding how neurons are able to represent vowel identity robustly across a variety of listening conditions and noise environments will enhance hearing aid and cochlear implant design.</gtr:abstractText><gtr:technicalSummary>Failure to understand speech in a noisy environment is one of the principal complaints of the hearing impaired. Our remarkable ability to recognize and understand speech across many different speakers, voice pitches and listening conditions likely depends on the auditory brain extracting those acoustic cues which provide reliable information about a particular sound feature in order to form a neural representation which robustly identifies the stimulus regardless of task-irrelevant 'nuisance' variables. This proposal employs a systems neuroscience approach to examine how auditory cortex forms invariant neural representations of vowel identity enabling a listener to differentiate vowels such as /ae/ from /ih/ irrespective of the voice pitch or location in space, and how this is maintained despite changes in the background noise environment. Three specific questions will be addressed: (1) How (with what neural code) and where (in which cortical field) do neurons support invariant perception of vowel identity? (2) Which cortical fields are necessary for an animal to perform invariant vowel recognition? (3) What role does visual information play in helping us to 'hear better'? By simultaneously measuring spiking activity, local field potentials and neural oscillations in trained ferrets performing a vowel identification task we will examine the incidence and location of invariant vowel timbre encoding. We will explore the neural codes which might support invariant coding and use reversible inactivation techniques examining causal relationships between the activity in specific cortical fields and vowel discrimination behaviour. Lastly we will examine how visual information is integrated within auditory cortex in order to aid listening in difficult conditions. Whilst multisensory integration has been documented in early sensory cortices very few studies have sought to correlate physiological measures with simultaneous assessment of any multisensory behavioural advantage.</gtr:technicalSummary><gtr:potentialImpactText>The proposed work is a fundamental neuroscience research project and will produce key insights into the functional organisation of mammalian sensory pathways and the processing of sounds by biological systems. Discriminating speech sounds in a noisy environment is one of the principle complaints of hearing impaired listeners. Understanding the neural processing which underlies this ability in normal hearing brains will offer key insights into how to better design signal processors in cochlear implants and hearing aids. Similar advantages will be afforded to communication technologies; we need only consider the very substantial shortcomings of even the most artificial speech recognition systems to be reminded of the remarkable sophistication of the auditory system. Collaboration with ENT surgeons and audiologists both within and beyond Oxford University ensures that our work maintains a clinical focus. Dr Hartley, a clinician-scientist has recently developed a ferret cochlear implant model meaning that the neural processing insights we gain from our efforts to relate perception to neural firing can be used to generate testable hypothesis which can be implemented within this animal model. We have established collaborations within Oxford with ENT clinicians. We maintain active collaborations with the growing number of sensory neuroscience groups, both in the UK and USA, who use ferrets as an animal model. Through regular dialogue we will continue to share new data, techniques and methods to the benefit of all. An example of such collaborative endeavour is the ferret brain atlas which is being developed by neuroanatomist Dr Sussane Radtke-Schuller (based in Munich), in collaboration with the University of Maryland based group of Dr Shihab Shamma, and the Oxford group. This atlas is the first of its kind for this species and will provide a free online resource containing cytoarchitectonic data and high resolution structural MRI scans which will be available to assist the increasing number of scientists who are working with ferrets. We increase the impact of our work by participate in the Deafness Research UK's public outreach programs. In the past year we have had a number of school work experience students, and a Nuffield bursary student spend time in the lab in an attempt to encourage more school leavers to consider pursuing a career in biomedical science. We maintain an uptodate website which details our most recent work as well as our research goals and includes routes through which members of the public can contact us. The research in this proposal addresses fundamental neuroscience issues with little immediate commercial exploitation potential. Its benefits will be in enhancing our scant knowledge of how the healthy brain operates to process sounds. This knowledge will be, through publication and dissemination at international meetings, made available to engineers and others who will be able to apply what we learn about neural coding to improving the signal processing capabilities of cochlear implants, hearing aids and telecommunication devices. Whilst cochlear implants have been tremendously successful they are only an option for those individuals with an intact auditory nerve. A knowledge of the neural coding mechanisms within auditory cortex will guide the stimulation strategies and design of neural prosthesis which targets auditory centres in the brainstem or midbrain.</gtr:potentialImpactText><gtr:fund><gtr:end>2011-01-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2011-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>384541</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of North Carolina at Chapel Hill</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Department of Psychiatry</gtr:department><gtr:description>UNC Chapel Hill, North Carolina</gtr:description><gtr:id>25420287-88C8-4548-96AC-0F22663252DC</gtr:id><gtr:impact>None yet.</gtr:impact><gtr:outcomeId>545be94dc4f766.87628131-1</gtr:outcomeId><gtr:partnerContribution>We have a collaborative project which has been funded by HFSP and which commences this month.</gtr:partnerContribution><gtr:piContribution>We have a collaborative project which has been funded by HFSP and which commences this month.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Washington</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Prof Adrian KC Lee, University of Washington, Seattle</gtr:description><gtr:id>45DA3D96-18E5-4B4B-91E9-BAEDD5605478</gtr:id><gtr:impact>Publications: Bizley, Shinn-Cunningham and Lee 2012, J.Neurosci.
Manuscript under review (Maddox, Atilgan, Bizley, Lee).</gtr:impact><gtr:outcomeId>545be8b89f1445.39420020-1</gtr:outcomeId><gtr:partnerContribution>Our collaborators designed the original psychophysical paradigm which was run in parallel in our two labs. We are performing animal studies while our collaborators perform imaging - in both cases using the same stimuli</gtr:partnerContribution><gtr:piContribution>We have collaborated on a psychophysics project (currently under review - Maddox, Atilgan, Bizley and Lee), and continue to collaborate using invasive recordings in our animal model and non-invasive functional imaging in humans.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Auditory Scene Analysis Workshop HWK, Delmenhorst</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>4B9AF19D-C778-403F-AF42-8780925E7FDD</gtr:id><gtr:impact>Discussion and continuing collaboration</gtr:impact><gtr:outcomeId>545be33e915d28.84885996</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC documentary</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EA635BCD-ED51-44AC-989C-5870AC1FB363</gtr:id><gtr:impact>Participated in the making of a BBC4 documentary on sound &amp;quot;Soundwaves: the symphony of physics&amp;quot;. Developed demos, participated in filming both performing demos and being interviewed about hearing. Broadcast in March 2017.</gtr:impact><gtr:outcomeId>57e28d279b0d11.50241726</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/b08h5gk8</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>School visit ( Dr Challoner' High School)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>B0579E9B-1FAE-4DB8-B912-E7BDCF5BAAAD</gtr:id><gtr:impact>Cafe Scientific as part of Brain awareness week.</gtr:impact><gtr:outcomeId>58c2882c8c6403.65175597</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TEDx talk</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BC1CED4E-0AE5-456C-83FD-B59C5D0F3BB2</gtr:id><gtr:impact>Lots of discussion, &amp;gt;1000 views on youtube</gtr:impact><gtr:outcomeId>545a4f659144d2.19819773</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.youtube.com/watch?v=GWUVRjtlg1Y</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings have been presented at a variety of national and international meetings. We have published two review papers and have a paper detailing some behavioural work undergoing a second revision. Our neural findings are currently being prepared for submission for peer review and subsequent publication.
Our software and methods have been shared with a number of other researchers. In addition the methods that we developed for freely moving recordings have led to me being able to assisted ano</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>982D8A7B-11A5-4B0D-A6D8-2BF610AE5D4F</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54614ce59d85f2.40680018</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Software to interface with TDT equipment to control behavioural testing apparatus. This was developed by the post-doc and PI named on this grant (Town and Bizley) and has been distributed to at least two other laboratories.</gtr:description><gtr:id>D8987AB8-396D-4CDA-A77B-8E07E6E38A62</gtr:id><gtr:impact>No actual Impacts realised to date</gtr:impact><gtr:outcomeId>r-3513990625.2864666fd1dec4</gtr:outcomeId><gtr:title>GoFerret</gtr:title><gtr:type>software</gtr:type><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>043FED67-8B89-4A7E-AD91-35D06810C646</gtr:id><gtr:title>Acute Inactivation of Primary Auditory Cortex Causes a Sound Localisation Deficit in Ferrets.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4bdc6edb43181df7897f4bb166462cda"><gtr:id>4bdc6edb43181df7897f4bb166462cda</gtr:id><gtr:otherNames>Wood KC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>58c28752795d08.02470048</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>351C9022-F793-477C-B24B-B539EC96ABCA</gtr:id><gtr:title>Auditory cortical processing in real-world listening: the auditory system going real.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c0e3d88ec34f0ede1e31b15375e2c767"><gtr:id>c0e3d88ec34f0ede1e31b15375e2c767</gtr:id><gtr:otherNames>Nelken I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>545cfefd0573e3.66999294</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A09071A-B8DE-46D5-B4C0-1CF7FE60A967</gtr:id><gtr:title>The what, where and how of auditory-object perception.</gtr:title><gtr:parentPublicationTitle>Nature reviews. Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b7cc714e4bb39b109bfedce9a878dce6"><gtr:id>b7cc714e4bb39b109bfedce9a878dce6</gtr:id><gtr:otherNames>Bizley JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1471-003X</gtr:issn><gtr:outcomeId>545a4a03ba5250.57781754</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>331658AC-FDE2-42B9-823D-E4A038E8FFA3</gtr:id><gtr:title>Spectral timbre perception in ferrets: discrimination of artificial vowels under different listening conditions.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b7cc714e4bb39b109bfedce9a878dce6"><gtr:id>b7cc714e4bb39b109bfedce9a878dce6</gtr:id><gtr:otherNames>Bizley JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>545a4a036e8c70.15205051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>91CDC4A8-5DBD-4A90-8649-DDE2445D3A9A</gtr:id><gtr:title>Auditory cortex represents both pitch judgments and the corresponding acoustic cues.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b7cc714e4bb39b109bfedce9a878dce6"><gtr:id>b7cc714e4bb39b109bfedce9a878dce6</gtr:id><gtr:otherNames>Bizley JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>545a4a03959be8.71332340</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9FB1C818-B6D5-4BB1-B579-9015DA5B1973</gtr:id><gtr:title>Visual sensitivity is a stronger determinant of illusory processes than auditory cue parameters in the sound-induced flash illusion.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f27c4d027595a9a3c3c2d978fcf7075f"><gtr:id>f27c4d027595a9a3c3c2d978fcf7075f</gtr:id><gtr:otherNames>Kumpik DP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>545a4a04203da0.59125292</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>77D7049F-9BB8-40CD-A03E-538877BB5178</gtr:id><gtr:title>The role of spectral cues in timbre discrimination by ferrets and humans.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08fbf809767ea3e68e53e6e784d018f6"><gtr:id>08fbf809767ea3e68e53e6e784d018f6</gtr:id><gtr:otherNames>Town SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>5675e4e614a19</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5785F1F3-4BC3-4175-BCE3-C1C887406ED9</gtr:id><gtr:title>Neural and behavioral investigations into timbre perception.</gtr:title><gtr:parentPublicationTitle>Frontiers in systems neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08fbf809767ea3e68e53e6e784d018f6"><gtr:id>08fbf809767ea3e68e53e6e784d018f6</gtr:id><gtr:otherNames>Town SM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1662-5137</gtr:issn><gtr:outcomeId>545a4a03e2c576.65028770</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/H016813/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>E84B386D-B2B0-4E06-820D-A4BC2165AF83</gtr:id><gtr:grantRef>BB/H016813/1</gtr:grantRef><gtr:amount>384541.36</gtr:amount><gtr:start>2011-01-01</gtr:start><gtr:end>2011-01-01</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>78B4512B-DDAE-4A23-8B88-3C4FD5460E16</gtr:id><gtr:grantRef>BB/H016813/2</gtr:grantRef><gtr:amount>384541.36</gtr:amount><gtr:start>2011-10-10</gtr:start><gtr:end>2014-10-09</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>