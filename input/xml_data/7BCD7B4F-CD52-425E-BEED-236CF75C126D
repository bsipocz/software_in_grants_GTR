<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/252189B3-135F-4D99-BFB7-0799A34A409C"><gtr:id>252189B3-135F-4D99-BFB7-0799A34A409C</gtr:id><gtr:name>Loughborough University</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/ED6ACD61-0140-4C05-B93D-2C6A42CA753B"><gtr:id>ED6ACD61-0140-4C05-B93D-2C6A42CA753B</gtr:id><gtr:name>University of Salford</gtr:name><gtr:address><gtr:line1>University Crescent</gtr:line1><gtr:line4>Salford</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>M5 4WT</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/252189B3-135F-4D99-BFB7-0799A34A409C"><gtr:id>252189B3-135F-4D99-BFB7-0799A34A409C</gtr:id><gtr:name>Loughborough University</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/ED6ACD61-0140-4C05-B93D-2C6A42CA753B"><gtr:id>ED6ACD61-0140-4C05-B93D-2C6A42CA753B</gtr:id><gtr:name>University of Salford</gtr:name><gtr:address><gtr:line1>University Crescent</gtr:line1><gtr:line4>Salford</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>M5 4WT</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0"><gtr:id>09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0</gtr:id><gtr:firstName>Josef</gtr:firstName><gtr:surname>Kittler</gtr:surname><gtr:orcidId>0000-0002-8110-9205</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B1CDD839-FF89-4EBA-8F72-453446B6FE49"><gtr:id>B1CDD839-FF89-4EBA-8F72-453446B6FE49</gtr:id><gtr:firstName>Wenwu</gtr:firstName><gtr:surname>Wang</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH050000%2F1"><gtr:id>7BCD7B4F-CD52-425E-BEED-236CF75C126D</gtr:id><gtr:title>Audio and Video Based Speech Separation for Multiple Moving Sources Within a Room Environment</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H050000/1</gtr:grantReference><gtr:abstractText>Human beings have developed a unique ability to communicate within a noisy environment, such as at a cocktail party. This skill is dependent upon the use of both the aural and visual senses together with sophisticated processing within the brain. To mimic this ability within a machine is very challenging, particularly if the humans are moving, such as in a teleconferencing context, when human speakers are walking around a room. In the field of signal processing researchers have developed techniques to separate one speech signal from a mixture of such signals, as would be measured by a number of microphones, on the basis of only audio information with the assumption that the humans are static and typically no more than two humans are within the room. Such approaches have generally been found to fail, however, when the human speakers are moving and when there are more than two in number. Fundamentally new approaches are therefore necessary to advance the state-of-the-art in the field. Professor Chambers and his team at Loughborough University were the first in the UK to propose a new approach on the basis of combined audio and video processing to solve the source separation problem, but their preliminary approach identified major challenges in audio-visual speaker localization, tracking and separation which must be solved to provide a practical solution for speech separation for multiple moving sources within a room environment. These findings motivate this new project in which world-leading teams at the University of Surrey, led by Professor Kittler, and at the GIPSA Lab, Grenoble, France, headed by Professor Jutten, are ready to work with Professor Chambers and his team at Loughborough University to advance the state-of-the-art in the field.In this new project, two postdoctoral researchers will be employed, one at Loughborough and another at Surrey. The first will focus on the development of fundamentally new speech source separation algorithms for moving speakers by using geometrical room acoustic (for example location and number of sources, descriptions of their movement) information provided by the second researcher. The research team at Grenoble will provide technical guidance on the basis of their considerable experience in source separation throughout the project and will work on providing an acoustic noise model for the room environment which will also aid the speech separation process. To achieve these tasks, frequency domain based beamforming algorithms will be developed which exploit microphone arrays having more microphones than speakers so that new data independent superdirective robust beamformer design methods can be exploited using mathematical convex optimization. Additionally, further geometic information will be exploited to introduce robustness to errors in the localization information describing the desired source and the interference. To improve the localization information an array of collaborative cameras will be used and both audio and visual information will be used. Advanced methods from particle filtering and probabilistic data association will be exploited for improving the tracking performance. Finally, visual voice activity detection will be used to determine the active sources within the beamforming operations. We emphasize that this work is not implementation-driven, so computational complexity for real-time realization will not be a focus; this would be the subject of a future project.All of the new algorithms will be evaluated both in terms of objective and subjective performance measures on labelled audio and visual datasets acquired at Loughbourgh and Surrey, and from the CHIL seminar room at the Karlsruhe University (UKA), Germany. To ensure this pioneering work has maximum impact on the UK and international academic and research communities all the algorithms and datasets will be made available through the project website.</gtr:abstractText><gtr:fund><gtr:end>2013-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>359529</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Surrey</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>MILES</gtr:description><gtr:id>50E01DCA-9992-4A55-9F4B-3D4C3741F1CC</gtr:id><gtr:outcomeId>b9d42844b9d42858-1</gtr:outcomeId><gtr:piContribution>Internal inter-department collaboration was initiated with Department of Computing and School of Psychology, and a small feasibility study fund was awarded by the MILES (Models and Mathematics in Life and Social Sciences) project (12/2012-12/2013).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Loughborough University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Loughborough Univeristy</gtr:description><gtr:id>CF8232E7-5806-4DC0-9FDC-19821A587315</gtr:id><gtr:outcomeId>b963a0e2b963a0f6-1</gtr:outcomeId><gtr:piContribution>Enhanced collaborations with Loughborough University which have led to the success of the bid for the EPSRC/Dstl funded project ?Signal processing solutions for a networked battlespace?.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Salford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Spatial Audio</gtr:description><gtr:id>CB6EB72F-2718-423E-8137-78546F38E441</gtr:id><gtr:outcomeId>b9d6dad0b9d6daee-1</gtr:outcomeId><gtr:piContribution>External collaborations with University of Southampton and University of Salford were established during the project period, which have contributed to the design of the work package 3 of the newly funded EPSRC project &amp;quot;S3A: future spatial audio for an immersive listener experience at home&amp;quot;, where robust algorithms for audio-visual audio object separation, localisation and tracking will be studied.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Southampton</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Spatial Audio</gtr:description><gtr:id>7BC389A5-7B9B-4CFC-9EA9-BFBA69F8737E</gtr:id><gtr:outcomeId>b9d6d80ab9d6d81e-1</gtr:outcomeId><gtr:piContribution>External collaborations with University of Southampton and University of Salford were established during the project period, which have contributed to the design of the work package 3 of the newly funded EPSRC project &amp;quot;S3A: future spatial audio for an immersive listener experience at home&amp;quot;, where robust algorithms for audio-visual audio object separation, localisation and tracking will be studied.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>5800000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>S3A: future spatial audio for an immersive listener experience at home</gtr:description><gtr:end>2017-11-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>6EF8B2E4-6892-4C9A-AABB-7FEBD8FA99DB</gtr:id><gtr:outcomeId>5edde7005edde714</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3800000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Signal processing for the networked battlespace</gtr:description><gtr:end>2018-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/K014307/1</gtr:fundingRef><gtr:id>A6133641-2F1B-4446-BD73-87E5838D7633</gtr:id><gtr:outcomeId>5edbfe685edbfe7c</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>6104265</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC Programme Grant</gtr:description><gtr:end>2020-12-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N007743/1</gtr:fundingRef><gtr:id>78E417A8-4E54-4ED0-8C1F-5AA6C34FC9D9</gtr:id><gtr:outcomeId>56dd8b62c60326.17984380</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The major original objectives of this joint project between the Advanced Signal Processing group at Loughborough University and the Centre for Vision, Speech and Signal Processing at Surrey University were to



? design novel audio and video based advanced signal processing algorithms for speech separation of multiple active moving speakers by exploiting additional geometrical room acoustic information within the framework of convex-optimization-based beamformer design.

? propose multi-model solutions for human detection, localization, and tracking, to include situations where the sources exhibit complex motions including occlusions and interactions, within a room environment.



We have successfully progressed these topics and our key findings have been:-



1. A novel multimodal source separation approach was proposed for physically moving and stationary sources which exploits a circular microphone array, multiple video cameras, robust spatial beamforming and time-frequency masking. The challenge of separating moving sources, including higher reverberation time (RT) even for physically stationary sources, is that the mixing filters are time varying; as such the unmixing filters should also be time varying but these are difficult to determine from only audio measurements. Therefore in the proposed approach, the visual modality was used to facilitate the separation for both stationary and moving sources. The movement of the sources was detected by a three-dimensional tracker based on a Markov Chain Monte Carlo particle filter. The audio separation was performed by a robust least squares frequency invariant data-independent beamformer. The uncertainties in source localisation and direction of arrival information obtained from the 3D video-based tracker were controlled by using a convex optimisation approach in the beamformer design. In the final stage, the separated audio sources were further enhanced by applying a binary time-frequency masking technique in the cepstral domain. Experimental results showed that by using the visual modality, the proposed algorithm cannot only achieve performance better than conventional frequency-domain source separations algorithms, but also provide acceptable separation performance for moving sources.



2. A video-aided model-based source separation algorithm for a two-channel reverberant recording in which the sources are assumed static was designed. By exploiting cues from video, we first localized individual speech sources in the enclosure and then estimated their directions. The interaural spatial cues, the interaural phase difference and the interaural level difference, as well as the mixing vectors were probabilistically modelled. The models made use of the source direction information and were evaluated at discrete time-frequency points. The model parameters were refined with the well-known expectation-maximization (EM) algorithm. The algorithm generated time-frequency masks that were used to reconstruct the individual sources. Simulation results showed that by utilizing the visual modality the proposed algorithm could produce better time-frequency masks thereby giving improved source estimates. We provided experimental results to test the proposed algorithm in different scenarios and provided comparisons with both other audio-only and audio-visual algorithms and achieved improved performance both on synthetic and real data. We also included dereverberation based pre-processing in our algorithm in order to suppress the late reverberant components from the observed stereo mixture and further enhance the overall output of the algorithm. This advantage makes our algorithm a suitable candidate for use in under-determined highly reverberant settings where the performance of other audio-only and audio-visual methods is limited.



3. Novel solutions to the following challenges in visual tracking of multiple human speakers in an office environment were proposed: (1) robust and computationally efficient modelling and classification of the changing appearance of the speakers in a variety of different lighting conditions and camera resolutions; (2) dealing with full or partial occlusions when multiple speakers cross or come into very close proximity; (3) automatic initialisation of the trackers, or re-initialisation when the trackers have lost lock caused by e.g. the limited camera views. First, we developed new algorithms for appearance modelling of the moving speakers based on dictionary learning (DL), using an off-line training process. In the tracking phase, the histograms (coding coefficients) of the image patches derived from the learned dictionaries were used to generate the likelihood functions based on Support Vector Machine(SVM) classification. This likelihood function was then used in the measurement step of the classical particle filtering (PF) algorithm. To improve the computational efficiency of generating the histograms, a soft voting technique based on approximate Locality-constrained Soft Assignment (LcSA) was proposed to reduce the number of dictionary atoms (codewords) used for histogram encoding. Second, an adaptive identity model was proposed to track multiple speakers whilst dealing with occlusions. This model was updated online using Maximum a Posteriori (MAP) adaptation, where we controlled the adaptation rate using the spatial relationship between the subjects. Third, to enable automatic initialisation of the visual trackers, we exploited audio information, the Direction of Arrival (DOA) angle, derived from microphone array recordings. Such information provided, a priori, the number of speakers and constrained the search space for the speaker's faces. The proposed system was tested on a number of sequences from three publicly available and challenging data corpora (AV16.3, EPFL pedestrian data set and CLEAR) with up to five moving subjects.



4. A novel method for determining coarse head pose orientation purely from audio information, exploiting the direct to reverberant speech energy ratio (DRR) within a reverberant room environment was developed. Our hypothesis was that a speaker facing towards a microphone will have a higher DRR and a speaker facing away from the microphone will have a lower DRR. This method has the advantage of actually exploiting the reverberations within a room rather than trying to suppress them. This also has the practical advantage that most everyday enclosed spaces, such as meeting rooms or offices are highly reverberant environments. In order to test this hypothesis we also collected a new data set featuring 39 subjects adopting 8 different head poses in 4 different room positions captured with a 16 element microphone array. We believe that this data set is unique and will make a significant contribution to further work in the area of audio head pose estimation. 



9th December 2013</gtr:description><gtr:exploitationPathways>The research results feed into a follow up EPSRC/dstl project. DSTL and the industrial partners of this Programme Grant will provide a route to exploitation of the research results. The industrial partners include Selex, Thales, Texas Instruments, Mathworks.



Apart from the above, a potential use of the signal processing solutions is in next generation hearing aids. This would have both commercial and societal impact. Companies interested in this technology have been contacted to gauge their interest. -The research results will be used by the investigators in new research.



-The research results will be used by the academic community to build on with a view to developing new solutions for the blind source separation problem.



-The research results feed into a follow up EPSRC/dstl project. DSTL and the industrial partners of this Programme Grant will provide a route to exploitation of the research results.</gtr:exploitationPathways><gtr:id>22646A32-1155-4245-8413-27B94966865A</gtr:id><gtr:outcomeId>r-997319818.47185627763a2ec</gtr:outcomeId><gtr:sectors><gtr:sector>Electronics</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A new dataset was recorded in the audio media engineering lab for evaluating the head pose estimation algorithm. More than 39 subjects attended in the data collection. To our knowledge, this dataset is the first dataset on real audio recordings for head pose recognition tasks. The dataset may be accessible from http://www.cvssp.org/avbss/</gtr:description><gtr:id>CE05AAA6-5194-4AC0-83B7-8E00E739CE74</gtr:id><gtr:impact>The published data was used in experiments conducted by peer groups.</gtr:impact><gtr:outcomeId>r-8818083581.34523466d88784</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Audio-Visual Data Set</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.cvssp.org/avbss/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>866DE4B0-30A4-4BCD-9BAB-64B3B75156EE</gtr:id><gtr:title>Algorithm to estimate biometric performance change over time</gtr:title><gtr:parentPublicationTitle>IET Biometrics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/69f40e7b3b679711a5a7b68fef3f7eae"><gtr:id>69f40e7b3b679711a5a7b68fef3f7eae</gtr:id><gtr:otherNames>Poh N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd91ada6efa0.62655207</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>131959E8-1ACE-4652-A73F-DEEC8EE604F4</gtr:id><gtr:title>Source Separation of Convolutive and Noisy Mixtures Using Audio-Visual Dictionary Learning and Probabilistic Time-Frequency Masking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d05f05fa875151</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>33F4F7E0-5A0A-4F35-B316-80686D105285</gtr:id><gtr:title>Face Spoofing Detection Based on Multiple Descriptor Fusion Using Multiscale Dynamic Binarized Statistical Image Features</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Information Forensics and Security</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d92cf96059fcb25b802ebc091336e554"><gtr:id>d92cf96059fcb25b802ebc091336e554</gtr:id><gtr:otherNames>Arashloo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd91ade5d5f6.59526316</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>58E6BF64-A23A-4D31-8633-44B0C817FFCF</gtr:id><gtr:title>Robust Multi-Speaker Tracking via Dictionary Learning and Identity Modeling</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2545b516d74f82259d77b9b02e19165"><gtr:id>d2545b516d74f82259d77b9b02e19165</gtr:id><gtr:otherNames>Barnard M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_71571823411369d846</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4D8E5FEF-B8AB-4E12-9F43-FAFA10DA503E</gtr:id><gtr:title>Multimodal (audio-visual) source separation exploiting multiple speaker tracking, robust beamforming and time frequency masking</gtr:title><gtr:parentPublicationTitle>IET Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/08092a6a3c0d73aedd3563c706a75145"><gtr:id>08092a6a3c0d73aedd3563c706a75145</gtr:id><gtr:otherNames>Sayed Naqvi (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_88717180561369fe52</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B057EC82-F479-4A9D-8462-77E4FDA35C0E</gtr:id><gtr:title>Mean-Shift and Sparse Sampling-Based SMC-PHD Filtering for Audio Informed Visual Speaker Tracking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/704fc7816ca45ac4c40425b2b2aa38d5"><gtr:id>704fc7816ca45ac4c40425b2b2aa38d5</gtr:id><gtr:otherNames>Kilic V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a5dbe1d196d5.01849676</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>50D89F71-92A4-44AB-8FA8-343E3C0193B2</gtr:id><gtr:title>Face Detection, Bounding Box Aggregation and Pose Estimation for Robust Facial Landmark Localisation in the Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e20f649187b603b7d0c7bf10989944f4"><gtr:id>e20f649187b603b7d0c7bf10989944f4</gtr:id><gtr:otherNames>Feng Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9dfa1d6a7540.58458093</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>078C8C17-5027-4FBA-A373-4D7686D8C7F0</gtr:id><gtr:title>Cascaded Collaborative Regression for Robust Facial Landmark Detection Trained Using a Mixture of Synthetic and Real Images With Dynamic Weighting.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b327f822bf0873e275194d90b5ad767d"><gtr:id>b327f822bf0873e275194d90b5ad767d</gtr:id><gtr:otherNames>Feng ZH</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>56dd91adc1cf92.86008923</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>85A751A2-BE63-4510-96A8-F43EF2032F12</gtr:id><gtr:title>Multi-label classification using stacked spectral kernel discriminant analysis</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fce242c6484754ca439b336688f80b07"><gtr:id>fce242c6484754ca439b336688f80b07</gtr:id><gtr:otherNames>Tahir M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd91ad887965.54047314</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CCDAAC04-AAEC-45C8-8615-BC5EEBC6F054</gtr:id><gtr:title>Audio head pose estimation using the direct to reverberant speech ratio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2545b516d74f82259d77b9b02e19165"><gtr:id>d2545b516d74f82259d77b9b02e19165</gtr:id><gtr:otherNames>Barnard M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d0580583896524</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>686A51B9-969F-439E-8DCA-F32626A66FA9</gtr:id><gtr:title>Joint Mixing Vector and Binaural Model Based Stereo Source Separation</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af463c0336f79ef0dbc212494ac75443"><gtr:id>af463c0336f79ef0dbc212494ac75443</gtr:id><gtr:otherNames>Alinaghi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f977977bf61ab2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15F18AC6-FBF6-4DA8-83BA-1B27A08E1879</gtr:id><gtr:title>Report on the FG 2015 Video Person Recognition Evaluation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/122ab9126b0ff02ace2b7d48f8e73964"><gtr:id>122ab9126b0ff02ace2b7d48f8e73964</gtr:id><gtr:otherNames>Beveridge J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5a9dfa1e26e0f1.01632865</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7C96C254-79A8-451D-89A5-D5CFD4A44CB3</gtr:id><gtr:title>Fitting 3D Morphable Face Models using local features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a3bd0ce629968c023bfbaae72be9a7f"><gtr:id>6a3bd0ce629968c023bfbaae72be9a7f</gtr:id><gtr:otherNames>Huber P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5a9dfa1decb615.76205906</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B3996861-E870-4592-BFAF-61FD47DA4D8C</gtr:id><gtr:title>Sparse coding with adaptive dictionary learning for underdetermined blind speech separation</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd9c82d54f841388e52c6a8216d950f"><gtr:id>afd9c82d54f841388e52c6a8216d950f</gtr:id><gtr:otherNames>Xu T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f95395308e076f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D7001C53-DDB8-4111-A96C-1FA0C674F09C</gtr:id><gtr:title>A dictionary learning approach to tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2545b516d74f82259d77b9b02e19165"><gtr:id>d2545b516d74f82259d77b9b02e19165</gtr:id><gtr:otherNames>Barnard M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>doi_53d05805831ebba1</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9884A051-1596-43B7-9242-D8817F85BDE9</gtr:id><gtr:title>Random Cascaded-Regression Copse for Robust Facial Landmark Detection</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e20f649187b603b7d0c7bf10989944f4"><gtr:id>e20f649187b603b7d0c7bf10989944f4</gtr:id><gtr:otherNames>Feng Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd91ae0ea423.87007881</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D002BE97-07E0-4547-B142-5248D30097CD</gtr:id><gtr:title>Adaptive particle filtering approach to audio-visual tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2839cf02c2e155d3d58231bf96ed0e1a"><gtr:id>2839cf02c2e155d3d58231bf96ed0e1a</gtr:id><gtr:otherNames>Volkan Kilic (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_332088302314097d60</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>234111F9-6250-497A-8159-533F624884D0</gtr:id><gtr:title>Audio constrained particle filter based visual tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/704fc7816ca45ac4c40425b2b2aa38d5"><gtr:id>704fc7816ca45ac4c40425b2b2aa38d5</gtr:id><gtr:otherNames>Kilic V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d05805836ad464</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9B4B622-E4EC-45AA-A709-3CE93F6EE427</gtr:id><gtr:title>Robust multi-camera audio-visual tracking</gtr:title><gtr:parentPublicationTitle>Proceedings of the 11th Annual UK Workshop on Computational Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fe043045e251a0fb9f8c033752b3f04d"><gtr:id>fe043045e251a0fb9f8c033752b3f04d</gtr:id><gtr:otherNames> Steven Grima (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_32133374141407833e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>72792E4D-B02B-4F34-90D4-54DDB5E109D5</gtr:id><gtr:title>Audio Assisted Robust Visual Tracking With Adaptive Particle Filtering</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/704fc7816ca45ac4c40425b2b2aa38d5"><gtr:id>704fc7816ca45ac4c40425b2b2aa38d5</gtr:id><gtr:otherNames>Kilic V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f0c43de44</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AF401593-2B3B-4A6C-8EF4-530E37BB4DD6</gtr:id><gtr:title>Audio-visual face detection for tracking in a meeting room environment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bb988954257ac5688efa5c14d985af7f"><gtr:id>bb988954257ac5688efa5c14d985af7f</gtr:id><gtr:otherNames>Josef Kittler (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_138977689213e5ef12</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DDCCE3F8-4EA3-4E62-B49E-B649D43E9C36</gtr:id><gtr:title>Reverberant speech separation based on audio-visual dictionary learning and binaural cues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0182-4</gtr:isbn><gtr:outcomeId>doi_53d05c05c79caed8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EEAC000-F111-4670-A38B-43320D79978F</gtr:id><gtr:title>Video-Aided Model-Based Source Separation in Real Reverberant Rooms</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/614ab942f7b5ca90143931e5d006b3f2"><gtr:id>614ab942f7b5ca90143931e5d006b3f2</gtr:id><gtr:otherNames>Khan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f9539530981d01</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H050000/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>