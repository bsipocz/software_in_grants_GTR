<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3DCB489B-CB83-4E36-A3C4-B550B305092B"><gtr:id>3DCB489B-CB83-4E36-A3C4-B550B305092B</gtr:id><gtr:name>James I University (Jaume I)</gtr:name><gtr:address><gtr:line1>Avd Sos Vaynat s/n</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/36712AC7-25CB-47F2-8E69-8F154ED3C9CD"><gtr:id>36712AC7-25CB-47F2-8E69-8F154ED3C9CD</gtr:id><gtr:name>Centre for Research and Technology Hellas (CERTH)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3DCB489B-CB83-4E36-A3C4-B550B305092B"><gtr:id>3DCB489B-CB83-4E36-A3C4-B550B305092B</gtr:id><gtr:name>James I University (Jaume I)</gtr:name><gtr:address><gtr:line1>Avd Sos Vaynat s/n</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/36712AC7-25CB-47F2-8E69-8F154ED3C9CD"><gtr:id>36712AC7-25CB-47F2-8E69-8F154ED3C9CD</gtr:id><gtr:name>Centre for Research and Technology Hellas (CERTH)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/8A3A695B-82C9-43E0-A57B-16B07669B9CD"><gtr:id>8A3A695B-82C9-43E0-A57B-16B07669B9CD</gtr:id><gtr:firstName>Ioannis</gtr:firstName><gtr:surname>Patras</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG033935%2F1"><gtr:id>DFDBB9FC-E958-426C-8DAC-BBBFDBB079B6</gtr:id><gtr:title>Recognition and Localisation of Human Actions in Image Sequences</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G033935/1</gtr:grantReference><gtr:abstractText>The explosion in the amount of generated and distributed digital visual data that we nowadays witness can only be paralleled to the similar explosion in the amount of textual data that has been witnessed the decade before. However, while retrieval based on textual information made great progress and resulted in commercially usable search engines (e.g. Google, Yahoo), vision-based retrieval of multimedia material remains an open research question. As the amount of produced and distributed videos increases at an unprecedented pace, the significance of having efficient methods for content-based indexing in terms of the depicted actions can hardly be overestimated. In particular in the domain of analysis of human motion progress is expected to boost applications in human computer interaction, health care, surveillance, computer animation and games, and multimedia retrieval. However, mapping low level visual descriptors to high level action/object models is open problem and the analysis faces major challenges to the degree that the analysed image sequence exhibits large variability in appearance and the spatiotemporal structure of the actions, occlusions, cluttered backgrounds and large motions. In addition learning structure and appearance models is hindered by the fact that segmentation and annotation for the creation of training datasets are onerous tasks. For these reasons, there is a great incentive for the development of recognition and localisation methods that can either learn from few annotated examples or in a way that minimizes the amount of required manual segmentation and annotation.This project will build on recent development in Computer Vision and Pattern Recognition in order to develop methods for recognition and localisation of human and animal action categories in image sequences. Once trained, the methods should be able to detect and localise in a previously unknown image sequence, all the actions that belong to one of the known categories. The methods will allow learning the models in an incremental way starting from few examples and will allow computer assisted manual interaction using appropriate interfaces in order to facilitate model refinement. The methodologies will allow training the models in image sequences in which there is significant background clutter, that is in the presence of multiple objects/actions in the scene and moving cameras. No prior knowledge of the anatomy of the human body is a-priori considered, and therefore the models will be able to identify a large class of action categories, including facial/hand/body actions, animal motion, as well as interaction between humans and objects in their environment (such as drinking a glass of water).</gtr:abstractText><gtr:fund><gtr:end>2012-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>340932</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Centre for Research and Technology Hellas (CERTH)</gtr:collaboratingOrganisation><gtr:country>Greece, Hellenic Republic</gtr:country><gtr:description>Collaboration with Institute on Telematics and Informatics</gtr:description><gtr:id>0BAA91DE-8BD9-445A-9C17-12A7E027200E</gtr:id><gtr:impact>In the period 2009 - 2015 the collaboration has resulted in 25 publications</gtr:impact><gtr:outcomeId>56d58143adac29.59526319-1</gtr:outcomeId><gtr:partnerContribution>Informatics and Telematics Institute (ITI-CERTH, Greece) funded the salaries and paid the fees of the researchers, provides equipment and travel costs and co-supervision of the research.</gtr:partnerContribution><gtr:piContribution>Within the framework of a Doctorate Programme that I initiated, Informatics and Telematics Institute (ITI-CERTH, Greece) funded the salaries and paid the fees of several researchers enrolled as PhD students in QMUL under my supervision. 2 students have already graduated, two will graduate until 2017 and 1 will enrol in Spring 2017.

I am co-supervisor of the students.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Universitat Jaume I</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:description>Academic Visit of Dr. Javier Trevor</gtr:description><gtr:id>5B421439-E8D7-4DFF-8088-097FD99CC964</gtr:id><gtr:outcomeId>b972065ab972066e-1</gtr:outcomeId><gtr:piContribution>Dr. Javier Trevor, an academic at Universitat Jaume I, Spain, collaborated with me during a 6 month visit at QMUL.

The visit was funded by an award obtained by the Spanish Ministry of Education and Research. The related proposal was entitled:

Human Action Recognition with partial and hidden information</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Imperial College London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Imperial College</gtr:description><gtr:id>D4638302-2230-43D1-967E-AF0D796933CE</gtr:id><gtr:outcomeId>b972a8f8b972a916-1</gtr:outcomeId><gtr:piContribution>Ongoing collaboration with the group of Prof. Pantic in facial and body gesture analysis that resulted in several publications. Joint supervision of Antonis Oikonomopoulos which resulted in several papers in action recognition.

Collaboration in pose-invariant facial expression recognition in the framework of the work of Ognjen Rudovic</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>150000</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Direct Industrial Funding (from Yamaha Motors Ltd)</gtr:description><gtr:end>2016-11-02</gtr:end><gtr:fundingOrg>Yamaha Motors</gtr:fundingOrg><gtr:id>1DB1E8E0-F35D-43E9-B1BD-FD4902BDF435</gtr:id><gtr:outcomeId>56d57deb4cda93.80790917</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2014-11-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work on localisation of human actions, and in particular the works on part-based models laid the foundations for research that led to collaboration with Yamaha Motors Ltd. That collaboration led to a follow up project with Yamaha, and a submitted patent for a pedestrian detection system (Spring2016).

The work on human motion analysis is also supportive of a recently awarded InnovateUK project (SensingFeeling) that aims to monitor and access the affective state of people in the retail environment.</gtr:description><gtr:id>5AEC0D43-334F-4AF0-8473-6F65FEFD9361</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d5ad9ef2cb28.79199120</gtr:outcomeId><gtr:sector>Creative Economy,Leisure Activities, including Sports, Recreation and Tourism,Manufacturing, including Industrial Biotechology,Transport</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project has developed machine learning and computer vision methods for analysis of facial expressions and body gestures so as to recognise behaviour and actions of human in natural environments. We have advanced the state of the art and have shown that machines are getting better at performing such tasks.</gtr:description><gtr:exploitationPathways>1) Content providers and distributors could utilise the results on facial expression analysis for inferring people's affective and cognitive state while watching films and TV programs.

2) Robot manufacturers could utilise the methods for facial expression analysis and gesture recognition for natural interfaces.

3) Gaming companies could use both the pose estimation and the gesture recognition results for game control.

4) Applications like interactive programs that guide people through their daily exercises could be built based on the technology for gesture recognition and pose estimation.
 The research can be utilised by companies and academic institutions that are interested in behaviour analysis. This includes analysis of human behaviour for assisted living (e.g of elderly people), or restaurants/shops that monitor costumer behaviour and/or preferences and/or interaction with products and/or reaction to provided services.



Our work on facial expression analysis can be used for analysing human reactions (e.g. affective states) to presentation of multimedia content. In the later direction, and in collaboration with partners from the FP7 Network of excellence Petamedia, results are already obtained and published.



Digital media companies can also utilise the findings. Specifically, the action spotting and action recognition algorithms developed in this project can be used for video annotation and/or retrieval system for better managing digital media.



Academic researchers can also utilised the theoretical findings of our work. In particular our works on tensor-based regression/classification or our works on max-margin non negative matrix factorisation are core pattern recognition methodologies with applications beyond the field of Computer Vision.



The dissemination efforts include a dedicate website (http://www.eecs.qmul.ac.uk/~ioannisp/ralis.htm)



Source code for several of our methods is provided online (http://www.eecs.qmul.ac.uk/~ioannisp/source.htm)</gtr:exploitationPathways><gtr:id>E678786F-7201-43C7-84D2-C2C297B6436C</gtr:id><gtr:outcomeId>r-701764682.961408777609db8</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.eecs.qmul.ac.uk/~ioannisp/ralis.htm</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Methods for data analysis, classification and regression.</gtr:description><gtr:id>13BF562F-53EE-4D73-81FB-4C9080F7156C</gtr:id><gtr:impact>The code has been used by a few researchers worldwide.</gtr:impact><gtr:outcomeId>56d5914141f584.14572371</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Machine Learning codes</gtr:title><gtr:type>Data analysis technique</gtr:type><gtr:url>http://www.eecs.qmul.ac.uk/~ioannisp/source.htm</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This code implements Support Tensor Regression (STR) as presented in Weiwei Guo, Irene Kotsia and Ioannis Patras, &amp;quot;Tensor Learning for Regression&amp;quot;, in IEEE Transactions on Image Processing, 2011.</gtr:description><gtr:id>F8E8704E-6BFF-4455-AFF1-59650BE1786B</gtr:id><gtr:impact>The paper has been cited by 30 researchers since 2012</gtr:impact><gtr:outcomeId>r-1555253380.49535946fd4d3f4</gtr:outcomeId><gtr:title>Tensor Regression</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.eecs.qmul.ac.uk/~ioannisp/source.htm</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This code implements the paper Max-Margin Semi-NMF (MNMF) as presented in Vijay Kumar, Irene Kotsia and Ioannis Patras, &amp;quot;Max-Margin Semi-NMF&amp;quot;, in BMVC 2011.</gtr:description><gtr:id>4FF949F3-3C28-498C-B281-D9AA7FA53C28</gtr:id><gtr:impact>The paper has been cited 10 times since 2012.</gtr:impact><gtr:outcomeId>r-879687239.30389366fd4cf76</gtr:outcomeId><gtr:title>Max-Margin Semi-NMF</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.eecs.qmul.ac.uk/~ioannisp/source.htm</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This code implements Support Tucker Machines (STuMs) and Sw-STuMs, as presented in Irene Kotsia and Ioannis Patras, &amp;quot;Support Tucker Machines&amp;quot;, in CVPR 2011, 2011.</gtr:description><gtr:id>3AB26A3B-F42A-4AB6-A961-B690F65017D5</gtr:id><gtr:impact>The paper has been cited 22 times since 2012.</gtr:impact><gtr:outcomeId>r-6838862958.2141186fd4d1ce</gtr:outcomeId><gtr:title>Support Tucker Machines</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.eecs.qmul.ac.uk/~ioannisp/source.htm</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D0666C51-F1E3-480A-A921-4A6B4A42D0D8</gtr:id><gtr:title>Support tucker machines</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec471a560e55c104b1fdcc758015f5b5"><gtr:id>ec471a560e55c104b1fdcc758015f5b5</gtr:id><gtr:otherNames>Kotsia I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0394-2</gtr:isbn><gtr:outcomeId>doi_53d057057410d2fc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>443BAA84-5861-4C05-9365-E32A64259D81</gtr:id><gtr:title>Multiplicative Update Rules for Multilinear Support Tensor Machines</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec471a560e55c104b1fdcc758015f5b5"><gtr:id>ec471a560e55c104b1fdcc758015f5b5</gtr:id><gtr:otherNames>Kotsia I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7542-1</gtr:isbn><gtr:outcomeId>doi_53d058058e3155ac</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1206735E-99B5-4B81-AE30-C694504A7F9D</gtr:id><gtr:title>Tensor learning for regression.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57dbc6e79ca5f4673a01187a03d4ae0d"><gtr:id>57dbc6e79ca5f4673a01187a03d4ae0d</gtr:id><gtr:otherNames>Guo W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>doi_53d05e05e3f9df2a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D6EED984-813D-4FE9-90E3-5185BBD2BD99</gtr:id><gtr:title>Supervised Dictionary Learning for Action Detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/590546611d35011ae88e657a81dc7fde"><gtr:id>590546611d35011ae88e657a81dc7fde</gtr:id><gtr:otherNames>Vijay Kumar (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_521566998714095b00</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1EA63B35-D289-44BB-8F03-005B6D461EFF</gtr:id><gtr:title>Coupled Gaussian processes for pose-invariant facial expression recognition.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ca97c3da5025fae211a933687e55edf"><gtr:id>2ca97c3da5025fae211a933687e55edf</gtr:id><gtr:otherNames>Rudovic O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05efddf3a4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F9E30799-A1B2-46A1-B9FD-51B5FA545B91</gtr:id><gtr:title>Support tensor action spotting</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec471a560e55c104b1fdcc758015f5b5"><gtr:id>ec471a560e55c104b1fdcc758015f5b5</gtr:id><gtr:otherNames>Kotsia I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-2534-9</gtr:isbn><gtr:outcomeId>doi_53d058058bec7f74</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D8A4F58-C0EC-450B-9229-AE2D3418BB8B</gtr:id><gtr:title>Regression-Based Multi-view Facial Expression Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ca97c3da5025fae211a933687e55edf"><gtr:id>2ca97c3da5025fae211a933687e55edf</gtr:id><gtr:otherNames>Rudovic O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7542-1</gtr:isbn><gtr:outcomeId>doi_53d058058e26af9a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CFBB6446-A8E3-4290-AEEA-917FFB3039C4</gtr:id><gtr:title>Discriminative 3D human pose estimation from monocular images via topological preserving hierarchical affinity clustering</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/438aedad065717fe77a0b8c7cf86bd3a"><gtr:id>438aedad065717fe77a0b8c7cf86bd3a</gtr:id><gtr:otherNames>Weiwei Guo</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4442-7</gtr:isbn><gtr:outcomeId>doi_53d0580586f1f95a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9E6C2033-6C08-4EF6-B518-2EFE7EAD1C01</gtr:id><gtr:title>Max-margin Non-negative Matrix Factorization</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/100277489c2daead851b364387aaa738"><gtr:id>100277489c2daead851b364387aaa738</gtr:id><gtr:otherNames>Kumar B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53cff7ff7634e493</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B9095752-D21C-4843-A7A5-1C6F2D675ABB</gtr:id><gtr:title>Spatiotemporal localization and categorization of human actions in unsegmented image sequences.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ebdcb8d2e38bbfcac60db3c96c56b8e9"><gtr:id>ebdcb8d2e38bbfcac60db3c96c56b8e9</gtr:id><gtr:otherNames>Oikonomopoulos A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>doi_53d05e05e3c61e48</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>14F1206A-8EDD-4082-8C3D-58F980ECDA2D</gtr:id><gtr:title>Higher Order Support Tensor Regression for Head Pose Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/321c57b1b556aebaee1453dbfd1344ed"><gtr:id>321c57b1b556aebaee1453dbfd1344ed</gtr:id><gtr:otherNames>Weiwei Guo (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_7762977281140a1d38</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>09BFC8A1-0D6F-4F11-BE60-714B9441015E</gtr:id><gtr:title>Fusion of facial expressions and EEG for implicit affective tagging</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8e262f130bab038972c1f655daf40d60"><gtr:id>8e262f130bab038972c1f655daf40d60</gtr:id><gtr:otherNames>Koelstra S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53cff7ff763df86a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>833A2CDC-C9E5-41CF-8333-16D9EAB6B715</gtr:id><gtr:title>Max-Margin Semi-NMF</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/590546611d35011ae88e657a81dc7fde"><gtr:id>590546611d35011ae88e657a81dc7fde</gtr:id><gtr:otherNames>Vijay Kumar (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_705306402114095c18</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39DE2AE8-235A-490C-81D6-9E28B1EC294F</gtr:id><gtr:title>Human action localization with support tensor machines</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4d7184cbb79078e31a57715d67b32967"><gtr:id>4d7184cbb79078e31a57715d67b32967</gtr:id><gtr:otherNames>Irene Kotsia (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_354995188813e3216a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9055A401-F705-4713-A35A-A143E286BE49</gtr:id><gtr:title>A Discriminative Voting Scheme for Object De- tection using Hough Forests</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/590546611d35011ae88e657a81dc7fde"><gtr:id>590546611d35011ae88e657a81dc7fde</gtr:id><gtr:otherNames>Vijay Kumar (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_961831089314095fba</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2ED7FFA1-9BF3-41F7-A9DA-62EA1AFE2FC0</gtr:id><gtr:title>Higher rank Support Tensor Machines for visual recognition</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec471a560e55c104b1fdcc758015f5b5"><gtr:id>ec471a560e55c104b1fdcc758015f5b5</gtr:id><gtr:otherNames>Kotsia I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d00400451f293f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25320E62-A7F8-45F3-B5FE-E11C3AA46BEA</gtr:id><gtr:title>A dynamic texture-based approach to recognition of facial actions and their temporal models.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8e262f130bab038972c1f655daf40d60"><gtr:id>8e262f130bab038972c1f655daf40d60</gtr:id><gtr:otherNames>Koelstra S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05ef930a0f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0294084D-E910-433D-87CD-E8D2B1074C40</gtr:id><gtr:title>Coupled prediction classification for robust visual tracking.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eb39da1ec92b630e9c4a590b478291c1"><gtr:id>eb39da1ec92b630e9c4a590b478291c1</gtr:id><gtr:otherNames>Patras I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05ef4a211d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>754F6C67-62D3-4D5F-92C4-5CFCBC132CF3</gtr:id><gtr:title>Learning codebook weights for action detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/100277489c2daead851b364387aaa738"><gtr:id>100277489c2daead851b364387aaa738</gtr:id><gtr:otherNames>Kumar B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1611-8</gtr:isbn><gtr:outcomeId>doi_53d05705746ae016</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7502BB13-1723-44E8-8261-7F8A31035099</gtr:id><gtr:title>Privileged information-based conditional regression forest for facial feature detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27635d4ee8c7365893ca3f1effdd8022"><gtr:id>27635d4ee8c7365893ca3f1effdd8022</gtr:id><gtr:otherNames>Heng Yang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>doi_53d057057bab5a3f</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G033935/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>