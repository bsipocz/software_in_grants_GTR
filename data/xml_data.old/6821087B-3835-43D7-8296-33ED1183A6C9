<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/BF9517F2-9B69-40E3-BDE2-9E497D90E574"><gtr:id>BF9517F2-9B69-40E3-BDE2-9E497D90E574</gtr:id><gtr:firstName>Yiannis</gtr:firstName><gtr:surname>Demiris</gtr:surname><gtr:orcidId>0000-0003-4917-3343</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/772BC904-C347-46F3-B47C-0B29EAB1D3C7"><gtr:id>772BC904-C347-46F3-B47C-0B29EAB1D3C7</gtr:id><gtr:firstName>Danilo</gtr:firstName><gtr:surname>Mandic</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FP008461%2F1"><gtr:id>6821087B-3835-43D7-8296-33ED1183A6C9</gtr:id><gtr:title>Closed-Loop Multisensory Brain-Computer Interface for Enhanced Decision Accuracy</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/P008461/1</gtr:grantReference><gtr:abstractText>The goals of our interdisciplinary effort are to develop new methodologies for modeling multimodal neural activity underlying multisensory processing and decision making, and to use those methodologies to design closed-loop adaptive algorithms for optimized exploitation of multisensory data for brain-computer communication. We are motivated by the observation that a dismounted soldier or a tank driver routinely makes decisions in time-pressured and stressful conditions based on a multiplicity of multisensory information presented in cluttered and distracting environments. We envision a closed-loop brain-computer interface (BCI) architecture for enhancing decision accuracy. The architecture will collect multimodal neural, physiological, and behavioral data, decode mental states such as attention orientation and situational awareness, and use the decoded states as feedback to adaptively change the multisensory cues provided to the subject, thus closing the loop. To realize such an architecture we will make fundamental advances on four fronts, constituting our research Thrusts: (1) modeling multisensory integration, attention, and decision making, and the associated neural mechanisms; (2) machine-learning algorithms for high-dimensional multimodal data fusion; (3) adaptive tracking of the neural and behavioral models during online operation of the BCI; and (4) adaptive BCI control of multisensory cues for optimized performance. We have assembled a multidisciplinary team with expertise spanning engineering, computer science, and neuroscience. We will take a fully integrated approach to address these challenges by combining rare state-of-the-art experimental capabilities with novel computational modeling. Complementary experiments in rodents, monkeys, and humans will collect multimodal data to study and model multisensory integration, attention, and decision making, and to prototype a BCI for enhanced decision accuracy. Our modeling efforts will span Bayesian inference, stochastic control, adaptive signal processing, and machine learning to develop: novel Bayesian and control-theoretic models of the brain mechanisms; new stochastic models of multimodal data and adaptive inference algorithms for this data; and novel adaptive stochastic controllers of multisensory cues based on the feedback of users' cognitive state.</gtr:abstractText><gtr:fund><gtr:end>2019-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>522179</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>97528250-2A01-4B0F-86F1-25EA0EAE7D4F</gtr:id><gtr:title>Quantifying team cooperation through intrinsic multi-scale measures: respiratory and cardiac synchronization in choir singers and surgical teams.</gtr:title><gtr:parentPublicationTitle>Royal Society open science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ccf12e6cfad62fd2630f4af59265041d"><gtr:id>ccf12e6cfad62fd2630f4af59265041d</gtr:id><gtr:otherNames>Hemakom A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2054-5703</gtr:issn><gtr:outcomeId>5a6ef10f166fd9.28314008</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AC060962-BC9F-4284-9D82-C37B9FD08AE8</gtr:id><gtr:title>Discrimination of emotional states from scalp- and intracranial EEG using multiscale R&amp;eacute;nyi entropy.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dff446539f7b22534cebe3de1cb8958d"><gtr:id>dff446539f7b22534cebe3de1cb8958d</gtr:id><gtr:otherNames>Tonoyan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a6ef0f2923471.58287499</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3001E155-94C6-479E-B701-095B5CE39FE1</gtr:id><gtr:title>Adaptive user modelling in car racing games using behavioural and physiological data</gtr:title><gtr:parentPublicationTitle>User Modeling and User-Adapted Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a1b74dac4caa61c000445c99306bcb7"><gtr:id>2a1b74dac4caa61c000445c99306bcb7</gtr:id><gtr:otherNames>Georgiou T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a5789e94c7177.30177692</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52BC23AC-6E54-4226-8604-781ED941E3FC</gtr:id><gtr:title>Resolving Ambiguities in the LF/HF Ratio: LF-HF Scatter Plots for the Categorization of Mental and Physical Stress from HRV.</gtr:title><gtr:parentPublicationTitle>Frontiers in physiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6278ef3760f39698cf201a8e6c1d1911"><gtr:id>6278ef3760f39698cf201a8e6c1d1911</gtr:id><gtr:otherNames>von Rosenberg W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1664-042X</gtr:issn><gtr:outcomeId>5a578b8dd9d389.65185424</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E80E8FAD-8365-4891-B097-4CB142CFE6A7</gtr:id><gtr:title>Physiological artifacts in scalp EEG and ear-EEG.</gtr:title><gtr:parentPublicationTitle>Biomedical engineering online</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/02c57ce314093789f8a242f4446ee29b"><gtr:id>02c57ce314093789f8a242f4446ee29b</gtr:id><gtr:otherNames>Kappel SL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1475-925X</gtr:issn><gtr:outcomeId>5a578b1b0d8e42.36480101</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>692DD8FF-1918-42C3-8FE2-5E9527052EDD</gtr:id><gtr:title>Hearables: feasibility of recording cardiac rhythms from head and in-ear locations.</gtr:title><gtr:parentPublicationTitle>Royal Society open science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6278ef3760f39698cf201a8e6c1d1911"><gtr:id>6278ef3760f39698cf201a8e6c1d1911</gtr:id><gtr:otherNames>von Rosenberg W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2054-5703</gtr:issn><gtr:outcomeId>5a6eefe1d75334.96088253</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/P008461/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>