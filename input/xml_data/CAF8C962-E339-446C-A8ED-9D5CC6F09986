<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Institute of Biomedical Engineering</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F79C4700-36E9-45D1-9DC0-51C1904430A0"><gtr:id>F79C4700-36E9-45D1-9DC0-51C1904430A0</gtr:id><gtr:firstName>Ara</gtr:firstName><gtr:otherNames>Warkes</gtr:otherNames><gtr:surname>Darzi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6EB766C7-CF7E-47FD-8F42-2F6211709069"><gtr:id>6EB766C7-CF7E-47FD-8F42-2F6211709069</gtr:id><gtr:firstName>Guang-Zhong</gtr:firstName><gtr:surname>Yang</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD057213%2F1"><gtr:id>CAF8C962-E339-446C-A8ED-9D5CC6F09986</gtr:id><gtr:title>Perceptual Docking for Robotic Control (Equipment Rich Proposal)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D057213/1</gtr:grantReference><gtr:abstractText>The quest for protecting humans from direct exposure to hazardous or safety critical environments has been the driving force behind technological developments in robotics. Whilst robotics and other related technologies continue to grow, researchers across the globe are turning their attention to what is perhaps the most challenging safety critical environment of all / the human body. Deploying robots around and within the human body, particularly for robotic surgery presents a number of unique and challenging problems that arise from the complex and often unpredictable environments that characterise the human anatomy. Master-slave based robots such as the daVinci system, which embodies the movements of trained minimal access surgeons through motion scaling and compensation, are gaining clinical significance. Micro-machines possessing sensors and actuators based on the MEMS technology are also rapidly emerging. Under the dichotomy of autonomous and manipulator technologies in robotics, intelligence of the robot is typically pre-acquired through high-level abstraction and environment modelling. For procedures that involve complex anatomy and large tissue deformation, however, this is known to create major difficulties. The regulatory, ethical and legal barriers imposed on interventional surgical robots also give rise to the need of a tightly integrated control between the operator and the robot when autonomy is being pursued. The aim of this project is to research into a new concept of perceptual docking for robotic control. The word docking is different in meaning to the conventional term used in mobile robots. It represents a fundamental paradigm shift of perceptual learning and knowledge acquisition for robotic systems in that operator specific motor and perceptual/cognitive behaviour is assimilated in situ through a gaze contingent framework. We hypothesise that saccadic eye movements and ocular vergence can be used for attention selection and recovering 3D motion and deformation of the soft tissue during MIS procedures. It is expected that the method will also open up a range of completely new opportunities for effective human-machine interaction. This proposal seeks equipment and research funding for establishing an integrated core experimental facility at Imperial College for investigating key challenges related to perceptual docking in robotics and allied research issues related to human-machine interaction, visual perception and machine learning, ergonomics, kinematics and actuation design, intra-operative image guidance, motion and biomechanical modelling, tissue-instrument interaction, and robotic control. Matching funding from the Institute of Biomedical Engineering at Imperial has already been secured, and the facility is expected to greatly enhance the multidisciplinary research capacity and facilitate the interaction and initiation of new research programmes across a number of different disciplines.</gtr:abstractText><gtr:fund><gtr:end>2010-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>878017</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>INTERACT workshop on HCI and HRI</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>0773AA34-2F54-401D-B1AA-CF75D9FFE4E3</gtr:id><gtr:impact>Workshop hosted by Sheffield Robotics to explore potential collaborations between the Human-Robot Interaction and Human-Computer Interface research communities. Attendees: 43 delegates, 23 UK universities represented. Outcome: ambition for the universities of Sheffield, Hertfordshire and Nottingham to apply for an EPSRC Network to keep the momentum going, network members identified</gtr:impact><gtr:outcomeId>58c7d54e8a3385.39517572</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>485228</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Academy of Engineering Fellowship</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>Royal Academy of Engineering</gtr:fundingOrg><gtr:fundingRef>Danail Stoyanov</gtr:fundingRef><gtr:id>5EDF0884-32BF-4DF1-9A85-4474C68EBB4A</gtr:id><gtr:outcomeId>5ede94025ede9416</gtr:outcomeId><gtr:sector>Learned Society</gtr:sector><gtr:start>2009-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1381868</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>ARAKNES</gtr:description><gtr:end>2012-04-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>FP7 224565</gtr:fundingRef><gtr:id>523C8559-EF06-4E10-B0AE-CF8EB348D2EA</gtr:id><gtr:outcomeId>5ec5498e5ec549a2</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2008-05-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>879943</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Robotic Assisted Surgical Guidance and Visualisation</gtr:description><gtr:end>2010-11-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>DT/E011101/1</gtr:fundingRef><gtr:id>C746CE62-8794-4635-9147-EF74889A9251</gtr:id><gtr:outcomeId>r-7053972368.17886406a86434</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2007-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3000000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Robotic Assisted Microsurgery (Wolfson Foundation)</gtr:description><gtr:end>2012-04-02</gtr:end><gtr:fundingOrg>The Wolfson Foundation</gtr:fundingOrg><gtr:id>93A5014E-FEED-43A8-846D-211037885C9C</gtr:id><gtr:outcomeId>5ec164ae5ec164c2</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2010-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The project has allowed the establishment of the research facility on perceptual docking at Imperial College and strengthened the collaboration with industry, particularly Intuitive Surgical Inc. This has led to joint patent development and initiation of new UK/EU projects for developing a range of new surgical robot platforms.</gtr:description><gtr:firstYearOfImpact>2008</gtr:firstYearOfImpact><gtr:id>A5DE9C1B-0402-4B47-B4B2-97C91D751486</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545f3d603dbfa8.28021157</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The aim of this project is to research into a new concept of perceptual docking for robotic control. It represents a fundamental paradigm shift of perceptual learning and knowledge acquisition for robotic systems in that operator specific motor and perceptual/cognitive behaviour is assimilated in situ through a gaze contingent framework. We hypothesise that saccadic eye movements and ocular vergence can be used for attention selection and recovering 3D motion and deformation of the soft tissue during robotically assistred surgical procedures. The project has resulted in a range gaze contingent (eye-controlled) robotic control platforms based on binocular eye tracking for anatomical registration, motion stabilisation, active constraints and dynamic motor channelling. The project has openned up many new opportunities for effective human-machine interaction for robotically assisted surgery.</gtr:description><gtr:exploitationPathways>The use of eye control for human computer/machine interaction has a range of applications from entertainment, general computer interfacing, controlling of mobile devices, to assistive technologies. This is an equipment rich grant for establishing an integrated core experimental facility at Imperial College for investigating key challenges related to perceptual docking in robotics and allied research issues related to human-robot interaction.</gtr:exploitationPathways><gtr:id>B8747946-1432-4C9F-B18A-487F10BC4A2B</gtr:id><gtr:outcomeId>r-8531051919.3588377710220</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.imperial.ac.uk/hamlyn</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>47509589-94D2-4902-B45A-3A8045AEB9C5</gtr:id><gtr:title>Laser-induced fluorescence and reflected white light imaging for robot-assisted MIS.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on bio-medical engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b6b2f5efc1322dc8008417a4d82e2caf"><gtr:id>b6b2f5efc1322dc8008417a4d82e2caf</gtr:id><gtr:otherNames>Noonan DP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0018-9294</gtr:issn><gtr:outcomeId>doi_53d05d05d0384da5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A5EA1153-757A-4211-866B-D390E9F646FD</gtr:id><gtr:title>Super resolution in robotic-assisted minimally invasive surgery</gtr:title><gtr:parentPublicationTitle>Computer Aided Surgery</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95011e32ee546e8c05f5f74619e66b55"><gtr:id>95011e32ee546e8c05f5f74619e66b55</gtr:id><gtr:otherNames>Lerotic M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53d03d03dad510f2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>502DADC7-D0E0-4460-83AE-C985FCE0C20F</gtr:id><gtr:title>Gaze-Contingent Motor Channelling, haptic constraints and associated cognitive demand for robotic MIS.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5e6604645d3384f4a2162e02b710ac6a"><gtr:id>5e6604645d3384f4a2162e02b710ac6a</gtr:id><gtr:otherNames>Mylonas GP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>doi_53d000000f92c7b4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F5C18366-8369-4724-B8E0-7C71553F1622</gtr:id><gtr:title>U.K. Robotics Week [Competitions]</gtr:title><gtr:parentPublicationTitle>IEEE Robotics &amp; Automation Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97a2d622d7f36f5a14d63106d30e887d"><gtr:id>97a2d622d7f36f5a14d63106d30e887d</gtr:id><gtr:otherNames>Merrifield R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa6c1ab0bfd13.39242695</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06A025FF-B3E5-4CF8-80B9-0724D0F894F3</gtr:id><gtr:title>Motion-compensated MR valve imaging with COMB tag tracking and super-resolution enhancement.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7d5d1a4be0ef24025d79d87af9f636ba"><gtr:id>7d5d1a4be0ef24025d79d87af9f636ba</gtr:id><gtr:otherNames>Dowsey AW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>doi_53d000000f34cb93</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E99C8572-23C0-4AD6-B2FD-370E84F90A0A</gtr:id><gtr:title>Intra-Operative Visualizations: Perceptual Fidelity and Human Factors</gtr:title><gtr:parentPublicationTitle>Journal of Display Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2b640396ebc4bb19ef8b61f459eed807"><gtr:id>2b640396ebc4bb19ef8b61f459eed807</gtr:id><gtr:otherNames>Stoyanov D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d05a05a2de2265</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E70F26EE-D931-40AB-AC97-2750AB8CD06C</gtr:id><gtr:title>Surgical Robot Challenge 2015 [Competitions]</gtr:title><gtr:parentPublicationTitle>IEEE Robotics &amp; Automation Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97a2d622d7f36f5a14d63106d30e887d"><gtr:id>97a2d622d7f36f5a14d63106d30e887d</gtr:id><gtr:otherNames>Merrifield R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5aa6c12690cf46.68543298</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C635124-DF2A-490C-9184-4E87DC089CE3</gtr:id><gtr:title>Super resolution in robotic-assisted minimally invasive surgery.</gtr:title><gtr:parentPublicationTitle>Computer aided surgery : official journal of the International Society for Computer Aided Surgery</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95011e32ee546e8c05f5f74619e66b55"><gtr:id>95011e32ee546e8c05f5f74619e66b55</gtr:id><gtr:otherNames>Lerotic M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:issn>1092-9088</gtr:issn><gtr:outcomeId>585d646b0338e9.56419468</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D057213/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>53F27348-198B-4AEF-A34B-8307067F507C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Systems engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A72E6EED-2213-4658-9EB5-5A5DD2F65F2D</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Intelligent &amp; Expert Systems</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>