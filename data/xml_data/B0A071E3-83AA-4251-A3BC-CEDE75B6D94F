<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/21CDD154-5F58-4D80-9253-B6E1C61F0A20"><gtr:id>21CDD154-5F58-4D80-9253-B6E1C61F0A20</gtr:id><gtr:name>Mae Fah Luang University</gtr:name><gtr:address><gtr:line1>Mae Fah Luang University</gtr:line1><gtr:line2>333 Moo1</gtr:line2><gtr:line3>Thasud    Muang</gtr:line3><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/04500B3A-1219-4E25-96B6-5AFBA5839CAA"><gtr:id>04500B3A-1219-4E25-96B6-5AFBA5839CAA</gtr:id><gtr:name>French National Institute of Agricultural Research</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/EF390CF0-ECD3-47D8-B9A8-7602AF319BEE"><gtr:id>EF390CF0-ECD3-47D8-B9A8-7602AF319BEE</gtr:id><gtr:name>Northumbria University</gtr:name><gtr:department>Fac of Engineering and Environment</gtr:department><gtr:address><gtr:line1>Ellison Place</gtr:line1><gtr:line4>Newcastle upon Tyne</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>NE1 8ST</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF390CF0-ECD3-47D8-B9A8-7602AF319BEE"><gtr:id>EF390CF0-ECD3-47D8-B9A8-7602AF319BEE</gtr:id><gtr:name>Northumbria University</gtr:name><gtr:address><gtr:line1>Ellison Place</gtr:line1><gtr:line4>Newcastle upon Tyne</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>NE1 8ST</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/21CDD154-5F58-4D80-9253-B6E1C61F0A20"><gtr:id>21CDD154-5F58-4D80-9253-B6E1C61F0A20</gtr:id><gtr:name>Mae Fah Luang University</gtr:name><gtr:address><gtr:line1>Mae Fah Luang University</gtr:line1><gtr:line2>333 Moo1</gtr:line2><gtr:line3>Thasud    Muang</gtr:line3><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/04500B3A-1219-4E25-96B6-5AFBA5839CAA"><gtr:id>04500B3A-1219-4E25-96B6-5AFBA5839CAA</gtr:id><gtr:name>French National Institute of Agricultural Research</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/53B0421A-9109-4F1E-9AF2-D3D691D1C53B"><gtr:id>53B0421A-9109-4F1E-9AF2-D3D691D1C53B</gtr:id><gtr:name>Kinesio UK</gtr:name><gtr:address><gtr:line1>Cobalt 3.1, Silverfox Way</gtr:line1><gtr:postCode>NE27 0QJ</gtr:postCode><gtr:region>North East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F655F5DB-170D-44FD-BDAA-D9F1135C369A"><gtr:id>F655F5DB-170D-44FD-BDAA-D9F1135C369A</gtr:id><gtr:name>Cadence Cycling Performance Centre</gtr:name><gtr:address><gtr:line1>Cadence Cycling Performance Centre</gtr:line1><gtr:line2>2a Anerley Hill</gtr:line2><gtr:postCode>SE19 2AA</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/36DECD3D-2BD2-4D20-AE3F-B7B563A46A24"><gtr:id>36DECD3D-2BD2-4D20-AE3F-B7B563A46A24</gtr:id><gtr:name>Nine Health CIC</gtr:name><gtr:address><gtr:line1>QU2, 15 Queen Square</gtr:line1><gtr:postCode>LS2 8AJ</gtr:postCode><gtr:region>Yorkshire and the Humber</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FB811B63-EA20-493B-A353-E58B431C87D8"><gtr:id>FB811B63-EA20-493B-A353-E58B431C87D8</gtr:id><gtr:firstName>Hubert P. H.</gtr:firstName><gtr:surname>Shum</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM002632%2F1"><gtr:id>B0A071E3-83AA-4251-A3BC-CEDE75B6D94F</gtr:id><gtr:title>Interaction-based Human Motion Analysis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M002632/1</gtr:grantReference><gtr:abstractText>In this project, we propose a new method to analyze human motion based on the interaction with the surrounding environment, which provides a better understanding about the nature of the performed motion and enhances the performance of modern motion-related applications. 
 
Understanding human movement is a central problem for motion-related applications such as behaviour monitoring for smart homes and movement evaluation for physical therapy. Most traditional 3D motion analysis algorithms are human-centered, meaning that they only consider features of the human body but not the interaction with the surrounding environment. Imagine the movement of an older person standing on the floor while doing some arm-stretching exercises, and another standing on a chair to fix a light bulb, which is considered to be dangerous. The two motions have completely different contextual meanings, but are surprisingly similar in terms of human body posture. Traditional computer-based motion analysis methods disregard the relationship between the human and the environment, and thus cannot accurately tell the difference between the two motions. 
 
We observe that real humans usually comprehend the context of a motion based on its interaction with the surrounding environment, such as sitting on a sofa, watching the television, riding a bicycle, etc. We believe that by considering these types of interactions, higher quality motion analysis can be improved as a result of better understanding on the context of the motion. Therefore, we propose a new algorithm to analyze human motion based on the interaction between the human and the surrounding environment, which will enhance the accuracy of motion identification and the performance of movement evaluation. Our algorithm evaluates detailed 3D movement features with respect to the environment, such as analyzing the subtle movement of the feet of a Parkinson's disease patient with respect to the position of the stairs during a stair climbing motion. It can therefore (1) analyze movement features from the interaction point of view, and (2) identify what kind of motion the user is performing based on the interaction context. 
 
The system proposed in this research could be used to enhance the quality of life of older people to enable greater independence and reduce the burden on emergency and care services caused by a rising ageing population. Our algorithm accurately identifies human motion, which is an important step towards a smart home system that takes care of older people autonomously. It also aims to evaluate human movement, which can significantly reduce the labour cost of rehabilitation and coaching for older people, as well as early stage motion-related disease (such as the Parkinson's disease) diagnosis.</gtr:abstractText><gtr:potentialImpactText>This research aims to enhance the quality of life and reduce the cost of supporting older people, which are major challenges in the UK due to population ageing. This target will be achieved via a new algorithm that better identifies and analyzes human motion, such that services that are usually required by older people including smart home, rehabilitation/sport coaching and movement disorder diagnosis can be enhanced/automated. 
 
According to the 2011 census, the population of the UK aged 65 and over was 10.4 million, which is equivalent to 16 per cent of the UK population. The Office for National Statistics has projected that in England in 2030, there will be 51% more people aged 65 and over compared to 2010. By 2050, a 65-year-old man in Britain can expect to live to 91, whereas in 1950 his life expectancy was 76. This trend presents significant challenges for services we depend on to take care of older people and also for an increasing proportion of the working age population which provides informal care. This research directly addresses this issue by proposing a new technology for motion analysis, which is a key part of several major services required by older people, including smart homes, rehabilitation and movement disorder diagnosis. 

Smart home, also known as home automation, describes technologies that facilitate in-house support such as automatically calling for help when accidents occur. An ideal smart home allows older people to live in their own home while maintaining access to high quality support, minimizing their dependency on costly healthcare facilities like care homes. One major challenge of smart homes is to understand what the user is doing, in order to decide how to support the user. The proposed research can accurately identify dangerous behaviours like standing on a chair to fix a light bulb, as well as accidents such as falling and colliding with obstacles. With this kind of technology, the labour cost of supporting older people can be reduced, and older people can live with greater independence. 
 
Rehabilitation and sport coaching are two major services used by older people because of the deterioration of physical fitness. The former focuses on recovering from physical injury, while the latter is to strengthen body fitness and prevent injury. The current main streams of these services are highly labour intensive, involving professional trainers to guide the user throughout the training process. Due to high running costs, the availability of these services is under heavy stress, and will likely worsen as a higher proportion of the population lives longer. This research can automatically evaluate the motion of a person when he/she interacts with the environment. This is especially useful for rehabilitation and sport coaching, in which the user usually needs to interact with equipment such as riding exercise bikes and stepping over obstacles. The proposed method can analyze how well the user handles the equipment, and point out how body parts should move with respect to the equipment. It is an important step towards autonomous rehabilitation and coaching. 
 
Movement disorder diagnosis is another service often required by older people. This is because many health issues suffered by older people such as Parkinson's disease, muscle injury, and degenerative joint disease, result in movement disorder. Early stage diagnosis of these issues usually involves physiologists observing the degree and the symptoms of the disorder. However, even for professionals, it is a challenging problem to accurately diagnose these diseases. The proposed algorithm can evaluate information considering the relationship between the motion and the environment, such as the subtle movement of the feet with respect to the stair in a stair climbing motion. Such kinds of human-environment relationship features have not been fully explored in traditional research, and can potentially improve diagnosis accuracy.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-10-26</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-02-27</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>99054</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Mae Fah Luang University</gtr:collaboratingOrganisation><gtr:country>Thailand, Kingdom of</gtr:country><gtr:description>Mae Fah Luang Thailand</gtr:description><gtr:id>5704C991-586F-4C65-A479-8F92F8566019</gtr:id><gtr:impact>Newton Fund application: &amp;pound;100,000
Newton Mobility Fund application: &amp;pound;24,000</gtr:impact><gtr:outcomeId>58c6dca4266a28.61887582-1</gtr:outcomeId><gtr:partnerContribution>Mae Fah Luang University has worked with our research team to produce two joint funding applications. This can potentially bring in further research money to enhance the scale of this project.</gtr:partnerContribution><gtr:piContribution>I have collaborated with Mae Fah Luang University for two funding applications. The first one is a Newton Fund application that is worth &amp;pound;100,000. The second one is a Newton Mobility Fund application that is worth &amp;pound;24,000. Both applications are about human motion analysis using human-environment interactions, which is the main themes of this research.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>French National Institute of Agricultural Research</gtr:collaboratingOrganisation><gtr:country>France, French Republic</gtr:country><gtr:description>INRIA France</gtr:description><gtr:id>452EACF3-BAD1-47BC-A263-3FDC53B35976</gtr:id><gtr:impact>Publications:
Pierre Plantard, Hubert P. H. Shum and Franck Multon, &amp;quot;Filtered Pose Graph for Efficient Kinect Pose Reconstruction,&amp;quot; Journal of Multimedia Tools and Applications, vol. 76, no. 3, pp. 4291-4312, Springer-Verlag, 2017.
Pierre Plantard, Hubert P. H. Shum, Anne-Sophie Le Pierres and Franck Multon, &amp;quot;Validation of an Ergonomic Assessment Tool using Kinect Data in Real Workplace Conditions,&amp;quot; Applied Ergonomics, 2016.
Pierre Plantard, Hubert P. H. Shum and Franck Multon, &amp;quot;Ergonomics Measurements using Kinect with a Pose Correction Framework,&amp;quot; in DHM '16: Proceedings of the 2016 International Digital Human Modeling Symposium, Montreal, Canada, Jun 2016.</gtr:impact><gtr:outcomeId>58c6ddd3714a42.65634831-1</gtr:outcomeId><gtr:partnerContribution>INRIA has worked with my research team to research on human motion analysis. We have produced multiple papers and co-supervised a PhD funded by INRIA. INRIA has also paid for a trip such that I can visit the France institute and give a seminar.</gtr:partnerContribution><gtr:piContribution>I have co-supervised a PhD funded by INRIA and have consultancy. I have visited INRIA to give a research seminar.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Program Chair of the ACM SIGGRAPH Conference on Motion in Games 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A7F8BF04-DD94-4E3F-A2CA-AD673E9C50C0</gtr:id><gtr:impact>The goal of the Motion in Games conference is to bring together researchers and industrial practitioners from this variety of fields to present their most recent results, to initiate collaborations, and to contribute to the establishment of the research area. The conference will consist of regular paper sessions, poster presentations, and as well as presentations by a selection of internationally renowned speakers in all areas related to games and simulation in the context of motion. The conference includes entertaining cultural and social events that foster casual and friendly interactions among the participants.

I organised this conference in 2016 as the Program Chair, and the conference had more than around 100 registered attendances coming from both the academic and the industry. The conference received 47 submissions. Its program consisted of 25 papers (10 long and 15 short) and a poster session. It was sponsored by ACM SIGGRAPH, with papers appearing in the ACM digital library. It was also in cooperation with Eurographics. Financial support was generously provided by Disney Research. For the first time, MIG was being co-located with AIIDE, the Twelfth Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment.</gtr:impact><gtr:outcomeId>587cb9f72eade9.75667947</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>https://mig2016.inria.fr/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We are working with various companies in applying our research findings to practical applications. We are working with Kinesio UK, which is an international company building the popular Kinesio Tape for sport training and rehabilitation, to build a motion analysis application using our findings. The focus is to analyse motion from patients suffering from musculoskeletal injury. On top of our research team members, we have co-supervised an undergraduate student with the company for such an application. We are also working with Cadence Performance, which is professional cycling training company in London, to build another application that analyses cyclers' performance using real-time 2.5D cameras. Such a system can analyse 3D motion in real-time, as well as gathering the 3D information of the bike and the environment. In the future, we are interested in working with home monitoring companies and surveillance companies by adapting our findings to improve the performance of activities understanding.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>EF704290-8FC2-40E2-8CCD-679B52362A00</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal,Economic,Policy &amp; public services</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56cc80866cc435.92937826</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Other</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In this research, we have investigated several areas of human motion analysis, including 3D motion retrieval, 3D motion analysis, motion-based human computer interaction with 2.5D point cloud, human body deformation based on skeletal movement, environment driven crowd simulation, video-based human action recognition and posture classification.

One of the major findings in this research is that the movement of a single human cannot convey enough information for many motion related applications. Instead, considering the interaction between multiple human, or between the human and the environment, can greatly enhance the understanding of the motion, thereby enhancing motion analysis and synthesis. This argument has opened a new direction in human motion related research that can be applied in computer graphics, computer animation, serious games, sport and rehabilitation training, etc. 

For further details of our findings, please refer to the following website: http://info.hubertshum.com</gtr:description><gtr:exploitationPathways>Our research has generated good academic impact, and our research team has made a name for ourselves in the research circle. We have published our findings in top-quartile journals, in the field of artificial intelligence, computer graphics, computer vision and human motion analysis. 

We have purchased open access for all of our research publications, so as to improve the popularity of our research. Moreover, we have built a website to showcase our research findings and attract potential collaboration opportunities, which can be found at: http://info.hubertshum.com

We have worked with multiple companies to transfer some of the knowledge of this project into practical uses, including companies from health sector, sport training and rehabilitation. We are interested in extending our impact in motion understanding to smart homes and automatics surveillance.</gtr:exploitationPathways><gtr:id>FC282050-674F-4FD9-BAD9-533A12DDA71C</gtr:id><gtr:outcomeId>56cc7fe4816733.69600188</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Other</gtr:sector></gtr:sectors><gtr:url>http://info.hubertshum.com</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>We introduced the first synthetic 2D and 3D training dataset for view-invariant transfer dictionary learning. The database were released for public usage.</gtr:description><gtr:id>06E6C32C-29D6-4542-96D4-9259C066D77E</gtr:id><gtr:impact>The database benefited the academic and the industry in action recognition. It attracted a collaborative project with Toyota EU, which was implementing a healthcare robot that required action recognition for older people.</gtr:impact><gtr:outcomeId>587cc8314267b8.24143476</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Human Motion Database for Action Recognition</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://hubertshum.com/info/icra2016.htm</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>677CE6DE-65E4-44D5-8D3E-99E865CC30A6</gtr:id><gtr:title>Validation of an ergonomic assessment method using Kinect data in real workplace conditions.</gtr:title><gtr:parentPublicationTitle>Applied ergonomics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76da70771c3f43b1d6cdcd37287daf0f"><gtr:id>76da70771c3f43b1d6cdcd37287daf0f</gtr:id><gtr:otherNames>Plantard P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0003-6870</gtr:issn><gtr:outcomeId>585d61b3e753e0.47150151</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D07E066B-D384-4994-9DA5-8700D1FB8F4C</gtr:id><gtr:title>Manifold Regularized Experimental Design for Active Learning.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/917fedf7d4ac4a5d4fbb9db6070723d7"><gtr:id>917fedf7d4ac4a5d4fbb9db6070723d7</gtr:id><gtr:otherNames>Zhang L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>586d3be015b7e3.22520414</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A05084D-10E1-414E-B9E6-70C529F3F55F</gtr:id><gtr:title>Towards sparse rule base generation for fuzzy rule interpolation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/838fb3b433832b2cd9e881a96c21779c"><gtr:id>838fb3b433832b2cd9e881a96c21779c</gtr:id><gtr:otherNames>Tan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>586d3b9f4cd035.08649590</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C71A8734-BF46-4023-9AC0-ADE107A3B625</gtr:id><gtr:title>Filtered pose graph for efficient kinect pose reconstruction</gtr:title><gtr:parentPublicationTitle>Multimedia Tools and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76da70771c3f43b1d6cdcd37287daf0f"><gtr:id>76da70771c3f43b1d6cdcd37287daf0f</gtr:id><gtr:otherNames>Plantard P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>57e2e4744dd933.82696243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5DC201F5-FFAD-4822-9871-6AE1408DBE73</gtr:id><gtr:title>Differential evolution algorithm as a tool for optimal feature subset selection in motor imagery EEG</gtr:title><gtr:parentPublicationTitle>Expert Systems with Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb8c550abb4bfb07683b0ee0dfa4c099"><gtr:id>cb8c550abb4bfb07683b0ee0dfa4c099</gtr:id><gtr:otherNames>Baig M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe32b4765a2.46173786</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>76A274D6-2AB9-4911-A9CA-9C34C232A505</gtr:id><gtr:title>Multi-layer Lattice Model for Real-Time Dynamic Character Deformation</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/979081b59a99cdfd71eba279c9e08a0a"><gtr:id>979081b59a99cdfd71eba279c9e08a0a</gtr:id><gtr:otherNames>Iwamoto N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5649e7be3d9361.87931668</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D42CF78B-8CC5-4CDB-94B0-4A3EE62057D8</gtr:id><gtr:title>Kinect Posture Reconstruction Based on a Local Mixture of Gaussian Process Models.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/360e80a17b26c2edb5cfe8db595159bb"><gtr:id>360e80a17b26c2edb5cfe8db595159bb</gtr:id><gtr:otherNames>Liu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>569fb60042f5e2.88725416</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9F45B43-C2D4-40BF-ACCB-A8EC3747DFFC</gtr:id><gtr:title>Handbook of Human Motion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8519379ca74594bab2b6baf7028f7afa"><gtr:id>8519379ca74594bab2b6baf7028f7afa</gtr:id><gtr:otherNames>Shen Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>586d3b9f0638f0.63941132</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92890593-803F-4C94-B0EF-D0A8C9956F77</gtr:id><gtr:title>Inverse dynamics based on occlusion-resistant Kinect data: Is it usable for ergonomics?</gtr:title><gtr:parentPublicationTitle>International Journal of Industrial Ergonomics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76da70771c3f43b1d6cdcd37287daf0f"><gtr:id>76da70771c3f43b1d6cdcd37287daf0f</gtr:id><gtr:otherNames>Plantard P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe0af117089.53914130</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AF520002-4524-460D-B8F8-FA8B0B79AF0D</gtr:id><gtr:title>Arbitrary view action recognition via transfer dictionary learning on synthetic training data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd0d66b1ecaa19049f23b60afbe84901"><gtr:id>cd0d66b1ecaa19049f23b60afbe84901</gtr:id><gtr:otherNames>Jingtian Zhang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>57e2e4579b0342.65863895</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>145C2A2D-ADD8-4A92-9D75-A3D6BA3FA705</gtr:id><gtr:title>Posture-based and action-based graphs for boxing skill visualization</gtr:title><gtr:parentPublicationTitle>Computers &amp; Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8519379ca74594bab2b6baf7028f7afa"><gtr:id>8519379ca74594bab2b6baf7028f7afa</gtr:id><gtr:otherNames>Shen Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a902d56d830a3.06973029</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4BE596E5-C92D-4613-A4D4-59EF40147486</gtr:id><gtr:title>Improving posture classification accuracy for depth sensor-based human activity monitoring in smart environments</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b45cc0e5408c0cded5ccc9991146951"><gtr:id>0b45cc0e5408c0cded5ccc9991146951</gtr:id><gtr:otherNames>Ho E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>569fb6af2aa699.72165799</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>09E7A417-A553-4736-9438-77B585C53680</gtr:id><gtr:title>CCESK: A Chinese Character Educational System Based on Kinect</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Learning Technologies</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/614499d6dfd60e0aac13fcc6156c92cd"><gtr:id>614499d6dfd60e0aac13fcc6156c92cd</gtr:id><gtr:otherNames>Yang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe12bc97640.86482152</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9655DE91-F20A-4F5F-8FA7-55C507933E34</gtr:id><gtr:title>Coordinated Crowd Simulation With Topological Scene Analysis</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/50fc20c94d4893b8e1e527e6d125cf3d"><gtr:id>50fc20c94d4893b8e1e527e6d125cf3d</gtr:id><gtr:otherNames>Barnett A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5649e7be745231.96773383</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5C645FCC-15B8-46D3-A9C5-BDB3475305CE</gtr:id><gtr:title>Discriminative Semantic Subspace Analysis for Relevance Feedback.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/917fedf7d4ac4a5d4fbb9db6070723d7"><gtr:id>917fedf7d4ac4a5d4fbb9db6070723d7</gtr:id><gtr:otherNames>Zhang L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>569fb70c8d7cc8.68462588</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M002632/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>FD25826C-8B50-43A3-8871-3FF08D051906</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Biomechanics &amp; Rehabilitation</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>