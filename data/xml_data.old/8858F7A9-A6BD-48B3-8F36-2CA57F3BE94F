<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F"><gtr:id>6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F</gtr:id><gtr:name>Technicolor</gtr:name><gtr:address><gtr:line1>1, avenue de Belle Fontaine</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/CF058FF3-A17B-43BF-B828-7507DD791A82"><gtr:id>CF058FF3-A17B-43BF-B828-7507DD791A82</gtr:id><gtr:name>Apical</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B"><gtr:id>CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B</gtr:id><gtr:name>Oxford Brookes University</gtr:name><gtr:department>Faculty of Tech, Design and Environment</gtr:department><gtr:address><gtr:line1>Headington Campus</gtr:line1><gtr:line2>Gipsy Lane</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:line5>Oxfordshire</gtr:line5><gtr:postCode>OX3 0BP</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B"><gtr:id>CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B</gtr:id><gtr:name>Oxford Brookes University</gtr:name><gtr:address><gtr:line1>Headington Campus</gtr:line1><gtr:line2>Gipsy Lane</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:line5>Oxfordshire</gtr:line5><gtr:postCode>OX3 0BP</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F"><gtr:id>6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F</gtr:id><gtr:name>Technicolor</gtr:name><gtr:address><gtr:line1>1, avenue de Belle Fontaine</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CF058FF3-A17B-43BF-B828-7507DD791A82"><gtr:id>CF058FF3-A17B-43BF-B828-7507DD791A82</gtr:id><gtr:name>Apical</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CBCF51D8-792D-4907-9E56-FD4E1BC5B8E0"><gtr:id>CBCF51D8-792D-4907-9E56-FD4E1BC5B8E0</gtr:id><gtr:name>OMG plc</gtr:name><gtr:address><gtr:line1>OMG plc</gtr:line1><gtr:line2>14 Minns Business Park</gtr:line2><gtr:line3>West Way</gtr:line3><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX2 0JB</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6D419C9B-1ACA-435A-B8B1-36FA3461BF52"><gtr:id>6D419C9B-1ACA-435A-B8B1-36FA3461BF52</gtr:id><gtr:firstName>Philip</gtr:firstName><gtr:otherNames>Hilaire</gtr:otherNames><gtr:surname>Torr</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI001107%2F1"><gtr:id>8858F7A9-A6BD-48B3-8F36-2CA57F3BE94F</gtr:id><gtr:title>Scene Understanding using New Global Energy Models</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I001107/1</gtr:grantReference><gtr:abstractText>This proposal concerns scene understanding from video. Computer vision algorithms for individual tasks such as objectrecognition, detection and segmentation has now reached some level of maturity. The next challenge is to integrate all thesealgorithms and address the problem of scene understanding. The problem of scene understanding involves explaining thewhole image by recognizing all the objects of interest within an image and their spatial extent or shape in 3D.The application to drive the research will be the problem of automated understanding of cities from video usingcomputer vision, inspired by the availability of massive new data sets such as that of Google's Street Viewhttp://maps.google.com/help/maps/streetview/, Yotta http://www.yotta.tv/index.php (who have agreed to supply OxfordBrookes with data) and Microsoft's Photosynth http://labs.live.com/photosynth/. The scenario is as follows: a van drivesaround the roads of the UK, in the van are GPS equipment and multiple calibrated cameras, synchronized to capture andstore an image every two metres; giving a massive data set. The task is to recognize objects of interest in the video, fromroad signs and other street furniture, to particular buildings, to allow them to be located exactly on maps of the environment.A second scenario would be to perform scene understanding for indoor scenes such as home or office, with video taken froma normal camera and Z-cam.</gtr:abstractText><gtr:potentialImpactText>Impact Plan The aim of this project is twofold, first to engage in basic science, second to produce a commercially useful set of outcomes that will improve UK competitiveness. The former will be a set of papers on object recognition combined with structure, models thereof and combinatorial algorithms to bring the work to fruition. The latter will be greatly helped by interaction with the Oxford Metrics Group, both 2d3 and Yotta who have undertaken to meet with the project members regularly to ensure the outcomes are commercially useful. Professor Torr's contacts with Sony and Microsoft will enable him to steer the project along lines that should provide maximum benefit to the UK economy. This project clearly relates to the Digital Economy, which is highlighted as a key area in the EPSRC Delivery plan 2008-11, and in particular to Transport and the Creative industries, which are highlighted as an area of particular importance within the Digital Economy-thus this proposal lies exactly in accord with fundamental directions outlined in the EPSRC's Delivery Plan. Transport Industry: Our starting target industry is the highways industry who are interested in their 'asset inventories', e.g. location of street furniture, heights of bridges. The UK Government has now adopted a policy to implement Resource Accounting and Budgeting and Whole Government Accounting (WGA). The use of Asset Management Plans is essential to underpinning this policy. This is currently not obligatory, but is likely to be legislated within the next couple of years (i.e. a requirement in order to apply for road maintenance funding). At that point, we would expect that every local authority in the UK will be required to provide an inventory for their entire road network, totaling about 400,000km. The inventory would need to be updated annually. A typical rate for asset inventory is about 30/km. We expect that the USA will follow within the next 10 years; the Europe market similarly. Creative Industries: The second application that would be considered would be the identification of objects in indoor scenes. The scenes might be of rooms in the home, public building or workplace. Professor Torr works closely with Sony on the EyeToy (http://en.wikipedia.org/wiki/EyeToy), the EyeToy is typically placed in the living room and being able to recognize objects within the living room would significantly help with the design of games. It is anticipated the the release of the time of flight camera as a peripheral for Microsoft's project Natal would revolutionize not only the gaming industry but research in computer vision as well. With the deep penetration of the Xbox it would be expected that over five million units would be sold. That means five million Z-cams in people's living rooms. Currently research on time of flight cameras is not the main stream, but Natal will change this and the commercial desire for such things as object recognition will be immediate in games, HCI, advertising using Z-cam data. Exploitation: Intellectual Property Rights management and exploitation will be managed by the Research and Business Development Office (RBDO) at Oxford Brookes University, which has access to financial and other resources to enable Intellectual Property and its commercial exploitation to be effectively managed, whilst maximizing the widespread dissemination of the research results. This includes, as appropriate, finance for patenting and proof of concept funding; IP, technology and market assessment; resources for defining and implementing a commercialization strategy though licensing, start-up company or other routes. Oxford Brookes Computer Vision group already has an established record for exploiting IP, and interactions with several companies. Agreements are in place with Sony and OMG.</gtr:potentialImpactText><gtr:fund><gtr:end>2013-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-09-07</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>439228</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Technicolor</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>segmentation with technicolor</gtr:description><gtr:id>D3DF66B8-4B5F-4BCF-9772-64AEDFA82FB2</gtr:id><gtr:impact>tech</gtr:impact><gtr:outcomeId>54492f21bb2c69.53419470-1</gtr:outcomeId><gtr:partnerContribution>tech</gtr:partnerContribution><gtr:piContribution>segmentation tech</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Apical</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Real time processing on Apical</gtr:description><gtr:id>6C321229-B6D7-4447-B625-B792B810973B</gtr:id><gtr:impact>just started</gtr:impact><gtr:outcomeId>54492eb6c18a53.80323786-1</gtr:outcomeId><gtr:partnerContribution>160K fund student</gtr:partnerContribution><gtr:piContribution>tech</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Many mentions in News Media TV</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E0E06CE4-D906-452D-B97C-2E757E04222B</gtr:id><gtr:impact>many BBC, news papers etc etc

some of it listed here
http://www.robots.ox.ac.uk/~tvg/projects/SemanticPaint/index.php

google semantic paint


see also here http://www.va-st.com/smart-specs/</gtr:impact><gtr:outcomeId>56b1f2b6e7c122.18344255</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.va-st.com/smart-specs/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2200000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Towards Total Scene Understanding using Structured Models</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:id>9A063D7D-C574-4A81-9DB8-114EE3F9347F</gtr:id><gtr:outcomeId>5ec6fb8a5ec6fba8</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>2200000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Towards Total Scene Understanding using Structured Models</gtr:description><gtr:end>2018-02-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:id>57E2DE2D-DA36-42A7-9CD8-4F35DE415C2C</gtr:id><gtr:outcomeId>r-4885655990.69526106a19456</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-12-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We are integrating our object recognition into glasses for the partially sighted.

We are working with a chip company Apical to put it on mobile phones.

We are working with Microsoft to put it in games


We are working with Technicolor to put the segmentation into films

see EP/I001107/2</gtr:description><gtr:id>DD0FBBDD-BC76-40CB-BE98-4FA0B67BE21D</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54480d97eafff8.03766719</gtr:outcomeId><gtr:sector>Creative Economy,Electronics,Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>face tracker</gtr:description><gtr:grantRef>EP/I001107/1</gtr:grantRef><gtr:id>AF31CC17-DEB3-4925-BB8B-568D67D022FB</gtr:id><gtr:impact>licensed to Real D</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>5448e777727880.19046622</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>face tracking</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>oxsight</gtr:companyName><gtr:description>xSight is a University of Oxford venture that uses the latest smart glasses to improve sight for blind and partially sighted people. OxSight's aim is to develop sight enhancing technologies to improve the quality of life for blind and partially sighted people around the world. Our current commercial products can enhance vision for people affected by conditions like glaucoma, diabetes and retinitis pigmentosa as well as some other degenerative eye diseases.</gtr:description><gtr:id>7809C56E-A6D9-4ABD-B0BC-6D5601A0703A</gtr:id><gtr:impact>see http://smartspecs.co/</gtr:impact><gtr:outcomeId>58c293e39b39c9.26611300</gtr:outcomeId><gtr:url>http://smartspecs.co/</gtr:url><gtr:yearCompanyFormed>2016</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>8FD4E53D-F077-469F-B710-6B423A2DE433</gtr:id><gtr:title>Exploiting projective geometry for view-invariant monocular human motion analysis in man-made environments</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/083b1a3ac649da20e04b06f2e13f8052"><gtr:id>083b1a3ac649da20e04b06f2e13f8052</gtr:id><gtr:otherNames>Rogez G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56af4fa8097b72.79184457</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>90E13740-0022-4060-A1B2-FD80A85FEEAE</gtr:id><gtr:title>Automatic dense visual semantic mapping from street-level imagery</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fc70088db3e5c706d92a1402b1affd20"><gtr:id>fc70088db3e5c706d92a1402b1affd20</gtr:id><gtr:otherNames>Sengupta S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1737-5</gtr:isbn><gtr:outcomeId>doi_53d059059a1a1ee7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E08C8A3-759C-4C5D-93B2-3F49B8EB1363</gtr:id><gtr:title>Improved Initialisation and Gaussian Mixture Pairwise Terms for Dense Random Fields with Mean-field Inference</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0fd42485365b0c4c1a85081cd2c15689"><gtr:id>0fd42485365b0c4c1a85081cd2c15689</gtr:id><gtr:otherNames>Vibhav Vineet (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_7403419646140951fa</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E32DCDE8-DFE8-433B-B9B6-D8490A9E4927</gtr:id><gtr:title>Urban 3D semantic modelling using stereo vision</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fc70088db3e5c706d92a1402b1affd20"><gtr:id>fc70088db3e5c706d92a1402b1affd20</gtr:id><gtr:otherNames>Sengupta S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>doi_53d058058ed8b90f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7CD5B775-8948-4600-A77E-BBF63E027903</gtr:id><gtr:title>Inference Methods for CRFs with Co-occurrence Statistics</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b487cb2a06228c4f89ba259ef32f29ba"><gtr:id>b487cb2a06228c4f89ba259ef32f29ba</gtr:id><gtr:otherNames>Ladick? L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53cfdefdef9181d0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A124E0B3-F8F0-4F45-AD98-10F0B90CB4F7</gtr:id><gtr:title>DenseCut: Densely Connected CRFs for Realtime GrabCut</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56af4ca6e36e49.28553726</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCAC0A25-0FA5-44F6-B46D-AEE6850263CB</gtr:id><gtr:title>Global contrast based salient region detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0394-2</gtr:isbn><gtr:outcomeId>5438302504f944.38398442</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1EDA764C-249F-4E14-9975-19CDA9074924</gtr:id><gtr:title>SalientShape: group saliency in image collections</gtr:title><gtr:parentPublicationTitle>The Visual Computer</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53cfd5fd5978c88c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D30E609E-F150-4E9A-80AC-69BFBD28B491</gtr:id><gtr:title>Dense Semantic Image Segmentation with Objects and Attributes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a340fbb42265c9ec6bc733e0a04c590"><gtr:id>2a340fbb42265c9ec6bc733e0a04c590</gtr:id><gtr:otherNames>Zheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>543295597867d5.31357831</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ED1BB8AF-516B-45DB-93FB-FC16655257E0</gtr:id><gtr:title>Approximate structured output learning for Constrained Local Models with application to real-time facial feature detection and tracking on low-power devices</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a340fbb42265c9ec6bc733e0a04c590"><gtr:id>2a340fbb42265c9ec6bc733e0a04c590</gtr:id><gtr:otherNames>Zheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn><gtr:outcomeId>doi_53d057057ba1da12</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D12F1B91-20DA-4EC8-A2C0-1D9D63EA5631</gtr:id><gtr:title>Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4e4dabbd6b89ad7e7ae0aaddcd5732f"><gtr:id>c4e4dabbd6b89ad7e7ae0aaddcd5732f</gtr:id><gtr:otherNames>Vineet V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56af4d898d2404.75276662</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7339DE2D-CE0D-4516-B52B-9CE896BB6697</gtr:id><gtr:title>Object Proposal Generation Using Two-Stage Cascade SVMs.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0355658aee7a567caa922e6adbda88b6"><gtr:id>0355658aee7a567caa922e6adbda88b6</gtr:id><gtr:otherNames>Zhang Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>56af4ca6c000b1.50920504</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B0AE061D-49ED-49BB-BDA1-C4923DAB7BA2</gtr:id><gtr:title>Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bc59f5fb736c3da3ed9d0d9ee91dddab"><gtr:id>bc59f5fb736c3da3ed9d0d9ee91dddab</gtr:id><gtr:otherNames>Valentin J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d05705742bc0a7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>05743E71-90CF-425C-A00C-183186648F90</gtr:id><gtr:title>ImageSpirit</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5438318ead7910.19950817</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CDB996D9-0C60-4F7C-B700-4BF8E95D5EA5</gtr:id><gtr:title>A tiered move-making algorithm for general pairwise MRFs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4e4dabbd6b89ad7e7ae0aaddcd5732f"><gtr:id>c4e4dabbd6b89ad7e7ae0aaddcd5732f</gtr:id><gtr:otherNames>Vineet V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1226-4</gtr:isbn><gtr:outcomeId>doi_53d057057422cee7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD28505F-415F-45E6-8594-2577B747F8BA</gtr:id><gtr:title>Efficient Salient Region Detection with Soft Image Abstraction</gtr:title><gtr:parentPublicationTitle>IEEE International Conference on Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/92874551456ea00ed1840fc236d95cdd"><gtr:id>92874551456ea00ed1840fc236d95cdd</gtr:id><gtr:otherNames> Ming-Ming Cheng (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_124709941513f0afb0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CFCA8A56-D720-4222-8944-ED93738358E5</gtr:id><gtr:title>Robust Non-parametric Data Fitting for Correspondence Modeling</gtr:title><gtr:parentPublicationTitle>IEEE International Conference on Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3eb486b2d1380e3d3f83b08315d9a119"><gtr:id>3eb486b2d1380e3d3f83b08315d9a119</gtr:id><gtr:otherNames> Wen-Yan Lin (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_8437715361140a2bf2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BD14870D-1F87-4BB3-96E4-220B97799B56</gtr:id><gtr:title>Filter-Based Mean-Field Inference for Random Fields with Higher-Order Terms and Product Label-Spaces</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4e4dabbd6b89ad7e7ae0aaddcd5732f"><gtr:id>c4e4dabbd6b89ad7e7ae0aaddcd5732f</gtr:id><gtr:otherNames>Vineet V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56af501d63f122.11075695</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BB7A752-5F82-4C74-A1F7-29F1AD2F3B1A</gtr:id><gtr:title>Learning Discriminative Space-Time Action Parts from Weakly Labelled Videos</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d51c9d60cb7ff175c0874f811f2391fd"><gtr:id>d51c9d60cb7ff175c0874f811f2391fd</gtr:id><gtr:otherNames>Sapienza M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56af501d8de2e8.84009295</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E7161AE-18BE-43F6-B5A8-C4F02CE0895B</gtr:id><gtr:title>Struck: Structured Output Tracking with Kernels.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2425a4d328ce2edcb596c40787925c07"><gtr:id>2425a4d328ce2edcb596c40787925c07</gtr:id><gtr:otherNames>Hare S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>56bbd52d6058b4.07783796</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB2B2C85-440C-4285-9DCE-B43865F49CC7</gtr:id><gtr:title>BING: Binarized Normed Gradients for Objectness Estimation at 300fps</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>543830a8d7e251.77319117</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I001107/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>8858F7A9-A6BD-48B3-8F36-2CA57F3BE94F</gtr:id><gtr:grantRef>EP/I001107/1</gtr:grantRef><gtr:amount>439228.41</gtr:amount><gtr:start>2011-09-07</gtr:start><gtr:end>2013-09-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>A8A6A881-0AC7-4B46-9ADF-2513845B1B63</gtr:id><gtr:grantRef>EP/I001107/2</gtr:grantRef><gtr:amount>327698.99</gtr:amount><gtr:start>2013-10-01</gtr:start><gtr:end>2016-02-29</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>