<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/883E05B9-AD63-4EFF-AA95-DAA17A43B1BF"><gtr:id>883E05B9-AD63-4EFF-AA95-DAA17A43B1BF</gtr:id><gtr:name>Conservatory of Italian Switzerland (Conservatorio della svizzera italiana)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/883E05B9-AD63-4EFF-AA95-DAA17A43B1BF"><gtr:id>883E05B9-AD63-4EFF-AA95-DAA17A43B1BF</gtr:id><gtr:name>Conservatory of Italian Switzerland (Conservatorio della svizzera italiana)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/4BA665DE-58CB-4522-83ED-59AB16CA9D7E"><gtr:id>4BA665DE-58CB-4522-83ED-59AB16CA9D7E</gtr:id><gtr:firstName>Daniel</gtr:firstName><gtr:surname>Leech-Wilkinson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/374CC611-8BEE-4FC5-906D-5BA19049C87B"><gtr:id>374CC611-8BEE-4FC5-906D-5BA19049C87B</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:otherNames>Palmer</gtr:otherNames><gtr:surname>McPherson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2026A4F7-C872-4123-9247-CA1646190CF3"><gtr:id>2026A4F7-C872-4123-9247-CA1646190CF3</gtr:id><gtr:firstName>Elaine</gtr:firstName><gtr:surname>Chew</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=AH%2FJ013145%2F1"><gtr:id>A751E031-C2DA-4599-8C6C-D8B9A89AC328</gtr:id><gtr:title>Measuring and Enhancing Expressive Musical Performance with Digital Instruments: Pilot Study and Research Workshop</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/J013145/1</gtr:grantReference><gtr:abstractText>This collaborative project between members of the Centre for Digital Music (C4DM) at Queen Mary, University of London and the multi-institution AHRC Research Centre for Musical Performance as Creative Practice (CMPCP) addresses the following questions:

* How can digital technology help explain what makes a performance expressive?
* How can an understanding of expressive performance guide the creation of new musical instruments and technologies?

Two emerging trends, if combined, hold the potential for transformative change in musical performance practice. First, the latest digital musical instruments capture the performer's actions with unprecedented detail, allowing continuous, precise control over every aspect of the resulting sound. Second, the study of musical performance as a creative act has taken a central role in musicology, with performers and scholars producing a vibrant interaction between theory and practice.

Music and technology have long been linked, but technology alone is not a driver of musical creativity. By extension, more dimensions of control do not make a digital instrument more expressive, and excessively complex interfaces can even become an impediment to expressive performance. The perspectives of performers and performance scholars are required to shape a new generation of digital instruments that are ideally suited to musicians' creative requirements.

On the other hand, digital technology is indispensable in the measurement and modelling of musical performance. Controlled quantitative studies of performers' actions complement qualitative techniques such as interviews and questionnaires to produce a detailed, balanced picture of performance practice. Digital musical instruments, including traditional acoustic instruments augmented with electronic sensors, provide a valuable data source concerning a performer's physical gestures.

This project promotes collaboration and knowledge exchange between performance scholars and digital music researchers through two main components:

First, a pilot study will be conducted using a sensor-enhanced acoustic piano. The study will focus on the link between expressive intent and physical gesture at the keyboard, and it will serve as a model for future extended cross-disciplinary collaborations. A refereed article will be published on the results, contributing to longstanding debates on the nature of physical keyboard technique (commonly known as &amp;quot;touch&amp;quot;).

Second, a research networking event will take place as a special paper session of the Computer Music Modeling and Retrieval (CMMR) conference in June 2012. The event will draw academics, postdocs and students from musicological and technological disciplines with the goal of identifying areas of shared interest. A concert performance will be held afterward with submissions invited from composers, performers and musicologists. Performers and scholars will be encouraged to attend both paper session and performance, promoting a wide range of perspectives at each event. Following the event, the project investigators will draft a document outlining potential areas of collaboration emerging from the session.

The proposed research will be directed by PI Andrew McPherson of C4DM with assistance from a postdoctoral researcher. Co-investigators Elaine Chew of C4DM and Daniel Leech-Wilkinson of King's College London/CMPCP will assist in the design of the pilot study and the organisation of the special session. All members of C4DM and CMPCP will be invited to contribute ideas on how digital technology can model and enhance expressive performance. Long-term impacts emerging from or influenced by this Research Development project include musical instruments that dynamically adapt to the abilities and tastes of the performer, interfaces for non-experts to express themselves musically, new pedagogical techniques and mathematical models of shape, phrasing and gesture in performance.</gtr:abstractText><gtr:potentialImpactText>The benefits of multidisciplinary research into expressive performance extend beyond academia. Beneficiaries in the commercial private sector include:

* Musical instrument companies. C4DM maintains relationships with manufacturers of both traditional instruments (e.g. Yamaha) and new interfaces (e.g. Focusrite/Novation). The proposed research could result in commercialisation of more expressive digital instruments.

* Musicians and sound artists, who benefit in two ways: (1) the deployment of new performance technologies in concert, and (2) the use of expressive performance models for reflection and self-analysis, potentially improving their skills.

* Computer hardware and software companies involved in creating human interfaces. Creating an intuitive interface is challenging, and understanding the subconscious processes involved in human-instrument musical interaction could inspire new approaches to interface design in other domains.

Beneficiaries in the public sector, third sector and the wider public:

* School music teachers, who can use digital instruments to improve and enhance both classroom curriculum and private lessons.

* Museums, through interactive exhibits exploring music performance (e.g. through live visualisation of sensor data).

* Amateur musicians, who benefit from (1) real-time feedback on their physical technique provided by sensor-equipped instruments, (2) a potentially faster learning curve provided by new instruments that are designed collaboratively with musicians and performance scholars, and (3) novel interfaces for musical expression which may emerge from collaborative study of expressive performance.

Cross-disciplinary activity of the type proposed in this project holds benefits for the UK at large. C4DM and CMPCP are internationally-leading centres in the fields of digital music and performance studies, respectively, and their activities draw students, academics and commercial partners from around the world. Joint efforts between the centres will open up exciting areas of study unique to the UK. What makes a performance expressive and how digital technology can influence it are challenging questions, and answering them will have lasting economic and cultural influence. By forging interdisciplinary collaborations, this proposal represents an initial step toward addressing this fascinating area of research.</gtr:potentialImpactText><gtr:fund><gtr:end>2012-09-19</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2012-02-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>23994</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>Three-concert performance series as part of the Computer Music Modeling and Retrieval conference, 2012, which blended traditional acoustic instruments with digital technology.</gtr:description><gtr:id>FA807189-0B36-408C-8E7E-1EC0B0129464</gtr:id><gtr:impact>Positive feedback from audiences and follow-up collaborations with participants, including a later project to provide a magnetic resonator piano kit (my digital instrument research) to a well-known UK composer/researcher to be used in a new piece.</gtr:impact><gtr:outcomeId>545f897812b757.69945583</gtr:outcomeId><gtr:title>CMMR concert</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Conservatory of Italian Switzerland (Conservatorio della svizzera italiana)</gtr:collaboratingOrganisation><gtr:country>Switzerland, Swiss Confederation</gtr:country><gtr:description>Jennifer MacRitchie CSI/UWS</gtr:description><gtr:id>D081DA38-5FCC-4F5F-8988-CA24F288628E</gtr:id><gtr:impact>J. MacRitchie and A. McPherson. Integrating optical finger motion tracking with surface touch events. Frontiers in Psychology, 2015, 6:702.

This project was also supported by a travel grant from the Swiss National Science Foundation.</gtr:impact><gtr:outcomeId>545b9ad42348b6.62332416-1</gtr:outcomeId><gtr:partnerContribution>Partner conservatory provided access to facilities (rehearsal spaces and piano) and time of piano teachers and students. Research partner also set up high-speed camera system for tracking hand and arm motion at the keyboard. Research collaborator has since moved to University of Western Sydney (2014) where data analysis continues.</gtr:partnerContribution><gtr:piContribution>Set up capacitive touch sensing technology we developed on an acoustic piano in a music conservatory environment, in order to capture detailed information of the finger motion of expert and student pianists.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>CMMR workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A09F2968-1CB3-46A3-A042-AF3BB543A7D6</gtr:id><gtr:impact>Organised an interdisciplinary workshop on expressive performance at the Computer Music Modelling and Retrieval conference, 2012. Brought together participants from scientific, artistic and humanities backgrounds. Paper presentations sparked discussions and helped make connections between participants.

Led to further collaborations with individuals in the workshop and wider interest in my instrument research.</gtr:impact><gtr:outcomeId>545f88e642afc0.26856139</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TouchKeys IRCAM</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>4DD66B7D-1CDC-49F7-BBC8-1C05F78757D6</gtr:id><gtr:impact>Live demo of TouchKeys multi-touch keyboard generated significant audience interest and led to online media coverage.

Interest from public in purchasing their own TouchKeys keyboards.</gtr:impact><gtr:outcomeId>545bb7f71cd331.88772704</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>30500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Queen Mary Innovation</gtr:department><gtr:description>QTech Commercialisation Fund</gtr:description><gtr:end>2015-01-02</gtr:end><gtr:fundingOrg>Queen Mary University of London</gtr:fundingOrg><gtr:id>9633E222-449F-43F3-A214-62441723ABF7</gtr:id><gtr:outcomeId>545baaeedc2d85.59865004</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2014-07-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Queen Mary Innovation</gtr:department><gtr:description>Queen Mary Innovation Fund</gtr:description><gtr:end>2013-09-02</gtr:end><gtr:fundingOrg>Queen Mary University of London</gtr:fundingOrg><gtr:id>F1A36863-CBA4-4908-9ED4-6D516569F524</gtr:id><gtr:outcomeId>545baac2c44477.14604705</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2013-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3510</gtr:amountPounds><gtr:country>Switzerland, Swiss Confederation</gtr:country><gtr:currCode>CHF</gtr:currCode><gtr:currCountryCode>Switzerland</gtr:currCountryCode><gtr:currLang>it_CH</gtr:currLang><gtr:description>International Short Visits</gtr:description><gtr:end>2013-07-02</gtr:end><gtr:fundingOrg>Swiss National Science Foundation</gtr:fundingOrg><gtr:id>B95CDE13-7D87-42CD-8E67-BDB3333FB627</gtr:id><gtr:outcomeId>545ba9a7829646.20675399</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2013-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The pilot study of keyboard technique in this project led to new mappings between motion and sound on the TouchKeys keyboard: these mappings let the performer add vibrato and pitch bends to each note in a way that did not interfere with traditional piano technique. This compatibility between existing experience and new techniques made it possible to develop the TouchKeys into a product suitable for a wider performer community.

Over the 2012-2013 period, an improved prototype of the TouchKeys hardware was developed incorporating feedback from user studies (for example: two-dimensional sensing on both black and white keys, and better ergonomics). 

In July 2013, I launched a Kickstarter crowd-funding campaign [1] to produce and distribute TouchKeys instruments to musicians. Both prebuilt instruments and self-install sensor kits were available. The campaign raised over &amp;pound;46,000, exceeding its goal of &amp;pound;30,000. Instruments were shipped to musicians in 20 countries. The campaign also generated a large amount of publicity, including 25 media article, 7,000+ Facebook likes and 150,000+ YouTube plays. The publicity led to invited presentations at the IRCAM Forum (Paris, November 2013) and the Innovation in Music conference (York, December 2013).

Since the instruments have shipped, musicians who bought the TouchKeys have begun uploading their own videos. Examples include using it to simulate the string bending found in many rock guitar solos, and splitting the keys into multiple segments by touch location in order to play microtonal Turkish maqam music. 

The software has been released open source (GNU Public Licence).

In 2015, I launched a second production run of TouchKeys. In total sales of kits and instruments have now totalled &amp;pound;100k. In 2015, QMUL began the process of spinning out the project into an independent company, TouchKeys Instruments Ltd. In December 2016, the company launched a web shop for continuing public sale of the kits and instruments, and remains in ongoing talks with major music manufacturers.

[1] https://www.kickstarter.com/projects/instrumentslab/touchkeys-multi-touch-musical-keyboard</gtr:description><gtr:id>E4ED517F-1D19-4444-A274-F86CBA0846C3</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5464ae05c58171.02391736</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Covers data processing methods for adding expressive controls to piano playing using the TouchKeys multi-touch keyboard sensors. The notable feature of the new methods are that they naturally integrate with existing keyboard technique by relying on particular patterns of motion of the fingers rather than raw sensor readings.</gtr:description><gtr:grantRef>AH/J013145/1</gtr:grantRef><gtr:id>64392A44-5823-4CCB-A66D-7D3BFB5D68EC</gtr:id><gtr:impact>Commercial licensing discussions are actively underway at the time of writing, but have not formally concluded.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56de0858b00e77.23432323</gtr:outcomeId><gtr:patentId>WO 2015/028793</gtr:patentId><gtr:protection>Patent application published</gtr:protection><gtr:title>Control methods for expressive musical performance</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>This project built connections among musicians, musicologists, computer scientists and engineers to study expressive musical performance. Instrument design was a particular focus of the project since it draws on each of these areas.

The project was organised around two activities: an interdisciplinary workshop on expressive performance held at the Computer Music Modeling and Retrieval conference, Queen Mary University of London, in June 2012; and a pilot study of keyboard technique using the TouchKeys multi-touch keyboard technology previously developed by the PI.

The workshop included 10 speakers from a range disciplines, coming from across Europe and North America. Each presented a short paper followed by a brief group discussion. The workshop was complemented by a three-evening series of concerts featuring new instruments and new performance technologies. These were held at the historic Wilton's Music Hall in London.

The pilot study of keyboard technique yielded new insights into the physical motion of the pianist's fingers while playing the keyboard: though sound production on the piano itself is percussive in nature (hammers striking strings), the motions needed to play the piano are by nature continuous. Pianists nearly universally agree that the subtle details of physical motion, known as &amp;quot;touch&amp;quot;, are crucial to achieving the right expressive results.

A major finding of the pilot study was how on a new augmented instrument, novel techniques could be integrated alongside familiar ones. Playing the piano is already a complex activity, and it is crucial that any new techniques, such as those made possible by the TouchKeys sensors, do not interfere with traditional playing. We developed new methods of adding vibrato and pitch bends to each note on the keyboard and verified their usability in a study of conservatory pianists.</gtr:description><gtr:exploitationPathways>The findings on expressive piano touch hold relevance to piano teachers and students as a way of better understanding the subtleties of motion needed to perform at an expert level. The sensor technology and analytical methods could potentially be deployed in the context of piano lessons, for example through visualisation of how the fingers move on the keys.

Insights into expressive piano touch are also highly relevant to instrument designers, where connecting to a performer's existing experience is important to achieving a good reaction. The more a new instrument can draw on existing technique, the faster the path to expertise on that instrument. Where the new instrument introduces completely new techniques, it is important that they fit with the constraints of traditional performance. Our research yielded insights in this area, and they could be put to use by instrument builders through analysis of our motion data (specifically in the case of the keyboard) or in following similar methodologies (applicable to many instruments).

Finally, the TouchKeys multi-touch keyboard, used for the studies, was itself significantly improved as a result of the project, and it has already had wide-ranging commercial and musical impact. Further details are found in the RCUK Narrative Impact section.</gtr:exploitationPathways><gtr:id>48E57C81-76C2-4042-AD30-EDE700ADD73C</gtr:id><gtr:outcomeId>5464ab6c128981.77151233</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Open source control software for the TouchKeys multi-touch keyboard sensors, which use capacitive sensing to transform any keyboard into an expressive multi-touch surface.</gtr:description><gtr:id>CF6A5061-DFD0-4D42-8112-904CE35BED8F</gtr:id><gtr:impact>Successful Kickstarter crowd-funding campaign raised &amp;pound;46k to build and distribute instruments to musicians in 20 countries; a second production run commenced in 2015, bringing the total raised to around &amp;pound;100k. Musicians using the instrument have contributed their own videos, and development of the project is ongoing. Invitation to present TouchKeys in several international conferences and venues.</gtr:impact><gtr:outcomeId>545bb71d5bf4e1.96018248</gtr:outcomeId><gtr:title>TouchKeys software</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/projects/touchkeys/</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>TouchKeys Instruments Ltd.</gtr:companyName><gtr:description>TouchKeys Instruments Ltd. is a spin-out focused on commercialising the TouchKeys multi-touch keyboard research developed by Andrew McPherson in the Centre for Digital Music at Queen Mary University of London. TouchKeys transforms the piano-style keyboard into an expressive multi-touch control surface using capacitive sensing on the surface of every key. With TouchKeys the player can naturally add vibrato, pitch bends and other forms of continuous note shaping to each note.

The company aims to support a license arrangement for the TouchKeys IP to a larger keyboard manufacturer, while also selling self-install sensor kits directly to the public.</gtr:description><gtr:id>A8235DB3-A0F9-4E91-AF17-3D71E3DBAE31</gtr:id><gtr:impact>The company was formed in mid-2015 and began commercial licensing discussions with an established European keyboard manufacturer in early 2016. These discussions are still ongoing. In the meantime, in December 2016 the company launched a new web site with an online shop supporting sales of kits and keyboards to the general public. In addition to kit sales, this activity has generated media publicity and led to several invited talks.</gtr:impact><gtr:outcomeId>56de2075baec73.63309878</gtr:outcomeId><gtr:url>http://touchkeys.co.uk</gtr:url><gtr:yearCompanyFormed>2015</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>DB8ED819-3A2A-41C4-9BF4-0E0B26BC4E21</gtr:id><gtr:title>Buttons, Handles, and Keys: Advances in Continuous-Control Keyboard Instruments</gtr:title><gtr:parentPublicationTitle>Computer Music Journal</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f330dc5ee7b3e936e7721bb795e40609"><gtr:id>f330dc5ee7b3e936e7721bb795e40609</gtr:id><gtr:otherNames>McPherson A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de04563b3db0.32845803</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E147D809-2848-44F9-99C6-5ED7F137E727</gtr:id><gtr:title>Integrating optical finger motion tracking with surface touch events.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3ef3a11797f28a9a0e49d539d8b66e75"><gtr:id>3ef3a11797f28a9a0e49d539d8b66e75</gtr:id><gtr:otherNames>MacRitchie J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>56de047341a621.27316436</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">AH/J013145/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>47D79871-3D9E-42F7-83E1-354D081901C7</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Classical Music</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>55773495-BB0B-43EB-B99D-D5C15272A52F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musical Performance</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>120E89AC-386D-4E25-9417-A3FE4D6FE83A</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musicology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>