<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C514A3A9-E140-4888-AFB2-5814D127FD79"><gtr:id>C514A3A9-E140-4888-AFB2-5814D127FD79</gtr:id><gtr:name>MRC Centre Cambridge</gtr:name><gtr:department>MRC Cognition and Brain Sciences Unit</gtr:department><gtr:address><gtr:line1>Hills Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 0QH</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C514A3A9-E140-4888-AFB2-5814D127FD79"><gtr:id>C514A3A9-E140-4888-AFB2-5814D127FD79</gtr:id><gtr:name>MRC Centre Cambridge</gtr:name><gtr:address><gtr:line1>Hills Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 0QH</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A8AB4D99-6257-4691-A685-760851A7D21C"><gtr:id>A8AB4D99-6257-4691-A685-760851A7D21C</gtr:id><gtr:firstName>Karl</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Friston</gtr:surname><gtr:orcidId>0000-0001-7984-8909</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/5AA55F7C-FB63-4ED6-91C1-2197B0B993A0"><gtr:id>5AA55F7C-FB63-4ED6-91C1-2197B0B993A0</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:surname>Furl</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D40B6B6D-82AA-4DAA-BC1B-A733F7DA3B21"><gtr:id>D40B6B6D-82AA-4DAA-BC1B-A733F7DA3B21</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Calder</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/31D382D5-AE31-473A-A1DF-24F20058A5F5"><gtr:id>31D382D5-AE31-473A-A1DF-24F20058A5F5</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Henson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FI01134X%2F1"><gtr:id>C905D2E0-3381-4086-93CD-E79ABF8501C1</gtr:id><gtr:title>Neural Representation of the Identities and Expressions of Human Faces</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/I01134X/1</gtr:grantReference><gtr:abstractText>&lt;p>Although a person's facial identity is immutable, faces are dynamic and undergo complex movements which signal critical social cues (viewpoint, eye gaze, speech movements, expressions of emotion and pain).&amp;nbsp; These movements can confuse automated systems, yet humans recognise moving faces robustly.&lt;/p>

&lt;p>Our objective is to discover the stimulus information, neural representations and computational mechanisms that the human brain uses when recognising social categories from moving faces. We will use human brain imaging to put an existing theory to the test. This theory proposes that recognition of changeable attributes (eg, expression) and facial identity are each recognised separately by two different brain pathways, each in a different part of the temporal lobe of the brain.&lt;/p>

&lt;p>The evidence we provide might indeed support and fill in many gaps in this theory. Nevertheless, we expect instead to instantiate a new alternative theory. By this new theory, some brain areas can recognise both identities and expressions, using unified representations, with one of the two pathways specialised for representing movement. Thus, the successful completion of our project will provide a new theoretical framework sufficient to motivate improved automated visual systems and advance new directions of research on human social perception. &lt;/p></gtr:abstractText><gtr:fund><gtr:end>2014-06-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2011-09-26</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>257979</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Please note that ES/I01134X/1 and ES/I01134X/2 are the same grant, held by the PI when he was at different institutions. These have the same impacts.

The outputs of this grant were designed from the beginning to be primarily academic in nature. Although there are currently few documented demonstrations of influence outside of academia at present, there is potential that this grant will have indirect influence in the long term.

This grant addresses the issue of how the visual system recognises facial identities and expressions from dynamic videos. Understanding how emotional information is detected by the neural mechanisms in visual system may, in the long term, help those suffering disorders in face perception, such as prosopagnosia, or those suffering anxiety disorders, who are adversely affected when encountering emotional or threatening visual information.

As a result of this grant, our laboratory has developed a method for extracting and quantifying facial motion from video. We also have developed methods for animating computer rendered 3D models of faces with extracted motion data. These results have not yet been published. We have submitted one paper and other studies on these methods are ongoing. With further study, these methods may improve how automated systems process video data from faces. They may also create new graphics applications.

The grant has had a positive training outcome on Dr Furl's career, as he has transitioned from a soft money principal investigator at the MRC CBU to a permanent lectureship post at Royal Holloway, University of London.</gtr:description><gtr:id>EC06A2F1-FD9E-43F2-A84A-64665A3562D1</gtr:id><gtr:impactTypes/><gtr:outcomeId>56e04dc5a765e8.58359118</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The brain is composed, in part, of pathways: connected networks of areas that represent information and perform an important function. The main objective of our ESRC grant was to explore whether there are multiple distinguishable brain pathways responsible for recognition of identities versus recognition of expressions and how these pathways relate to neural representations of facial form and facial motion. We hypothesised that an analysis of brain responses to dynamic videos of facial movements would reveal new insight into this pathway structure in the brain. One of our aims was to use computational models (e.g., dynamic causal modelling, DCM) to test models of pathway structure against our data. 
Below I provide brief descriptions of the ESRC-funded studies that use dynamic faces and/or DCM methods to reveal these brain pathways, their facial form and motion representations, and how they can interact together when recognising facial expressions. We have also used connectivity models and methods to learn more about these pathways including (a) &amp;quot;face-blind&amp;quot; individuals, who are less able to recognise faces (developmental prosopagnosics), show deficits in how brain areas are connected, suggesting a facial identity-related pathway has been disrupted in these individuals; (b) brain areas that respond to faces use a characteristic &amp;quot;cross-frequency coupling&amp;quot; pattern to communicate, which may be a signature for the transmission of information in the face recognition pathways. Our grant is also associated with new studies, that remain ongoing and unpublished but are coming soon, and that further explore these topics.

Please note that part of this research was conducted at the MRC Cognition Brain Sciences Unit (CBU) in Cambridge and part of it was performed at Royal Holloway, University of London (RHUL). The work at these institutions derived from one grant project but are listed on ResearchFish as separate grants and so we have reported separate outputs for each. Here, we describe the details of outputs resulting from work at the CBU (ES/I01134X/1). Please see ResearchFish report for ES/I01134X/2 for details of outputs resulting from work conducted at RHUL.


(1) The first output that we published was an analysis of existing macaque functional magnetic resonance imaging data (fMRI, a type of brain imaging that creates detailed maps of brain responses to stimuli, such as faces). Using special statistical methods known as multivariate decoding, we found evidence for facial expression representations in brain areas that ordinarily respond to motion. This paper suggests an important role for motion-sensitive brain areas in recognition of both dynamic and static facial expressions.

Furl N*, Hadj-Bouziane F*, Liu N, Averbeck BB, Ungerleider LG. 2012. Dynamic and static facial expressions decoded from motion-sensitive areas in the macaque monkey. J Neurosci 32:15953-62. *co-first authors

(2) The second output that we published is also an analysis of a previously existing dataset. These data come from magnetoencephalography (MEG). MEG allows us to track brain responses very quickly so that we can analyse the frequency with which they increase and decrease in activity at the millisecond level (neural oscillations). Using DCM as a connectivity modelling technique, we discovered a characteristic frequency signature that areas responding to faces use to communicate with each other. This finding of a fundamental mechanism could aide future research that wishes to track how the different face recognition pathways transfer information.

Furl N, Coppola R, Averbeck BB, Weinberger D. 2014. Cross-frequency power coupling among hierarchically-organized face-selective areas. Cerebral Cortex 24:2409-20.


(3) The third output that we published also used DCM to develop pathway models of the face recognition system. However, in this study, DCM was applied to fMRI data. We aimed to discover how brain areas representing form (e.g., FFA) and those representing motion (e.g., V5) interact when confronted with dynamic and static fearful expressions. We found that fearful faces enhanced responses in motion areas when faces were moving but they enhanced responses in form areas when faces were static. Our connectivity modelling showed this occurred because of the influence of the amygdala, an area involved in emotion. Thus, it seems form and motion pathways can each detect fearful information, depending on whether the face is dynamic, and they are controlled by the amygdala.

Furl N, Henson RN, Friston KJ, Calder AJ. 2013. Top-down control of visual responses to fear by the amygdala. J Neurosci 33:17435-43.


(4) A fourth output that we published investigated why one brain area, the superior temporal sulcus, responds only to faces that are moving, and not to objects or static stimuli. Our DCM analysis showed that the form and motion pathways work together to produce responses in the STS that are specific to facial form and movement. We conclude, as our grant hypothesised, that dynamic facial stimuli can reveal interactions between form and motion pathways subserving face recognition.

Furl N, Henson RN, Friston KJ, Calder AJ. 2015. Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus. Cerebral Cortex 25:2876-82.</gtr:description><gtr:exploitationPathways>Our research has applications in developing artificial visual recognition systems for video information as well as developing clinical models of brain disorders. These studies suggest ways that abstract visual information can be coded by neurons as well as the computations these neurons perform when coding. This knowledge can be used to devise and improve artificial visual recognition systems. Particularly, our results using dynamic facial stimuli can help develop software which can visually recognise video. Our research using the macaque monkey will help develop animal models of visual function. Our research on oscillatory communication between brain regions is a first step towards developing sophisticated models of disorders such as schizophrenia.</gtr:exploitationPathways><gtr:id>9742A54E-8086-49C5-B146-0B8A926DEA58</gtr:id><gtr:outcomeId>r-1096435395.1592329b2f2fc12</gtr:outcomeId><gtr:sectors><gtr:sector>Education</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F877B344-4CF2-403B-BFFA-6C860EB89A46</gtr:id><gtr:title>Network Interactions Explain Sensitivity to Dynamic Faces in the Superior Temporal Sulcus.</gtr:title><gtr:parentPublicationTitle>Cerebral cortex (New York, N.Y. : 1991)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b3e119c6de2dd7da3498604603acb757"><gtr:id>b3e119c6de2dd7da3498604603acb757</gtr:id><gtr:otherNames>Furl N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1047-3211</gtr:issn><gtr:outcomeId>545a4f98bda141.66328662</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D3D5ED1-80C8-48E0-8884-3A323304EE16</gtr:id><gtr:title>Top-down control of visual responses to fear by the amygdala.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b3e119c6de2dd7da3498604603acb757"><gtr:id>b3e119c6de2dd7da3498604603acb757</gtr:id><gtr:otherNames>Furl N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>pm_53cc027c0273a8429</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EF09AF51-692D-4371-A9BB-12AC5122ED4C</gtr:id><gtr:title>Dynamic and static facial expressions decoded from motion-sensitive areas in the macaque monkey.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b3e119c6de2dd7da3498604603acb757"><gtr:id>b3e119c6de2dd7da3498604603acb757</gtr:id><gtr:otherNames>Furl N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>545a4f98989af1.62540247</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BBE49799-FD0A-47F7-A383-CB25E7B9C656</gtr:id><gtr:title>Cross-frequency power coupling between hierarchically organized face-selective areas.</gtr:title><gtr:parentPublicationTitle>Cerebral cortex (New York, N.Y. : 1991)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b3e119c6de2dd7da3498604603acb757"><gtr:id>b3e119c6de2dd7da3498604603acb757</gtr:id><gtr:otherNames>Furl N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1047-3211</gtr:issn><gtr:outcomeId>56deb8bf02b2d7.31381267</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/I01134X/1</gtr:identifier><gtr:identifier type="RES">RES-062-23-2925</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>C905D2E0-3381-4086-93CD-E79ABF8501C1</gtr:id><gtr:grantRef>ES/I01134X/1</gtr:grantRef><gtr:amount>257979.56</gtr:amount><gtr:start>2011-09-26</gtr:start><gtr:end>2014-06-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>F6BBADB2-9818-473F-B80D-A693D6D34F7B</gtr:id><gtr:grantRef>ES/I01134X/2</gtr:grantRef><gtr:amount>61166.49</gtr:amount><gtr:start>2014-06-02</gtr:start><gtr:end>2015-12-01</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>FFA3A6C9-532B-4B83-B6F7-AAF3E63D34F0</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Social Stats., Comp. &amp; Methods</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>