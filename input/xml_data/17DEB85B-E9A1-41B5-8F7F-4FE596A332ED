<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Electrical and Electronic Engineering</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7C777568-AEA8-464E-8AF8-B1DFE6358842"><gtr:id>7C777568-AEA8-464E-8AF8-B1DFE6358842</gtr:id><gtr:name>Bar-Ilan University</gtr:name><gtr:address><gtr:line1>Bar-Ilan University</gtr:line1><gtr:line4>Ramat-Gan</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Israel</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E"><gtr:id>598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E</gtr:id><gtr:name>Friedrich-Alexander University</gtr:name><gtr:address><gtr:line1>Schlossplatz 4</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/64C9E3D9-D3AA-4878-931B-1B3A9F45A351"><gtr:id>64C9E3D9-D3AA-4878-931B-1B3A9F45A351</gtr:id><gtr:firstName>Christine</gtr:firstName><gtr:surname>Evers</gtr:surname><gtr:roles><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FP001017%2F1"><gtr:id>17DEB85B-E9A1-41B5-8F7F-4FE596A332ED</gtr:id><gtr:title>Acoustic Signal Processing and Scene Analysis for Socially Assistive Robots</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/P001017/1</gtr:grantReference><gtr:abstractText>The interaction between users and a robot often takes place in busy environments in the presence of competing speakers and background noise sources such as televisions. The signals received at the microphones of the robot are hence a mixture of the signals from multiple sound sources, ambient noise, and reverberation due to reflections of sound waves. Thus, in order to focus on stimuli of interest, the robot has to learn and adapt to the acoustic environment.

The aim of this research is to provide robots and machines with the ability to understand and adapt to the surrounding acoustic environment. Acoustic scene analysis combines salient features from the observed audio signals in order to create situational awareness of the environment; Sound sources are detected, localised and identified, whilst acoustic properties of the room itself can be characterised. Using the information acquired by analysing the acoustic scene, a three-dimensional map of the environment is created, and can be used to identify sounds or recognise the intent of speech signals. Moreover, by moving within the environment, the robot can explore and learn about the acoustic properties of its surrounding. 

However, many of the tasks required for analysis of the acoustic scene are jointly dependent. For example, localising the sources of sounds buried in noise and reverberation is a challenging problem. Sound source localisation can be improved by enhancing the signals of desired sources, such as human speakers, whilst suppressing interfering sources, such as a television. However, for source enhancement, desired and interfering sources must be spatially distinguished, hence requiring knowledge of the source directions. 

The novel objective of this research is therefore to identify and exploit constructively the joint dependencies between the tasks required for acoustic scene analysis. To achieve this objective, the project will take advantage of the motion of the robot in order to look at uncertain events from different perspectives. Techniques will be developed to constructively exploit motion of the robot's arms by fusing microphones attached to the robot's limbs with microphone arrays installed in the robot head. Furthermore, approaches will be investigated that allow multiple robots to share their experience and knowledge about the acoustic environment.

The research will be conducted at Imperial College London, within the Department of Electrical and Electronic Engineering with academic advice from national, European, and international project partners at the University of Edinburgh, UK; International Audio Laboratories Erlangen, Germany; and Bar-Ilan University, Israel.</gtr:abstractText><gtr:potentialImpactText>Equipping machines with an understanding of the acoustic environment allows a robot to engage in verbal interactions with humans. The robot can also adapt to the environment, e.g., by stepping closer or increasing the volume. Moreover, abnormal sounds, such as the noise due to a household accident, can be detected and appropriate actions can be taken, e.g., calling an ambulance.

The potential impact of this project therefore spans across the sectors of healthcare, industry, academia, and the government.

From the perspective of healthcare, socially assistive robots that are capable of providing physical aid and non-physical interaction could facilitate low-cost assistance for the 1.6 million people in the UK who provide over 50 hours a week of unpaid care to family and friends, as well as patients who cannot rely on the support of relatives.

For the industry, the research results have the potential to impact a wide range of applications that rely on processing of sounds in realistic environments, including search-and-rescue technology, hearing aids, home entertainment systems, and automatic speech recognition systems. 

For academia, the trans-disciplinary nature of the project has the potential to help bridge the gap between the fields of robotics, digital signal processing, and acoustics. 

Finally, from a societal perspective, research targeting intuitive human-robot interaction promotes public acceptance of robots in everyday life, thereby supporting the government's initiative to become one of the world leading nations in the field of Robotics and Autonomous Systems, a market with estimated global economic impact of USD 1.7-4.5 trillion annually by 2025.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2017-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>330104</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>30F87ADF-BD0C-4B44-9B12-549F2CAB84E9</gtr:id><gtr:title>Sparse parametric modeling of the early part of acoustic impulse responses</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df5998a20513c4927c14e4c90f6c7b3b"><gtr:id>df5998a20513c4927c14e4c90f6c7b3b</gtr:id><gtr:otherNames>Papayiannis C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa66fc6978d59.39071381</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B7DE912A-E8B0-4793-BE0D-0792722C7E52</gtr:id><gtr:title>Optimized Self-Localization for SLAM in Dynamic Scenes Using Probability Hypothesis Density Filters</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/982c0a9310c399b15362fc3130fc7883"><gtr:id>982c0a9310c399b15362fc3130fc7883</gtr:id><gtr:otherNames>Evers C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2ff0fb0ec1a7.02119709</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/P001017/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F78E4567-DD59-4364-9D1F-0A778996E941</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Instrumentation Eng. &amp; Dev.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>