<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Institute for Global Health</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D7342094-0342-4B1B-8995-AB151CF6797B"><gtr:id>D7342094-0342-4B1B-8995-AB151CF6797B</gtr:id><gtr:name>Helen Hamlyn Trust</gtr:name><gtr:address><gtr:line1>64 Old Church Street</gtr:line1><gtr:postCode>SW3 6EB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F79C4700-36E9-45D1-9DC0-51C1904430A0"><gtr:id>F79C4700-36E9-45D1-9DC0-51C1904430A0</gtr:id><gtr:firstName>Ara</gtr:firstName><gtr:otherNames>Warkes</gtr:otherNames><gtr:surname>Darzi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6EB766C7-CF7E-47FD-8F42-2F6211709069"><gtr:id>6EB766C7-CF7E-47FD-8F42-2F6211709069</gtr:id><gtr:firstName>Guang-Zhong</gtr:firstName><gtr:surname>Yang</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI027769%2F1"><gtr:id>6C3F0931-4206-4EDE-8697-12BA2CC1BC48</gtr:id><gtr:title>SMART-Endomicroscopy (Sensing and Mechatronically Assisted Real-Time Endomicroscopy)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I027769/1</gtr:grantReference><gtr:abstractText>As surgical techniques become more minimally invasive, there is an inevitable increase in complexity of the operating platforms. Clinically, it is of vital importance to integrate real-time microscopic visualisation into the surgical workflow in a seamless way and in combination with the existing imaging modalities such as MRI, CT and ultrasound. The aim of this proposal is to develop a new SMART-endomicroscopic probe that integrates in situ sensing and mechatronic control to allow for flexible and consistent tissue characterisation for Minimally Invasive Surgery (MIS). It addresses specific research and engineering challenges related to biophotonics, mechanical/optical miniaturisation and integration, super-resolution image reconstruction, as well as tracking, localisation and large area coverage for in vivo, in situ tissue characterisation. Through multi-scale minimally invasive imaging integration, the proposed SMART-Endomicroscope will potentially allow cancer staging and intervention to be performed as a single procedure, permitting histological and vascular examination to be performed at all stages of the operation, such that management decisions can be altered intra-operatively to ensure best treatment for the individual patient. This will help to transform surgical treatment options and patient cancer outcomes as personalised healthcare becomes a reality. The project echoes the current paradigm shift in MIS towards miniaturised smart instruments with integrated imaging and sensing, enhanced by robotic control. It is supported by a multidisciplinary team with complementary skills in physics, engineering, medical image computing and surgery.</gtr:abstractText><gtr:potentialImpactText>The proposed project is in response to the current paradigm shift and clinical demand in healthcare for bringing cellular and molecular imaging modalities to an in vivo - in situ setting to allow for real-time tissue characterisation, functional assessment, and intraoperative guidance. The types of stakeholders will benefit from this research include: Academia (both UK and international) in medical imaging, biophotonics, sensing, vision, robotics, surgical oncology and general biomedical engineering; Public and private sectors in healthcare provision; Medical devices industry; Charities championing improved healthcare provision and disease focussed organisations particularly on cancer therapy; and more importantly patients and the general public. The benefit from this research includes potentially significant technological, social, commercial and economic impact. It not only enhances research capacity but also brings tangible knowledge transfer opportunities. The project addresses important research and development challenges, and the end results are likely to transform clinical research and practices in cancer staging, minimally invasive surgery, interventional imaging, and the development of molecular imaging techniques. The proposed project will potentially allow cancer staging and intervention to be performed as a single procedure, permitting histological and vascular examination to be performed at all stages of the operation, such that management decisions can be altered intra-operatively to ensure best treatment for the individual patient. The project provides an exciting opportunity in cross-scale morphological and functional integration of surgical information, not only because of the acquisition of real-time images which can be relayed to the surgeon using the same interface as other imaging modalities, but also because of the relatively small and flexible nature of the hardware probe. As the technical innovations described are introduced we are likely to generate opportunities in clinical applications which are not imaginable with current operating platforms, aiding in the minimisation of patient trauma and personalisation of healthcare treatment.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-03-25</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-09-26</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>996323</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Surgical Imaging Workshop at Hamlyn Symposium</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>21F109AF-4B8D-4CAD-90E2-D51132C6ED69</gtr:id><gtr:impact>The Surgical Imaging Workshop at the Hamlyn Symposium in 2012, 2013, 2014, 2015 and 2016 brought together clinicians, computer scientists and engineers from academia and industry to share the latest developments in the fields of surgical imaging, image guidance and
augmented reality in surgery. The scope was broad, covering imaging technologies (including clinical
systems and applications) as well as the technical aspects of registration, modelling and visualisation.
Through the development and fusion of imaging modalities, and the enhancement of the surgeon's
sensory experience, participants in this exciting area of research hope to transform surgical practice.</gtr:impact><gtr:outcomeId>58c7107c1d18f2.57332723</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.hamlyn-robotics.org/</gtr:url><gtr:year>2012,2013,2014,2015,2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>249549</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Horizon Scan</gtr:description><gtr:end>2016-05-02</gtr:end><gtr:fundingOrg>Bill and Melinda Gates Foundation</gtr:fundingOrg><gtr:fundingRef>OPP1127324</gtr:fundingRef><gtr:id>C3ED4844-5AE0-46B3-B0F3-FC313A6A7128</gtr:id><gtr:outcomeId>58c91407815824.09893457</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>249493</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Translational Alliance Platform</gtr:description><gtr:end>2018-04-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/N022521/1</gtr:fundingRef><gtr:id>EB988EF9-8FA1-4567-98FF-4B6DD111B478</gtr:id><gtr:outcomeId>58c15c98495832.13948972</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-05-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The potential of the technology to be applied to global healthcare challenges (unanticipated in the original Pathways to Impact) has been identified by the Bill and Melinda Gates Foundation who we have now partnered with in an EPSRC Translational Alliance Platform (EP/NO22521/1) to explore this further. Key pieces of know-how developed under the project have been transferred to Imperial Innovations and are being considered for future exploitation. The project provided excellent training for several early career scientists and engineers, with three postdoctoral researchers associated with this grant (a named researcher and two staff employed on the project) gaining permanent academic positions in the UK and China, while a student and a further named researcher have moved into industry in technical roles, benefiting the UK's wider high-tech economy.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>BAAFE805-8CA8-41FB-88E5-D8003E39B146</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545f350907a4c1.05447780</gtr:outcomeId><gtr:sector>Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>N/A</gtr:description><gtr:grantRef>EP/I027769/1</gtr:grantRef><gtr:id>BBCDD409-2F51-4379-B182-8E508BD1CA2C</gtr:id><gtr:impact>N/A</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>58c70da847f6b0.84089657</gtr:outcomeId><gtr:protection>Protection not required</gtr:protection><gtr:title>Know-how registered with Imperial Innovations - Mosaicking algorithm</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>N/A</gtr:description><gtr:grantRef>EP/I027769/1</gtr:grantRef><gtr:id>DC0133FF-997F-4393-A461-8390789581B7</gtr:id><gtr:impact>N/A</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>58c70d15a625d6.29135384</gtr:outcomeId><gtr:protection>Protection not required</gtr:protection><gtr:title>Know-how registered with Imperial Innovations - Know-how around endomicrocsopy instrument and design</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>N/A</gtr:description><gtr:grantRef>EP/I027769/1</gtr:grantRef><gtr:id>A8E8CB21-48F7-46E6-9231-7E2FB0DB49EF</gtr:id><gtr:impact>N/A</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>58c70d7080e1f7.23070529</gtr:outcomeId><gtr:protection>Protection not required</gtr:protection><gtr:title>Know-how registered with Imperial Innovations - Method of performing white light microscopy via a fibre imaging bundle</gtr:title></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>The aim of this work was to develop a SMART-Endomicroscopic probe with integrated imaging, sensing and mechatronic control for consistent tissue characterisation during minimally invasive surgery. During the project, we developed several new approaches to endomicroscopy, including white-light fibre bundle endocytoscopy, which provides a simple, low-cost means of imaging tissue stained topically with human-approved, non-fluorescence dyes such as toluidine blue. Our new designs for fluorescence endomicroscopes can acquire images at 120 fps, a 10-fold increase in the framerate over conventional systems, made possible by shifting to a line-scanning design using a virtual detector slit. While this reduced the axial resolution, we also demonstrated a real-time method for compensating for this. We showed that high frames rates make it more practical to form large image mosaics (stitching together individual image frames to produce a large-area image), and that we could generate these mosaics in real time at 120 fps. We also demonstrated endomicroscopy at other wavelengths, including multi-wavelength fluorescence imaging at high speeds.

The endomicroscopes were integrated with the da Vinci surgical robot as well as novel scanning devices for large area imaging. We showed that the endomicroscopy images can be used for real-time monitoring of the robot trajectory (visual servoing), allowing on-the-fly correction for tissue deformation. We developed custom devices for imaging during breast surgery and transanal endoscopic microsurgery, demonstrating that we could generate mosaics many times larger than possible with manual probe manipulation. We tackled the problem of maintaining consistent tissue contact in several ways, including development of a passive force-control probe for use with the da Vinci and an active force control device (using a force sensor and voice coil) for imaging in the rectum. We integrated the endomicroscope with other imaging modalities, including ultrasound and optical coherence tomography, and demonstrated multi-scale image fusion with the stereo reconstruction from the laparoscopic camera view.

We also explored new clinical applications of endomicroscopy, with a particular focus on breast surgery. In a large ex vivo study we showed that confocal endomicroscopy can identify normal and cancerous tissue with an accuracy of 92% when read by surgeons. We also demonstrated that methylene blue, when used with our high speed endomicroscope variant at 660 nm, was suitable for staining breast tissue. As this is a safe and widely used stain, unlike previous work with acriflavine, this could greatly simplify the clinical introduction of endomicroscopy into breast surgery, and so help accelerate the benefits to patients.</gtr:description><gtr:exploitationPathways>We are currently exploring potential translational routes for the know-how developed during this project. One potential, but initially unexpected, direction is towards low cost imaging devices for developing world use. Partly as a result of this work, we were awarded funding by the Bill and Melinda Gates Foundation to prepare a horizon scan report on potential applications of endomicroscopy and other optical biopsy techniques to gut disease in the developing world. We are now working with the Gates Foundation and an industrial design company, Smallfry, under an EPSRC translational alliance partnership grant, to further explore frugal innovation in endomicroscopy and bring devices developed under this grant closer to clinical use. We are also currently exploring further clinical applications, together with clinical collaborators in two main areas: lung and breast surgery, with on-going studies on human tissue samples which will lead towards future clinical trials.</gtr:exploitationPathways><gtr:id>88B47D00-9342-4051-8BD7-BAB5FDA26402</gtr:id><gtr:outcomeId>r-1761922251.2114077d659598</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.imperial.ac.uk/hamlyn</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>1F619BC0-2CD9-40CE-8211-B5DADE518CF8</gtr:id><gtr:title>Surgical Robot Challenge 2015 [Competitions]</gtr:title><gtr:parentPublicationTitle>IEEE Robotics &amp; Automation Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97a2d622d7f36f5a14d63106d30e887d"><gtr:id>97a2d622d7f36f5a14d63106d30e887d</gtr:id><gtr:otherNames>Merrifield R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5aa6c12690cf46.68543298</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8F2C6B06-BC74-466B-95C8-B7F164608535</gtr:id><gtr:title>Development of a large area scanner for intraoperative breast endomicroscopy</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/78327d80f4bfd61b1f1989a7cad1f70b"><gtr:id>78327d80f4bfd61b1f1989a7cad1f70b</gtr:id><gtr:otherNames>Zuo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545f4cc8e86902.61216504</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A741BB18-208C-4557-A962-3A9B954C347E</gtr:id><gtr:title>Multi-view Multi-modal Feature Embedding for Endomicroscopy Mosaic Classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f2ff93192dec71a3af4d8933d02728f3"><gtr:id>f2ff93192dec71a3af4d8933d02728f3</gtr:id><gtr:otherNames>Gu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a9e8a8f912481.20397326</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EA5A841A-460F-4010-A9E7-5748B9A7262A</gtr:id><gtr:title>Online tracking and retargeting with applications to optical biopsy in gastrointestinal endoscopic examinations.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/55418e0a2e5a3017eba9269ec196abdc"><gtr:id>55418e0a2e5a3017eba9269ec196abdc</gtr:id><gtr:otherNames>Ye M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>585d59d8edc347.51114474</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D2CF395A-4437-4388-8368-6131F0E59FA9</gtr:id><gtr:title>Novel Balloon Surface Scanning Device for Intraoperative Breast Endomicroscopy.</gtr:title><gtr:parentPublicationTitle>Annals of biomedical engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/78327d80f4bfd61b1f1989a7cad1f70b"><gtr:id>78327d80f4bfd61b1f1989a7cad1f70b</gtr:id><gtr:otherNames>Zuo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0090-6964</gtr:issn><gtr:outcomeId>56e02c2dc05618.05142227</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8399AB59-525F-472C-B6E1-0CC45D995CD0</gtr:id><gtr:title>Endomicroscopy for Computer and Robot Assisted Intervention.</gtr:title><gtr:parentPublicationTitle>IEEE reviews in biomedical engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/78327d80f4bfd61b1f1989a7cad1f70b"><gtr:id>78327d80f4bfd61b1f1989a7cad1f70b</gtr:id><gtr:otherNames>Zuo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1937-3333</gtr:issn><gtr:outcomeId>5a578af4692f89.66841186</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3B853DFE-A1F7-4A90-B442-418C6E4235E1</gtr:id><gtr:title>U.K. Robotics Week [Competitions]</gtr:title><gtr:parentPublicationTitle>IEEE Robotics &amp; Automation Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/97a2d622d7f36f5a14d63106d30e887d"><gtr:id>97a2d622d7f36f5a14d63106d30e887d</gtr:id><gtr:otherNames>Merrifield R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa6c1ab0bfd13.39242695</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F9DC75DB-8A35-42A0-A722-7811B6C267E5</gtr:id><gtr:title>Imaging breast cancer morphology using probe-based confocal laser endomicroscopy: towards a real-time intraoperative imaging tool for cavity scanning.</gtr:title><gtr:parentPublicationTitle>Breast cancer research and treatment</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eb1e5e79beeb0868d7d3c164ac9eacf3"><gtr:id>eb1e5e79beeb0868d7d3c164ac9eacf3</gtr:id><gtr:otherNames>Chang TP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0167-6806</gtr:issn><gtr:outcomeId>56e02c2e010a84.01633675</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A2AE226A-E3E8-45EA-9CDD-89D7EB9B849D</gtr:id><gtr:title>Robot assisted endomicroscopic image mosaicing with optimal surface coverage and reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/931cf27055eba06f2e31899bce3b6ea2"><gtr:id>931cf27055eba06f2e31899bce3b6ea2</gtr:id><gtr:otherNames>Simaiaki V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-6456-0</gtr:isbn><gtr:outcomeId>545f4cc8c200c3.94777141</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>577DC181-465D-4B8F-A531-B2730F90A2B6</gtr:id><gtr:title>Force adaptive robotically assisted endomicroscopy for intraoperative tumour identification.</gtr:title><gtr:parentPublicationTitle>International journal of computer assisted radiology and surgery</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23c9277b7df01d4a5897f0ef0c7ba0fe"><gtr:id>23c9277b7df01d4a5897f0ef0c7ba0fe</gtr:id><gtr:otherNames>Giataganas P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1861-6410</gtr:issn><gtr:outcomeId>5675e2f82379b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AF116F4F-3A3C-467D-B968-3CAD7CBBFB4F</gtr:id><gtr:title>A Hand-held Instrument for in vivo Probe-based Confocal Laser Endomicroscopy during Minimally Invasive Surgery.</gtr:title><gtr:parentPublicationTitle>Proceedings of the ... IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE/RSJ International Conference on Intelligent Robots and Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ef3f5ee2eb51bbee62620c9611a78be3"><gtr:id>ef3f5ee2eb51bbee62620c9611a78be3</gtr:id><gtr:otherNames>Latt WT</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>2153-0858</gtr:issn><gtr:outcomeId>545fe8ce787648.28959389</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AD965C0-3D2A-44EA-8C0D-AAF9C3D075A8</gtr:id><gtr:title>A balloon endomicroscopy scanning device for diagnosing barrett's oesophagus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/78327d80f4bfd61b1f1989a7cad1f70b"><gtr:id>78327d80f4bfd61b1f1989a7cad1f70b</gtr:id><gtr:otherNames>Zuo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9ed1fa0a6bd8.40514897</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EDD4AF89-B5A6-42F1-9974-9878983546ED</gtr:id><gtr:title>Toward Intraoperative Breast Endomicroscopy With a Novel Surface-Scanning Device.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on bio-medical engineering</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/78327d80f4bfd61b1f1989a7cad1f70b"><gtr:id>78327d80f4bfd61b1f1989a7cad1f70b</gtr:id><gtr:otherNames>Zuo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0018-9294</gtr:issn><gtr:outcomeId>56e02c2e244aa4.36364366</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C427859-5830-4A27-9DE6-3F5ED4031443</gtr:id><gtr:title>Methylene-blue aided rapid confocal laser endomicroscopy of breast cancer</gtr:title><gtr:parentPublicationTitle>Journal of Biomedical Optics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3aa675613281202774129f27a91c942b"><gtr:id>3aa675613281202774129f27a91c942b</gtr:id><gtr:otherNames>Vyas K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c15a4ea42a75.87678651</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F8929054-B909-42B0-9BD5-298061239F9B</gtr:id><gtr:title>A miniaturised robotic probe for real-time intraoperative fusion of ultrasound and endomicroscopy</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c38ba48cb8012bbf9f374793cf6bd71c"><gtr:id>c38ba48cb8012bbf9f374793cf6bd71c</gtr:id><gtr:otherNames>Dwyer G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e02d2461e045.72054951</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E396872-C468-4B17-8FB2-F87B3C9B3AEA</gtr:id><gtr:title>Color reflectance fiber bundle endomicroscopy without back-reflections.</gtr:title><gtr:parentPublicationTitle>Journal of biomedical optics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61f0cf0f16d76c3fe54012bce667060a"><gtr:id>61f0cf0f16d76c3fe54012bce667060a</gtr:id><gtr:otherNames>Hughes M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1083-3668</gtr:issn><gtr:outcomeId>545f4cc8049985.39146322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EC123E28-0766-4D01-AA9C-85B85241D582</gtr:id><gtr:title>Flexible Robotic Scanning Device for Intraoperative Endomicroscopy in MIS</gtr:title><gtr:parentPublicationTitle>IEEE/ASME Transactions on Mechatronics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/78327d80f4bfd61b1f1989a7cad1f70b"><gtr:id>78327d80f4bfd61b1f1989a7cad1f70b</gtr:id><gtr:otherNames>Zuo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9ed229798f54.80048159</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF19E0ED-1ABB-4D3A-81E3-C04B5F209FD3</gtr:id><gtr:title>Cooperative in situ microscopic scanning and simultaneous tissue surface reconstruction using a compliant robotic manipulator</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23c9277b7df01d4a5897f0ef0c7ba0fe"><gtr:id>23c9277b7df01d4a5897f0ef0c7ba0fe</gtr:id><gtr:otherNames>Giataganas P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>545f4cc896e981.44674679</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E8BAC7A-FF93-4349-9600-865C9C952D51</gtr:id><gtr:title>Fiber bundle endocytoscopy.</gtr:title><gtr:parentPublicationTitle>Biomedical optics express</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61f0cf0f16d76c3fe54012bce667060a"><gtr:id>61f0cf0f16d76c3fe54012bce667060a</gtr:id><gtr:otherNames>Hughes M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>2156-7085</gtr:issn><gtr:outcomeId>545f4637d20454.33982466</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>47A725B4-DA2C-4F43-9C31-42614844D1FC</gtr:id><gtr:title>Fiber-shifting endomicroscopy for enhanced resolution imaging</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3aa675613281202774129f27a91c942b"><gtr:id>3aa675613281202774129f27a91c942b</gtr:id><gtr:otherNames>Vyas K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9e8a282315d2.82549372</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7F508F27-ECBE-43B3-8957-5981023EBC3F</gtr:id><gtr:title>Three-dimensional robotic-assisted endomicroscopy with a force adaptive robotic arm</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3d0a08ba5ec1655906ce5c19b6e256cb"><gtr:id>3d0a08ba5ec1655906ce5c19b6e256cb</gtr:id><gtr:otherNames>Wisanuvej P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a578aeacf4557.56869481</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6877DA3D-2FF0-45CA-AB30-01D3E6268DFB</gtr:id><gtr:title>High speed, line-scanning, fiber bundle fluorescence confocal endomicroscopy for improved mosaicking.</gtr:title><gtr:parentPublicationTitle>Biomedical optics express</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61f0cf0f16d76c3fe54012bce667060a"><gtr:id>61f0cf0f16d76c3fe54012bce667060a</gtr:id><gtr:otherNames>Hughes M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>2156-7085</gtr:issn><gtr:outcomeId>doi_55f977977fecd135</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1E2A9B45-B95B-4A69-BF35-EA21C9A94DEB</gtr:id><gtr:title>Line-scanning fiber bundle endomicroscopy with a virtual detector slit.</gtr:title><gtr:parentPublicationTitle>Biomedical optics express</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61f0cf0f16d76c3fe54012bce667060a"><gtr:id>61f0cf0f16d76c3fe54012bce667060a</gtr:id><gtr:otherNames>Hughes M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2156-7085</gtr:issn><gtr:outcomeId>585d48323566d6.38171310</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>23D1C251-408E-4B0C-9BB4-51CF45DA58ED</gtr:id><gtr:title>Dual mode fibre bundle confocal endomicroscopy: combining reflectance and fluorescence imaging</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61f0cf0f16d76c3fe54012bce667060a"><gtr:id>61f0cf0f16d76c3fe54012bce667060a</gtr:id><gtr:otherNames>Hughes M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545f4cc8362c04.28900373</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5577CB8-4866-4797-B7E0-CAEB2BE551B0</gtr:id><gtr:title>Gaze gesture based human robot interaction for laparoscopic surgery.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/81e610aa8dd9a388c8fbe27c46413573"><gtr:id>81e610aa8dd9a388c8fbe27c46413573</gtr:id><gtr:otherNames>Fujii K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>5aa7be1ebadbc8.12498830</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I027769/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>