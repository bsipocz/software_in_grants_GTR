<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Computer Science</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/9C2D8593-EA64-4918-A394-C7BCBEDC9331"><gtr:id>9C2D8593-EA64-4918-A394-C7BCBEDC9331</gtr:id><gtr:firstName>Ales</gtr:firstName><gtr:surname>Leonardis</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN019415%2F1"><gtr:id>7379503B-00F0-4D6E-84DC-E3FE4CD55904</gtr:id><gtr:title>Understanding scenes and events through joint parsing, cognitive reasoning and lifelong learning</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N019415/1</gtr:grantReference><gtr:abstractText>The goal of this MURI team is to develop machines that have the following capabilities:
i) Represent visual knowledge in probabilistic compositional models in spatial, temporal, and causal hierarchies augmented with rich attributes and relations, use task-oriented representations for efficient task-dependent inference from an agent's perspective, and preserve uncertainties;
ii) Acquire massive visual commonsense via web scale continuous lifelong learning from large and small data in weakly supervised HCI, and maintain consistence via dialogue with humans;
iii) Achieve deep understanding of scenes and events through joint parsing and cognitive reasoning about appearance, geometry, functions, physics, causality, intents and belief of agents, and use joint and long-range reasoning to fill the performance gap with human vision;
iv) Understand human needs and values, interact with humans effectively, and answer human queries about what, who, where, when, why and how in storylines through Turing tests.

Collaboration with US:
Principal Investigator: Dr. Song-Chun Zhu
Tel. 310-206-8693, Fax. 310-206-5658, email: sczhu@stat.ucla.edu
Institution: University of California, Los Angeles
Statistics and Computer Science
8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095
Institution proposal no. 20153924

Other universities in the US
CMU: Martial Hebert Computer Vision, Robotics &amp;amp; AI
Abhinav Gupta Computer Vision, Lifelong Learning
MIT: Joshua Tenenbaum Cognitive Modeling and Learning
Nancy Kanwisher Cognitive Neuroscience
Stanford: Fei-Fei Li Computer Vision, Psychology &amp;amp; AI
UIUC Derek Hoiem Computer Vision, Machine Learning
Yale Brian Scholl Psychology, Cognitive Science</gtr:abstractText><gtr:fund><gtr:end>2018-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>504832</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>AD12045E-632A-440E-8B24-66268156C9C0</gtr:id><gtr:title>Towards Categorization and Pose Estimation of Sets of Occluded Objects in Cluttered Scenes from Depth Data and Generic Object Models Using Joint Parsing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/629381ab5ae9daf8d97a6271ccc6cb00"><gtr:id>629381ab5ae9daf8d97a6271ccc6cb00</gtr:id><gtr:otherNames>Basevi H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>589de478c9cf02.78669305</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4A39530D-C563-4335-8D96-E55E29881330</gtr:id><gtr:title>Robust Real-Time Music Transcription with a Compositional Hierarchical Model.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25485985702c7da79309f693b2448bd8"><gtr:id>25485985702c7da79309f693b2448bd8</gtr:id><gtr:otherNames>Pesek M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a9e73fe61b596.45937658</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>59B57A89-E6E9-44E2-B0B0-DC858A1B1E22</gtr:id><gtr:title>Learning part-based spatial models for laser-vision-based room categorization</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23061423206a62c546cb5e74ede31ac2"><gtr:id>23061423206a62c546cb5e74ede31ac2</gtr:id><gtr:otherNames>Ur?ic P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9e734a1b1076.05927182</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9E57635C-C0D3-4C49-B7D8-0B11A4B9F999</gtr:id><gtr:title>Visual Stability Prediction and Its Application to Manipulation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce73e661495882f5061c864c6e2c1985"><gtr:id>ce73e661495882f5061c864c6e2c1985</gtr:id><gtr:otherNames>Li W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>589ef741e734c0.23109687</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9BF932B1-5263-4DDB-A94E-8110C1E2F6CE</gtr:id><gtr:title>Robust Fusion of Color and Depth Data for RGB-D Target Tracking Using Adaptive Range-Invariant Depth Models and Spatio-Temporal Consistency Constraints.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/03f36af74ec4f9850aa93b5c629d1a42"><gtr:id>03f36af74ec4f9850aa93b5c629d1a42</gtr:id><gtr:otherNames>Xiao J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>5a2fe61e340870.81471040</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3E311457-6CE3-4D07-9A37-724AEDE57338</gtr:id><gtr:title>Beyond Standard Benchmarks: Parameterizing Performance Evaluation in Visual Object Tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3b0b73d096e2755236d244c25eef44c9"><gtr:id>3b0b73d096e2755236d244c25eef44c9</gtr:id><gtr:otherNames>Zajc L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9e754dcab0c1.05884186</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF79F3C5-9D46-4EE2-B96C-445CAC1C0F55</gtr:id><gtr:title>Dynamic multi-level appearance models and adaptive clustered decision trees for single target tracking</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/03f36af74ec4f9850aa93b5c629d1a42"><gtr:id>03f36af74ec4f9850aa93b5c629d1a42</gtr:id><gtr:otherNames>Xiao J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9e722559ba36.25626293</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DA491CDE-4016-4887-AF31-988595C2C8B8</gtr:id><gtr:title>Rolling Shutter Correction in Manhattan World</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e54ce2039d2cb5948d0f5b9fff0cd95b"><gtr:id>e54ce2039d2cb5948d0f5b9fff0cd95b</gtr:id><gtr:otherNames>Purkait P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a9e74611fb125.68926827</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21298583-AC7E-415B-AC2D-B90EBF3CDB63</gtr:id><gtr:title>Visual Stability Prediction and Its Application to Manipulation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce73e661495882f5061c864c6e2c1985"><gtr:id>ce73e661495882f5061c864c6e2c1985</gtr:id><gtr:otherNames>Li W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>589dede4a1cef2.77074781</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7EF4F5EC-7C19-48A2-80B8-CE349CD74E48</gtr:id><gtr:title>A local-global coupled-layer puppet model for robust online human pose tracking</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/647f04fce88d423f200b199a6505db10"><gtr:id>647f04fce88d423f200b199a6505db10</gtr:id><gtr:otherNames>Ma M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>589df2377bacd7.28049923</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AEF5E899-CF19-4596-BEE0-B25F0111CC9B</gtr:id><gtr:title>Towards Deep Compositional Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/95531f6880d722cc38fe8bc82adf5374"><gtr:id>95531f6880d722cc38fe8bc82adf5374</gtr:id><gtr:otherNames>Tabernik D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>589df4a0875e19.28144614</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E75D822C-87F9-48C6-8C32-33DBD37E1185</gtr:id><gtr:title>Region-sequence based six-stream CNN features for general and fine-grained human action recognition in videos</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/647f04fce88d423f200b199a6505db10"><gtr:id>647f04fce88d423f200b199a6505db10</gtr:id><gtr:otherNames>Ma M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a9e73c9d282c7.39155223</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N019415/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E457FFDE-A4C1-4907-AE12-A394D95A3AE5</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Cognitive Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>