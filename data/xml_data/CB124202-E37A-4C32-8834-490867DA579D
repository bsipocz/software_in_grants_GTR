<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/4462E15D-62FF-4C65-89A8-2ABDCAA8F5C7"><gtr:id>4462E15D-62FF-4C65-89A8-2ABDCAA8F5C7</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>David</gtr:otherNames><gtr:surname>Clough</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F9994A4C-84DA-4D3A-B028-702A78E6A6C0"><gtr:id>F9994A4C-84DA-4D3A-B028-702A78E6A6C0</gtr:id><gtr:firstName>Emma</gtr:firstName><gtr:otherNames>Jane</gtr:otherNames><gtr:surname>Barker</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A09E4835-5982-4111-987F-CD39C410CCCC"><gtr:id>A09E4835-5982-4111-987F-CD39C410CCCC</gtr:id><gtr:firstName>Robert</gtr:firstName><gtr:surname>Gaizauskas</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/358A0C83-1C2D-4DD9-8E9B-9E92800152B3"><gtr:id>358A0C83-1C2D-4DD9-8E9B-9E92800152B3</gtr:id><gtr:firstName>Ahmet</gtr:firstName><gtr:surname>Aker</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK019082%2F1"><gtr:id>CB124202-E37A-4C32-8834-490867DA579D</gtr:id><gtr:title>VisualSense</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K019082/1</gtr:grantReference><gtr:abstractText>Recent years have witnessed an unprecedented growth in the number of image and video collections, partially due to the increased popularity of photo and video sharing websites. One such website alone (Flickr) stores billions of images. And this is not the only way in which visual content is present on the Web: in fact most web pages contain some form of visual content. However, while most traditional tools for search and retrieval can successfully handle textual content, they are not prepared to handle heterogeneous documents. This new type of content demands the development of new efficient tools for search and retrieval.


The large number of readily accessible multi-media data-collections pose both an opportunity and a challenge. The opportunity lies in the potential to mine this data to automatically discover mappings between visual and textual content. The challenge is to develop tools to classify, filter, browse and search such heterogeneous data. In brief, the data is available, but the tools to make sense of it are missing.

The Visual Sense project aims to automatically mine the semantic content of visual data to enable &amp;quot;machine reading&amp;quot; of images. In recent years, we have witnessed significant advances in the automatic recognition of visual concepts. These advances allowed for the creation of systems that can automatically generate keyword-based image annotations. However, these annotations, e.g. &amp;quot;man&amp;quot; and &amp;quot;pot&amp;quot;, fall far short of the sort of more meaningful descriptive captions necessary for indexing and retrieval of images, for example,&amp;quot;Man cooking in kitchen&amp;quot;. The goal of this project is to move a step forward and predict semantic image representations that can be used to generate more informative sentence-based image annotations, thus facilitating search and browsing of large multi-modal collections. It will address the following key open research challenges:

1) Develop methods that can derive a semantic representation of visual content. Such representations must go beyond the detection of objects and scenes and also include a wide range of object relations.
2) Extend state-of-the-art natural language techniques to the tasks of mining large collections of multi-modal documents and generating image captions using both semantic representations of visual content and object/scene type models derived from semantic representations of the textual component of multi-modal documents.
3) Develop learning algorithms that can exploit available multi-modal data to discover mappings between visual and textual content. These algorithms should be able to leverage 'weakly' annotated data and be robust to large amounts of noise.

Thus, the main focus of the Visual Sense project is the development of machine learning methods for knowledge and information extraction from large collections of visual and textual content and for the fusion of this information across modalities. The tools and techniques developed in this project will have a variety of applications. To demonstrate them, we will address three case studies: 1) evaluation of generated descriptive image captions in established international image annotation benchmarks, 2) re-ranking for improved image search and 3) automatic illustration of articles with images.

To address these broad challenges, the project will build on expertise from multiple disciplines, including computer vision, machine learning and natural language processing (NLP). It brings together four research groups from University of Surrey (Surrey, UK), Institut de Robotica i Informatica Industrial (IRI, Spain), Ecole Centrale de Lyon (ECL, France), and University of Sheffield (Sheffield, UK) having each well established and complementary expertise in their respective areas of research.</gtr:abstractText><gtr:fund><gtr:end>2016-06-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>310676</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The following are the key findings of the award from our perspective:
1. A proposal for how to better evaluate system-generated image descriptions in comparison with human-authored gold standard descriptions. This addresses the problem of how to score system descriptions which are at a different level of granularity from those in a human gold standard, for example, when a human suggests &amp;quot;dog&amp;quot; as an image description a system that says &amp;quot;poodle&amp;quot; is better than one that says &amp;quot;horse&amp;quot;. Having meaningful evaluation measures is important to assess whether our systems are getting better or worse and is particularly important these measures become objective functions that learning methods try to optimise. See: J. Wang, F. Yan, A. Aker and R. Gaizauskas (2014). &amp;quot;A Poodle or a Dog? Evaluating Automatic Image Annotation Using Human Descriptions at Different Levels of Granularity&amp;quot; in In Proceedings of the 3rd Workshop on Vision and Language (VL'14). 
2. A proposal for a specific subtask, which we called &amp;quot;content selection&amp;quot;, as part of the process of generating image descriptions. Content selection is the choice of which elements of an image as recognised by an objection detection system, should be included in a natural language description of image. We defined the task, developed baseline and state-of-the-art systems to carry out the task and proposed and ran an international shared task evaluation challenge around this task. Addressing this task is an important part of understanding which elements in an image humans chose to talk about and of designing systems to do this automatically. See: 
o J. Wang and R. Gaizauskas (2015). &amp;quot;Generating Image Descriptions with Gold Standard Visual Inputs: Motivation, Evaluation and Baselines&amp;quot;. In Proceedings of the 15th European Workshop on Natural Language Generation (ENLG), 117--126.
o A. Gilbert, L. Piras, . Wang, F. Yan, E. Dellandrea, R. Gaizauskas, M. Villegas, K. Mikolajczyk (2015). &amp;quot;Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task&amp;quot;. In CLEF2015 Working Notes.
o Josiah Wang and Robert Gaizauskas (2016). &amp;quot;Don't Mention the Shoe! A Learning to Rank Approach to Content Selection for Image Description Generation&amp;quot;. In Proceedings of the 9th International Natural Language Generation Conference (INLG16).
3. A proposal for how to define visually descriptive language and a set of annotation guidelines based on this definition to allow humans to annotate examples of visually descriptive language. This is important as to date language used to train automatic image description software has largely been derived from existing image caption which is a limited and noisy resource. Our scheme allows visually descriptive language to be annotated in any sort of text and could be used to train classifiers to recognise visually descriptive language in arbitrary text, allowing it to be harvested for training the next generation of image description software. See: R. Gaizauskas, J. Wang and A. Ramisa (2015). &amp;quot;Defining Visually Descriptive Language&amp;quot;. In Proceedings of the Fourth Workshop on Vision and Language, 10--17.
4. A method for using knowledge about visual and semantic similarity between object classes to improve the adaptation of image classifiers into image detectors, allowing better object detection by Deep Learning techniques. See: Yuxing Tang, Josiah Wang, Boyang Gao, Emmanuel Dellandrea, Robert Gaizauskas and Liming Chen (2016). &amp;quot;Large Scale Semi-supervised Object Detection using Visual and Semantic Knowledge Transfer&amp;quot;. In Proceedings of the IEEE Conference on Computer Vision &amp;amp; Pattern Recognition (CVPR 2016). 
5. A proposal for the task of predicting the preposition that best expresses the relation between a subject and an object (&amp;quot;man on horse&amp;quot;). This is important for the subsequent task of automatically generating textual descriptions for an image. We constructed a dataset containing instances of  extracted from human-authored image descriptions, thus allowing us to explore real-world usage of prepositions. We proposed methods to tackle the preposition prediction task given two objects, combining different cues -- text, image and the geometric arrangements between objects. We found that all three cues play a role in preposition prediction. See: A. Ramisa, J. Wang, Y. Lu, E. Dellandrea, F. Moreno-Noguer, R. Gaizauskas (2016). &amp;quot;Combining Geometric, Textual and Visual Features for Predicting Prepositions in Image Descriptions&amp;quot;. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015.</gtr:description><gtr:exploitationPathways>Our methods for evaluating image system-generated image descriptions in comparison with human-authored gold standard descriptions could be used or further developed by anyone wanting to improve the quality of automatic image description (and in turn web-based image retrieval).
Pursuing the content selection and predicting the preposition tasks could also be used by anyone wanting to improve the quality of automatic image description, especially those interested in developing compositional approaches to image description generation.
As indicated above, building classifiers to recognise visually descriptive language, as we have defined it, could increase the voiume of data available to train/inform image description generators by several orders of magnitude.
Our findings on how to use knowledge of object similarities to improve the adaptation of image classifiers into image detectors can be used anyone wishing to build better object detectors.</gtr:exploitationPathways><gtr:id>F92905BA-48E0-4001-88A3-628174726CCB</gtr:id><gtr:outcomeId>56e19580d700f8.62600692</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Education,Culture, Heritage, Museums and Collections,Security and Diplomacy</gtr:sector></gtr:sectors><gtr:url>https://sites.google.com/site/visenproject/home/publications</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>We contributed to the ImageCLEF2015 Scalable Concept Image Annotation Challenge dataset. Our contribution was (1) to select concepts to be identified within images by challenge participants (2) to annotate 5000 images at the image level with the concepts from *(1) (3) to annotate 5000 images with full sentence descriptions of image content (4) to annotate ~1000 images with correspondences between bounding boxes around instances of concept categories in the image and terms in the associated full sentence descriptions from (3).</gtr:description><gtr:id>7081EFA1-DD9E-40A2-8DDD-CA879DFD38E8</gtr:id><gtr:impact>The dataset was used by 11 participants from 7 countries who participated in the ImageCLEF2015 Scalable Concept Image Annotation Challenge. Its design is being re-used in the ImageCLEF 2016 international challenge, as is an extended version of the data.</gtr:impact><gtr:outcomeId>56e19a41ee6fb7.35383195</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>ImageCLEF2015</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.imageclef.org/2015/annotation</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Visen Prepositions Dataset is a dataset with instances of  and the bounding box localisations of entity1 and entity2 in the corresponding picture (e.g. &amp;quot;boy on sled&amp;quot; and the bounding boxes for &amp;quot;boy&amp;quot; and &amp;quot;sled&amp;quot; in the image being described). This dataset has two main appeals: (i) it is extracted from two large-scale image datasets with human-authored descriptions, with a reasonable amount of noise as extraction was performed automatically; (ii) the prepositions are based on real-world usage as used by humans in image descriptions, making it attractive for exploring prepositional usage specifically in image descriptions. The dataset is used in Ramisa et al. (2015), and is publicly available at http://preposition.github.io/ 

Arnau Ramisa*, Josiah Wang*, Ying Lu, Emmanuel Dellandrea, Francesc Moreno-Noguer and Robert Gaizauskas. Combining Geometric, Textual and Visual Features for Predicting Prepositions in Image Descriptions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2015). 2015.</gtr:description><gtr:id>BEBF05BF-9423-406B-BB7E-34D4CEFB4D8D</gtr:id><gtr:impact>This database will progress research in the field of natural language processing and computer vision, by bridging the gap between both text and image modality, based on how humans use prepositions in an image description in the context of two entities and how these entities are actually depicted in a picture.</gtr:impact><gtr:outcomeId>56e04fd84bc086.49330129</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Visen Prepositions Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://preposition.github.io/</gtr:url></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>BA371891-353F-45A5-8280-3EF9AB0A967D</gtr:id><gtr:title>Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fedc1685ea8.09422256</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EE55A8A-803E-44DE-A0EE-44C1F2853EFB</gtr:id><gtr:title>Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task</gtr:title><gtr:parentPublicationTitle>CLEF2015 Working Notes</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/42a7336e4dc582e7aab8aaafd3d93d67"><gtr:id>42a7336e4dc582e7aab8aaafd3d93d67</gtr:id><gtr:otherNames>Gilbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e04cb82deb58.67285296</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8E8C4A16-0F47-434B-8715-0F2D32BC1B1C</gtr:id><gtr:title>Large Scale Semi-Supervised Object Detection Using Visual and Semantic Knowledge Transfer</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c71fa2a20649.46388882</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ACEA07F0-5750-4AA0-BA61-4F8D5E4949D1</gtr:id><gtr:title>General Overview of ImageCLEF at the CLEF 2015 Labs</gtr:title><gtr:parentPublicationTitle>Unpublished</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05644025d33ab146e0840b9643b20de0"><gtr:id>05644025d33ab146e0840b9643b20de0</gtr:id><gtr:otherNames>Villegas M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e04cb8badef1.62940599</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K019082/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>