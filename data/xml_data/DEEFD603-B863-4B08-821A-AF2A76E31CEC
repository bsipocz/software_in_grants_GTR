<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Engineering</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/739761A3-8A28-452B-8209-4F1F4C8FC3EA"><gtr:id>739761A3-8A28-452B-8209-4F1F4C8FC3EA</gtr:id><gtr:firstName>James</gtr:firstName><gtr:otherNames>Robert</gtr:otherNames><gtr:surname>Hopgood</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD051207%2F1"><gtr:id>DEEFD603-B863-4B08-821A-AF2A76E31CEC</gtr:id><gtr:title>A Model Based Approach Towards Practical Blind Enhancement of Audio Signals Acquired in Real Acoustic Environments</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D051207/1</gtr:grantReference><gtr:abstractText>This proposal concerns enhancing the quality and intelligibility of audio.The ubiquitousness of digital audio in broadcasting, storage, and multimedia applications, each offering crystal clear sound quality, has resulted in a heightened awareness and expectation of the achievable performance of applications involving audio signals: digital hearing aids should outperform their analogue counterparts in concert halls, speech recognition software should achieve high recognition rates in office environments, and hands-free telephones must produce intelligible speech when used in car cabins.The quality and intelligibility of speech obtained in these scenarios is constrained not just by the reproduction quality of the hardware itself; rather, it is dependent on the acoustical properties of the environment in which the audio is acquired.Specifically, audio signals in confined acoustic environments exhibit reverberation; this causes problems in two major classes of signal processing applications. The first is in automatic speech recognition in which it is more difficult to identify reverberant speech than closely coupled speech. This prevents hands-free interaction without the undesirable constraint that a user must carry a microphone close to their mouth. The second class involves the desire to improve speech quality and intelligibility from devices such as mobile and hands-free telephones and next generation digital hearing aids. In each scenario, the presence of reverberation should be reduced to adequate levels by a robust speech enhancement algorithm that can be applied in any acoustic environment, and which does not rely on the acoustic properties being known a priori. Since neither the acoustic impulse response (AIR) nor the source audio is known in this situation, the process of removing the effects of reverberation is known as blind dereverberation.Previously, blind dereverberation has often been approached assuming the AIR between the source and sensor is time-invariant. This might be appropriate in scenarios where the source-sensor geometry is not rapidly varying, for example, a hands-free kit in a car cabin, in which the driver and the microphone are approximately fixed relative to one another, or in a work environment where a user is seated in front of a computer terminal in roughly the same configuration. However, there are many applications where the source-sensor geometry is subject to change; the wearer of a hearing-aid will typically wish to move around a room, as might users of hands-free conference telephony equipment.Moreover, it is not beyond possibility that the acoustics of the room itself vary; the changing state of doors, windows, or items being moved in the room will influence the room dynamics, as will a moving person. Consequently, in order to develop a blind dereverberation algorithm that is suitable for practical applications, it is important to account for source-sensor movement, and for possible changes in the acoustical properties of the room.This proposal uses model based signal processing, robust Bayesian statistical parameter estimation and numerical optimisation methods, in order to obtain practical algorithms to tackle this problem. Model-based signal processing is fundamentally based on the availability of realistic, tractable, and extensiblemodels that reflect the underlying processes and systems involved. This proposal focuses on developing, implementing, testing, and applying a number of models that have not previously been investigated in blind speech dereverberation. These include- a complete speech model that accounts for both voiced and unvoiced speech;- a more realistic room acoustic model;- subband methods for dealing with the complicated acoustic responses that occur in realistic acoustic environments;- models that can account for varying source-sensor geometries;- models that can be estimated using batch and sequential Monte Carlo methods.</gtr:abstractText><gtr:fund><gtr:end>2008-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>124179</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D2AC3476-4D45-4FB0-BF1C-356EC78F28C0</gtr:id><gtr:title>Speech Dereverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aa0f8511934d9f95dc72d6c3855eadee"><gtr:id>aa0f8511934d9f95dc72d6c3855eadee</gtr:id><gtr:otherNames>Naylor, Patrick A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-84996-056-4</gtr:isbn><gtr:outcomeId>i-356394453.30611843bd149fe</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>71E6155B-C4A2-40E0-A07C-6D9803C4A068</gtr:id><gtr:title>Speech Classification for Enhancing Single Channel Blind Dereverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/141d217282cc6d1461c3b529df7fddec"><gtr:id>141d217282cc6d1461c3b529df7fddec</gtr:id><gtr:otherNames>J Hopgood</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_673977541413e3d8c6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B40790DD-C598-452B-9004-21E851374230</gtr:id><gtr:title>Blind speech dereverberation using batch and sequential MC methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1442e6b858f61396cbfaa304e78cc225"><gtr:id>1442e6b858f61396cbfaa304e78cc225</gtr:id><gtr:otherNames>C Evers</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_302621821313da1d0e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>10612022-1C57-461C-8EBD-10ABAFE9414E</gtr:id><gtr:title>Bayesian Single Channel Blind Dereverberation of Speech from a Moving Speaker</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/141d217282cc6d1461c3b529df7fddec"><gtr:id>141d217282cc6d1461c3b529df7fddec</gtr:id><gtr:otherNames>J Hopgood</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_437519814413aa0772</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>717AC446-10BD-4147-B7FC-110F4E4DCB24</gtr:id><gtr:title>Speech Classification for Enhancing Single Channel Blind Dereverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/51dc1bbfe7fe2602f3ac993ad15979b0"><gtr:id>51dc1bbfe7fe2602f3ac993ad15979b0</gtr:id><gtr:otherNames>James Hopgood (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>r_659400173964277d2a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2FC993EE-0AD5-4C91-8DE5-56F52065CADA</gtr:id><gtr:title>Parametric modelling for single-channel blind dereverberation of speech from a moving speaker</gtr:title><gtr:parentPublicationTitle>IET Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/982c0a9310c399b15362fc3130fc7883"><gtr:id>982c0a9310c399b15362fc3130fc7883</gtr:id><gtr:otherNames>Evers C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d031031e5233c3</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D051207/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>