<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA"><gtr:id>1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>Claverton Down</gtr:line1><gtr:city>Bath</gtr:city><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA"><gtr:id>1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>Claverton Down</gtr:line1><gtr:city>Bath</gtr:city><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/77201EFF-0EDB-4AF0-8DC6-8908902F301D"><gtr:id>77201EFF-0EDB-4AF0-8DC6-8908902F301D</gtr:id><gtr:firstName>Gabriel</gtr:firstName><gtr:otherNames>Julian</gtr:otherNames><gtr:surname>Brostow</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ021458%2F1"><gtr:id>82BD6723-B5CF-4160-9D23-351FF4F53B86</gtr:id><gtr:title>Learning Models of Handwriting for Structured Texture Synthesis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J021458/1</gtr:grantReference><gtr:abstractText>The proposed research aims to produce images of handwriting using a new generative mathematical model of different people's handwriting styles and the specified text strings to be &amp;quot;forged.&amp;quot; Our model uses image-fragments of real handwriting samples and learns what fragments are compatible under what deformations.

Our intended analysis/synthesis applications include quantitative authentication of handwriting in legal cases, inpainting of destroyed sections of historical documents, imitation of handwriting for banking purposes, and eventually, writer-specific handwriting recognition and drawn-sketch interpretation.

The specific objectives of this project are to:
- Automatically synthesize handwriting of novel text but in a particular person's style.
- Measure the likelihood that a given sample was written by a specific individual.
- Build a system that suggests what examples of handwriting should be captured next.


The project scope is limited to just handwriting to assess feasibility. At its conclusion, we expect to further pursue the following additional objectives:
- Demonstrate the synthesis and analysis approaches on large-scale forgery studies. The goal is to measure our forgery and forgery-detection rates to help transfer these techniques into industrial practice. Abuse of the technology for forgery creation could be limited by filming an individual while writing, to confirm that the synthesis was not automatic.
- Build a similar adaptive authoring system to help synthesize cartoon animation. Replace strings of characters used in the present project with a structure based on limb-configurations (2D joint-angles) of a stick-figure.
- Explore a unified framework for example-based generative models. In many domains, better synthesis should be possible when multiple examples are available. Findings from this near-term research on handwriting should generalize, so that structured acquisition of training examples steadily improves the quality of synthesized drawings, 3D shapes, and video post-production.</gtr:abstractText><gtr:potentialImpactText>This project will have impact on at least three communities: i) professional Forensic Document Examiners, ii) forensic scientists who study handwriting (both synthesis and recognition), and iii) vision and graphics researchers studying texture synthesis. With the UK's imminent dissolution of the Forensic Science Services (FSS) because of cutbacks, there is no substantial national hub for forensic science to be developed, and for the expertise to be disseminated to investigators and prosecutors. At least in the case of handwriting synthesis and analysis, some of that burden could be taken on by the PI and the host organization, as initiated in this project.

The impact of the proposed work will be far-reaching. Benefits include the ability to synthesize convincing imitations of different individuals' handwriting, and critically, the ability to numerically measure the probability that the handwriting on a questioned document was written by a known individual. Our intended analysis/synthesis applications include quantitative authentication of handwriting in legal cases, inpainting of destroyed sections of historical documents, imitation of handwriting for banking purposes, and in the longer term, writer-specific handwriting recognition, and eventually drawn-sketch interpretation.

To increase the likelihood of impact, to share expertise, and to raise the profile for the new approach of &amp;quot;structured texture synthesis&amp;quot;, the PI and RA will organize a one-day workshop near the completion of the one-year project. Naturally, all research findings will be subjected to peer review and disseminated at conferences, in journals, and in specialized seminars for professional handwriting analysis / synthesis practitioners.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-01-20</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-01-21</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>99141</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Bath</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Department of Computer Science</gtr:department><gtr:description>OAK Project partnership</gtr:description><gtr:id>ECB02974-27F5-4B7C-9E59-6B107F9E344A</gtr:id><gtr:impact>Computer Science and Film Special Effects: multiple papers have resulted (listed elsewhere in Research Fish under the OAK project EP/K023578/1). Also collaboration on pending research papers, and exchange of data and expertise.</gtr:impact><gtr:outcomeId>56d9ffa7f324b8.90867199-1</gtr:outcomeId><gtr:partnerContribution>Help with shape models for rotoscoping, building of prototype and running of user-study at The Foundry.</gtr:partnerContribution><gtr:piContribution>Research on computer-controlled gimballs, 3D reconstruction of specular surfaces, and rotoscoping.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Playfair Capital 2015 AI Summitt</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>0554ABA1-CB58-4C56-9EF9-6A58BE68AF06</gtr:id><gtr:impact>Probably about 300 people attended this event organized to showcase how AI and Machine Learning progress could potentially be explored by and for private industry. G. Brostow was an invited speaker for the panel, and the event was held at Bloomberg News headquarters in London.</gtr:impact><gtr:outcomeId>56da075f62fbd7.88906851</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Interview for Handwriting Synthesis project</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3FC64417-0457-4AB9-93DA-74D6B3B31D52</gtr:id><gtr:impact>Rory Cellan-Jones, BBC Technology correspondent came and interviewed me and postdoc Dr Tom Haines, to showcase our research. The project allows us to scan someone's handwriting sample, and then to create new text in that person's handwriting. The report and BBC video from this interview were featured as the main Technology story on the BBC website. Our youtube video has been viewed over 52,000 times.</gtr:impact><gtr:outcomeId>58c7dd18a108d7.83985410</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/news/technology-37046477</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>30000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Innovation and Enterprise</gtr:department><gtr:description>EPSRC Impact Acceleration Award</gtr:description><gtr:end>2017-03-02</gtr:end><gtr:fundingOrg>University College London</gtr:fundingOrg><gtr:fundingRef>M.2.35</gtr:fundingRef><gtr:id>2E2F8D7C-F1B5-4873-B3C3-9AB5E27C8683</gtr:id><gtr:outcomeId>58cb174e819b34.47704964</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-11-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Findings were picked up and reported first by the BBC, then by Reuters and AP, then in various new agencies internationally, and across social media. 
This coverage resulted in commercialisation opportunities that we are now pursuing. The research project was a success in making it possible to train a machine learning system to replicate a person's handwriting style. Now we are exploring ways to make this technology accessible and easy to use by casual home-users.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>8752DE02-08AA-4CA1-B7D3-1110307F3341</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56da0494a4c041.36914107</gtr:outcomeId><gtr:sector>Creative Economy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have developed a set of computer vision principles for semi-automatically extracting the letter-specific pen-strokes and style of any person's handwriting, whether joined-up, print-style, or mixed. We have also developed a comprehensive optimization algorithm that harmonizes the glyph samples to generate non-repetitive looking handwriting, that convinces human observers that it was hand-written by the intended author.</gtr:description><gtr:exploitationPathways>This is the first work to demonstrate structure texture-synthesis. It shows that at least in the handwriting domain, it is possible to do non-parametric texture-synthesis by letting the end-user control just part of the output (the new strings of letters to be generated) while letting the system take care of the style-specific details. This is in contrast to previous texture-synthesis, which just copy-pasted patches of pixels in an attempt to look compatible, but with no substantial purpose. Extensions could work on other forms of data, such as speech synthesis, video textures, and geometry synthesis, where the main blocky shapes could be user-specified, but the details would be generated automatically.</gtr:exploitationPathways><gtr:id>3C54713B-7948-4FCE-AEA1-F6087291B873</gtr:id><gtr:outcomeId>56da08fc6f38e2.47124709</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Government, Democracy and Justice,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://visual.cs.ucl.ac.uk/pubs/handwriting/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset contains photos and calibration for many camera views of each specular object. It also contains mask images, ground-truth 3D laser-scans, and environment photographs.</gtr:description><gtr:id>E9ED5BD3-9FC1-4655-B9F1-6942131C7AFF</gtr:id><gtr:impact>N/A</gtr:impact><gtr:outcomeId>56da00c0e385a6.12035804</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Specular Surfaces 3DV 2015 datasets</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/shapefromreflections/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset has our code and annotated samples of people's handwriting, allowing new users to process these inputs and generate newly authored text in these writers' handwriting.</gtr:description><gtr:id>EA0D5417-3D3F-4E42-B18F-9AE9AD41E9D7</gtr:id><gtr:impact>Just posted online last week.</gtr:impact><gtr:outcomeId>56da0184aefb82.22808229</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>My Text in Your Handwriting</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/handwriting/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The software has two parts:
1) Interactive software that helps a user annotate a scanned image of handwritten text, extracting and recognizing the characters (or glyphs) to make them re-usable by the synthesis algorithm.

2) Synthesis software that takes a learned model of a specific user's handwriting, and also expects some typed text that the user wishes to render in the handwritten style. The system produces images that can be printed on paper and look convincingly like handwriting.</gtr:description><gtr:id>55B4CEDA-1C56-4332-86F1-AE7B204727D7</gtr:id><gtr:impact>N/A: This software has just been released online, and the public announcement is pending.</gtr:impact><gtr:outcomeId>56da05ba2bb1a9.60070210</gtr:outcomeId><gtr:title>Handwriting Annotation and Synthesis Software</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/handwriting/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The released software allows users (typically researchers) to generate infinite amounts of synthetic images, where various looking scenes are partially affected by variously-shaped soft shadows. This data can then serve as ground-truth training data for supervised learning applications.</gtr:description><gtr:id>B8F55B16-929A-4ED2-9A0F-D42311394418</gtr:id><gtr:impact>The system was used to build the computational model used in our ACM Transactions on Graphics paper in 2015 &amp;quot;Learning to Remove Soft Shadows&amp;quot;, which was part of Maciej Gryka's EngD, co-funded by Anthropics Ltd.</gtr:impact><gtr:outcomeId>58cb1d23dbee13.80709288</gtr:outcomeId><gtr:title>Soft Shadow generation software</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/softshadows/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This software allows a user to 1st train a statistical model of appearance, and 2nd to count the number of instances of that object. It was designed and most tested for counting of hundreds of fly-eggs in microscope images.</gtr:description><gtr:id>B4D58523-E759-46EB-BEBA-CB442029FE62</gtr:id><gtr:impact>The UCL Genetics lab on Healthy Aging is using this software on a daily basis to measure the number of fly eggs in their vials. The number of eggs is an indicator of fly health.</gtr:impact><gtr:outcomeId>56da02bf6f0c64.98788720</gtr:outcomeId><gtr:title>Quantifly software</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127659</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8038BF74-E54A-4318-A353-F6992EB6D384</gtr:id><gtr:title>My Text in Your Handwriting</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c492bb39ff2308f7f364eb19212b22b"><gtr:id>9c492bb39ff2308f7f364eb19212b22b</gtr:id><gtr:otherNames>Haines T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d9c9938ffb47.51602429</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>11584E05-7FD3-444A-A878-52141D61C0B9</gtr:id><gtr:title>Learning to Remove Soft Shadows</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1dc28e29b11957a2fc67b13c5f89153"><gtr:id>d1dc28e29b11957a2fc67b13c5f89153</gtr:id><gtr:otherNames>Gryka M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9c82871a913.65387094</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0BD724F3-CAD6-4521-8472-BBB34D695AC0</gtr:id><gtr:title>Hierarchical Subquery Evaluation for Active Learning on a Graph</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ff51beb4a33e863f949a5b4c2437462"><gtr:id>2ff51beb4a33e863f949a5b4c2437462</gtr:id><gtr:otherNames>Aodha O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d9c828b195c9.28055038</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J021458/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>