<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/935B2919-2093-4FB9-A5D4-59F746D5235A"><gtr:id>935B2919-2093-4FB9-A5D4-59F746D5235A</gtr:id><gtr:name>University of California, Irvine</gtr:name><gtr:address><gtr:line1>UCI</gtr:line1><gtr:postCode>92697</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Statistics</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/935B2919-2093-4FB9-A5D4-59F746D5235A"><gtr:id>935B2919-2093-4FB9-A5D4-59F746D5235A</gtr:id><gtr:name>University of California, Irvine</gtr:name><gtr:address><gtr:line1>UCI</gtr:line1><gtr:postCode>92697</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/14D8EC0F-B4FA-4C50-9AE6-8E3672E6A2C8"><gtr:id>14D8EC0F-B4FA-4C50-9AE6-8E3672E6A2C8</gtr:id><gtr:firstName>Yee</gtr:firstName><gtr:otherNames>Whye</gtr:otherNames><gtr:surname>Teh</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK009362%2F1"><gtr:id>F0F8F872-18D2-451A-A960-0B03B77BABA5</gtr:id><gtr:title>Bayesian Inference for Big Data with Stochastic Gradient Markov Chain Monte Carlo</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K009362/1</gtr:grantReference><gtr:abstractText>We are in the midst of an information revolution, where advances in science and technology, as well as the day-to-day operation of successful organisations and businesses, are increasingly reliant on the analyses of data. Driving these advances is a deluge of data, which is far outstripping the increase in computational power available. The importance of managing, analysing, and deriving useful understanding from such large scale data is highlighted by high-profile reports by McKinsey and The Economist as well as other outlets, and by the EPSRC's recent ICT priority of &amp;quot;Towards an Intelligent Information Infrastructure&amp;quot;.

Bayesian analysis is one of the most successful family of methods for analysing data, and one now widely adopted in the statistical sciences as well as in AI technologies like machine learning. The Bayesian approach offers a number of attractive advantages over other methods: flexibility in constructing complex models from simple parts; fully coherent inferences from data; natural incorporation of prior knowledge; explicit modelling assumptions; precise reasoning of uncertainties over model order and parameters; and protection against overfitting. 

On the other hand, there is a general perception that they can be too slow to be practically useful on big data sets. This is because exact Bayesian computations are typically intractable, so a range of more practical approximate algorithms are needed, including variational approximations, sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). MCMC methods arguably form the most popular class of Bayesian computational techniques, due to their flexibility, general applicability and asymptotic exactness. Unfortunately, MCMC methods do not scale well to big data sets, since they require many iterations to reduce Monte Carlo noise, and each iteration already involves an expensive sweep through the whole data set.

In this project we propose to develop the theoretical foundations for a new class of MCMC inference procedures that can scale to billions of data items, thus unlocking the strengths of Bayesian methods for big data. The basic idea is to use a small subset of the data during each parameter update iteration of the algorithm, so that many iterations can be performed cheaply. This introduces excess stochasticity in the algorithm, which can be controlled by annealing the update step sizes towards zero as the number of iterations increases. The resulting algorithm is a cross between an MCMC and a stochastic optimization algorithm. An initial exploration of this procedure, which we call stochastic gradient Langevin dynamics (SGLD), was initiated by us recently (Welling and Teh, ICML 2011). 

Our proposal is to lay the mathematical foundations for understanding the theoretical properties of such stochastic MCMC algorithms, and to build on these foundations to develop more sophisticated algorithms. We aim to understand the conditions under which the algorithm is guaranteed to converge, and the type and speed of convergence. Using this understanding, we aim to develop algorithmic extensions and generalizations with better convergence properties, including preconditioning, adaptive and Riemannian methods, Hamiltonian Monte Carlo methods, Online Bayesian learning methods, and approximate methods with large step sizes. These algorithms will be empirically validated on real world problems, including large scale data analysis problems for text processing and collaborative filtering which are standard problems in machine learning, and large scale data from ID Analytics, a partner company interested in detecting identity theft and fraud.</gtr:abstractText><gtr:potentialImpactText>The Bayesian approach to data analysis offers significant advantages over conventional techniques. It has become very popular over the past twenty years thanks to Markov chain Monte Carlo (MCMC) algorithms which allow us to carry out Bayesian computation for complex models. Unfortunately, current MCMC methods do not scale to big data sets and consequently Bayesian techniques are almost never used in a data rich environment. At a time where the amount of data available is growing exponentially fast, this project proposes to unlock the strengths of Bayesian analysis for big data by developing new MCMC-type algorithms that can scale to billions of data items.

The methodologies developed in this proposal will allow the development of Bayesian approaches to large scale data analysis problems now frequently encountered in healthcare, industries and services to the general public. In the medium term, the applications to collaborative filtering, text processing, and to the data of our industrial partner ID Analytics will benefit the general public with more powerful search engines, recommender systems, better credit card scoring techniques and improved methods for identity fraud detection. Other industrial beneficiaries include business analytics, finance, pharmaceuticals, and the defence industry, where the developments of this proposal will have a significant impact on how large scale data can be analysed.

The longer term benefits of this project are also closely linked to the RCUK &amp;quot;Digital Economy&amp;quot; programme. For example the &amp;quot;digital hospital&amp;quot; component of this programme involves the real-time accurate data fusion and tracking of patients. It could directly benefit from the techniques we aim to develop in this proposal.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-08-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>200069</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of California, Irvine</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Donald Bren School of Information and Computer Sciences (ICS)</gtr:department><gtr:description>SGMCMC</gtr:description><gtr:id>84550A14-149F-41BE-B2D3-C8DF529EC0D5</gtr:id><gtr:impact>We have organised a series of 3 workshops (one in Amsterdam, one in Oxford and one at Neural Information Processing Systems (Montreal, Canada). Workshop websites:

https://github.com/BigBayes/bigbayes.github.io/wiki/BIBiD-2015

http://babaks.github.io/ScalableMonteCarlo/</gtr:impact><gtr:outcomeId>56cdbe82b022e3.69920016-1</gtr:outcomeId><gtr:partnerContribution>See above.</gtr:partnerContribution><gtr:piContribution>This is a joint NSF-EPSRC funded project, with US participants based at UCI and Amsterdam funded by NSF, and Oxford and Bristol participants funded by EPSRC. We have been working towards methods for scaling up Bayesian inference using Markov chain Monte Carlo to big data settings. The partnership have provided a forum to exchange ideas on this topic. We have organised a series of 3 workshops (one in Amsterdam, one in Oxford and one at Neural Information Processing Systems (Montreal, Canada).</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have met the original objectives of the project.

The major question we set out to investigate on the first phase of this grant is on the theoretical foundations of stochastic gradient MCMC methods for Bayesian inference on Big Data problems. A first answer is a recently accepted paper (Teh, Thiery, Vollmer 2016) at the Journal of Machine Learning Research (JMLR), the top journal in machine learning. In that paper we showed that SGLD is indeed a consistent method, and developed a central limit theorem which indicates that the rate of convergence is O(m^{-1/3}) where m is the number of iterations of the algorithm. This is slower than standard Monte Carlo convergence rates of O(m^{-1/2}). We are following this result up with a finite time analysis of SGLD without decreasing step size, where we found the same convergence rate of O(m^{-1/3}). This has now been accepted as a second paper to JMLR (Vollmer, Zygalakis, Teh 2016). Our postdoc Vollmer has continued working in the direction and has a number of follow up works (not funded by this project). 

We have published an extension of SGLD (Patterson and Teh) taking into account Riemannian manifold structure of probabilistic models at Neural Information Processing Systems 2013, the premier international conference in machine learning. This allowed SGLD type inference algorithms to be applicable to a large class of models defined over probability simplices. A second extension develops Relativistic Monte Carlo (Lu et al, accepted at International Conference on Artificial Intelligence and Statistics) which imposes a maximum speed (of light) on updates to parameters, which is important in applications in deep learning.

We have also developed methods for Bayesian inference for Big Data using distributed and parallel computing architectures. This culminated in two recent papers (Xu et al and Paige et al) at NIPS 2014. We have recently extended (Xu et al), where we developed a novel alternative to EP which is more robust to Monte Carlo errors in moment estimates, and a system for distributed learning which we call the Posterior Server, which was recently accepted at Journal of Machine Learning Research (Hasenclever et al (accepted)).

In addition a number of side projects investigated computational methods based on sequential Monte Carlo and Markov chain Monte Carlo and have been published/in review as well, and we have also worked on scalable Bayesian learning methods based on Mondrian forests (Lakshminarayanan et al 2014, 2015, 2016).</gtr:description><gtr:exploitationPathways>Development of Bayesian inference algorithms that are scalable are hugely important in scaling up the powerful methods for data analysis based on Bayesian statistics to the age of Big Data. The project partners held a project meeting in August 2014, with a summary appearing as an invited letter to the President's Newsletter of the International Society for Bayesian Analysis. We have also followed this meeting with another one in summer of 2015 in Oxford, and a NIPS workshop in December 2015. All workshops were well-attended and instigated interesting discussions both among research project members as well as others.

A number of research groups around the world have built on our work and developed a number of sophisticated extensions. This includes the stochastic gradient Hamilitonian Monte Carlo method (Chen et al ICML 2014) and the stochastic gradient thermostat (Ding et al NIPS 2014, Shang &amp;amp; Leimkuhler 2015). Our original paper on the SGLD (Welling &amp;amp; Teh 2011) has garnered over 250 citations on Google Scholar, demonstrating the impact and excitement around research in this area.</gtr:exploitationPathways><gtr:id>AFAED294-0407-45CF-A2E8-95FA67A6E8A3</gtr:id><gtr:outcomeId>5462419f6667b0.97133352</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://mlcs.stats.ox.ac.uk/projects/sgmcmc/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The Python code implements the stochastic gradient Riemannian Langevin dynamics (SGRLD) algorithm presented in the paper &amp;quot;Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex&amp;quot; by Sam Patterson and Yee Whye Teh at NIPS 2013.</gtr:description><gtr:id>28DC1B75-1815-48ED-B204-AD587E7901A4</gtr:id><gtr:impact>Our software was released as research software (rather than production software) for transparency and to help the research community build on and make use of our EPSRC funded work. We have not noticed notable impacts besides other researchers using our software and citing our work.</gtr:impact><gtr:outcomeId>546295ba6e3f74.30002445</gtr:outcomeId><gtr:title>Stochastic Gradient Riemannian Langevin Dynamics</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/BigBayes/SGRLD</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Method for performing Markov chain Monte Carlo in a distributed networked setting. Development software based on MATLAB for research purposes only.</gtr:description><gtr:id>44B1BC49-6D2C-43BD-977F-1A9F055EE34E</gtr:id><gtr:impact>None.</gtr:impact><gtr:outcomeId>56cdc00937fa77.65426840</gtr:outcomeId><gtr:title>Sampling via Moment Sharing (SMS)</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/BigBayes/SMS</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>SGMCMC.jl is a Julia test bed for stochastic gradient Markov chain Monte Carlo algorithms. There is a large range of SGMCMC algorithms but it can be difficult for practitioners to get a feel for different algorithms. We provide a simple package to try out different samplers on commonly used models. SGMCMC.jl is can also speed up experiments for researchers working on SGMCMC. It is simple to define new samplers and test them against existing ones on a number of commonly used models.</gtr:description><gtr:id>97884551-0FF0-4AD5-943D-A445CCFEBC1F</gtr:id><gtr:impact>None yet. Just released.</gtr:impact><gtr:outcomeId>58c7185f624039.36443109</gtr:outcomeId><gtr:title>SGMCMC.jl</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/BigBayes/SGMCMC.jl</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Research software for approximate Bayesian inference in graphical models based on EPBP (Lienart NIPS 2015). Software written in Julia.</gtr:description><gtr:id>D2E476B7-D1B6-41C1-A2B8-B58CB31A8AAD</gtr:id><gtr:impact>None.</gtr:impact><gtr:outcomeId>56cdc0b8f11402.15241166</gtr:outcomeId><gtr:title>Expectation Particle Belief Propagation (EPBP)</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/tlienart/EPBP</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This Julia code implements stochastic natural gradient expectation propagation, a novel algorithm for distributed Bayesian learning (see http://arxiv.org/abs/1512.09327). In addition, there is code for various SGD methods such as asynchronous SGD (https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) and elastic averaging SGD (https://papers.nips.cc/paper/5761-deep-learning-with-elastic-averaging-sgd.pdf). The code is research code that is fairly modular to allow testing on various different models. It is not production code meant to compete eg with standard neural network packages. We believe it should be possible to implement our algorithm much more efficiently in such a package.</gtr:description><gtr:id>28EA28B0-86B6-481B-AE33-69502FC09100</gtr:id><gtr:impact>None yet. Software associated with paper that was just accepted.</gtr:impact><gtr:outcomeId>58c71802d07989.40891761</gtr:outcomeId><gtr:title>Posterior Server and SNEP</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/BigBayes/PosteriorServer</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>81FDA5CA-90B2-44BF-B043-DD826E01D00C</gtr:id><gtr:title>Scalable Gaussian Processes for Characterizing Multidimensional Change Surfaces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9fd77ccd98141081681f9e66c70f2966"><gtr:id>9fd77ccd98141081681f9e66c70f2966</gtr:id><gtr:otherNames>Herlands W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cdc45f6184f1.39694762</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1FEAB183-8F0D-42AD-BBD6-D71F0DFD07B9</gtr:id><gtr:title>Asynchronous Anytime Sequential Monte Carlo</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed2ffb085ec98ef4279621072602bbea"><gtr:id>ed2ffb085ec98ef4279621072602bbea</gtr:id><gtr:otherNames>Paige B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546244766819b8.33128701</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8CA0FBC9-CEDA-4156-825A-72952E948857</gtr:id><gtr:title>Consistency and Fluctuations for Stochastic Gradient Langevin Dynamics</gtr:title><gtr:parentPublicationTitle>Journal of Machine Learning Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/32c33aa90ab10ba82a6957aaa9395f4f"><gtr:id>32c33aa90ab10ba82a6957aaa9395f4f</gtr:id><gtr:otherNames>Teh YW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5462439519a026.65044226</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61DFF908-6CF8-4D97-9833-3E8ABD40FBD2</gtr:id><gtr:title>Distributed Bayesian learning with stochastic natural-gradient expectation propagation and the posterior server</gtr:title><gtr:parentPublicationTitle>Journal of Machine Learning Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26413c585354c081795ed3ada95615d9"><gtr:id>26413c585354c081795ed3ada95615d9</gtr:id><gtr:otherNames>Hasenclever L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>56cdb7cb888c63.15717499</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9EF40CCC-2EBF-40D0-A56D-461B4D0DBB18</gtr:id><gtr:title>The Wang&amp;acirc;??Landau algorithm reaches the flat histogram criterion in finite time</gtr:title><gtr:parentPublicationTitle>The Annals of Applied Probability</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/60704af733b815b0c7069388738ceb7c"><gtr:id>60704af733b815b0c7069388738ceb7c</gtr:id><gtr:otherNames>Jacob P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54632311edeb73.31650568</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>66395BB9-1E71-424E-8EA1-24F1A0C73694</gtr:id><gtr:title>Distributed Bayesian Posterior Sampling via Moment Sharing</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aeaabec9f90a0072ffd2bea2f82fea62"><gtr:id>aeaabec9f90a0072ffd2bea2f82fea62</gtr:id><gtr:otherNames>Xu M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54624435b001f9.27241193</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A23C902F-352F-4798-AB9C-74D02833C496</gtr:id><gtr:title>Stochastic Gradient Riemannian Langevin Dynamics on the Probability Simplex</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/282ae484e5c075253c677437743d1964"><gtr:id>282ae484e5c075253c677437743d1964</gtr:id><gtr:otherNames>Patterson S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5462432f719e09.54949638</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E06C89AE-A0E5-4812-8DA8-EA203ECEEED1</gtr:id><gtr:title>Parallel Resampling in the Particle Filter</gtr:title><gtr:parentPublicationTitle>Journal of Computational and Graphical Statistics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2f1bc7bbf2d9a6533a0c9aa5158fcd9"><gtr:id>d2f1bc7bbf2d9a6533a0c9aa5158fcd9</gtr:id><gtr:otherNames>Murray L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>54625cfbdae543.38791012</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4EA0B1C2-3758-4CFD-AF28-8EFDB183874A</gtr:id><gtr:title>Exploration of the (non-)asymptotic bias and variance of stochastic gradient Langevin dynamics</gtr:title><gtr:parentPublicationTitle>Journal of Machine Learning Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1b288d714900581c40ac984d6d2154d0"><gtr:id>1b288d714900581c40ac984d6d2154d0</gtr:id><gtr:otherNames>Vollmer SJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5464db6f6a9d01.20566282</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>36924D24-D5EF-4C8F-A0ED-E0AB0B6DC1EC</gtr:id><gtr:title>Expectation Particle Belief Propagation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76ebc08a91a1dc603eea2fcfd172de69"><gtr:id>76ebc08a91a1dc603eea2fcfd172de69</gtr:id><gtr:otherNames>Lienart T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdb5992339b6.85529790</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5976C681-FC2A-4C11-A4BC-25BFB7323A13</gtr:id><gtr:title>Relativistic Monte Carlo</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fe7c89fddc71b72db69dbca9d36a56be"><gtr:id>fe7c89fddc71b72db69dbca9d36a56be</gtr:id><gtr:otherNames>Lu X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c716e09c0883.40520484</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0AF04543-6760-4869-8564-F0B13850E90B</gtr:id><gtr:title>Path storage in the particle filter</gtr:title><gtr:parentPublicationTitle>Statistics and Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/60704af733b815b0c7069388738ceb7c"><gtr:id>60704af733b815b0c7069388738ceb7c</gtr:id><gtr:otherNames>Jacob P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546240aab67591.50506515</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2783EB06-16A7-4B1B-9355-45E109F670AD</gtr:id><gtr:title>Bayesian Inference in Non-Markovian State-Space Models With Applications to Battery Fractional-Order Systems</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Control Systems Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/60704af733b815b0c7069388738ceb7c"><gtr:id>60704af733b815b0c7069388738ceb7c</gtr:id><gtr:otherNames>Jacob P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2fe6b0287062.02906747</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4982A4C6-4087-4B64-B10C-1A34A63D7FE8</gtr:id><gtr:title>Particle Gibbs for Bayesian additive regression trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dcee8994115239ac86b0ae2f0b2e134b"><gtr:id>dcee8994115239ac86b0ae2f0b2e134b</gtr:id><gtr:otherNames>Lakshminarayanan B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdb5dd9ae1c7.36949154</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>28BBD4C2-BA9F-45B3-9713-52DB9FE34CB0</gtr:id><gtr:title>Bayesian inference and big data: A snapshot from a workshop</gtr:title><gtr:parentPublicationTitle>ISBA Bulletin</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aeabfeb5fba28e5db008d7c4d95bf312"><gtr:id>aeabfeb5fba28e5db008d7c4d95bf312</gtr:id><gtr:otherNames>Welling M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdb66d3b56a9.53216437</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ABBCEA9E-12E6-49F4-955D-48C3239D6EC7</gtr:id><gtr:title>On nonnegative unbiased estimators</gtr:title><gtr:parentPublicationTitle>The Annals of Statistics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/60704af733b815b0c7069388738ceb7c"><gtr:id>60704af733b815b0c7069388738ceb7c</gtr:id><gtr:otherNames>Jacob P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>doi_55f982982f1c365d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>59CE2F75-CC3E-465B-AF8E-396701F40930</gtr:id><gtr:title>Mondrian Forests for Large-Scale Regression when Uncertainty Matters</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dcee8994115239ac86b0ae2f0b2e134b"><gtr:id>dcee8994115239ac86b0ae2f0b2e134b</gtr:id><gtr:otherNames>Lakshminarayanan B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56cdb5545a9021.85519372</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K009362/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>DEA11FBC-BEED-4EDD-890B-97D728462D26</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Mathematical sciences</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>62309876-5C71-411C-B1A7-1B2907AFB5A8</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Statistics &amp; Appl. Probability</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>