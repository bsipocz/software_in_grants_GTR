<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CAA9A40D-0226-4A4F-AC0D-D8299E30A1EF"><gtr:id>CAA9A40D-0226-4A4F-AC0D-D8299E30A1EF</gtr:id><gtr:name>Loughborough University</gtr:name><gtr:department>Electronic, Electrical &amp; Systems Enginee</gtr:department><gtr:address><gtr:line1>Loughborough University</gtr:line1><gtr:line4>Loughborough</gtr:line4><gtr:line5>Leicestershire</gtr:line5><gtr:postCode>LE11 3TU</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CAA9A40D-0226-4A4F-AC0D-D8299E30A1EF"><gtr:id>CAA9A40D-0226-4A4F-AC0D-D8299E30A1EF</gtr:id><gtr:name>Loughborough University</gtr:name><gtr:address><gtr:line1>Loughborough University</gtr:line1><gtr:line4>Loughborough</gtr:line4><gtr:line5>Leicestershire</gtr:line5><gtr:postCode>LE11 3TU</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2365F024-E37D-45EF-B7B0-A7972FB3D7A4"><gtr:id>2365F024-E37D-45EF-B7B0-A7972FB3D7A4</gtr:id><gtr:firstName>Sangarapillai</gtr:firstName><gtr:surname>Lambotharan</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/EDF5C5A7-BBFD-4BC0-940E-E81E043CDD67"><gtr:id>EDF5C5A7-BBFD-4BC0-940E-E81E043CDD67</gtr:id><gtr:firstName>Jonathon</gtr:firstName><gtr:surname>Chambers</gtr:surname><gtr:orcidId>0000-0002-5820-6509</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH049665%2F1"><gtr:id>540E8761-43BA-4B3F-BA5D-60FF706CEAFA</gtr:id><gtr:title>Audio and Video Based Speech Separation for Multiple Moving Sources Within a Room Environment</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H049665/1</gtr:grantReference><gtr:abstractText>Human beings have developed a unique ability to communicate within a noisy environment, such as at a cocktail party. This skill is dependent upon the use of both the aural and visual senses together with sophisticated processing within the brain. To mimic this ability within a machine is very challenging, particularly if the humans are moving, such as in a teleconferencing context, when human speakers are walking around a room. In the field of signal processing researchers have developed techniques to separate one speech signal from a mixture of such signals, as would be measured by a number of microphones, on the basis of only audio information with the assumption that the humans are static and typically no more than two humans are within the room. Such approaches have generally been found to fail, however, when the human speakers are moving and when there are more than two in number. Fundamentally new approaches are therefore necessary to advance the state-of-the-art in the field. Professor Chambers and his team at Loughborough University were the first in the UK to propose a new approach on the basis of combined audio and video processing to solve the source separation problem, but their preliminary approach identified major challenges in audio-visual speaker localization, tracking and separation which must be solved to provide a practical solution for speech separation for multiple moving sources within a room environment. These findings motivate this new project in which world-leading teams at the University of Surrey, led by Professor Kittler, and at the GIPSA Lab, Grenoble, France, headed by Professor Jutten, are ready to work with Professor Chambers and his team at Loughborough University to advance the state-of-the-art in the field.In this new project, two postdoctoral researchers will be employed, one at Loughborough and another at Surrey. The first will focus on the development of fundamentally new speech source separation algorithms for moving speakers by using geometrical room acoustic (for example location and number of sources, descriptions of their movement) information provided by the second researcher. The research team at Grenoble will provide technical guidance on the basis of their considerable experience in source separation throughout the project and will work on providing an acoustic noise model for the room environment which will also aid the speech separation process. To achieve these tasks, frequency domain based beamforming algorithms will be developed which exploit microphone arrays having more microphones than speakers so that new data independent superdirective robust beamformer design methods can be exploited using mathematical convex optimization. Additionally, further geometic information will be exploited to introduce robustness to errors in the localization information describing the desired source and the interference. To improve the localization information an array of collaborative cameras will be used and both audio and visual information will be used. Advanced methods from particle filtering and probabilistic data association will be exploited for improving the tracking performance. Finally, visual voice activity detection will be used to determine the active sources within the beamforming operations. We emphasize that this work is not implementation-driven, so computational complexity for real-time realization will not be a focus; this would be the subject of a future project.All of the new algorithms will be evaluated both in terms of objective and subjective performance measures on labelled audio and visual datasets acquired at Loughbourgh and Surrey, and from the CHIL seminar room at the Karlsruhe University (UKA), Germany. To ensure this pioneering work has maximum impact on the UK and international academic and research communities all the algorithms and datasets will be made available through the project website.</gtr:abstractText><gtr:potentialImpactText>The project entails fundamental algorithmic research and evaluation. We are therefore not involving an industrial partner directly nor providing a commercial roadmap for exploitation; however, as listed below, our industrial contacts provide clear routes for longer term engagement with industry. Who will benefit from this research? The PDRAs working on the project and the research students in the associated laboratories at Loughborough and Surrey. The UK and international academic research communities working in the field of combined audio and video processing will be major beneficiaries, in particular those with an interest in speech separation. In the longer term, UK industries in the areas of automatic speech recognition and human machine interfaces, defence and security (MoD), and healthcare (NHS) are likely to benefit from the work, but this is expected to happen after the three-year duration of this project. How will they benefit from this research? The project intends to advance the state-of-the-art in terms of audio and video-based algorithms for localization, tracking and source separation of moving sources within a room environment. The signal processing algorithms developed and the related audio-video datasets used for evaluation will become important research tools for the UK and international academic and industrial research communities. Industrial contacts, such as through QinetiQ, for which Professor Chambers is the first QinetiQ Visiting Fellow, and with BAE Systems, for which Professor Kittler is currently managing collaborative projects, will ensure that the route is open for the longer term, beyond the three year period of the project, commercialization of new technological breakthroughs - particularly in the defence and security areas. In the wider context of the digital economy, the research has the potential to improve the quality of life of those who have disabilities and wish to remain living independently. What will be done to ensure they benefit from the research? The PDRAs will be trained by the investigators on the project all of whom have outstanding track records in their respective research areas. The research results will be published regularly in the foremost journals and presented at international conferences. We will attend other key international conferences to disseminate our results to the academic and industrial research communities. Quarterly meetings will be set up with our international collaborators to review the project progress and to transfer knowledge and skills. A website dedicated to our audio and video based research will be developed with the aim of attracting wide audience from academia, and industrial research laboratories.</gtr:potentialImpactText><gtr:fund><gtr:end>2013-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>300746</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The algorithms developed have been published in international conferences and journals.</gtr:description><gtr:id>93BF3B43-895A-4438-A535-8385971D4A87</gtr:id><gtr:impactTypes/><gtr:outcomeId>54579c603dca38.25406979</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In this joint project between the Advanced Signal Processing Group at Loughborough University and the Centre for Vision, Speech and Signal Processing at Surrey University we have developed new methods for solving the machine cocktail party problem in an enclosed environment; namely, to mimic the ability of a human to separate sounds from moving speakers using both their ears and eyes. In the machine these sounds are measured at multiple microphones and visual information is acquired by video cameras. One technique we have proposed exploits a circular microphone array, multiple video cameras, robust spatial beamforming and time-frequency masking. Use of the video modality allows the processing to adapt to whether the sources are statistic or moving. The second method limits the number of microphones to two, in the same way that a human only uses two ears, whilst retaining the ability to separate multiple sources. The processing is based upon exploiting both audio and visual information, that is interaural level and phase difference cues, together with video-informed mixing terms in the form of probabilistic models. Given that the sources can be moving the development of tracking algorithms has been crucial to the work and we have developed new solutions that are based upon (1) robust and computationally efficient modelling and classification of the changing appearance of the speakers in a variety of different lighting conditions and camera resolutions; (2) dealt with full or partial occlusions when multiple speakers cross or come into very close proximity; (3) automatically initialised the trackers, or performed re-initialisation, when the trackers have lost lock caused by e.g. the limited camera views. Finally, we have developed an audio-only method for estimation of head pose orientation. All of the methods have had success on real datasets and new databases have been recorded for future work in the field.</gtr:description><gtr:exploitationPathways>The natural interface between man and machines is through speech and therefore our research findings have attracted broad interest both nationally and internationally. Internet and telecommunication companies are very keen to progress the field, for example in expanding the use of speech interfaces in noisy and challenging environments. We believe the use of video in this domain is crucial to generate transformative solutions, and the work we have undertaken is providing a basis for part of the research activity in our new five-year &amp;pound;4.4M project entitled &amp;quot;Signal Processing for the Networked Battlespace&amp;quot; funded by Dstl and EPSRC. The research findings have been published in leading international journals published by the IEEE such as Trans. on Audio, Speech and Language Processing, Signal Processing and Multimedia; and presented at key UK and international conferences. Collaboration with the University of Grenoble through funding from Dstl and DGA (France) has allowed international exchange of knowledge and transfer to key defence stakeholders. Research staff and students have benefitted from the very best training in research. This quality training has been the platform for success in the transfer of personal to both academic and new research positions at the end of the project.</gtr:exploitationPathways><gtr:id>8B519BD4-EAB4-4D71-8819-C1BB328772FA</gtr:id><gtr:outcomeId>r-8786421780.275574777466ea</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.lboro.ac.uk/departments/eese/research/communications/signal-processing/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C8EC68AE-4993-49E7-81E0-DE304D20CB4E</gtr:id><gtr:title>A Multimodal Approach to Blind Source Separation of Moving Sources</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01c69a027bae739bd5e502b251904cde"><gtr:id>01c69a027bae739bd5e502b251904cde</gtr:id><gtr:otherNames>Naqvi S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>541a8ee93fc762.45185124</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2054BA07-EA43-4F01-A9EF-F459548C885D</gtr:id><gtr:title>One class boundary method classifiers for application in a video-based fall detection system</gtr:title><gtr:parentPublicationTitle>IET Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a0029150620f65feff65f8b10d27745"><gtr:id>6a0029150620f65feff65f8b10d27745</gtr:id><gtr:otherNames>Yu M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d0310318225562</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BBF7B47B-09AA-4968-AB88-B04D2D25FFFE</gtr:id><gtr:title>Video-Aided Model-Based Source Separation in Real Reverberant Rooms</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/614ab942f7b5ca90143931e5d006b3f2"><gtr:id>614ab942f7b5ca90143931e5d006b3f2</gtr:id><gtr:otherNames>Khan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d05c05cf3d9bf5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8FC068BF-E48D-4FC6-8137-F7B195DD28EF</gtr:id><gtr:title>Adaptive step size independent vector analysis for blind source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/16c11141da1f37cf666bf71d1556f422"><gtr:id>16c11141da1f37cf666bf71d1556f422</gtr:id><gtr:otherNames>Yanfeng Liang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0273-0</gtr:isbn><gtr:outcomeId>doi_53d058058806e32f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C4A8CBDF-5770-41B8-B5D3-9D6C01DEAABD</gtr:id><gtr:title>Multimodal Blind Source Separation with a Circular Microphone Array and Robust Beamforming</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e024b4df43c74ac60f8df5eae249223d"><gtr:id>e024b4df43c74ac60f8df5eae249223d</gtr:id><gtr:otherNames>Syed Mohsen Naqvi (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_96955775661407cb46</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D778CC4-37B6-4AC9-AD4F-0D222AE0C364</gtr:id><gtr:title>Robust Multi-Speaker Tracking via Dictionary Learning and Identity Modeling</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2545b516d74f82259d77b9b02e19165"><gtr:id>d2545b516d74f82259d77b9b02e19165</gtr:id><gtr:otherNames>Barnard M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>m_50195175921369b71c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4028DD48-7C40-4A61-B0C3-53145A586257</gtr:id><gtr:title>Multimodal (audio-visual) source separation exploiting multi-speaker tracking, robust beamforming and time-frequency masking</gtr:title><gtr:parentPublicationTitle>IET Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3adfe0e4a6f8620452a7bccef115fb3b"><gtr:id>3adfe0e4a6f8620452a7bccef115fb3b</gtr:id><gtr:otherNames>Mohsen Naqvi S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d031031e17d713</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E9FF242-A34A-43CD-8332-BD57815C87AC</gtr:id><gtr:title>Audio video based fast fixed-point independent vector analysis for multisource separation in a room environment</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Advances in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96925e58fc50223a95dd6d10f9850bc0"><gtr:id>96925e58fc50223a95dd6d10f9850bc0</gtr:id><gtr:otherNames>Liang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d07b07bbe3503c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2E8D91D-DB62-4669-9950-94140896F9EC</gtr:id><gtr:title>Multimodal blind source separation for moving sources based on robust beamforming</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/01c69a027bae739bd5e502b251904cde"><gtr:id>01c69a027bae739bd5e502b251904cde</gtr:id><gtr:otherNames>Naqvi S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0538-0</gtr:isbn><gtr:outcomeId>doi_53d0580583034601</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C292207E-0A2A-4904-BC48-61A314C3D224</gtr:id><gtr:title>Robust Feature Selection for Scaling Ambiguity Reduction in Audio-Visual Convolutive BSS</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e024b4df43c74ac60f8df5eae249223d"><gtr:id>e024b4df43c74ac60f8df5eae249223d</gtr:id><gtr:otherNames>Syed Mohsen Naqvi (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_12126326271407cc54</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3ECEEC27-1B7D-4F07-A0D3-E84262F8324B</gtr:id><gtr:title>Fast pose invariant face recognition using super coupled multiresolution Markov Random Fields on a GPU</gtr:title><gtr:parentPublicationTitle>Pattern Recognition Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1473381d2e6453f183e678ae8489e4a3"><gtr:id>1473381d2e6453f183e678ae8489e4a3</gtr:id><gtr:otherNames>Rahimzadeh Arashloo S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f9759756d8af31</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H049665/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>