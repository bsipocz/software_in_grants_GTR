<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/BECA2763-0902-40AA-8DB2-836A5731CF58"><gtr:id>BECA2763-0902-40AA-8DB2-836A5731CF58</gtr:id><gtr:name>Brunel University</gtr:name><gtr:department>Information Systems &amp; Computing</gtr:department><gtr:address><gtr:line1>Brunel University</gtr:line1><gtr:line4>Uxbridge</gtr:line4><gtr:line5>Middlesex</gtr:line5><gtr:postCode>UB8 3PH</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BECA2763-0902-40AA-8DB2-836A5731CF58"><gtr:id>BECA2763-0902-40AA-8DB2-836A5731CF58</gtr:id><gtr:name>Brunel University</gtr:name><gtr:address><gtr:line1>Brunel University</gtr:line1><gtr:line4>Uxbridge</gtr:line4><gtr:line5>Middlesex</gtr:line5><gtr:postCode>UB8 3PH</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C8026C9D-7BE7-435B-933D-9DCBBAF6C449"><gtr:id>C8026C9D-7BE7-435B-933D-9DCBBAF6C449</gtr:id><gtr:firstName>Yongmin</gtr:firstName><gtr:surname>Li</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FC007654%2F1"><gtr:id>A6A18D6F-0C64-4021-ACE4-F9CD2B342554</gtr:id><gtr:title>Video Annotation Using Human Activities and Visual Context</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/C007654/1</gtr:grantReference><gtr:abstractText>This proposal aims to develop a system for video annotation, i.e. assigning meaningful semantic labels to video units. As opposed to many previous studies where ad-hoc concepts such as 'grasses', 'sky' and 'explosion' are adopted, we will address the problem from a different perspective by combining human activity and visual context. On the one hand, humans are usually the subjects of video semantics, e.g. the doer of an action. Their presence, activities and interactions are often the key factors to video contents. On the other hand, context, the physical or informative environment or situation where human activities are undertaken, can greatly clarify ambiguity and reduce complexity in video content understanding. Based on our previous work on human face processing and semantic video analysis, we will develop new algorithms and methods for appearance based non-rigid object (e.g. face) tracking, incremental and robust person-specific facial model updating, and unsupervised automatic contextual analysis. The system will detect and track human faces and provide probabilistic descriptions of each individual human face as well as their trajectories in a video. It will also formulate person-specific facial appearance models online by incrementally and robustly updating a generic facial model. Meanwhile, the system will perform unsupervised visual context analysis from low-level features on each video segments.</gtr:abstractText><gtr:fund><gtr:end>2009-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>126876</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>552CE101-8743-402D-8934-44C91257388B</gtr:id><gtr:title>Set-Membership Filtering with State Constraints</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Aerospace and Electronic Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7c05c5fa650837e820e71d7efcfa94ab"><gtr:id>7c05c5fa650837e820e71d7efcfa94ab</gtr:id><gtr:otherNames>Fuwen Yang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d05c05c9e86b18</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>05009D0D-53EC-46A3-9CB7-F2B0F1A4549F</gtr:id><gtr:title>Set-membership fuzzy filtering for nonlinear discrete-time systems.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1c4b012d8125b7b44c79b0b4f6555913"><gtr:id>1c4b012d8125b7b44c79b0b4f6555913</gtr:id><gtr:otherNames>Yang F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1083-4419</gtr:issn><gtr:outcomeId>doi_53d05f05f7927314</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8AFDCDF9-D84B-49E7-A1B2-9A570DF55329</gtr:id><gtr:title>Set-Membership Filtering for Discrete-Time Systems With Nonlinear Equality Constraints</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Automatic Control</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7c05c5fa650837e820e71d7efcfa94ab"><gtr:id>7c05c5fa650837e820e71d7efcfa94ab</gtr:id><gtr:otherNames>Fuwen Yang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d05c05c8bc67b7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21BE7AFC-3845-465E-8559-62388B8CF7E1</gtr:id><gtr:title>Set-membership filtering for systems with sensor saturation</gtr:title><gtr:parentPublicationTitle>Automatica</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1c4b012d8125b7b44c79b0b4f6555913"><gtr:id>1c4b012d8125b7b44c79b0b4f6555913</gtr:id><gtr:otherNames>Yang F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53cfe5fe5df21b79</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/C007654/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>