<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/961756BF-E31F-4A13-836F-0A09BA02385C"><gtr:id>961756BF-E31F-4A13-836F-0A09BA02385C</gtr:id><gtr:name>University of Exeter</gtr:name><gtr:department>Biosciences</gtr:department><gtr:address><gtr:line1>University of Exeter</gtr:line1><gtr:line2>Clydesdale House</gtr:line2><gtr:line3>Clydesdale Road</gtr:line3><gtr:line4>Exeter</gtr:line4><gtr:postCode>EX4 4QX</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/961756BF-E31F-4A13-836F-0A09BA02385C"><gtr:id>961756BF-E31F-4A13-836F-0A09BA02385C</gtr:id><gtr:name>University of Exeter</gtr:name><gtr:address><gtr:line1>University of Exeter</gtr:line1><gtr:line2>Clydesdale House</gtr:line2><gtr:line3>Clydesdale Road</gtr:line3><gtr:line4>Exeter</gtr:line4><gtr:postCode>EX4 4QX</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/426EB813-25A4-44A5-A9B3-7C8CF775872E"><gtr:id>426EB813-25A4-44A5-A9B3-7C8CF775872E</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Everson</gtr:surname><gtr:orcidId>0000-0002-3964-1150</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B809EC4D-175A-4F30-81C4-52C21316C991"><gtr:id>B809EC4D-175A-4F30-81C4-52C21316C991</gtr:id><gtr:firstName>Tom</gtr:firstName><gtr:surname>Tregenza</gtr:surname><gtr:orcidId>0000-0003-4182-2222</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=NE%2FI000852%2F1"><gtr:id>D325EC18-B557-4195-AA5A-AAA7CFF0F273</gtr:id><gtr:title>Video image recognition for ecological monitoring</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>NE/I000852/1</gtr:grantReference><gtr:abstractText>Advances in video technology over the last decade mean it is now feasible to study insects and other small animals in their natural habitats. By tagging and genotyping individuals, their lives can be observed and the relationships between individuals, including how many offspring they leave can be quantified. The main bottleneck preventing a range of fantastic new studies is the time it takes to collect basic data from digital video recordings. We will develop new software to automate individual recognition of tagged invertebrates and other small animals in the wild. The project is a collaboration between the School of Biosciences and the School of Engineering, Computing &amp;amp; Mathematics (SECAM) at the University of Exeter. In Biosciences we have developed an 80 camera integrated network of video cameras monitoring a tagged population of wild crickets in a field in Spain. This provides a perfect platform for the development of a system that will have potential applications across field biology. In SECAM we have developed new approaches to object recognition and video tracking that are ideally suited to the requirements of these new monitoring techniques. This technology has the potential to catalyse a host of new studies that will ultimately provide insights into the natural ecology of invertebrates, essential for understanding ecosystems. Potential applications The availability of internet protocol (IP) CCTV cameras, which are flexible to deploy and can be wireless, has enormous potential for use in environmental research. Initially species that predictably visit small areas such as nests, flowers and burrow entrances will be the main subjects of study, but higher resolution cameras enable larger areas to be monitored. Many of our potential end users will not yet be aware of the potential studies they could be carrying out. We anticipate our software will allow behavioural ecologists to monitor the behaviour of beetles, allow pollination studies that automatically track which individual bees have visited which real or artificial flowers, and will allow studies of how the spatial distribution of terrestrial and marine gastropods arises from individual movement patterns. Technology We propose to develop new pattern recognition techniques that will allow any researcher working on a system in which individual animals are tagged to automate key aspects of data collection and analysis. We will use our database of 100,000+ hours of recordings of tagged crickets as a development platform. The crickets have tags with a 2 character code that can be seen on the video recordings. We will use image segmentation via deformable object contours and tracking to automatically distinguish and follow individuals. Variational Bayesian methods will permit efficient handling of multi-modal densities, enabling us to track more than one individual and distinguish animals from noise such as the shadows of grass moving in the breeze. Character recognition will be used to identify individuals from their tags; as these are often not visible or are obliquely viewed, we shall fuse estimates from tracked frames to provide an optimal estimate. We shall also investigate the use of two dimensional error-correcting barcodes (e.g., QR codes) as a robust alternative to alphanumeric codes. By the end of the project we expect the system to be operational in the field and available to researchers for testing and for further development - i.e. at TRL level 4.</gtr:abstractText><gtr:fund><gtr:end>2012-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/8A03ED41-E67D-4F4A-B5DD-AAFB272B6471"><gtr:id>8A03ED41-E67D-4F4A-B5DD-AAFB272B6471</gtr:id><gtr:name>NERC</gtr:name></gtr:funder><gtr:start>2010-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>96732</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We are continuing to work on this project with the aim of producing a commercially valuable object recognition and tracking application for biological research. We currently have 1 PhD student in the UK and 1 masters students in Denmark working on the project. However as yet, there have been no documentable impacts</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>87BFBB4F-6559-4912-967C-E3D554D1897B</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545c96a922f0b4.32317828</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have worked on the problem of automatically locating, tracking and identifying wild field crickets (Gryllus campestris) in their natural habitat through digital video recordings. Our original aims were to:

1. Detect, count and log the presence of objects of a particular size in our video recordings

2. Make repeated estimates of the characters on a tag providing an estimate of ID.

3. Read tags from more than one individual at once.

4. Track and quantify movement of a single individual.

5. Track and quantify movement of more than one individual at once.

6. Flag novel video segments (e.g. bird devouring a cricket) for manual inspection.

7. Any number of increasingly sophisticated additional features.

Of these, the most important aims were the automatic tracking and identification of crickets, with programs written in Matlab. The data are stored in Matlab MAT files and key data extracted to CSV files for importation into a Microsoft Access database in which manually collected data has already been stored. A prototype GUI has been demonstrated which enables the user to view selected videos, with or without overlaid tracks.

We have demonstrated a system that is able automatically to detect and log the presence of a moving object within the frame, track and quantify the movement of a single individual, determine to some degree of accuracy that the individual is a cricket, flag some types of unusual events, and make repeated estimates of the characters on a tag. Our tracking algorithm should be able to locate and track more than one object at once (which would allow the reading of more than one tag at once), but we discovered that this was computationally very intensive and the speed of the tracking part of the algorithm needs to be improved. Our work has provided a framework for this development.

We have demonstrated that the algorithm can be generalised to other tracking scenarios by using it to locate and track bees in a laboratory environment. The only significant change required was to the prior information about the anticipated size of the object being tracked.

Progress against the specific aims of the original NERC proposal is as follows:

Detect, count and log the presence of objects of a particular size that move within the frame.

The algorithms detect and log the presence of a moving object within each frame and they are able to incorporate a prior belief regarding the size of the object to be tracked. I believe that they are able to track multiple objects at once, but the tracking program is currently too slow to test this functionality to any useful degree.

Figure 2 shows an example of a cricket being tracked, compared with the ground truth. The location of the cricket is accurately estimated. Figure 3 shows the tracks made by crickets in a number of videos with the position of the (manually located) burrows marked. The patterns of the tracks are closely associated with the burrows.

Make repeated estimates of the characters on a tag providing an integrated best estimate of ID.

We have developed an algorithm that uses information from the tracking program to home in on the location of the tag and then compare the extracted tag image with library images of the tags we know to be in use. The algorithm ranks each library tag according to how likely they are to match the extracted tag image. This process can be repeated across all the frames in a video and the resulting ranks used to estimate the ID.

If the crickets are being tracked effectively, then it is not necessary to identify them in every frame of a video. In future, the identification process might be performed only on a selected subset frames and it might use information from across the field to inform the estimated tag identity in a particular location at a particular time.

Read tags from more than one individual at once.

The tag-reading algorithm is able to attempt to locate and read tags from as many objects as the tracking algorithm has tracked. At present this is only single objects.

Track and quantify movement of a single individual.

The algorithms track and quantify the movement of a single individual.

Track and quantify movement of more than one individual at once.

As stated above, I believe that they are able to track multiple objects at once, but this has not yet been tested.

Flag novel video segments (e.g. bird devouring a cricket) for manual inspection.

The cricket tracks are represented by a series of ellipses centred on the cricket. The five parameters that describe each ellipse (xy coordinates of the centre, long and short axis lengths and orientation angle) are estimated as probability distributions. Thus we may identify when the cricket makes an unexpectedly large jump in location or when the size of the ellipse changes unexpectedly.

Any number of increasingly sophisticated additional features - in particular the potential to infer behaviour of individuals (e.g. fighting, foraging, mating).

Most of the behaviour that we wish to infer is based on the interaction between two or more crickets. Since we are only tracking one individual in each video, we are not able to identify behaviour such as fighting or mating. Foraging and basking behaviour might be plausibly inferred from the shape of the tracks.

One particularly useful outcome would be to estimate the location of the burrows. These are often difficult, if not impossible, to see by eye, but should be the focus of large jumps by the (frightened) cricket. Thus multiple, unexpectedly large jumps to a similar location might be used to infer the burrow location.

Analysis should be automatic, i.e. the videos should be able to be processed and analysed without human intervention.

The processing from video to tracks is entirely automatic. Classification of tracks into cricket and non-cricket, and the location and identification of tags are also automatic and could easily be incorporated into the main process.

Cricket tracking data to be uploaded into a database.

Rather than an SQL database, the data is stored in Matlab MAT files and key data extracted to CSV files for importation into a Microsoft Access database in which manually collected data has already been stored.

Designing and implementing an SQL database would be relatively straightforward. Building a useful front end to it would be more time-consuming, but not too difficult. Long-term maintenance of the front end wold require specialist support on an ad-hoc basis.

A graphical user interface (GUI) through which the results could be reviewed.

A prototype Matlab-based GUI has been demonstrated which enables the user to view selected videos, with or without overlaid tracks.

Other outcomes

The tracking algorithm is able to track a range of objects in different formats of video with minimal change. For example, it has been demonstrated on a video of caged bees, with changes only to the video format (an uncompressed AVI rather that the JPEGs for the cricket videos) and the prior expectation of the object size.

The research outcomes for computer science are a novel method of probabilistic background modelling (paper in review with Machine Learning) and a novel object tracking algorithm which does away with a separate object (re)aquisition step (paper in preparation).</gtr:description><gtr:exploitationPathways>We are working with commerical CCTV provider i-code systems to develop their i-catcher software. This will enable users including non-academic researchers, land managers, conservationists, outdoor wildlife attraction managers and so forth to use our software. We are seeking further funding to develop this research. We are working with commerical CCTV provider i-code systems to develop their i-catcher software to enable them to sell it to researchers</gtr:exploitationPathways><gtr:id>BEE79EDD-25B6-4312-BB95-990506E3C9A9</gtr:id><gtr:outcomeId>r-3664657076.508418677649a6c</gtr:outcomeId><gtr:sectors><gtr:sector>Electronics,Environment,Leisure Activities/ including Sports/ Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>30EF314D-1C64-4BB0-823E-EF55FC08F540</gtr:id><gtr:title>Comparing pre- and post-copulatory mate competition using social network analysis in wild crickets.</gtr:title><gtr:parentPublicationTitle>Behavioral ecology : official journal of the International Society for Behavioral Ecology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8683eb5220016d2c08c13f1e119d1e4b"><gtr:id>8683eb5220016d2c08c13f1e119d1e4b</gtr:id><gtr:otherNames>Fisher DN</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1045-2249</gtr:issn><gtr:outcomeId>585d7930ed5e28.07964255</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C338973D-4942-48B5-8160-791141F287B6</gtr:id><gtr:title>The effect of size and sex ratio experiences on reproductive competition in Nicrophorus vespilloides burying beetles in the wild.</gtr:title><gtr:parentPublicationTitle>Journal of evolutionary biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5e75b77346c82a542413af7f71588c37"><gtr:id>5e75b77346c82a542413af7f71588c37</gtr:id><gtr:otherNames>Hopwood PE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1010-061X</gtr:issn><gtr:outcomeId>56ba1d6f1f1140.83183982</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C5D10FED-5AB2-4491-856F-DC79CE646097</gtr:id><gtr:title>Variational Bayesian tracking: Whole track convergence for large-scale ecological video monitoring</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b33f56c0cf90bcdc55cc7c3fea90a123"><gtr:id>b33f56c0cf90bcdc55cc7c3fea90a123</gtr:id><gtr:otherNames>Christmas J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545c78d0385ae2.78269018</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">NE/I000852/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>1A1A6805-9DC4-4BCE-BC70-9F2AA4FD093B</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Ecol, biodivers. &amp; systematics</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>C23BE70F-1EF6-4CC7-88B0-841CB18DD875</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Genetics &amp; development</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>685A8D5E-BD8A-4D8D-BAC5-607439217156</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Behavioural Ecology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5020ECC8-E0E8-434D-BDD5-663A9C04EFA2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Conservation Ecology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>DE30777A-E4A8-486B-875D-58CC92FD5525</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Population Ecology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A4209D5A-2E41-4290-9D1A-3172C1F48962</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Population Genetics/Evolution</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>