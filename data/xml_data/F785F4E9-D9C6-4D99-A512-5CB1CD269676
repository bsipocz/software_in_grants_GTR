<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1C0C0EC8-3AEE-4671-A717-FC834D00338C"><gtr:id>1C0C0EC8-3AEE-4671-A717-FC834D00338C</gtr:id><gtr:name>Nissan Motor Company</gtr:name><gtr:address><gtr:line1>1-1 Morinosatoaoyama, Atsugi-shi</gtr:line1><gtr:postCode>243-0123</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1C0C0EC8-3AEE-4671-A717-FC834D00338C"><gtr:id>1C0C0EC8-3AEE-4671-A717-FC834D00338C</gtr:id><gtr:name>Nissan Motor Company</gtr:name><gtr:address><gtr:line1>1-1 Morinosatoaoyama, Atsugi-shi</gtr:line1><gtr:postCode>243-0123</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D9111769-DF66-4DB1-8D4A-842FEB13D7ED"><gtr:id>D9111769-DF66-4DB1-8D4A-842FEB13D7ED</gtr:id><gtr:name>Department for Transport</gtr:name><gtr:address><gtr:line1>Gt Minster House</gtr:line1><gtr:line2>76 Marsham Street</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW1P 4DR</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF81E303-6E50-4348-9599-52B81DED7BC3"><gtr:id>EF81E303-6E50-4348-9599-52B81DED7BC3</gtr:id><gtr:name>BAE Systems</gtr:name><gtr:address><gtr:line1>Chelmsford Office and Technology Park</gtr:line1><gtr:line2>West Hanningfield Road</gtr:line2><gtr:line3>Great Baddow</gtr:line3><gtr:line4>Chelmsford</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CM2 8HN</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D535F886-24F2-4E8C-BB78-369651E9530F"><gtr:id>D535F886-24F2-4E8C-BB78-369651E9530F</gtr:id><gtr:name>Navtech Radar Limited</gtr:name><gtr:address><gtr:line1>16 Home Farm</gtr:line1><gtr:line2>Ardington</gtr:line2><gtr:line4>Wantage</gtr:line4><gtr:line5>Oxfordshire</gtr:line5><gtr:postCode>OX12 8PD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/00FE3D00-46A6-4EDB-9672-1565C1E10DB8"><gtr:id>00FE3D00-46A6-4EDB-9672-1565C1E10DB8</gtr:id><gtr:name>Massachusetts Institute of Technology</gtr:name><gtr:address><gtr:line1>77 Massachusetts Avenue</gtr:line1><gtr:postCode>02139</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/273F7312-D608-4BF1-917F-0E2FFE42E1D0"><gtr:id>273F7312-D608-4BF1-917F-0E2FFE42E1D0</gtr:id><gtr:name>Guidance Navigation Ltd.</gtr:name><gtr:address><gtr:line1>4 Dominus Way</gtr:line1><gtr:line2>Meridian Business Park</gtr:line2><gtr:postCode>LE19 1RR</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C74C9612-26CA-4D91-B65F-260FD21EB7A4"><gtr:id>C74C9612-26CA-4D91-B65F-260FD21EB7A4</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>Michael</gtr:otherNames><gtr:surname>Newman</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI005021%2F1"><gtr:id>F785F4E9-D9C6-4D99-A512-5CB1CD269676</gtr:id><gtr:title>Life-Long Infrastructure Free Robot Navigation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/I005021/1</gtr:grantReference><gtr:abstractText>In the future, autonomous vehicles will play an important part in our lives. They will come in a variety of shapes and sizes and undertake a diverse set of tasks on our behalf. We want smart vehicles to carry, transport, labour for and defend us. We want them to be flexible, reliable and safe. Already robots carry goods around factories and manage our ports, but these are constrained, controlled and highly managed workspaces. Here the navigation task is made simple by installing reflective beacons or guide wires. This project is about extending the reach of robot navigation to truly vast scales without the need for such expensive, awkward and inconvenient modification of the environment. It is about enabling machines to operate for, with and beside us in the multitude of spaces we inhabit, live and work. Even when GPS is available, it does not offer the accuracy required for robots to make decisions about how and when to move safely. Even if it did, it would say nothing about what is around the robot and that has a massive impact on autonomous decision-making.Perhaps the ultimate application is civilian transport systems. We are not condemned to a future of congestion and accidents. We will eventually have cars that can drive themselves, interacting safely with other road users and using roads efficiently, thus freeing up our precious time. But to do this the machines need life-long infrastructure-free navigation, and that is the focus of this work.We will use the mathematics of probability and estimation to allow computers in robots to interpret data from sensors like cameras, radars and lasers, aerial photos and on-the-fly internet queries. We will use machine learning techniques to build and calibrate mathematical models which can explain the robot's view of the world in terms of prior experience (training), prior knowledge (aerial images, road plans and semantics) and automatically generated Web queries. The goal is to produce technology which allows robots always to know precisely where they are and what is around them. Robots have a big role to play in our future economy, but underpinning this role will be life-long infrastructure-free navigation.</gtr:abstractText><gtr:potentialImpactText>Impact Summary It is hard to understate the importance of the transport of goods and people in daily life. We depend on it totally. Any increase in efficiency, access, safety or reliability will have a major economic and societal impact. This proposal aims to achieve just those things. We will use information engineering, computing and robotics to provide a low cost underpinning for smart vehicles in civil, defence and industrial domains. Such vehicles offer the possibilty of end-to-end goods transportation - from raw materials to point of sale. They promise improved efficiency and safety on our roads. They give our aged, infirm and sensorially impaired citizens the hope of independent personal transport. Our aim is to enable this without requiring new navigation infrastructure. Car manufacturers have long been interested in improving driver experience and safety through sensing and processing. The vision of this proposal reaches beyond better parking sensors or lane detectors to real automotive autonomy and assisted driving. Robotics science has a massive role to play. In 2008, 2000 people were killed on UK roads due to driver error or concentration loss. We can address this by building cars which interpret their surroundings hundreds of times a second, never grow weary and access the cumulative experience of all cars. We need not wait for complete autonomy. The path to driverless transport is rich with exploitation opportunities. For example, smart cars can aid congestion control systems where humans currently respond to flow control signals in overly cautious and far from optimal ways. Cars that control themselves efficiently will improve flow, reduce congestion, pollution and transit times. There are energy considerations too. The CO2 cost of manufacture of the average sedan is 11% of its total life footprint. A fleet of autonomous cars operating ceaselessly in our cities would increase transport capacity and increase the person miles/kg CO2. Access to independent transport is another huge motivation. We insist, with good reason, that only the able can drive cars. We often preclude the infirm, partially-sighted and otherwise disabled, demanding that their access to roads be dependent on the munificence of others and public transport. Ultimately, autonomous cars, capable of operating on our roads as they exist now, operating beside and amongst traditional cars will extend the reach of transport to these disadvantaged citizens. This research is driven by this ultimate goal and the belief that this can be achieved by putting smarts in individual cars alone rather than on the roads themselves. The UK Department for Transport is a partner in this project and will ensure relevance to the UK's transport and logistics portfolio. Industry already benefits from precision robot navigation. For years now factories have been serviced by automated guided vehicles (AGVs) but they require awkward installation of beacons or buried wires. Using similar methods, ports in Australia and Singapore are fully automated. Infrastructure-free navigation will benefit AGV producers through market generation and users through increased efficiency. We will see increased coverage, cheaper operation and greater flexibility. There is a prima-facie need for autonomous vehicles in the defence sector. Our armed forces are keen to use smart robots for urban reconnaisance in anti-terrorist and rescue roles. The importance of mobile autonomy in national security is made clear by a US Congressional Mandate that an astounding one-third of all ground vehicles in the US Armed Forces will be unmanned by 2015. Impact will be driven by the involvement of partners representing automotive (Nissan), defence (BAe), industrial (Guidance Ltd), sensing (Navtech), international academic (MIT) and policy-making (DfT) sectors. It will be managed through licensing and inter-partner coorperation. It will be generated by first-class robotics research.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1655489</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Nissan Motor Company</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>Nissan Collaboration</gtr:description><gtr:id>D44CF0EC-D634-4DE0-B0D3-7F300802362B</gtr:id><gtr:outcomeId>b96459ceb96459e2-1</gtr:outcomeId><gtr:piContribution>0.5M GBP income raise by a collaboration with Nissan Motor Company. This resulted in a demonstration of driverless car technology to the worlds media in Feb 2012.


In October 2010 a collaborative deal was signed with Nissan Motor Company Inc. They are placing, at their own cost, an RA to work in my group and ?general support funding? for the group. Again, an I.P. deal is at the heart of the agreement
. This has resulted in a demonstration of driverless technology in the UK

See www.robotcar.org.uk</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>RSS 2014</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9325C166-05C6-41EE-AA5C-206202179D23</gtr:id><gtr:impact>Attendance at RSS at Berkeley, USA

Pending</gtr:impact><gtr:outcomeId>546363a65d4e24.15698110</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:url>http://rll.berkeley.edu/RSS2014/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Reception to Celebrate British Science, 10 Downing Street</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>922A46EE-21F4-4DFF-9BF0-98C39D974368</gtr:id><gtr:impact>Attendance at an evening reception to celebrate British Science - hosted by the Prime Minister David Cameron.

Networking contacts made in various industries.</gtr:impact><gtr:outcomeId>54634c35455b70.80812933</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Policymakers/parliamentarians</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>IET Young Professionals Event 2014: Robotics Science for Autonomous Vehicles</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>634D349C-8C61-4450-A214-F6863D4DD045</gtr:id><gtr:impact>This talk was designed to inspire young engineers and promote discussion amongst the next generation about the challenges ahead for robotics. The talk covered many areas of robotics and the problems we have solved and those that are set to vex the next generation of engineers.

Request for further participation</gtr:impact><gtr:outcomeId>5448c0b6656615.09508270</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://tv.theiet.org/channels/news/19815.cfm</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Michael Tanner (PhD student) judged the &quot;regionals&quot; of Vex Robotics competition (primarily a STEM-outreach program) hosted at Stowe School</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>2AC78ED9-4116-4F9B-9602-EC0959432A30</gtr:id><gtr:impact>From: Michael Tanner 
Subject: Vex Robotics Competition
Date: 6 February 2017 at 12:59:07 GMT
To: Paul Newman , Ingmar Posner 

[This is purely an FYI/feel-good email. No action required.]

Paul/Ingmar,

Last week I took a day off to judge the &amp;quot;regionals&amp;quot; of Vex Robotics competition (primarily a STEM-outreach program) hosted at Stowe School
 http://www.vexrobotics.com/vexedr/competition/

I thoroughly enjoyed the event and was fascinated at the creative designs students developed for this year's challenge. Here is a YouTube video showing the types of robots students develop:
 https://www.youtube.com/watch?v=FCck9_vk8H4

The students were quite diverse (boys/girls, 10 - 17 years old, hail from UK/US/China, etc.), but they all shared a deep passion regarding their respective robot designs. The level of knowledge some of the students demonstrated was impressive (e.g., describing the chemical/material property trade-offs between various plastics included in their designs). Once the students learned I was studying robotics at Oxford, I was inuidated with questions.

I went home and immeditally ordered one of Vex Robotics' cheaper toy robotics kits (http://www.vexrobotics.com/vexiq) for my daughters to play with at home.

--
Michael Tanner
Oxford Robotics Institute
Department of Engineering Science
University of Oxford
Comm +44 7514 119187</gtr:impact><gtr:outcomeId>58ca6708922090.94266661</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:url>http://www.vexrobotics.com/vexedr/competition/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Shell Eco-Marathon June / July 2016</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>0B1EDB30-65ED-4B6F-8460-82BA21C26D73</gtr:id><gtr:impact>Shell Eco-marathon challenges student teams around the world to design, build, test and drive ultra-energy-efficient vehicles.</gtr:impact><gtr:outcomeId>58c17cf1b3a989.81038712</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.youtube.com/watch?v=kU7OYLgnlkM</gtr:url><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Talk at AHRC Research Network Workshop</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>9E799CA5-4511-41C2-BB5C-82B276498862</gtr:id><gtr:impact>The action-based alternative to 3D coordinate-frame representation was the main topic of our video meetings throughout 2016, and of our workshop at St John's College, Oxford, in January 2017. This workshop brought together experts from diverse disciplines for a focussed multidisciplinary discussion across three days, testing the action-based hypothesis by assessing its philosophical, computational and neuroscientific consequences. You can see the final discussion of the workshop here. As well as discussions led by Andrew and me, the workshop featured talks by:</gtr:impact><gtr:outcomeId>58ca641fd18677.08782749</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://jamesstazicker.com/research/the-action-based-brain/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC News Article - Oxford's robots and the funding of innovation</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BFFF1832-9CC4-470D-BD73-262722A19F2C</gtr:id><gtr:impact>Interview with BBC News Tech corespondent for an article discussing &amp;quot;Oxford's robots and the funding of innovation&amp;quot;

Increase in contact from potential students and enquiries regarding industry partnerships.</gtr:impact><gtr:outcomeId>545a489fc1a808.18898742</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/news/technology-29877706</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Venturefest Oxford 2014</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>76D7747E-2419-4DDC-A8C0-677E90DF9971</gtr:id><gtr:impact>Demonstrated RobotCar and presented MRG team plus press interviews.

Promotion of MRG.</gtr:impact><gtr:outcomeId>5463577f243402.07968455</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.venturefestoxford.com/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>IET Thought Leadership Debate</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>68795DD7-C726-40AA-8C58-85EA5F6C672A</gtr:id><gtr:impact>Debate and networking event afterwards

Dissemination of nascent information</gtr:impact><gtr:outcomeId>54635b491e9b51.20071364</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Motoring of the Future Inquiry, House of Commons</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5BD37A88-257E-4EDF-A8E6-C8F8BD27DFF2</gtr:id><gtr:impact>Member of a panel giving evidence to inquiry panel of MP's

Direct interaction with policy makers</gtr:impact><gtr:outcomeId>54634a48de7991.28117242</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Policymakers/parliamentarians</gtr:primaryAudience><gtr:url>http://www.parliament.uk/business/committees/committees-a-z/commons-select/transport-committee/inquiries/parliament-2010/motoring-of-the-future/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Radio 4 Interview - The Today Programme</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>B7FDB7B2-7DE8-4FEE-AEDD-81791448950F</gtr:id><gtr:impact>Go to 1:34:14 for Paul's interview on the Today Programme:</gtr:impact><gtr:outcomeId>58ca652ea5ea72.19685209</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/b08hl5rt</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Safe and Secure Internet of Things, House of Lords</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E0BF3D68-28FA-4D25-8F8A-9748570E2BF7</gtr:id><gtr:impact>Gave a talk on driverless cars

Dissemination of technology and engagement with influential policy makers</gtr:impact><gtr:outcomeId>546362179ca259.41409402</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Policymakers/parliamentarians</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Atacama Mars Rover Field Trial Blog</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>416A86CE-87C5-415A-8EFA-8F42704218DC</gtr:id><gtr:impact>Pending.

Pending</gtr:impact><gtr:outcomeId>5448c33aba2603.86530285</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.scisys.co.uk/where-we-work/space/atacama-field-trial-2014-blog.html</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Jenkin Lecture: &quot;The Oxford RobotCar&quot; - University of Oxford Engineering Dept. Alumni Day</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>60AD0020-5D74-437D-9F32-9DD655294B6F</gtr:id><gtr:impact>180 current and former members of the Engineering Department attended the Jenkin Lecture during the Alumni weekend, which showcased the exciting work done by the Department and stimulated much discussion.

Pending</gtr:impact><gtr:outcomeId>5448c23bd204c7.38537333</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.eng.ox.ac.uk/about/news/the-2014-alumni-weekend-a-first-for-the-department</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPSRC Robotics, Automation &amp; Artificial Intelligence (RAAI) Theme Day</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>F30DCFBD-28C3-478F-8E1E-162E38E69158</gtr:id><gtr:impact>EPSRC are undertaking a review of our robotics, automation and artificial intelligence portfolios of relevance to Robotics and Autonomous Systems (RAS) in order to evaluate the quality and importance of EPSRC's portfolio of research and training in the area. To facilitate this we are hosting a Theme Day on the 31st January 2017 in Central London.
 
The Theme Day will involve poster presentations from holders of current and recent related grants from across the EPSRC portfolio. A panel of internationally leading experts chaired by Prof David Hogg will use the posters and discussions with attendees to draw conclusions about the portfolio as a whole. The outcomes of the review will be used to inform future strategy in the area of RAAI and will not impact on future funding decisions at a PI level. 
 
The Theme day will be an opportunity for PIs to present their research to the review panel. The day will also give attendees an opportunity to view work of relevance to RAAI from across the EPSRC portfolio and to network with leaders in the area from across the UK.
 
As a holder of such a related grant(s) (details below) we would like to invite you to attend the event.
 
Related Grant(s): EP/I005021/1, EP/J012017/1, EP/M019918/1</gtr:impact><gtr:outcomeId>58ca64c4053467.40485308</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Supporters</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>4991610</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Mobile Robotics: Enabling a Pervasive Technology of the Future</gtr:description><gtr:end>2020-02-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/M019918/1</gtr:fundingRef><gtr:id>C970A324-E2F7-448E-A208-E4B09BAF58A1</gtr:id><gtr:outcomeId>56e00c2e57cab2.27619783</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1093660</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Intelligent Workspace Acquisition, Comprehension and Exploitation for Mobile Autonomy in Infrastructure Denied Environments</gtr:description><gtr:end>2017-10-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/J012017/1</gtr:fundingRef><gtr:id>9E82BAC0-EF8F-4C6B-8A26-D0C0D76B58CC</gtr:id><gtr:outcomeId>56e00d32e69ff7.88431421</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Paul Newman and Ingmar Posner run the Mobile Robotics Group (mrg.robots.ox.ac.uk). One
of their flagship projects has been the Oxford Robotcar (www.robotcar.org.uk) which has
received remarkable international attention. This project is the instantiation of 5 year's of work
within MRG, bringing together threads of work on navigation, mapping planning and
perception. A particular focus and success of their work is the use of commodity sensors
(rather than expensive bespoke ones) and value of the algorithms developed across multiple
application domains.

UK's First Autonomous Car on Public Roads: Robotcar's demonstration to policy makers
has been a major contributing factor in the strategic review of autonomous cars by the UK
government. Newman and Posner worked with the Dept for Transport to use the vehicle's
development as pilot scheme resulting in the Robotcar being the first autonomous car in the
UK being given permission to run on the public roads.

National Impact:Demonstrations of the Robotcar (a ride) to the UK minister for science was
instrumental in robotics being named as one of the eight great british technologies. On the
back of this, the UK government has recently launched a nationwide (&amp;pound;10M) competition for
councils to prepare their cities for autonomous cars operation. The Robotcar has been the
focal point of a deep collaboration with Nissan bringing 600K of investment from Tokyo into
the University

Impact Beyond Cars: The infrastructure free navigation and perception algorithms and
software developed to run on the Robotcar have found application beyond the vehicle itself.
For example the vision system has been licensed for use on the ESA ExoMars mission, and
the autonomy system is being incorporated into a series of &amp;quot;pods&amp;quot; small electric vehicles that will operate in Milton Keynes in 2016 as part of the LUTZ project.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>16DD8DB7-FE83-4CD8-88E8-7FDED365ECED</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic,Policy &amp; public services</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56bc65f4e9c939.47082276</gtr:outcomeId><gtr:sector>Construction,Digital/Communication/Information Technologies (including Software),Education,Security and Diplomacy,Transport</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>Parsing Traffic Lights Version 2</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>EA9E52D0-1969-46EA-9601-33404D005F7E</gtr:id><gtr:impact>Parsing Traffic Lights Version 2</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b6b8b3017.90684867</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Parsing Traffic Lights Version 2</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman, William Maddern, Alex Stewart, Colin McManus, Winston Churchill &amp;quot;Vehicle Localisation&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>5979F77C-423B-4989-9BA1-D193AEF0EE6A</gtr:id><gtr:impact>Paul Newman, William Maddern, Alex Stewart, Colin McManus, Winston Churchill &amp;quot;Vehicle Localisation&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04771057515.90338034</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Vehicle Localisation</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Winston Churchill and Paul Newman &amp;quot;Visual Inertial Odometry&amp;quot; (an upgrade package to oVo) Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>91461DB0-9BDC-49CE-BC17-DBF05FAE907B</gtr:id><gtr:impact>Winston Churchill and Paul Newman &amp;quot;Visual Inertial Odometry&amp;quot; (an upgrade package to oVo) Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e04460d30fa4.27688159</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Visual Inertial Odometry&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman, Colin McManus and Ben Davis &amp;quot;Distraction suppression</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>693EE077-2439-4238-917C-FDB6D0B2981F</gtr:id><gtr:impact>Paul Newman, Colin McManus and Ben Davis &amp;quot;Distraction suppression</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e0482c020955.04197609</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Distraction suppression</gtr:title><gtr:yearProtectionGranted>2014</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dub4</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>9519A9E4-2BBD-42B0-AA03-043466E5031E</gtr:id><gtr:impact>Dub4</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94c1d4f2973.63240372</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Dub4</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Lina Paz and Pinies PN &amp;quot;Pain of the Plain Plane - Using Priors in Dense Reconstruction&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>6730197A-D41D-4A83-A09F-D4543172A793</gtr:id><gtr:impact>Lina Paz and Pinies PN &amp;quot;Pain of the Plain Plane - Using Priors in Dense Reconstruction&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e046a76274d4.41387105</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Pain of the Plain Plane - Using Priors in Dense Reconstruction</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Jonathan Attias and Paul Newman &amp;quot;Scenario Planner - a software tool for creating simulation scenarios for robot environments&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>6549A273-6A05-4CDC-B5FD-C1E84125F3A4</gtr:id><gtr:impact>Jonathan Attias and Paul Newman &amp;quot;Scenario Planner - a software tool for creating simulation scenarios for robot environments&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044374ab846.37031858</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Scenario Planner - a software tool for creating simulation scenarios for robot environments&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Daniel Wilde and Ingmar Posner &amp;quot;3D to 2D label projection&amp;quot; Not registered yet. Not licensed yet</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>952AF2E9-F59E-4318-9C90-584342AEB477</gtr:id><gtr:impact>Daniel Wilde and Ingmar Posner &amp;quot;3D to 2D label projection&amp;quot; Not registered yet. Not licensed yet</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044caacaaa3.59618833</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>3D to 2D label projection</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Asa Eckert-Erdheim, Scott Terry, Christopher Prahacs and Paul Newman &amp;quot;NABU Man Portable Navigation and Survey Scanner&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>4F7EE460-B54C-4E1A-97A6-3800A4633556</gtr:id><gtr:impact>Asa Eckert-Erdheim, Scott Terry, Christopher Prahacs and Paul Newman &amp;quot;NABU Man Portable Navigation and Survey Scanner&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e046fc4ceb47.34714585</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;NABU Man Portable Navigation and Survey Scanner&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Lina Paz, Pedro Pinies and Paul Newman &amp;quot;A Variational Approach to Online Road and Path Segmentation with Monocular Vision&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>49E494C6-5BCA-4F92-8F15-9404DABD5FBC</gtr:id><gtr:impact>Lina Paz, Pedro Pinies and Paul Newman &amp;quot;A Variational Approach to Online Road and Path Segmentation with Monocular Vision&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e046d7043be2.02886818</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>A Variational Approach to Online Road and Path Segmentation with Monocular Vision</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Ashley Napier &amp;quot;Extrinsic Calibration of Imaging Sensing Devices And 2D Lidars Mounted On Transportable Apparatus&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>A810284C-9EA4-46EF-9AE9-972E50A59231</gtr:id><gtr:impact>Paul Newman and Ashley Napier &amp;quot;Extrinsic Calibration of Imaging Sensing Devices And 2D Lidars Mounted On Transportable Apparatus&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e047b397c703.28338687</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Extrinsic Calibration of Imaging Sensing Devices And 2D Lidars Mounted On Transportable Apparatus</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Winston Chruchill &amp;quot;Visual Odometry (OVO software)&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>D5A8591C-7564-4593-B18D-4749FEC6ABCE</gtr:id><gtr:impact>Paul Newman and Winston Chruchill &amp;quot;Visual Odometry (OVO software)&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04a56268b12.83477217</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Visual Odometry (OVO software)</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Lina Paz, Pedro Pinnies, Michael Tanner, and Paul Newman &amp;quot;Borg cube&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>B9D35642-2CFB-44FE-A2ED-407DDAD49B62</gtr:id><gtr:impact>Lina Paz, Pedro Pinnies, Michael Tanner, and Paul Newman &amp;quot;Borg cube&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e0465e2d95c8.76978218</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Borg cube</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman, Geoff Pascoe, Will Maddern and Alex Stewart &amp;quot;Localising Portable Apparatus</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>B114C08B-E733-463B-8BA2-455CE58D47D0</gtr:id><gtr:impact>Paul Newman, Geoff Pascoe, Will Maddern and Alex Stewart &amp;quot;Localising Portable Apparatus</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04682673247.51490169</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Localising Portable Apparatus</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Chris Linegar and Paul Newman &amp;quot;Patch-based localisation technique using camera images&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>85CD7BBD-1D45-4C9A-AED9-D1316FA4CF6B</gtr:id><gtr:impact>Chris Linegar and Paul Newman &amp;quot;Patch-based localisation technique using camera images&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044052de4e4.84062739</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Patch-based localisation technique using camera images&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Deep Image-based Detection</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>E90AABB2-55EB-42D7-8F2B-1A82CDA55908</gtr:id><gtr:impact>Deep Image-based Detection</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94bd8136c90.22556150</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Deep Image-based Detection</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Semi-supervised Training for deep semantic Segmentation</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>9C666348-7DB0-4133-8C4C-E0BB7A81DA26</gtr:id><gtr:impact>Semi-supervised Training for deep semantic Segmentation</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94e02b23909.78604174</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Semi-supervised Training for deep semantic Segmentation</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Scene Prior Builder v2</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>EB9B022F-411A-4EF3-AB0C-680D9A366FDC</gtr:id><gtr:impact>Scene Prior Builder v2</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94c41090493.08958717</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Scene Prior Builder v2</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Ashley Napier &amp;quot;Use of Synthetic Overhead Images For Vehicle Localisation&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>77889DFC-9362-4C2D-A19C-0795FC91ACCC</gtr:id><gtr:impact>Paul Newman and Ashley Napier &amp;quot;Use of Synthetic Overhead Images For Vehicle Localisation&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04b28c4fb84.73208159</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Use of Synthetic Overhead Images For Vehicle Localisation</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dan Barnes, William Maddern and Ingmar Posner &amp;quot;Label Projection from Semantic Map Priors&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>C7A18D79-7496-476C-90F0-8740389D6632</gtr:id><gtr:impact>Dan Barnes, William Maddern and Ingmar Posner &amp;quot;Label Projection from Semantic Map Priors&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e0456dcc9782.71654251</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Label Projection from Semantic Map Priors&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Pedro Pinies, Lina Paz, Michael Tanner and Paul Newman &amp;quot;BORG2&amp;quot; Patent not published yet. Licensed.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>E7344435-7259-4D44-9EB8-49C3339BB82A</gtr:id><gtr:impact>Pedro Pinies, Lina Paz, Michael Tanner and Paul Newman &amp;quot;BORG2&amp;quot; Patent not published yet. Licensed.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04637221da4.79727530</gtr:outcomeId><gtr:protection>Patent granted</gtr:protection><gtr:title>BORG2</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dense Laser Stereo</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>01C8FE14-6F1A-4129-B151-B23C58BB35B9</gtr:id><gtr:impact>Dense Laser Stereo</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b19c27ec0.43526342</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Dense Laser Stereo</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Akshay Morye, Dominic Wang, Chi Tong and Ingmar Posner &amp;quot;TrackLib 1.0&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>FEE9BB84-2D9A-41B5-8F91-1594AF3DD22D</gtr:id><gtr:impact>Akshay Morye, Dominic Wang, Chi Tong and Ingmar Posner &amp;quot;TrackLib 1.0&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e0450667b2b6.94176013</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;TrackLib 1.0&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Pedro Pinies, Lina Paz and Paul Newman &amp;quot;Dense disparity estimation for stereo vision&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>7FBBBCF9-7E45-482C-B87C-9FF1D88C3FEC</gtr:id><gtr:impact>Pedro Pinies, Lina Paz and Paul Newman &amp;quot;Dense disparity estimation for stereo vision&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e043714b0655.29883523</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Dense disparity estimation for stereo vision&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dan Barnes, William Maddern and Ingmar Posner &amp;quot;Parsing Traffic Lights&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>6755B230-9411-410B-9FA7-2FB8277BCDA9</gtr:id><gtr:impact>Dan Barnes, William Maddern and Ingmar Posner &amp;quot;Parsing Traffic Lights&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e045495ad654.07916299</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Parsing Traffic Lights&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Ian Baldwin &amp;quot;Lidar Point Cloud&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>A9DA5664-63D9-452C-803C-A77B14BE1752</gtr:id><gtr:impact>Paul Newman and Ian Baldwin &amp;quot;Lidar Point Cloud&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04791ac7382.54780834</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Lidar Point Cloud</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Alastair Harrison &amp;quot;Clock Synchronisation / TICSync</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>19630872-D50D-4708-B418-B0AB10085293</gtr:id><gtr:impact>Paul Newman and Alastair Harrison &amp;quot;Clock Synchronisation / TICSync</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04b49974fc6.50301887</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Clock Synchronisation / TICSync</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Daniel Wilde and Ingmar Posner &amp;quot;Laser Simulation in virtual environments&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>F9261B46-85BF-42C0-AC40-9065127A7F3B</gtr:id><gtr:impact>Daniel Wilde and Ingmar Posner &amp;quot;Laser Simulation in virtual environments&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044e8c80601.49295155</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Laser Simulation in virtual environments&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>William Maddern, Geoff Pascoe, and Paul Newman &amp;quot;Leveraging Experience for Large-Scale LIDAR Localisation in Changing Cities&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>DA554C3A-18D4-42F2-9BEE-9010093A3584</gtr:id><gtr:impact>William Maddern, Geoff Pascoe, and Paul Newman &amp;quot;Leveraging Experience for Large-Scale LIDAR Localisation in Changing Cities&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e047231e5ba8.06244208</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Leveraging Experience for Large-Scale LIDAR Localisation in Changing Cities</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Chris Prahacs and Paul Newman &amp;quot;NABU4 Design&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>986A06C6-AD48-4E9A-A47B-58FAEAFA8981</gtr:id><gtr:impact>Chris Prahacs and Paul Newman &amp;quot;NABU4 Design&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044a39d7453.39566715</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;NABU4 Design&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Pedro Pinies, Lina Paz and Paul Newman &amp;quot;Dense disparity estimation using laser and stereo vision&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>0FC87DEC-5BA4-4032-B70A-2481FE893222</gtr:id><gtr:impact>Pedro Pinies, Lina Paz and Paul Newman &amp;quot;Dense disparity estimation using laser and stereo vision&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e043a76c4957.32939680</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Dense disparity estimation using laser and stereo vision&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Ian Baldwin &amp;quot;Vehicle Localisation with 2D Pushbroom Lasers&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>605EDB11-9401-471E-AFBA-9D63B397BDA0</gtr:id><gtr:impact>Paul Newman and Ian Baldwin &amp;quot;Vehicle Localisation with 2D Pushbroom Lasers&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e047fed90467.77854466</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Vehicle Localisation with 2D Pushbroom Lasers</gtr:title><gtr:yearProtectionGranted>2013</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Semantic Label Projection v2</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>17C68FE5-C9D2-462D-A9E5-E19F3ED3AD43</gtr:id><gtr:impact>Semantic Label Projection v2</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b4cb389b8.20775088</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Semantic Label Projection v2</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Ian Baldwin &amp;quot;Vehicle Localisation with 2D Laser Scanner And 3D Prior Scans&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>7E3B1B37-C308-4794-B859-EC28C5E4A28A</gtr:id><gtr:impact>Paul Newman and Ian Baldwin &amp;quot;Vehicle Localisation with 2D Laser Scanner And 3D Prior Scans&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04afd339c03.11126219</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Vehicle Localisation with 2D Laser Scanner And 3D Prior Scans</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman, Chris Linegar, and Winston Churchill &amp;quot;Work Smart not Hard - Path Memory for Intelligent Map Management&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>0DC57360-053B-4B9C-B6B6-4CAE3B6D461B</gtr:id><gtr:impact>Paul Newman, Chris Linegar, and Winston Churchill &amp;quot;Work Smart not Hard - Path Memory for Intelligent Map Management&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e0474c8faee6.69883864</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Work Smart not Hard - Path Memory for Intelligent Map Management</gtr:title><gtr:yearProtectionGranted>2014</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Winston Churchill &amp;quot;EBN Experience Based Life Long Navigation&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>03EF5422-7BA4-4970-A4D3-EAC6A4F95236</gtr:id><gtr:impact>Paul Newman and Winston Churchill &amp;quot;EBN Experience Based Life Long Navigation&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04a7eaad283.84628510</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>EBN Experience Based Life Long Navigation</gtr:title><gtr:yearProtectionGranted>2013</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman, Alastair Harrison and William Maddern &amp;quot;Fast Calibration For LidarS&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>9AB3E4DE-CDC9-4C56-A3F6-F9B988B25A66</gtr:id><gtr:impact>Paul Newman, Alastair Harrison and William Maddern &amp;quot;Fast Calibration For LidarS&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04aa92a1c17.66508714</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Fast Calibration For LidarS</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Geoff Hester, Ingmar Posner and Paul Newman &amp;quot;Planning in the Presence of Dynamic Obstacles&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>F183AD34-8E7C-4DA0-B0C2-1C2A04FD8169</gtr:id><gtr:impact>Geoff Hester, Ingmar Posner and Paul Newman &amp;quot;Planning in the Presence of Dynamic Obstacles&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e04525a6dc85.21010055</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Planning in the Presence of Dynamic Obstacles&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Alex Stewart &amp;quot;Monocular Camera Localisation Using Prior Point Clouds&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>2405C43E-D558-4A7D-9E0E-3B4C2026A95E</gtr:id><gtr:impact>Paul Newman and Alex Stewart &amp;quot;Monocular Camera Localisation Using Prior Point Clouds&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e04ad54d1358.51186686</gtr:outcomeId><gtr:protection>Patent application published</gtr:protection><gtr:title>Monocular Camera Localisation Using Prior Point Clouds</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Vote3Deep</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>2608CFCF-43D3-4CDB-ADDD-EE07EDF80E7C</gtr:id><gtr:impact>Vote3Deep</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94c603df216.11371563</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Vote3Deep</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Real-time Remote State Visualisation</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>FE2EFD63-DC4E-4D57-977D-6855CF2A5EC4</gtr:id><gtr:impact>Real-time Remote State Visualisation</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94b92bde466.03711918</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Real-time Remote State Visualisation</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Paul Newman and Winston Churchill &amp;quot;Generating Navigation Data&amp;quot;</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>0126AD2D-0DCA-414F-A863-4790C748B624</gtr:id><gtr:impact>Paul Newman and Winston Churchill &amp;quot;Generating Navigation Data&amp;quot;</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e047d599abb0.00454662</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Generating Navigation Data</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>William Maddern and Paul Newman &amp;quot;Dense Stereo&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>5CB55BE5-DC99-43A2-A1C4-CF64D75DD490</gtr:id><gtr:impact>William Maddern and Paul Newman &amp;quot;Dense Stereo&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e044818eb924.09868530</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Dense Stereo&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Visual Odometry System licensed for Mars Mission by ESA</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>D82FE48C-0FEC-4559-A72F-4252FA5E7CC0</gtr:id><gtr:impact>Visual Odometry System licensed for Mars Mission by ESA</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>5464d454358634.35270280</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>OvO</gtr:title><gtr:yearProtectionGranted>2011</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Path Discovery using Random Forests and Dense Stereo</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>DD2D8F8E-B8E0-4ADE-8C44-5E0397EC747A</gtr:id><gtr:impact>Path Discovery using Random Forests and Dense Stereo</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>58c94bfe5cfb06.20667223</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Path Discovery using Random Forests and Dense Stereo</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Akshay Morye, Dominic Wang, Chi Tong and Ingmar Posner &amp;quot;TrackLib 1.0&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>66260D60-A47A-4204-BF75-86532CF49872</gtr:id><gtr:impact>Akshay Morye, Dominic Wang, Chi Tong and Ingmar Posner &amp;quot;TrackLib 1.0&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e0450a9756a7.97655082</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;TrackLib 1.0&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Lina Paz, Pedro Pinies, and Paul Newman &amp;quot;Route segmentation and Tracking with Cameras&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>3082C289-F1C2-42C9-90E6-9EE0613DD53D</gtr:id><gtr:impact>Lina Paz, Pedro Pinies, and Paul Newman &amp;quot;Route segmentation and Tracking with Cameras&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56e043d96147b5.65466088</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Route segmentation and Tracking with Cameras&amp;quot;</gtr:title></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>Dominic Wang, Ingmar Posner and Paul Newman &amp;quot;Vote 3D 2.0&amp;quot; Not registered yet. Not licensed yet.</gtr:description><gtr:grantRef>EP/I005021/1</gtr:grantRef><gtr:id>70F0FCB1-AC0D-49FF-962E-902947F71812</gtr:id><gtr:impact>Dominic Wang, Ingmar Posner and Paul Newman &amp;quot;Vote 3D 2.0&amp;quot; Not registered yet. Not licensed yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>56e0458b0a6d22.19115369</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>&amp;quot;Vote 3D 2.0&amp;quot;</gtr:title></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>This grant has been transformational. Not only did it result in 54 publications and a flotilla of patents, but it produced the UK's first self driving car. It allowed me to build a team that is changing the landscape of robotics in the UK. A team that is technically strong enough to put its name against a fleet of autonomous vehicles carrying people around the streets of Milton Keynes. From my perspective the technical outputs surpassed , by a large margin , that which I hoped for. We now have infrastructure free navigation being demonstrated over 1000 km, we have low cost high resolution 3D maps of the nation's cities, we have fascinating and unusual algorithms that kick open the door to life long learning for mobile platforms.

Bringing all the work together on a platform as profound and challenging as the Robotcar acted as spring-board for more ambition. We rolled the impact agenda out to cover other kinds of platforms , space, warehousing , rail and nuclear. It allowed the creation of national platform for robotics research , and indeed a platform grant.

All of this was underpinned by a rich substrate of information engineering , machine learning and computing. The reader is invited to browse the list of publications can patents to witness and understand deeply this weaving of CompSci, Engineering, Design and application.</gtr:description><gtr:exploitationPathways>The story of EPSRC's support for research that my involvement with continues with a &amp;pound;5 million Programme Grant that has just begun (March 2015). I am already looking at how big data could be harnessed in this area and will be linking up with some new industrial names for robotics exploitation.
And finally, this has all resulted in the creation of one of the UK's most exciting spin outs Oxbotica, which seeks to apply the science of mobile autonomy, to anything that moves on land. We will start with cars.</gtr:exploitationPathways><gtr:id>ED416394-B21F-477D-A3FA-BDD555431C2E</gtr:id><gtr:outcomeId>56d9ae9991ba18.97472950</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Environment,Government, Democracy and Justice,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>Driverless Cars</gtr:description><gtr:geographicReach>National</gtr:geographicReach><gtr:id>FCD012FE-C043-44D0-8117-1931E5CD5F11</gtr:id><gtr:outcomeId>5464cfbfd88fd0.06559828</gtr:outcomeId><gtr:type>Gave evidence to a government review</gtr:type><gtr:url>http://www.policyexchange.org.uk/publications/category/item/eight-great-technologies</gtr:url></gtr:policyInfluenceOutput></gtr:policyInfluenceOutputs><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>Oxbotica</gtr:companyName><gtr:description>Oxbotica is a spin-out from Oxford University's internationally acclaimed Mobile Robotics Group. We specialise in mobile autonomy, navigation and perception, and draw on our heritage of world leading research into autonomous robotics. Our solutions allow robots, vehicles, machinery and people to precisely map, navigate and actively interact with their surroundings, delivering new capability and precision to a wide range of applications. Our 3D imaging and localisation solutions operate indoors and outdoors and are suitable for use in applications ranging from hand held survey devices to autonomous vehicles.

Oxbotica was founded by Prof. Ingmar Posner and Prof. Paul Newman - leaders of Oxford University's Mobile Robotics Group (MRG). MRG has an outstanding reputation for innovation and industrial collaborations (mrg.robots.ox.ac.uk). It has licensed navigation software for use on Mars rovers, developed the UK's first self-driving car, and has been a key and influential innovator in the area of Robotics and Autonomous Systems.</gtr:description><gtr:id>96C79127-257B-491B-BCE4-86D91B899092</gtr:id><gtr:impact>Oxbotica will leverage the innovative and world leading outputs of the UK's premier mobile robotics group, enabling rapid commercialisation with our industry partners and further application of spin-off technologies.</gtr:impact><gtr:outcomeId>5463530f914259.91764147</gtr:outcomeId><gtr:url>http://www.oxbotica.com/</gtr:url><gtr:yearCompanyFormed>2014</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication><gtr:id>D761AECC-FAA4-45E2-99BF-4FDDA496EF9D</gtr:id><gtr:title>How was Your Day? Online Visual Workspace Summaries Using Incremental Clustering in Topic Space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2dc667d5a7b6fa34f6ac7252d1109e"><gtr:id>0b2dc667d5a7b6fa34f6ac7252d1109e</gtr:id><gtr:otherNames>R. Paul</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54635ea9896a63.10432444</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BB6D639D-5E6F-49BF-B722-95C320C22F3E</gtr:id><gtr:title>Appearance-only SLAM at large scale with FAB-MAP 2.0</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/884b4174b28ddfabb86b1ca7f93dcb17"><gtr:id>884b4174b28ddfabb86b1ca7f93dcb17</gtr:id><gtr:otherNames>Cummins M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>546357065dc936.80734300</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0D9F10B-6E5F-481F-AD9D-997A9DFC51DC</gtr:id><gtr:title>Building, Curating, and Querying Large-scale Data Repositories for Field Robotics Applications</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d563bbae934438a053c13740cde2a48f"><gtr:id>d563bbae934438a053c13740cde2a48f</gtr:id><gtr:otherNames>P.Nelson</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd18433e4f5.20447752</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4FBB4E8F-3FE7-45EC-A50D-8B8D4DD769EB</gtr:id><gtr:title>Non-parametric learning for natural plan generation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/768077d1ee2fd994432b4d261d5af941"><gtr:id>768077d1ee2fd994432b4d261d5af941</gtr:id><gtr:otherNames>Baldwin I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-6674-0</gtr:isbn><gtr:outcomeId>54639103c997c2.60181489</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94BF014B-85B9-4B2B-95CA-7340062841A4</gtr:id><gtr:title>Cross-calibration of push-broom 2D LIDARs and cameras in natural scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23a40be487410666a3600d6fedd0afa8"><gtr:id>23a40be487410666a3600d6fedd0afa8</gtr:id><gtr:otherNames>Napier A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>54635ffc62c991.28056110</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BB0C66C7-0A94-4A96-B6A3-6DD056C9820E</gtr:id><gtr:title>Voting for Voting in Online Point Cloud Object Detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/199e360bdea6ffd3a7cc2931022d91ab"><gtr:id>199e360bdea6ffd3a7cc2931022d91ab</gtr:id><gtr:otherNames>D.Z. Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d71afe61af47.62921600</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FF87BA70-94D7-4D17-A6CF-79B01FBF458B</gtr:id><gtr:title>Parsing outdoor scenes from streamed 3D laser data using online clustering and incremental belief updates</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Conference on Artificial Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c99a95f42fec838b289cedaa0baf3873"><gtr:id>c99a95f42fec838b289cedaa0baf3873</gtr:id><gtr:otherNames>Triebel R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54635e57f3ea12.57905288</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7C2E3923-9626-49A3-8199-0D6050011571</gtr:id><gtr:title>Robust Direct Visual Localisation using Normalised Information Distance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d776fe9b6b1f44f3d9975f92058d77a"><gtr:id>0d776fe9b6b1f44f3d9975f92058d77a</gtr:id><gtr:otherNames>G. Pascoe</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdcd24d4ca12.64682054</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E683B74-77DF-4FCD-85E0-FBFF26F703A4</gtr:id><gtr:title>End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks (Best Paper in Workshop)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0cfcb9ce8c360ad04900d964d2f175f9"><gtr:id>0cfcb9ce8c360ad04900d964d2f175f9</gtr:id><gtr:otherNames>Peter Ondruska</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c295895eddc7.85272794</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B4A0014-1626-4C60-9B61-DF1A5F5334BC</gtr:id><gtr:title>Lighting Invariant Urban Street Classification</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26ae2050de62c6cd2df470ceb394513a"><gtr:id>26ae2050de62c6cd2df470ceb394513a</gtr:id><gtr:otherNames>B. Upcroft</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bc38623b70.09410676</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F1D5F2DE-B32E-40ED-9544-A3EB2382595E</gtr:id><gtr:title>FAB-MAP 3D: Topological mapping with spatial and visual appearance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-5038-1</gtr:isbn><gtr:outcomeId>54638f56da8686.13207424</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2CD41435-5349-492A-A599-1BF635D9A90C</gtr:id><gtr:title>Self help: Seeking out perplexing images for ever improving navigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-61284-386-5</gtr:isbn><gtr:outcomeId>doi_53d058058e4ef866</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BFC71584-1B69-4B1A-A42A-2FB16C277F78</gtr:id><gtr:title>How was your day? Online visual workspace summaries using incremental clustering in topic space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e8dbd10</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0F811BA-7418-4242-B485-EA610D8C9D74</gtr:id><gtr:title>Risky planning: Path planning over costmaps with a probabilistically bounded speed-accuracy tradeoff</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4be007341e28890767f48854a009a0d6"><gtr:id>4be007341e28890767f48854a009a0d6</gtr:id><gtr:otherNames>Murphy L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-61284-386-5</gtr:isbn><gtr:outcomeId>54635d1cd41908.15798770</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D5017404-8629-4B73-B95F-8D2E913DB955</gtr:id><gtr:title>Practice Makes Perfect? Managing and Leveraging Visual Experiences for Lifelong Navigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1aff0f72170e6247247271291d864f9f"><gtr:id>1aff0f72170e6247247271291d864f9f</gtr:id><gtr:otherNames>W. Churchill</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546372e30b5707.03634710</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>827EAF78-71FE-4267-A1BD-D071D73E2416</gtr:id><gtr:title>Shady Dealings: Robust, Long- Term Visual Localisation using Illumination Invariance</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/04ba25d1d77a3cb587920a23010d9b89"><gtr:id>04ba25d1d77a3cb587920a23010d9b89</gtr:id><gtr:otherNames>C. McManus</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bb0d33fb05.67024211</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F98CD62C-9FB6-40DD-AD2E-2A381EEC2CE6</gtr:id><gtr:title>Self-calibration for a 3D laser</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4b2cc33c47b63271465857847ab49319"><gtr:id>4b2cc33c47b63271465857847ab49319</gtr:id><gtr:otherNames>Sheehan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d079079a411078</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AD5DFA2-55C6-4839-9902-7FEFA71ABBDE</gtr:id><gtr:title>Lost in translation (and rotation): Rapid extrinsic calibration for 2D and 3D LIDARs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d81eaa1265fa908e57800b54c76f2ed"><gtr:id>5d81eaa1265fa908e57800b54c76f2ed</gtr:id><gtr:otherNames>Maddern W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e7920ed</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5564AB8E-E142-47B2-A2D1-F7138110D425</gtr:id><gtr:title>Lost in Translation (and Rotation): Fast Extrinsic Calibration for 2D and 3D LIDAR's</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0e561e1b5a65aac31882ff2578f944"><gtr:id>9a0e561e1b5a65aac31882ff2578f944</gtr:id><gtr:otherNames>W. Maddern</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54635f8f5a7b30.56658829</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>829522DC-4AE3-4C66-86D4-31229F8E37E7</gtr:id><gtr:title>Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8e400243a2424a1693884d9c0e1d7bb"><gtr:id>a8e400243a2424a1693884d9c0e1d7bb</gtr:id><gtr:otherNames>Corke P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546396a0073be8.25874481</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0687E5AF-33B1-4FC1-83F8-B550119E6A36</gtr:id><gtr:title>Generation and exploitation of local orthographic imagery for road vehicle localisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23a40be487410666a3600d6fedd0afa8"><gtr:id>23a40be487410666a3600d6fedd0afa8</gtr:id><gtr:otherNames>Napier A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-2119-8</gtr:isbn><gtr:outcomeId>doi_53d05a05a25ab89c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>637E24D7-1899-4393-93D3-68A10B55A7F6</gtr:id><gtr:title>Knowing when we don't know: Introspective classification for mission-critical decision making</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/38888467eee7d5e3ccaa70308f092111"><gtr:id>38888467eee7d5e3ccaa70308f092111</gtr:id><gtr:otherNames>Grimmett H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>546394f0efafb3.22756684</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E8097FC-9534-47E1-8C59-33BAF54D90E7</gtr:id><gtr:title>Scene Signatures: Localised and Point-less Features for Localisation</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/04ba25d1d77a3cb587920a23010d9b89"><gtr:id>04ba25d1d77a3cb587920a23010d9b89</gtr:id><gtr:otherNames>C. McManus</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bfd800d113.48364243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A50870B7-095D-40EA-9212-F9D1774BE152</gtr:id><gtr:title>Teaching a Randomized Planner to plan with Semantic fields</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/385f6faecff90876889d0851a731cabb"><gtr:id>385f6faecff90876889d0851a731cabb</gtr:id><gtr:otherNames>I. Baldwin</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>54639096eac8a5.70276639</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>01FD0F7B-E090-40A6-A5C4-3F6A98E562F7</gtr:id><gtr:title>Road vehicle localization with 2D push-broom LIDAR and 3D priors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/768077d1ee2fd994432b4d261d5af941"><gtr:id>768077d1ee2fd994432b4d261d5af941</gtr:id><gtr:otherNames>Baldwin I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058ec2193b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2F87514E-EFB7-4682-B56F-0C2ACADE4AD8</gtr:id><gtr:title>Continually improving large scale long term visual navigation of a vehicle in dynamic urban environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0003334c5042f8c1ccb89b16e90e8971"><gtr:id>0003334c5042f8c1ccb89b16e90e8971</gtr:id><gtr:otherNames>Churchill W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-3064-0</gtr:isbn><gtr:outcomeId>doi_53d05a05a0de6306</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA25263E-D9BE-4E9A-93B5-FA558214575A</gtr:id><gtr:title>Continuous vehicle localisation using sparse 3D sensing, kernelised r&amp;eacute;nyi distance and fast Gauss transforms</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4b2cc33c47b63271465857847ab49319"><gtr:id>4b2cc33c47b63271465857847ab49319</gtr:id><gtr:otherNames>Sheehan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54635fca398aa4.27081142</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F8649E11-F161-490C-A675-4ADEE525A0E8</gtr:id><gtr:title>Laser-only road-vehicle localization with dual 2D push-broom LIDARS and 3D priors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/768077d1ee2fd994432b4d261d5af941"><gtr:id>768077d1ee2fd994432b4d261d5af941</gtr:id><gtr:otherNames>Baldwin I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1737-5</gtr:isbn><gtr:outcomeId>546393d4e30731.67467414</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88E74233-6DBE-4D93-A0CA-30E85141E159</gtr:id><gtr:title>What could move? Finding cars, pedestrians and bicyclists in 3D laser data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c8216ad8b0edf7c3fbae30aac4d3a4d9"><gtr:id>c8216ad8b0edf7c3fbae30aac4d3a4d9</gtr:id><gtr:otherNames>Dominic Zeng Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e83c959</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E861848F-1DE3-4CEC-947B-C823B3706E00</gtr:id><gtr:title>Keep Geometry in Context: Using Contextual Priors for Very-Large-Scale 3D Dense Reconstructions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f934aaa600d9e1b0587cd5e493ae50a9"><gtr:id>f934aaa600d9e1b0587cd5e493ae50a9</gtr:id><gtr:otherNames>Michael Tanner</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2962a1fa668.27393519</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2E8A996-5A17-4D82-94D0-039D690611FA</gtr:id><gtr:title>Distraction suppression for vision-based pose estimation at city scales</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e069a2c6809b1f71a6ee517bc39e329f"><gtr:id>e069a2c6809b1f71a6ee517bc39e329f</gtr:id><gtr:otherNames>McManus C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn><gtr:outcomeId>5463602cefb931.94565918</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5282FB8-7BAE-4D73-BA84-88E658C3F87A</gtr:id><gtr:title>From Dusk till Dawn: Localisation at Night using Artificial Light Sources</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d563bbae934438a053c13740cde2a48f"><gtr:id>d563bbae934438a053c13740cde2a48f</gtr:id><gtr:otherNames>P.Nelson</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cdd217d7d490.58321986</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B08E073F-78D1-4DC8-A9A8-E6644BECBEBD</gtr:id><gtr:title>Semantic categorization of outdoor scenes with uncertainty estimates using multi-class gaussian process classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1737-5</gtr:isbn><gtr:outcomeId>doi_53d059059a255fc4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D2A57895-25B3-4CCA-B585-6E6A43BD7219</gtr:id><gtr:title>Visual Precis Generation using Coresets</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2dc667d5a7b6fa34f6ac7252d1109e"><gtr:id>0b2dc667d5a7b6fa34f6ac7252d1109e</gtr:id><gtr:otherNames>R. Paul</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463bcd6d96254.57495148</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8927F3A0-8D9D-405A-A0BD-474BA67B5750</gtr:id><gtr:title>Can Priors Be Trusted? Learning to Anticipate Roadworks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3bf846e4d9a713e3488d94692757d4fa"><gtr:id>3bf846e4d9a713e3488d94692757d4fa</gtr:id><gtr:otherNames>B. Mathibela</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546392a4bf52e7.01525304</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>411AAE0A-740B-4AAB-8E54-372F9D2600B9</gtr:id><gtr:title>On-line Scene Understanding for Closed Loop Control</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/00a4ad0d860328c530aef886f82f585f"><gtr:id>00a4ad0d860328c530aef886f82f585f</gtr:id><gtr:otherNames>Lina Maria Paz</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c28a7c5e8911.54516927</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F934AF47-3550-4EE9-AE6C-35A984D351F9</gtr:id><gtr:title>Practice makes perfect? Managing and leveraging visual experiences for lifelong navigation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0003334c5042f8c1ccb89b16e90e8971"><gtr:id>0003334c5042f8c1ccb89b16e90e8971</gtr:id><gtr:otherNames>Churchill W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>doi_53d058058e64e731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>658CDAC2-FD47-4C22-8D41-58796AC748E3</gtr:id><gtr:title>Self-help: Seeking out perplexing images for ever improving topological mapping</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bf532fde3dae60a54a708122637b2bbf"><gtr:id>bf532fde3dae60a54a708122637b2bbf</gtr:id><gtr:otherNames>Paul R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f97397357654b8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B8F8B301-52DD-4ED3-823F-FD251CDFE54E</gtr:id><gtr:title>A New Approach to Model-Free Tracking with 2D Lidar</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1927e29ec944c44da9b234a935eb55a7"><gtr:id>1927e29ec944c44da9b234a935eb55a7</gtr:id><gtr:otherNames>D. Z. Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463b6841b8270.84806145</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4973AB2F-4CEE-451F-A8A2-2196C2B9A95A</gtr:id><gtr:title>Can priors be trusted? Learning to anticipate roadworks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c383bcdbcc9faa1eb7bf2af1eaae0cf0"><gtr:id>c383bcdbcc9faa1eb7bf2af1eaae0cf0</gtr:id><gtr:otherNames>Mathibela B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-3064-0</gtr:isbn><gtr:outcomeId>doi_53d05a05a0d49659</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DF8289F4-5856-4C5F-A105-37BD914762CC</gtr:id><gtr:title>Can Priors Be Trusted? Learning to Anticipate Roadworks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3bf846e4d9a713e3488d94692757d4fa"><gtr:id>3bf846e4d9a713e3488d94692757d4fa</gtr:id><gtr:otherNames>B. Mathibela</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546392a62f6366.39626497</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B4BBA9F8-6799-4A31-87E6-51F927AD8E40</gtr:id><gtr:title>Model-free detection and tracking of dynamic objects with 2D lidar</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a0385ddf822030de0fee889762bbc89e"><gtr:id>a0385ddf822030de0fee889762bbc89e</gtr:id><gtr:otherNames>Wang D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675fc11a5653</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8FA7ABD5-F9DC-4E3C-A58C-0F1C03EA3A85</gtr:id><gtr:title>Real-time bounded-error pose estimation for road vehicles using vision</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/23a40be487410666a3600d6fedd0afa8"><gtr:id>23a40be487410666a3600d6fedd0afa8</gtr:id><gtr:otherNames>Napier A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7657-2</gtr:isbn><gtr:outcomeId>doi_53d05a05a0b6dfdd</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E0F1CD67-30EA-4887-B66F-7F087A54CE05</gtr:id><gtr:title>1 year, 1000 km: The Oxford RobotCar dataset</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d81eaa1265fa908e57800b54c76f2ed"><gtr:id>5d81eaa1265fa908e57800b54c76f2ed</gtr:id><gtr:otherNames>Maddern W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c27cc5d504d7.24907285</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0D1E8A44-7D3D-445F-810F-46FEB7C0490C</gtr:id><gtr:title>Toward automated driving in cities using close-to-market sensors: An overview of the V-Charge Project</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/330ada88bb0cf81531e53d016c16af4b"><gtr:id>330ada88bb0cf81531e53d016c16af4b</gtr:id><gtr:otherNames>Furgale P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463966feb9471.94946845</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>560D5E7C-BAD6-4F0F-9E96-0F9C33042D64</gtr:id><gtr:title>Checkout my map: Version control for fleetwide visual localisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5beb69924584b6a1ae2a929ac7e7c45c"><gtr:id>5beb69924584b6a1ae2a929ac7e7c45c</gtr:id><gtr:otherNames>Gadd M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2898e823299.22326472</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>402B4F2B-2974-4051-87AD-1FA50EA42654</gtr:id><gtr:title>Choosing where to go: Complete 3D exploration with stereo</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cc2b75ea0dd0ecbe90649dfd04cc85f3"><gtr:id>cc2b75ea0dd0ecbe90649dfd04cc85f3</gtr:id><gtr:otherNames>Shade R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-61284-386-5</gtr:isbn><gtr:outcomeId>54635b4eefeb74.64703529</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C8713AC8-52ED-4B2C-BD01-5C1BBBC323F2</gtr:id><gtr:title>A New Approach to Model-Free Tracking with 2D Lidar</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1927e29ec944c44da9b234a935eb55a7"><gtr:id>1927e29ec944c44da9b234a935eb55a7</gtr:id><gtr:otherNames>D. Z. Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546355f53552f6.84273425</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8613FEC7-06D3-4A19-BBE4-30F04F77EE66</gtr:id><gtr:title>End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints, 2016.</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0cfcb9ce8c360ad04900d964d2f175f9"><gtr:id>0cfcb9ce8c360ad04900d964d2f175f9</gtr:id><gtr:otherNames>Peter Ondruska</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2996a093d52.30834001</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13E54CB9-4636-4C16-9990-75000104178C</gtr:id><gtr:title>TICSync: Knowing when things happened</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4540e126fc4cd349ec5b03eb85310ce"><gtr:id>c4540e126fc4cd349ec5b03eb85310ce</gtr:id><gtr:otherNames>Harrison A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-61284-386-5</gtr:isbn><gtr:outcomeId>doi_53d058058e45be6f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F53F0D7D-F8C6-43F6-B380-363473304510</gtr:id><gtr:title>Planning to perceive: Exploiting mobility for robust object detection</gtr:title><gtr:parentPublicationTitle>ICAPS 2011 - Proceedings of the 21st International Conference on Automated Planning and Scheduling</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a0012aa1098b6748fd7047d2c03eccc"><gtr:id>6a0012aa1098b6748fd7047d2c03eccc</gtr:id><gtr:otherNames>Velez J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54635dcad2d171.56561381</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>655D739E-05FD-4C51-80BB-A2778407A93E</gtr:id><gtr:title>RSLAM: A System for Large-Scale Mapping in Constant-Time Using Stereo</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/537e465abf19bfe71b49331543c5fd3a"><gtr:id>537e465abf19bfe71b49331543c5fd3a</gtr:id><gtr:otherNames>Mei C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>5463575d0f8dc8.70787247</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C90FB1FF-7EED-4CF5-AA8D-9265F8F84BCE</gtr:id><gtr:title>LAPS - localisation using appearance of prior structure: 6-DoF monocular camera localisation using prior pointclouds</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/03f96d57f563aea58a8f5662d4f734f1"><gtr:id>03f96d57f563aea58a8f5662d4f734f1</gtr:id><gtr:otherNames>Stewart A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1403-9</gtr:isbn><gtr:outcomeId>54635c90925853.90091074</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B8F6789-BA32-4B67-AE6A-C6856A5E4580</gtr:id><gtr:title>Watch this: Scalable cost-function learning for path planning in urban environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/644e92ffe4453bc79f6ee65cf3ce993e"><gtr:id>644e92ffe4453bc79f6ee65cf3ce993e</gtr:id><gtr:otherNames>Wulfmeier M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c286bd5a4569.46385168</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96FD6C62-EEED-42FE-8AFB-52D09886F8E2</gtr:id><gtr:title>Adaptive compression for 3D laser data</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6174d851e88487fed8422110a02b660f"><gtr:id>6174d851e88487fed8422110a02b660f</gtr:id><gtr:otherNames>Smith M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54635d60ab4c22.71190705</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB5988F6-6AAF-46EE-998C-0E25D0BB4788</gtr:id><gtr:title>Real-time probabilistic fusion of sparse 3D LIDAR and dense stereo,</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8ed1064e0eb1fb644f6b8be607f6840f"><gtr:id>8ed1064e0eb1fb644f6b8be607f6840f</gtr:id><gtr:otherNames>Will Maddern</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2864c659af7.39429309</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C50FE7CC-2C30-4281-A06C-255491480F6B</gtr:id><gtr:title>Experience-based navigation for long-term localisation</gtr:title><gtr:parentPublicationTitle>The International Journal of Robotics Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0003334c5042f8c1ccb89b16e90e8971"><gtr:id>0003334c5042f8c1ccb89b16e90e8971</gtr:id><gtr:otherNames>Churchill W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463ba58823241.16628291</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B39EE3CD-DB59-4548-B0EA-5CA6D3897303</gtr:id><gtr:title>Illumination Invariant Imaging: Applications in Robust Vision-based Localisation, Mapping and Classification for Autonomous Vehicles</gtr:title><gtr:parentPublicationTitle>Pending</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0e561e1b5a65aac31882ff2578f944"><gtr:id>9a0e561e1b5a65aac31882ff2578f944</gtr:id><gtr:otherNames>W. Maddern</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463be94eaf5b9.21328333</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5A6D36E7-0422-4B48-8963-D3C81D41C336</gtr:id><gtr:title>Hidden view synthesis using real-time visual SLAM for simplifying video surveillance analysis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/537e465abf19bfe71b49331543c5fd3a"><gtr:id>537e465abf19bfe71b49331543c5fd3a</gtr:id><gtr:otherNames>Mei C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-61284-386-5</gtr:isbn><gtr:outcomeId>54635d93cfbad0.33067328</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1E1EE2BC-9889-4256-A23D-E29B0B8F7F39</gtr:id><gtr:title>Modelling observation correlations for active exploration and robust object detection</gtr:title><gtr:parentPublicationTitle>Journal of Artificial Intelligence Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a0012aa1098b6748fd7047d2c03eccc"><gtr:id>6a0012aa1098b6748fd7047d2c03eccc</gtr:id><gtr:otherNames>Velez J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>10769757</gtr:issn><gtr:outcomeId>54635f229755f6.99964985</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9664361A-9D48-4C38-B82C-07C433BEAAAF</gtr:id><gtr:title>Fit for purpose? Predicting Perception Performance based on Past Experience</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ffba4f4d6774b564ef2be77970f2ec9b"><gtr:id>ffba4f4d6774b564ef2be77970f2ec9b</gtr:id><gtr:otherNames>Corina Gurau</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c285213e8bf3.16761433</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I005021/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5E9AA4EC-49E3-4D6A-B545-79DA07CE39E0</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Management &amp; Business Studies</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>748617DE-4AB0-42C9-9514-B22ECFBE05E1</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Transport Ops &amp; Management</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>