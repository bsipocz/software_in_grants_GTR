<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Computing Science</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAC7749F-E3C7-41F3-81B0-45E308D04F30"><gtr:id>BAC7749F-E3C7-41F3-81B0-45E308D04F30</gtr:id><gtr:name>Nokia Research Centre</gtr:name><gtr:address><gtr:line1>21 J J Thomson Avenue</gtr:line1><gtr:line2>Madingley Road</gtr:line2><gtr:postCode>CB3 0FA</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Finland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C04EA393-5137-4DC1-850C-4C2BD8251E91"><gtr:id>C04EA393-5137-4DC1-850C-4C2BD8251E91</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Brewster</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B313BEA5-9100-4BD6-BFCB-99990CE3DA22"><gtr:id>B313BEA5-9100-4BD6-BFCB-99990CE3DA22</gtr:id><gtr:firstName>Roderick</gtr:firstName><gtr:surname>Murray-Smith</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF023405%2F1"><gtr:id>F1D7A046-B98B-4C83-9935-67838C32819B</gtr:id><gtr:title>GAIME: Gestural and Audio Interactions for Mobile Environments</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F023405/1</gtr:grantReference><gtr:abstractText>Most PDAs and smart phones have sophisticated graphical interfaces and commonly use small keyboards or styli for input. The range of applications and services for such devices is growing all the time. However, there are problems which make interaction difficult when a user is on the move. Much visual attention is needed to operate many of the applications, which may not be available in mobile contexts. Oulasvirta et al. [29] showed that attention can become very fragmented for users on the move as it must shift between navigating the environment and the device, making interaction hard. Our own research has shown that performance may drop by more than 20% when users are mobile [4]. Another important issue is that most devices require hands to operate many of the applications. These may not be available if the user is carrying bags, holding on to children or operating machinery, for example. The novel aspect of this proposal is to reduce the reliance on graphical displays and hands by investigating gesture input from other locations on the body com-bined with three-dimensional sound for output.Little work has gone into making input and control hands-free for mobile users. Speech recognition is still problematic in such settings due to its high processing requirements and the dynamic audio environments in which devices are used. Much of the research on ges-ture input still uses hands for making the gestures. There is some work on head-based input, often for users with disabilities [26], but little of this has been used in mobile settings. Our own previous work has begun to examine head pointing and showed that it might be a useful way to point and select on the move [3].Many other body locations could be useful for subtle and discreet input whilst mobile (e.g., users walking or sitting on a bumpy train). For example, wrist rotation has potential for controlling a radial menu as the wrist can be rotated to move a pointer across the menu. It is unobtrusive and could be tracked using the same sensor used for hand pointing gestures (in a watch for exam-ple). Small changes in gait are also a possibility for in-teraction. In previous work [12] we extracted gait in-formation from an accelerometer on a PDA to look at usability errors. We can adapt this technique so that users could slightly change the timing of a step to make input. There has been no systematic study of the differ-ent input possibilities across the body. We will develop a novel testing methodology using a Fitts' law analysis along with more subjective measures to find out which body locations are most useful for input on the move.Output is also a problem due to the load on visual atten-tion when users are mobile. We and others have begun to look at the use of spatialised audio cues for output when mobile as an alternative or complement to graph-ics [1, 6] [19, 32]. Many of these use very simple 3D audio displays, but, with careful design, spatial audio could provide a much richer display space. Our Audio-Clouds project built some foundations for 3D audio interactions, investigating basic pointing behaviour, target size and separation [1,3]. We need to now take this work forward and develop more sophisticated inter-actions. Key aspects here are to develop the use of ego-centric (fixed to the user) and exocentric (fixed to the world) displays, and how they can be combined to cre-ate a rich 3D display space for interaction.The final key part of this project is to create compelling applications which combine the best of the audio and gestures. We can then test these with users in more realistic settings over longer time periods to fine-tune how these interactions work in the real world.</gtr:abstractText><gtr:fund><gtr:end>2011-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>367098</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>40007</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia</gtr:description><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:fundingRef>Nokia university donations</gtr:fundingRef><gtr:id>EEC47586-38F4-4073-8C58-2C49342DFA6B</gtr:id><gtr:outcomeId>r-5910214992.15645306a39314</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>70000</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia</gtr:description><gtr:end>2013-12-02</gtr:end><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:fundingRef>1/2 funded PhD studentship</gtr:fundingRef><gtr:id>6BF66740-2CEA-4FB5-8C6A-4DC46D34540D</gtr:id><gtr:outcomeId>r-8823760430.16441206a39238</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2010-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>7000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>HP Labs</gtr:description><gtr:end>2011-12-02</gtr:end><gtr:fundingOrg>HP Laboratories</gtr:fundingOrg><gtr:fundingRef>HP project</gtr:fundingRef><gtr:id>70FE89B1-A507-4DD7-AA15-44F30177AFE3</gtr:id><gtr:outcomeId>5eb98e1e5eb98e32</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>40007</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia</gtr:description><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:fundingRef>Nokia university donations</gtr:fundingRef><gtr:id>88BD7F50-BE18-4C8A-A362-708A93FEEC29</gtr:id><gtr:outcomeId>5ec8e2385ec8e24c</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>collaborated with nokia on the project. they provided some funding and hosted a student for an internship.

we developed a range of new interaction techniques for lots of different settings.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>FD4402E2-497D-474E-AE47-32650B8EF097</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5464c0fb274a13.63186347</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>developed new methods to allow interaction without the use of eyes or hands.</gtr:description><gtr:exploitationPathways>we provided a set of 3d audio and gesture based interaction techniques that others could use to enrich mobile interactions</gtr:exploitationPathways><gtr:id>D4D00853-4A13-4D69-ACC1-541C71F6FA9E</gtr:id><gtr:outcomeId>5464c03d9576c4.99352337</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.gaime-project.org/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>72DB89D5-C6DE-411B-91C5-4307013E7B2A</gtr:id><gtr:title>Auditory display design for exploration in mobile audio-augmented reality</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7e1fd68dda54ea6f46c5530178feafc6"><gtr:id>7e1fd68dda54ea6f46c5530178feafc6</gtr:id><gtr:otherNames>Vazquez-Alvarez Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53cfd7fd7be72f13</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FBEAC703-7D46-442C-8BEB-66152A353626</gtr:id><gtr:title>Handbook of Research on User Interface Design and Evaluation for Mobile Technology</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b3dab4bd2c63e903ce08cb2a630cbe79"><gtr:id>b3dab4bd2c63e903ce08cb2a630cbe79</gtr:id><gtr:otherNames>Lumsden, Joanna</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-1-59904-871-0</gtr:isbn><gtr:outcomeId>i_68918963773c06b5a8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B7AF232-F535-4608-9EF5-27E37E5EDC15</gtr:id><gtr:title>Designing Interactions with Multilevel Auditory Displays in Mobile Audio-Augmented Reality</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Computer-Human Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7e1fd68dda54ea6f46c5530178feafc6"><gtr:id>7e1fd68dda54ea6f46c5530178feafc6</gtr:id><gtr:otherNames>Vazquez-Alvarez Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d46ba3afae8.07550849</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>842D8B8A-31D8-4409-B3D4-33F8D74DCAC6</gtr:id><gtr:title>Audio or Tactile Feedback: Which Modality When?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c3a36549e7ad48f90ec2c971a3e29be4"><gtr:id>c3a36549e7ad48f90ec2c971a3e29be4</gtr:id><gtr:otherNames>Eve Hoggan</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>r_3115557575cac2a8f0</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F023405/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>13B6D44B-6EAF-464B-A4CC-1C08F8DDE5A0</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Mobile Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>