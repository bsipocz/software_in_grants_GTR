<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/7EEAB738-BCBA-493E-94E9-60D394577D5E"><gtr:id>7EEAB738-BCBA-493E-94E9-60D394577D5E</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>Murray</gtr:surname><gtr:orcidId>0000-0001-5309-5080</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ014990%2F1"><gtr:id>6E97E9DF-4C7E-4534-8F7F-25A35F851A99</gtr:id><gtr:title>Constant-time wide-area monocular SLAM using absolute depth hinting</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J014990/1</gtr:grantReference><gtr:abstractText>Understanding the visual environment is key to allowing machines interact with us and the space we occupy, whether the machine is to take fully autonomous action or just provide us with extra information and advice. A core competence is the ability to reconstruct a 3D representation of a moving camera's surroundings and to locate the camera relative to them from moment to moment. Over the last years, enormous practical strides have been made in this problem of visual simultaneous localization and mapping (visual SLAM), to the point now where robust live reconstruction is possible on modest hardware using stereo cameras and, more challengingly, using just a single camera.

We are concerned here with single camera visual SLAM, of particular importance when the payload and power manifest has to be kept small. The state of the art allows reconstructions containing several tens of thousands of 3D point locations imaged from a few hundred viewpoints to be optimized on the fly. Though this sounds large, in practice these numbers restrict the operational scope to modestly-sized environments. Our aim in this proposal is to tackle the two chief impediments to increasing that scope. 

First is algorithmic computational complexity. Polynomial complexity in the number of map points and/or the number of camera positions gradually stifles live operation as the environment expands. This means that gains in processor speed do not lead to proportional gains in map size, and we cannot merely wait for cpu development to catch up. Here we will pursue our recent work on constant-time exploration in monocular SLAM, in which only a local region around the camera needs to be re-optimized frame-by-frame.

Second is monocular vision's inherent depth/speed scaling ambiguity. Using image motion alone only relative depth, rather than absolute depth, is observable. As the camera moves around, uncertainty builds up not only in position and orientation, but also in the scale of the surroundings. This leads to added difficulty when returning to a previously visited location: not only do the surroundings appear translated and twisted, but they also appear the wrong size. But a human one-eyed observer does not suffer in the same way: whether deprived of stereo vision or not, we use other visual clues to maintain our sense of scale, clues from objects, object classes, and from low-level image traits. It is these that this project plans to glean to provide partial information about absolute depth, sufficient to remove that extra degree of freedom in the solution.

We aim to produce a SLAM algorithm (i) that functions at video frame-rate; (ii) that functions in on-average constant time quite independently of the size of the map it is constructing, and (ii) that behaves gracefully whatever quality of depth information is provided to it.</gtr:abstractText><gtr:potentialImpactText>The immediate potential beneficiaries of this work are industries and agencies that have interests in using image data to automatically and quickly reconstruct environments about which nothing is known at the outset, and in recovering where the camera has been over time in that environment.

The knowledge and code base generated by the project could lead to new products for
 (a) reconnaissance from ground-based or low-flying cameras, where speed allows tactical decisions to be made in the field;
 (b) fast reconstruction and transmission of environments for architectural design.

The recovered camera position could be used for new methods of
 (c) camera tracking for graphical augmentation in live TV in marker-free locations;
 (d) providing spatially aware visual instructions for manufacturing/repair.

Examples of other possible routes for exploitation include
 (e) recovery and annotation of scenes for environment/asset management (e.g., by highways agency, emergency services, environment agency); and
 (f) archival storage and curation, where 3D abstractions may be compressed to a small fraction of the size of many thousands of video frames.

Working in this field allows information engineers to develop wide-ranging knowledge and skills from applied mathematics to systems integration. We have a strong record of our researchers continuing to work at the sharp, creative end of the technology sector. Industry would benefit from the expertise accumulated by the employed researcher, and more widely from the involvement in this research of doctoral students in engineering, and indeed from final year integrated masters students undertaking projects in the group.

So far we have considered high-value/low-volume applications. However, the ability to determine &amp;quot;what is where&amp;quot; will impact more widely in the longer term through high-volume/low-value
applications, delivered by smart phones communicating with cloud-based computing power and databases.

There are numerous applications in this broad area of augmented reality that will bring direct economic benefits to their creators, but that will bring societal benefits to the broader public through new opportunities in
 (a) education and training, where live versions of the traditional &amp;quot;cut-away diagram&amp;quot; might be overlaid on a piece of machine, or the body;
 (b) bringing cultural heritage to life; and
 (c) health care, particularly memory augmentation for an ageing population.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>393463</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>800000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Commercial funding (untitled)</gtr:description><gtr:end>2020-09-02</gtr:end><gtr:fundingOrg>Dyson</gtr:fundingOrg><gtr:id>CC7869DA-E183-44BE-B870-446D1A1CE068</gtr:id><gtr:outcomeId>56d6f7f0ce1277.98603585</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work has contributed to current commercial funding of research work in the laboratory.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>B85C3AAB-F820-4FB5-BF49-5585ABAFD1FF</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d6e138a07634.46531467</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>InfiniTAM an Open Source, multi-platform framework for real-time, large-scale depth fusion and tracking, released under an Oxford Isis Innovation Academic License. The framework currently supports dense volumes (using an implementation based on the Newcombe et al KinectFusion paper) and sparse volumes (using an implementation based on our ISMAR 2015 paper).</gtr:description><gtr:grantRef>EP/J014990/1</gtr:grantRef><gtr:id>7175CD48-8A39-4F8C-8BD3-CBB371925CA5</gtr:id><gtr:impact>Basis for industrial funding into the laboratory</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56d6cb1b3f8e70.32852059</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Infinitam</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>Without knowledge of the absolute baseline between images, the scale of the map built using single-camera simultaneous localization and mapping (SLAM) is subject to calamitous drift over time. Within EP/J014990 we have developed a monocular approach that seamlessly combines point measurements with measurements on objects of known size distribution into a bundle adjustment. By placing a prior on the size of detected objects, scale ambiguity is resolved in areas where they are observed. The local scale of the map is determined jointly with the camera pose in local adjustments that are run when object observations are available. We have shown that this framework reduces scale drift over long-range (c 40km) outdoor sequences.

The second strand of work in this grant is on dense SLAM. Volumetric methods provide efficient, flexible and simple ways of integrating multiple depth images into a full 3D model. They provide dense and photorealistic 3D reconstructions, and parallelised implementations on GPUs achieve real-time performance on modern graphics hardware. To run such methods on mobile devices, providing users with freedom of movement and instantaneous reconstruction feedback, remains challenging however. Within EP/J014990, and continuing from work started in EP/H050795 we have developed a range of modifications to existing volumetric integration methods based on voxel block hashing, considerably improving their performance and making them applicable to tablet computer applications. Current work in this area is focussed on loop closure in dense SLAM.</gtr:description><gtr:exploitationPathways>The outputs are of broad interest to the computer vision research community, and of particular interest to researchers and developers working on automatic reconstruction for navigation and augmented reality.</gtr:exploitationPathways><gtr:id>534B5A3A-11DC-4B22-AC4B-806CF1442CEE</gtr:id><gtr:outcomeId>56d6e119a13513.39977798</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.robots.ox.ac.uk/ActiveVision</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The Oxford Active Vision Library (OxVisionLib) is a loose collection of computer vision library and applications provided by the Active Vision Lab in the Department of Engineering Science, University of Oxford. These include
InfiniTAM - A multi-platform framework for real-time, large-scale depth fusion and tracking.
gSLICr - A library for much faster-than-real-time superpixel segmentation.
LibISR - A library for tracking multiple 3D objects from depth images.
PTAM - Parallel Tracking and Mapping
fastHOG \ Tech Report \ single GPU version \ multi GPU version \ git (by Ashwin Nanjappa) \ ubuntu build guide
PWP3D \ Paper \ single object and view, VS2008 32bit \ multiple objects and views, VS2010 64bit \ git (by Lu Ma at CU-Boulder)</gtr:description><gtr:id>2A72D27D-B9F1-4AFE-9625-5CB285782220</gtr:id><gtr:impact>The software has been successful in bringing industrial funding and consultancies into the laboratory. Some elements are licensed when used commercially.</gtr:impact><gtr:outcomeId>56d6cc985ebc55.74567670</gtr:outcomeId><gtr:title>Oxford Active Vision Library</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/OxVisionLib/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>6A58AF98-9DAE-454D-891F-B79A15E18CAB</gtr:id><gtr:title>Real-Time Tracking of Single and Multiple Objects from Depth-Colour Imagery Using 3D Signed Distance Functions</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8dfa044c136a49a413c2f3c6a6af6357"><gtr:id>8dfa044c136a49a413c2f3c6a6af6357</gtr:id><gtr:otherNames>Ren C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>15731405</gtr:issn><gtr:outcomeId>58c68ece547a12.37756643</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AE923762-E655-4797-A49C-2F063E8D04EB</gtr:id><gtr:title>Efficient 3D Scene Labelling using Fields of Trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c90bac0ae9ceca03214af79c31b5117a"><gtr:id>c90bac0ae9ceca03214af79c31b5117a</gtr:id><gtr:otherNames>Kahler, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54639700c01181.10807079</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>90EC3882-8F6E-42A9-97C8-9059F87A9C9F</gtr:id><gtr:title>Simultaneous 3D tracking and reconstruction on a mobile phone</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/66d436891ba6ed5b5b6299667c48a319"><gtr:id>66d436891ba6ed5b5b6299667c48a319</gtr:id><gtr:otherNames>Prisacariu V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463990681bdb8.43848899</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>85DCEEC7-0023-4457-B820-5382B7D9B7AC</gtr:id><gtr:title>Long Range Monocular SLAM</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ff2fe49e578d3a6863a20b140efb0e9b"><gtr:id>ff2fe49e578d3a6863a20b140efb0e9b</gtr:id><gtr:otherNames>Duncan Frost</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6a42e031ed6.77175721</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>08E2278F-EC32-4A00-BB13-A8401A254023</gtr:id><gtr:title>Hierarchical Voxel Block Hashing for Efficient Integration of Depth Images</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c080f96287ede86b62b705f02d46680e"><gtr:id>c080f96287ede86b62b705f02d46680e</gtr:id><gtr:otherNames>Kahler O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d6c5fa30d284.41281208</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E8181AB-286B-479B-9A25-B4E4CAD5339D</gtr:id><gtr:title>3D Tracking of Multiple Objects with Identical Appearance Using RGB-D Input</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8dfa044c136a49a413c2f3c6a6af6357"><gtr:id>8dfa044c136a49a413c2f3c6a6af6357</gtr:id><gtr:otherNames>Ren C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d6c5fab38199.76916719</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7AF2B04D-59D3-4FD9-8021-DD27A510028D</gtr:id><gtr:title>A Framework for the Volumetric Integration of Depth Images</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f02b593e2f36af98bffbc372cf7d418"><gtr:id>5f02b593e2f36af98bffbc372cf7d418</gtr:id><gtr:otherNames>Prisacariu, V.A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54639ae59cc040.29283697</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C7AB0199-9190-449F-B01A-4BEDDF23F0FE</gtr:id><gtr:title>Object-aware bundle adjustment for correcting monocular scale drift</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3c620bc2a7edc9f581737dfa60d44d22"><gtr:id>3c620bc2a7edc9f581737dfa60d44d22</gtr:id><gtr:otherNames>Frost D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>10504729</gtr:issn><gtr:outcomeId>58c68d917d31b0.89398727</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7339EF4E-515C-4830-8F23-467693146845</gtr:id><gtr:title>Very high frame rate volumetric integration of depth images on mobile devices.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/539791ebfe9edb5a4d8b2203f75df96e"><gtr:id>539791ebfe9edb5a4d8b2203f75df96e</gtr:id><gtr:otherNames>K?hler O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>56d6c5fa598de6.89611713</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>73121AF4-E732-4FD9-B75E-AA6D9A24ABCC</gtr:id><gtr:title>Real-Time 3D Tracking and Reconstruction on Mobile Phones.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0af101c6767c1ff1bbb6fd3667d44e75"><gtr:id>0af101c6767c1ff1bbb6fd3667d44e75</gtr:id><gtr:otherNames>Prisacariu VA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>546396b6b944e1.41207680</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0457541E-63F1-447E-BA45-4A2ACE031AF9</gtr:id><gtr:title>Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c4e4dabbd6b89ad7e7ae0aaddcd5732f"><gtr:id>c4e4dabbd6b89ad7e7ae0aaddcd5732f</gtr:id><gtr:otherNames>Vineet V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d6c5fa87a1a1.87768437</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J014990/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>