<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/0740327D-1A22-41B4-82F9-1CC65BB29610"><gtr:id>0740327D-1A22-41B4-82F9-1CC65BB29610</gtr:id><gtr:name>Edinburgh Napier University</gtr:name><gtr:address><gtr:line1>219 Colinton Road</gtr:line1><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH14 1DJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Informatics</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0740327D-1A22-41B4-82F9-1CC65BB29610"><gtr:id>0740327D-1A22-41B4-82F9-1CC65BB29610</gtr:id><gtr:name>Edinburgh Napier University</gtr:name><gtr:address><gtr:line1>219 Colinton Road</gtr:line1><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH14 1DJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/22D23CEC-7A38-4C24-A181-04F491B2BBE1"><gtr:id>22D23CEC-7A38-4C24-A181-04F491B2BBE1</gtr:id><gtr:name>Articulate Instruments Ltd</gtr:name><gtr:address><gtr:line1>Queen Margaret Campus</gtr:line1><gtr:line2>Queen Margaret University Drive</gtr:line2><gtr:postCode>EH21 6UU</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/DC224564-2EA0-435A-8FD0-A46221A72B3E"><gtr:id>DC224564-2EA0-435A-8FD0-A46221A72B3E</gtr:id><gtr:name>University of California, Merced</gtr:name><gtr:address><gtr:line1>PO Box 2039</gtr:line1><gtr:line4>Merced</gtr:line4><gtr:line5>CA 95344</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/82872A8F-54F4-4D75-A1F8-6AF48CAF5126"><gtr:id>82872A8F-54F4-4D75-A1F8-6AF48CAF5126</gtr:id><gtr:firstName>James</gtr:firstName><gtr:otherNames>M</gtr:otherNames><gtr:surname>Scobbie</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/EF2CA888-3185-4370-B5C5-059F7685570B"><gtr:id>EF2CA888-3185-4370-B5C5-059F7685570B</gtr:id><gtr:firstName>Korin</gtr:firstName><gtr:surname>Richmond</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/5D7F708A-5218-40FC-BD20-E55CBA1B14F6"><gtr:id>5D7F708A-5218-40FC-BD20-E55CBA1B14F6</gtr:id><gtr:firstName>Steve</gtr:firstName><gtr:surname>Renals</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D393D282-0920-4341-B3F4-6A8EB4B25C0A"><gtr:id>D393D282-0920-4341-B3F4-6A8EB4B25C0A</gtr:id><gtr:firstName>Joanne</gtr:firstName><gtr:surname>Cleland nee McCann</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI027696%2F1"><gtr:id>CA4C93AB-1B25-4089-8B1F-DB15CE2E4FD7</gtr:id><gtr:title>Ultrax: Real-time tongue tracking for speech therapy using ultrasound</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I027696/1</gtr:grantReference><gtr:abstractText>Speech Sound Disorders (SSDs) are the most common communication impairment in childhood, affecting 6.5% of all UK children, or 2 children in every classroom. SSDs make it difficult for people to communicate with peers and integrate with society, yet the efficacy of interventions for most types of SSDs is weak. Speech, Language and Communication Disorders (SLCD) are a key UK government priority at present, with 2011 designated the national year of speech, language and communication . A recent government report (Bercow, 2008) highlighted the need for a programme of research to enhance the evidence base for children and young people with SLCD. Our programme of research aims to fulfill this need by developing technology which will aid the assessment, diagnosis and treatment of SSDs. Currently in Speech and Language Therapy, technological support is sparse. Technologies that do exist have been expensive to run or complicated to operate and hence not adopted in clinical practice. This project will develop technology (Ultrax) to turn ultrasound into a tongue imaging device specifically designed to provide real-time visual feedback of tongue movements. Most interventions for SSDs rely heavily on auditory skills; clients must listen to their own productions and modify them. However, with Ultrax people with SSDs will actually be able to see the movements of their own tongues and use this information to modify their speech. It is already possible to capture tongue movements by placing a standard medical ultrasound probe under the chin. Ultrasound has the potential to provide powerful information about atypical speech and to enable speakers to modify their own incorrect articulations. However, the image is grainy, information (especially about the tongue tip) is often lost and the image is difficult to interpret. We will improve this image by exploiting prior knowledge about the range of possible tongue shapes and movements in order to provide valuable constraints in tracking tongue contours in sequences of ultrasound images. We will apply a tongue model to this problem, making use of explicit sequence-based optimization for dynamic tracking and smoothing through time. We will use this technology to enhance the ultrasound images, transforming them into a dynamic, real-time 2D video of the tongue's movements which we hypothesize will be A) more easily understood by children B) extend the range of visible tongue shapes from only vowels and /r/ to include /t/,/k/,/ch/ and other consonants which are often targets for therapy Ultrax will be used to provide bio-feedback therapy for people with SSDs and to provide a means for objectively assessing progress by comparing tongue shapes before and after therapy. We will collect a large database of ultrasound and MRI images of tongue movements from 12 adults (ultrasound and MRI) and 90 primary school children (ultrasound) on which to base the model of tongue contours and to test its performance. At the same time, we will split the 90 children into 3 groups and record each group's response to one of 3 types of ultrasound display: 1. Raw, unenhanced, ultrasound 2. Unenhanced ultrasound with added anatomical context (e.g. the position of the teeth and roof of the mouth) 3. Fully enhanced ultrasound, developed in this project. The ability of the children to imitate tongue shapes and movements will be evaluated to determine whether they find the enhanced images easier to interpret than unenhanced images, leading to an improved ability use ultrasound for bio-visual feedback. We will trial ultrasound therapy with 9 children with SSDs (3 children for each type of display) enabling us to evaluate practical issues arising during therapy and pave the way for a future clinical trial. At the conclusion of our research project we will have developed the basis for a new visual-feedback tool (Ultrax) for Speech and Language Therapists to use in the diagnosis and treatment of SSDs.</gtr:abstractText><gtr:potentialImpactText>Speech, Language and Communication difficulties (SLCD) are common in childhood. They pose a major challenge to society, affecting educational attainment and the social and emotional health of over one million children and young adults in the UK. The most common type of SLCD is a speech sound disorder (SSD), in which children's speech is difficult to understand. Few technologies exist which aim to help children with SSDs. In our research project we will develop a technology, Ultrax which will enhance ultrasound images (from a standard medical ultrasound machine) to visualise the tongue's movements during speech. The ultimate goal of our research is that Ultrax will be used by Speech and Language Therapists to assess, diagnose and treat SSDs by allowing children and adults with SSDs to actually see the movements of their own tongues and hence modify them. There are three large groups targeted to benefit from our research: 1. Children and adults with speech sound disorders. 2. Speech and language therapists learning about typical and disordered articulation. 3. Students of SLT or phonetics learning about typical and atypical articulation For children and adults with SSDs, Ultrax has the potential to be used for more accurate diagnosis of SSDs and ultimately better treatment with faster results, leading to a decrease in the cost of supplying therapy to children with SSDs. Within the life of our project we will trial various types of ultrasound image with 9 children with SSDs. A Speech and Language Therapist with extensive experience of similar types of therapy will design and implement individualised therapy programmes for these children with the goal of improving their speech and ultimately their quality of life. We will also run workshops demonstrating normal and disordered speech with ultrasound for Speech and Language Therapists. At Queen Margaret University, we have experienced success running similar workshops in the past and we have the technology to video-conference seminars to a wider audience. A website will be set up for both professionals and parents/user groups interested in speech and via this website we will communicate our research findings in an accessible manner. As part of our project we will collect one of the largest databases of ultrasound images of tongue movements (90 children and 12 adults). This will be made available to researchers all over the World who are interested in normal and disordered speech production and speech recognition technology. Locally, we will use this database in our teaching of both undergraduate and post-graduate students. Student Speech and Language Therapists can find learning about the dynamics of speech production challenging, our database of ultrasound (and MRI) tongue movements and shapes will be a valuable teaching tool in training the UK's future clinicians. Ultrax has strong commercialisation prospects. Articulate Instruments Ltd is one of only a handful of companies worldwide that supplies instrumentation to the speech therapy market. They have been involved from the outset of this project and are firmly committed to taking Ultrax forward, developing it further into a product for the assessment, diagnosis and treatment of speech disorders in both children and adults in collaboration with research Speech and Language Therapists at Queen Margaret University.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>586154</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University College London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Seeing Speech</gtr:description><gtr:id>59A78CB4-7F13-4F47-A0B9-581C57A87422</gtr:id><gtr:impact>Lawson, Eleanor and Stuart-Smith, Jane and Scobbie, James M and Nakai, Satsuki and Beavan, David and Edmonds, Fiona and Edmonds, Iain and Turk, Alice and Timmins, Claire and Beck, Janet M and Esling, John and LePlatre, Gregoire and Cowen, Steve and Barras, Will and Durham, Mercedes (2015) Seeing Speech: an articulatory web resource for the study of phonetics [website]. University of Glasgow.</gtr:impact><gtr:outcomeId>56b9e65f6c4bd5.29092777-2</gtr:outcomeId><gtr:partnerContribution>Glasgow led the project, and host the website (see URL below). &amp;quot;Ultrasound Tongue Imaging Resource&amp;quot; (2011-2013) funded by the Carnegie Trust for the Universities of Scotland provided ~&amp;pound;40,000 funding to a collaborative project hosted by the University of Glasgow (PI Dr Jane Stuart-Smith) in collaboration with Queen Margaret University and other partners to develop a free-to-use web-based resource for phonetics teaching using our novel articulatory data, primarily Ultrasound Tongue Imaging. Further development not using ULTRAX data took place 2013-2015 on this and another Dynamic Dialects website.</gtr:partnerContribution><gtr:piContribution>We contributed MRI recordings of a model talker collected as part of the ULTRAX project, as well as recording ultrasound recordings funded by Carnegie Trust and both MRI and Ultrasound recordings of model talkers funded by Carnegie Trust and the AHRC.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Edinburgh Napier University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Seeing Speech</gtr:description><gtr:id>6C546AED-9525-4A2B-B111-65B02B274556</gtr:id><gtr:impact>Lawson, Eleanor and Stuart-Smith, Jane and Scobbie, James M and Nakai, Satsuki and Beavan, David and Edmonds, Fiona and Edmonds, Iain and Turk, Alice and Timmins, Claire and Beck, Janet M and Esling, John and LePlatre, Gregoire and Cowen, Steve and Barras, Will and Durham, Mercedes (2015) Seeing Speech: an articulatory web resource for the study of phonetics [website]. University of Glasgow.</gtr:impact><gtr:outcomeId>56b9e65f6c4bd5.29092777-4</gtr:outcomeId><gtr:partnerContribution>Glasgow led the project, and host the website (see URL below). &amp;quot;Ultrasound Tongue Imaging Resource&amp;quot; (2011-2013) funded by the Carnegie Trust for the Universities of Scotland provided ~&amp;pound;40,000 funding to a collaborative project hosted by the University of Glasgow (PI Dr Jane Stuart-Smith) in collaboration with Queen Margaret University and other partners to develop a free-to-use web-based resource for phonetics teaching using our novel articulatory data, primarily Ultrasound Tongue Imaging. Further development not using ULTRAX data took place 2013-2015 on this and another Dynamic Dialects website.</gtr:partnerContribution><gtr:piContribution>We contributed MRI recordings of a model talker collected as part of the ULTRAX project, as well as recording ultrasound recordings funded by Carnegie Trust and both MRI and Ultrasound recordings of model talkers funded by Carnegie Trust and the AHRC.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Edinburgh</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Seeing Speech</gtr:description><gtr:id>C4FA0CA4-EFB7-4796-886C-F5E910D5D243</gtr:id><gtr:impact>Lawson, Eleanor and Stuart-Smith, Jane and Scobbie, James M and Nakai, Satsuki and Beavan, David and Edmonds, Fiona and Edmonds, Iain and Turk, Alice and Timmins, Claire and Beck, Janet M and Esling, John and LePlatre, Gregoire and Cowen, Steve and Barras, Will and Durham, Mercedes (2015) Seeing Speech: an articulatory web resource for the study of phonetics [website]. University of Glasgow.</gtr:impact><gtr:outcomeId>56b9e65f6c4bd5.29092777-3</gtr:outcomeId><gtr:partnerContribution>Glasgow led the project, and host the website (see URL below). &amp;quot;Ultrasound Tongue Imaging Resource&amp;quot; (2011-2013) funded by the Carnegie Trust for the Universities of Scotland provided ~&amp;pound;40,000 funding to a collaborative project hosted by the University of Glasgow (PI Dr Jane Stuart-Smith) in collaboration with Queen Margaret University and other partners to develop a free-to-use web-based resource for phonetics teaching using our novel articulatory data, primarily Ultrasound Tongue Imaging. Further development not using ULTRAX data took place 2013-2015 on this and another Dynamic Dialects website.</gtr:partnerContribution><gtr:piContribution>We contributed MRI recordings of a model talker collected as part of the ULTRAX project, as well as recording ultrasound recordings funded by Carnegie Trust and both MRI and Ultrasound recordings of model talkers funded by Carnegie Trust and the AHRC.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Glasgow</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>School of Critical Studies</gtr:department><gtr:description>Seeing Speech</gtr:description><gtr:id>D0EDA5AD-7B7E-4F4A-BCC3-423285F57EB0</gtr:id><gtr:impact>Lawson, Eleanor and Stuart-Smith, Jane and Scobbie, James M and Nakai, Satsuki and Beavan, David and Edmonds, Fiona and Edmonds, Iain and Turk, Alice and Timmins, Claire and Beck, Janet M and Esling, John and LePlatre, Gregoire and Cowen, Steve and Barras, Will and Durham, Mercedes (2015) Seeing Speech: an articulatory web resource for the study of phonetics [website]. University of Glasgow.</gtr:impact><gtr:outcomeId>56b9e65f6c4bd5.29092777-1</gtr:outcomeId><gtr:partnerContribution>Glasgow led the project, and host the website (see URL below). &amp;quot;Ultrasound Tongue Imaging Resource&amp;quot; (2011-2013) funded by the Carnegie Trust for the Universities of Scotland provided ~&amp;pound;40,000 funding to a collaborative project hosted by the University of Glasgow (PI Dr Jane Stuart-Smith) in collaboration with Queen Margaret University and other partners to develop a free-to-use web-based resource for phonetics teaching using our novel articulatory data, primarily Ultrasound Tongue Imaging. Further development not using ULTRAX data took place 2013-2015 on this and another Dynamic Dialects website.</gtr:partnerContribution><gtr:piContribution>We contributed MRI recordings of a model talker collected as part of the ULTRAX project, as well as recording ultrasound recordings funded by Carnegie Trust and both MRI and Ultrasound recordings of model talkers funded by Carnegie Trust and the AHRC.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Public information website (on ultrasound for speech therapy)</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>0ACD29D9-3763-4841-A211-69614986F546</gtr:id><gtr:impact>Website created to advertise the existence of this research direction, and to explain its motivation and approach. 
Domain paid for 15 years, and website will continue to be hosted and updated following the 3 year funding period. Research data arising from the funded project to be distributed, free for research use, via this website. Website also supported recruitment efforts of experiment subjects.

Supported media interest (e.g. see http://www.ultrax-speech.org/links/links)</gtr:impact><gtr:outcomeId>5460ba5ddaa0e7.73556920</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.ultrax-speech.org</gtr:url><gtr:year>2011,2012,2013,2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Sounds and ultrasound in Scotland</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C4AF25D9-A1C3-4976-BAF3-6CF5A7A67AC9</gtr:id><gtr:impact>Invited lecture at GIPSA-Lab, Grenoble.

Academic collaboration</gtr:impact><gtr:outcomeId>r-4382670879.7204042b05c8e4</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:year>2011</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Ultrasound Tongue Imaging: Insights into articulation in the laboratory, community and clinic</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8A802607-8250-4FBD-BDFF-F3AE437F9729</gtr:id><gtr:impact>Keynote talk at 14th Australasian International Conference on Speech Science and Technology, Macquarie University, Sydney. Interdisciplinary audience of general academic and Speech and Language Therapy researchers and students.

na</gtr:impact><gtr:outcomeId>r-2190570786.77282762b14849c</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Postgraduate Summer School Course (Groningen)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F1E9E587-ED0A-45E3-8119-E47C91B90B64</gtr:id><gtr:impact>Five days of doctoral student and masters student seminars at the LOT Summer School, Groningen, 17-23 June 2013.

At least one of the students has gone on to undertake ultrasound-based research for her doctorate</gtr:impact><gtr:outcomeId>r-7699938833.4340782a8fe78c</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.lotschool.nl/files/schools/2013_Summerschool_Groningen/schedule.php</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>ASHA UTI workshop 2016</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>703BB404-2CFD-47B8-933E-16BE512F655A</gtr:id><gtr:impact>The world's largest Speech and Language Therapy convention (ASHA) meets annually in the USA with 12,000 delegates, most of them practicing professional SLTs. Dr Cleland was invited to participate in a three hour workshop (led by Dr Jon Preston, Syracuse University, New York) on ultrasound visual biofeedback in speech therapy. She was the only international participant, alongside the other world leaders (all North American) in the field (e.g. Preston, McAllister-Byun, Leece, Boyce). Ultraphonix project data was demonstrated, highlighting the uniquely high quality, varied, and detailed longitudinal treatment dataset we have gathered. Our wider experience with a more complex and representative set of speech difficulties than is typically addressed also let us claim a leading role in this area in terms of clinical practice. The quantitative results of our successful interventions from Ultraphonix and ULTRAX are appearing in the literature, and the panel was a platform to promote our peer-reviewed outputs. Our aim in attending was to influence the future practice of SLTs in North America by establishing ourselves, in their minds, as a major authority and centre of excellence in this new field both in research and in clinical practice.</gtr:impact><gtr:outcomeId>5810b71c29a9f1.50690579</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>seminar at INESC 2012</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EA9927F4-0CCC-42F2-9521-82F4BA0EFE3A</gtr:id><gtr:impact>Talk entitled &amp;quot;Assistive Speech Technology&amp;quot; at INESC-ID, Lisbon</gtr:impact><gtr:outcomeId>56dfd0f3d79263.13136500</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Postgraduate Training (S?o Paulo)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6AA82B96-C20A-4572-A13D-B6BEEB953D2D</gtr:id><gtr:impact>one week of masters / doctoral seminars at Pontif&amp;iacute;cia Universidade Cat&amp;oacute;lica de S&amp;atilde;o Paulo, Brazil, primarily to a group of Speech Therapists and Therapy-oriented researchers.

Additional request for CAPES-funded international post-grad visit to QMU for 2015.</gtr:impact><gtr:outcomeId>54606a618f7227.51156527</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>SLT UTI Study Day (06/14)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>9442F556-AF8E-450C-9C14-8580E4FB560D</gtr:id><gtr:impact>Visualising Articulation: A Study Day for Speech &amp;amp; Language Therapists on Visual Biofeedback and Articulatory Models for treating Speech Sound Disorders. (5th June 2014), Queen Margaret University, Edinburgh. 
Attended by NHS Speech and Language Therapists who wanted to find out about research findings on ultrasound tongue imaging fundamentals, what it reveals about typical populations and development, and how it can be used for biofeedback therapy.
Input from researchers on multiple projects for a full day workshop for local SLT managers and therapists, upper undergraduate and masters-level students.
a. EPSRC ULTRAX (Renals PI)
a. ESRC Mimicry (Scobbie PI)
c. Carnegie Trust Seeing Speech (Stuart-Smith PI)
d. AHRC Dynamic Dialects (Stuart-Smith PI)
(An associated advanced training workshop in UTI was also run at the same time - it is recorded as a separate event.)

Increased awareness of
a. the importance and use of visual biofeedback methods in speech therapy, protocols used in current successful therapy at QMU and elsewhere,
b. datasets of and research into typical production of a range of sounds, including social variation in adulthood and typical childhood articulation patterns</gtr:impact><gtr:outcomeId>543e71c3550835.59753697</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Holyrood impact event 2015</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>3D5DB2EB-118D-4688-AF50-AA596B7E6CE6</gtr:id><gtr:impact>Prof Scobbie was the sole QMU representative at a Universities Scotland Evening Reception in the Garden Lobby of the Scottish Parliament titled &amp;quot;Innovation with Impact&amp;quot;. He demonstrated ultrasound tongue imaging and discussed its clinical use. MSPs visited the 11 stalls and met the representatives of the 19 HEIs present, plus an &amp;quot;Interface&amp;quot; impact-promotion organisation. The MSPs, other guests and the university representatives from other HEIs learned about our ultrasound research and the use of ultrasound as a clinical (diagnostic and intervention) tool. There were around 120 guests with &amp;quot;significant opportunities for networking&amp;quot;. Around 20 minutes were speeches (e.g. from the Higher Education minister).</gtr:impact><gtr:outcomeId>58aadddc018e84.40796525</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Policymakers/politicians</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Ultrafest VI</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>80B3D9E2-892E-47B5-9586-82ECA52A7438</gtr:id><gtr:impact>This was the 6th Ultrafest meeting - an international workshop bringing together researchers, practitioners and other stakeholders with interests in exploiting ultrasound imaging technology for speech science, technology and therapy purposes. The meeting was co-hosted by the CASL Research Centre (Centre for Audiology, Speech and Language, Queen Margaret University) and CSTR (the Centre for Speech Technology Research, University of Edinburgh), and took place from Wednesday 6th to Friday 8th November 2013 in the Informatics Forum at the University of Edinburgh. Ultrafest VI was organised by a committee including researchers from both Edinburgh University and Queen Margaret University: Eleanor Lawson, Korin Richmond, Zoe Roxburgh, Sonja Schaeffler, James M Scobbie, Claire Timmins, Alan Wrench and Natasha Zharkova. Presentations provoked much discussion, sparking new ideas for further work, as well as opening new links for collaboration.</gtr:impact><gtr:outcomeId>58c9394fc606d4.21946307</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.qmu.ac.uk/casl/conf/ultrafest_2013/</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UTI researcher training (06/14)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>799792D6-E728-4CAF-A2CF-6FE3579E334E</gtr:id><gtr:impact>Advanced and Intermediate Training in UTI data analysis for post-doc, PhD and active researchers from around the UK, June 2014.
(See also a related Speech and Language Therapist Training event).

Academic colleagues requested further, regular (eg annual) events. Small grant application written and submitted to British Academy with one of the participants on the basis of their attendance (unsuccessful).</gtr:impact><gtr:outcomeId>5460830e2bf270.22391511</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>138378</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>CSO Response Mode Grants</gtr:description><gtr:end>2016-11-02</gtr:end><gtr:fundingOrg>Chief Scientist Office (CSO)</gtr:fundingOrg><gtr:fundingRef>ETM/402</gtr:fundingRef><gtr:id>13E0AF9A-F56F-4C45-995A-CE912456A142</gtr:id><gtr:outcomeId>56e14ac1adae52.15223159</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2015-05-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>46421</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Project Grant</gtr:description><gtr:end>2018-07-02</gtr:end><gtr:fundingOrg>Action Medical Research</gtr:fundingOrg><gtr:fundingRef>GN2544</gtr:fundingRef><gtr:id>AECDE89B-5BC8-4B27-943D-DC44912A1D64</gtr:id><gtr:outcomeId>58a582762357d9.16446568</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2017-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>At the outset of the Ultrax project in 2011 there was very little work showing whether ultrasound-based speech therapy might be an effective treatment and no work using ultrasound to diagnose specific types of SSDs. Through the Ultrax project, and a related clinical project, we have shown that ultrasound can be used to help children with SSDs which were previously thought to be untreatable. The impact on children's quality of life is considerable, and feedback from parents has been positive and enthusiastic.</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>F671A350-5C96-484D-B9EE-F06C81CD8EF4</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545363f8b6e3e0.63955987</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The ULTRAX project was concerned with the use of ultrasound imaging for speech therapy. There were several findings.

1. Fast and robust tracking of tongue contours in ultrasound video sequence. By integrating global whole-frame features with local edge-gradient features, we implemented a subject-independent system for tracking tongue contour edges in real-time. This approach used a recursive Bayesian framework, and provided an acceptable level of robustness for online visual display. Offline processing of entire recordings can yield results which are more refined, and will provide additional benefit in use-cases which do not require on-line processing to extract tongue contours from ultrasound sequences.

2. Vocal tract shape modelling. Dimensionality-reduction techniques applied to 2D tracings of phonetically diverse midsagittal articulations from MRI scans were used to give a plausible and flexible model of vocal tract shape, which is straightforward to interpret. Furthemore, we have shown it is possible animate such models within the above mentioned recursive Bayesian estimation framework, using articulatory observation data such as ultrasound tongue contours or articulator points tracked using electromagnetic articulography.

3. Improved tongue movement model. Because visibility of the front of the tongue is limited in ultrasound recordings, we have found animating a generic tongue model (derived from 12 adult subject MRI scans) using ultrasound tongue contour observations alone results in apparently over-constrained tongue tip movement. Therefore, moving beyond the originally envisaged scope of the project, we have explored incorporating additional features that can be extracted from ultrasound data which indicate tongue movements in the absence of a clear contour for the tongue tip. Pilot experiments indicate we can indeed achieve greatly increased movement in the animated tongue tip using this approach.

4. Experimental investigation of real-time imaging of tongue movement. We have used real-time ultrasound images of the tongue's midline surface as it moves during speech as real-time feedback for a talker, who was being taught to pronounce new unfamiliar speech sounds. These experiments were motivated by the ultimate aim of using such feedback as an element of clinical intervention to help children with persistent speech disorders. The results showed, as expected, that some sounds are harder than others to acquire successfully, and that human-mediated intervention by the therapist was far more likely to be successful than a mere exposure to acoustic speech sounds or un-mediated ultrasound images. We also were able to clarify different models of intervention which we can use to explore the use of ultrasound in the future in clinical cases. 

5. Ultrasound corpus. The 60 children involved in the experiments above were recorded using simultaneous ultrasound of the tongue, audio, and video of the lips producing a wide range of speech sounds and real words, creating a unique new corpus of typical speech in children. These were used in research and also to provide models for the speech therapy component.

6. Ultrasound/MRI corpus. We also collected a matched corpus with ultrasound (and acoustic) data on the one hand and MRI (and acoustic) data on the other, from 12 phonetically trained adult speakers. The vocal tract surfaces for each speaker have been traced for a wide range of speech sounds in each corpus, and this unique resource was used to help estimate the tongue model. We also showed the effects on the location of the tongue during speech production when speakers lie on their backs (in order to emulate the necessary conditions for recording speech in an MRI machine). 

7. Speech therapy. Finally, nine courses of speech therapy were undertaken with 8 individual children. These children were all successfully treated and their speech improved. Particularly useful enhancements of the raw image resulting from our project and used during the therapy included the placement on the screen of a model of the speaker's hard palate and the use of the child's own stored productions. A number of developments to the analysis software were made, to assist automated tracking of the tongue surface, for example.</gtr:description><gtr:exploitationPathways>1. Experimental data. The core experimental data collection was accompanied by other speech corpora as noted above, and it is these which are expected to be particularly useful for other researchers. For example, the clinical corpus has already been used by other researchers at QMU, and a Carnegie Trust research project &amp;quot;Seeing Speech&amp;quot; has used some of our MRI corpus recordings as part of online teaching tool for phonetics - as of October 2014, speech sounds on the site had been viewed 127,250 times. 

2. Modelling and Algorithms. The recursive Bayesian estimation approach developed for this project has potential for application and impact in several other speech and visual processing problems which can be cast in similar terms of integrating local and global features. Drawing from the findings of this project, Richmond is adapting the approach to the requirements of speech synthesis and pitch tracking.

3. Ultrasound imaging. In addition to speech therapy, ultrasound is used to image tongue movements in such diverse research fields as: linguistic/phonetic investigation (e.g. related to speech production); psychology experimentation (e.g. speech-related cognition, or response timing); and speech technology (e.g. silent-speech interfaces, automatic speech recognition, and acoustic-articulatory inversion mapping). The methods available to track tongue contours in ultrasound video prior to this project were only semi-automatic and slower than realtime, so requiring laborious and costly human effort. Therefore, the development of a fast and robust method to track tongue contour in ultrasound video brings direct and lasting impact to all these and other research areas.</gtr:exploitationPathways><gtr:id>EC1BAAA9-CD1C-434F-AD69-69B7A1B44EB3</gtr:id><gtr:outcomeId>54536340bebbc7.20706735</gtr:outcomeId><gtr:sectors><gtr:sector>Communities and Social Services/Policy,Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.ultrax-speech.org</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Magnetic resonance imaging (MRI) scans: vocal tracts of 12 adults scanned, with audio, demonstrating a carefully designed phonetically diverse range of articulatory configurations
Ultrasound recordings of the tongue: 12 adults (same as MRI scans, with matched phonetic material), 60 child subjects</gtr:description><gtr:id>CEE94190-8C4F-4221-A389-167C23ED824D</gtr:id><gtr:impact>Beyond the needs of the project which funded the recording of this data, the corpus has so far been used by others to i) build an online visual International Phonetic Alphabet, whereby the user can see speech sounds articulated in short movies derived from the MRI data; ii) to build a 3D geometric model of the vocal tract for audio/visual speech synthesis.</gtr:impact><gtr:outcomeId>5460bd3f5776e2.05023936</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>MRI/Ultrasound corpus</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F2539A02-E9D7-4E2A-8C95-FB600F43A9C2</gtr:id><gtr:title>A fixed dimension and perceptually based dynamic sinusoidal model of speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4270d19e312228c6755fbae911c04249"><gtr:id>4270d19e312228c6755fbae911c04249</gtr:id><gtr:otherNames>Hu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56615bbd1c2f76.58444862</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55353935-A185-460A-A972-C08C97D516FC</gtr:id><gtr:title>Covert contrast and covert errors in persistent velar fronting.</gtr:title><gtr:parentPublicationTitle>Clinical linguistics &amp; phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0269-9206</gtr:issn><gtr:outcomeId>5809daecd2a624.64989054</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E3DF6CC-8343-4E35-A970-DFC0017AD8C0</gtr:id><gtr:title>A statistical shape space model of the palate surface trained on 3D MRI scans of the vocal tract.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c7312ddcc7ec908bf60035836b247dc3"><gtr:id>c7312ddcc7ec908bf60035836b247dc3</gtr:id><gtr:otherNames>Hewer A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c92b2f9e3f59.16920264</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>45BCBD5C-6AE2-4C2B-A0FB-6B1A32FB2758</gtr:id><gtr:title>On the evaluation of inversion mapping performance in the acoustic domain</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2e45bf486b896a69a7ae0bb1941313ec"><gtr:id>2e45bf486b896a69a7ae0bb1941313ec</gtr:id><gtr:otherNames>Richmond K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>546089f83f4905.43520440</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CD262AF7-E8BB-475C-9F06-86CC5A8313B6</gtr:id><gtr:title>Helping children learn non-native articulations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5809f0157569a7.88375118</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>683A7D82-5310-4663-8C0C-4D4B72BBCA9E</gtr:id><gtr:title>Vowel creation by articulatory control in HMM-based parametric speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a43a7c784b9eb3272073c4b1094c50f"><gtr:id>2a43a7c784b9eb3272073c4b1094c50f</gtr:id><gtr:otherNames>Ling Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54609bdeed22e0.45589050</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C2421324-D1D7-4E67-8555-8923437FC26A</gtr:id><gtr:title>Speech animation using electromagnetic articulography as motion capture data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2c8bbbbaad9b5cc83eb4f98830f2e6"><gtr:id>0b2c8bbbbaad9b5cc83eb4f98830f2e6</gtr:id><gtr:otherNames>Steiner I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460906de0caf8.99963233</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0F437C5B-CB7E-427E-BE75-973AE9FB6E43</gtr:id><gtr:title>Methods for applying dynamic sinusoidal models to statistical parametric speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4270d19e312228c6755fbae911c04249"><gtr:id>4270d19e312228c6755fbae911c04249</gtr:id><gtr:otherNames>Hu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56ddf7e0140b29.73223351</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F8734B55-7523-479F-8BDA-9492FD43C741</gtr:id><gtr:title>Seeing Speech: an articulatory web resource for the study of phonetics [website]</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bef7fd9f1b26be7fff3cd3cef5230aeb"><gtr:id>bef7fd9f1b26be7fff3cd3cef5230aeb</gtr:id><gtr:otherNames>Lawson E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de99bebb5342.41219640</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1B412EC4-38D5-4F85-A279-8A83E0FFCE7A</gtr:id><gtr:title>Mage - Reactive articulatory feature control of HMM-based parametric speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3c8bc8ff107c34db43cfc64ab6d9241a"><gtr:id>3c8bc8ff107c34db43cfc64ab6d9241a</gtr:id><gtr:otherNames>Astrinaki M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460992902dca6.96734255</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A947E3C-C5B8-445F-AB3C-44E380785720</gtr:id><gtr:title>The Edinburgh Speech Production Facility DoubleTalk Corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c1629ffb6608cc3c8323188f6e4f0f5e"><gtr:id>c1629ffb6608cc3c8323188f6e4f0f5e</gtr:id><gtr:otherNames>Scobbie J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460966664a022.30441899</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>40387051-5F61-4BCC-9887-DAF4BE65B1D3</gtr:id><gtr:title>Deep Architectures for Articulatory Inversion</gtr:title><gtr:parentPublicationTitle>NTERSPEECH 2012 13th Annual Conference of the International Speech Communication Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3b57bedea99d575d64d8240d6953fb42"><gtr:id>3b57bedea99d575d64d8240d6953fb42</gtr:id><gtr:otherNames>Uria, B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5433b7ba1d9c05.40523614</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>78CC5194-6BD0-48A1-94D0-CF9951A4CA9D</gtr:id><gtr:title>The magnetic resonance imaging subset of the mngu0 articulatory corpus.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2c8bbbbaad9b5cc83eb4f98830f2e6"><gtr:id>0b2c8bbbbaad9b5cc83eb4f98830f2e6</gtr:id><gtr:otherNames>Steiner I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>54609cba31c143.96524110</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4F53D02D-27BA-4A56-876C-6E04B4F275FB</gtr:id><gtr:title>Glottal Spectral Separation for Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0354e6b9c5de838fe3e29ef2e8d29910"><gtr:id>0354e6b9c5de838fe3e29ef2e8d29910</gtr:id><gtr:otherNames>Cabral J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54609740cd8aa3.02421865</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BF327C4-D6A0-423F-B93C-265CFA33E55F</gtr:id><gtr:title>Insights from ultrasound: Enhancing our understanding of clinical phonetics.</gtr:title><gtr:parentPublicationTitle>Clinical linguistics &amp; phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0269-9206</gtr:issn><gtr:outcomeId>5809daecb32cb6.42397933</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2169AC1A-A5D7-4CE4-9641-5ED1CC8335AF</gtr:id><gtr:title>Announcing the electromagnetic articulography (day 1) subset of the mngu0 articulatory corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1863a1af108e484cfbba1abf3daa6033"><gtr:id>1863a1af108e484cfbba1abf3daa6033</gtr:id><gtr:otherNames>Richmond K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5460ac2d1c3db2.38890418</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06224839-AD29-40E5-8C65-27D13BF7EFE9</gtr:id><gtr:title>Ultrax: An Animated Midsagittal Vocal Tract Display for Speech Therapy</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1863a1af108e484cfbba1abf3daa6033"><gtr:id>1863a1af108e484cfbba1abf3daa6033</gtr:id><gtr:otherNames>Richmond K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5433b7735b5f33.66938357</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3C25D03B-EE5D-49E5-91FF-630E4D2BEF0C</gtr:id><gtr:title>Recording speech articulation in dialogue: Evaluating a synchronized double electromagnetic articulography setup</gtr:title><gtr:parentPublicationTitle>Journal of Phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24179b49774713b48feec4cbdb6f059c"><gtr:id>24179b49774713b48feec4cbdb6f059c</gtr:id><gtr:otherNames>Geng C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433b721e06bb1.17436322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>73C5D699-2687-4CDD-AB04-11D6CD38C38D</gtr:id><gtr:title>An ultrasound protocol for comparing tongue contours: upright vs. supine</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5617565e6167c488573dadcf17eda685"><gtr:id>5617565e6167c488573dadcf17eda685</gtr:id><gtr:otherNames>Wrench AA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54614f5c1efad2.36972040</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B1403C3-7AB1-4200-9351-42170C132C21</gtr:id><gtr:title>Onset vs. Coda Asymmetry in the Articulation of English /r/</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6ebb7378417e1f6f06954e49d67b937"><gtr:id>e6ebb7378417e1f6f06954e49d67b937</gtr:id><gtr:otherNames>Scobbie JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b9e089e239a8.15232845</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DC3203F4-5BD6-4154-8652-F7C9AE885E15</gtr:id><gtr:title>A multilinear tongue model derived from speech related MRI data of the human vocal tract</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c7312ddcc7ec908bf60035836b247dc3"><gtr:id>c7312ddcc7ec908bf60035836b247dc3</gtr:id><gtr:otherNames>Hewer A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a9ff7f3e1d3b4.45160194</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B75A6483-E6A4-4010-81B2-E5CD1718D7F1</gtr:id><gtr:title>An Investigation of the Application of Dynamic Sinusoidal Models to Statistical Parametric Speech Synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4e4c09fc42bbe15fdd1b36d90d56f338"><gtr:id>4e4c09fc42bbe15fdd1b36d90d56f338</gtr:id><gtr:otherNames>Hu, Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56615c53ceec49.51715475</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A582FC0-2600-40F8-B48A-523BCD6A3FA4</gtr:id><gtr:title>Articulatory Control of HMM-Based Parametric Speech Synthesis Using Feature-Space-Switched Multiple Regression</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4d35805892fea2bd41853d9893397711"><gtr:id>4d35805892fea2bd41853d9893397711</gtr:id><gtr:otherNames>Zhen-Hua Ling</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54608e8a3f7c13.23391512</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6DC93613-5E8C-4F8E-B5B0-EFF3736E3592</gtr:id><gtr:title>Tongue reading: comparing the interpretation of visual information from inside the mouth, from electropalatographic and ultrasound displays of speech sounds.</gtr:title><gtr:parentPublicationTitle>Clinical linguistics &amp; phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0269-9206</gtr:issn><gtr:outcomeId>5809f015ccffc0.87101523</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96F17E23-5E4C-48A7-ADCD-EC36CAC1A305</gtr:id><gtr:title>Helping children learn non-native articulations: the implications for ultrasound-based clinical intervention</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b9de45cc4d65.32779540</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55F307B1-6EB2-413C-A83F-819E1DE6C47C</gtr:id><gtr:title>Vowel Creation by Articulatory Control in HMM-based Parametric Speech Synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a43a7c784b9eb3272073c4b1094c50f"><gtr:id>2a43a7c784b9eb3272073c4b1094c50f</gtr:id><gtr:otherNames>Ling Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54609b11dc7a81.50680681</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>798F197F-E20C-4519-8B49-161AE944C7D2</gtr:id><gtr:title>Investigating English Pronunciation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c31326fe2d7119af01381f29a5e77427"><gtr:id>c31326fe2d7119af01381f29a5e77427</gtr:id><gtr:otherNames>Thomas S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>978-1-349-56406-4</gtr:isbn><gtr:outcomeId>56b9e26132e0c4.51241170</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EB147A13-FBA9-4626-9FC9-32DED6DE0565</gtr:id><gtr:title>Comparing articulatory images: An MRI / Ultrasound Tongue Image database</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54614e9d100173.42933488</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BEE95D2-A7A1-4DD4-BF19-46593D888186</gtr:id><gtr:title>A Deep Neural Network for Acoustic-Articulatory Speech Inversion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b0a2802ab72d32f2f1872ad84c2e9258"><gtr:id>b0a2802ab72d32f2f1872ad84c2e9258</gtr:id><gtr:otherNames>Uria B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5433b7ecd4b5a0.08045788</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C9CE0625-6973-4AF6-8ADF-15B58083CD51</gtr:id><gtr:title>Using ultrasound visual biofeedback to treat persistent primary speech sound disorders.</gtr:title><gtr:parentPublicationTitle>Clinical linguistics &amp; phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442eea80226afba0f9f7053070e06437"><gtr:id>442eea80226afba0f9f7053070e06437</gtr:id><gtr:otherNames>Cleland J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0269-9206</gtr:issn><gtr:outcomeId>5675e08ac7acb</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EA38F382-3E0D-442F-BA13-A72CDA3B8C8E</gtr:id><gtr:title>Mage-HMM-based speech synthesis reactively controlled by the articulators</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3c8bc8ff107c34db43cfc64ab6d9241a"><gtr:id>3c8bc8ff107c34db43cfc64ab6d9241a</gtr:id><gtr:otherNames>Astrinaki M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546093a2b225c3.65869162</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>91A6F181-BAEC-4A52-8DA8-F1D6CC97F373</gtr:id><gtr:title>A common co-ordinate system for mid-sagittal articulatory measurement</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6ebb7378417e1f6f06954e49d67b937"><gtr:id>e6ebb7378417e1f6f06954e49d67b937</gtr:id><gtr:otherNames>Scobbie JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5460ee774e9576.65536508</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5135D65F-0CD7-40C0-B978-C4D40F8052D6</gtr:id><gtr:title>Feature-space transform tying in unified acoustic-articulatory modelling of articulatory control of HMM-based speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a43a7c784b9eb3272073c4b1094c50f"><gtr:id>2a43a7c784b9eb3272073c4b1094c50f</gtr:id><gtr:otherNames>Ling Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5460acf985ffa2.57330895</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3AC320E6-7CBD-43A9-80A8-D59348B2327F</gtr:id><gtr:title>HMM-based speech synthesiser using the LF-model of the glottal source</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0354e6b9c5de838fe3e29ef2e8d29910"><gtr:id>0354e6b9c5de838fe3e29ef2e8d29910</gtr:id><gtr:otherNames>Cabral J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0538-0</gtr:isbn><gtr:outcomeId>5460b008d94fb8.97626869</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6C5E6843-322C-43FC-83E0-A4677C7B17A9</gtr:id><gtr:title>Using multimodal speech production data to evaluate articulatory animation for audiovisual speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0b2c8bbbbaad9b5cc83eb4f98830f2e6"><gtr:id>0b2c8bbbbaad9b5cc83eb4f98830f2e6</gtr:id><gtr:otherNames>Steiner I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54608f99db6b12.53308765</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I027696/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>FD25826C-8B50-43A3-8871-3FF08D051906</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Biomechanics &amp; Rehabilitation</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>