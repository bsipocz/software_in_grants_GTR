<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>College of Medical, Veterinary &amp;Life Sci</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/06706DF4-EA2A-4E30-9F6B-67019B7A478D"><gtr:id>06706DF4-EA2A-4E30-9F6B-67019B7A478D</gtr:id><gtr:firstName>Joachim</gtr:firstName><gtr:surname>Gross</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3D55175C-135E-40F9-9EB1-C22FAB628925"><gtr:id>3D55175C-135E-40F9-9EB1-C22FAB628925</gtr:id><gtr:firstName>Christoph</gtr:firstName><gtr:surname>Kayser</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/BA1958FA-5D03-43A8-8CB2-469390168B8A"><gtr:id>BA1958FA-5D03-43A8-8CB2-469390168B8A</gtr:id><gtr:firstName>Philippe</gtr:firstName><gtr:otherNames>Georges</gtr:otherNames><gtr:surname>Schyns</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FL027534%2F1"><gtr:id>6D934B3F-CBFA-449F-8A6A-B22EF1566016</gtr:id><gtr:title>Pathways and mechanisms underlying the visual enhancement of hearing in challenging environments.</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/L027534/1</gtr:grantReference><gtr:abstractText>The perception of speech is of paramount importance for social communication and the quality of our daily life. Yet, many individuals have difficulties in speech comprehension, especially so under conditions where the speech signal is acoustically distorted (e.g. by background noise). Visual information from seeing the speakers face or lip movements can greatly enhance speech intelligibility under such challenging circumstances. Yet, the specific brain mechanisms that render hearing susceptible to such challenges and those that implement the visual facilitation of hearing remain poorly understood.

We here use two critical manipulations of natural sounds, the addition of background noise and changes in natural speech rhythm, to study where and how along auditory pathways the neural encoding of sounds is affected by these acoustic challenges. We then use these manipulations to study where and how proper sound encoding is restored by informative simultaneous visual information. To this end we use a combination of behavioural tests with high-density electric neuroimaging (Magnetoencephalography) and advanced computational signal analysis in adult human listeners, hence taking a systems approach studying large-scale brain activations during sensory perception. Our specific hypothesis and questions are based on recent insights demonstrating the importance of rhythmic patterns of neural activity for auditory perception. We directly investigate how specific signatures of speech encoding in the human brain are affected by degradation of the speech signal and how they are restored by visual information. 

Hearing difficulties are a highly common disability in the UK and a global burden to society (WHO), making this an important topic for the biosciences underpinning live-long health. The knowledge gained here is of interest for our basic understanding of the physiological processes underlying hearing and to understand how the brain merges visual and acoustic information to achieve a percept that can withstand challenging sensory environments. The insights gained here will also enhance our understanding of the neural mechanisms responsible for impaired auditory comprehension and hence could provide critical knowledge to facilitate the improvement of auditory prosthetic devices or the training of patients in using them. Finally, understanding the interplay between speech signals and their representation in the brain is of socio-economic interest as it is important for the design of human-computer voice-based interfaces, which are becoming ubiquitous in everyday life.</gtr:abstractText><gtr:technicalSummary>Speech is paramount for social communication and daily life. Yet, for many individuals hearing is challenged when speech is distorted by background noises or other sound irregularities. Seeing the speakers face greatly enhances intelligibility under such conditions. Yet, it remains unclear where and how visual information enhances auditory processing. Recent work described multiple rhythmic processes along auditory pathways as critical markers for speech encoding. Based on these we use a systems-approach combining psychophysics with measurements of brain activity (magnetic-neuroimaging; MEG) in adult human listeners to study this timely problem. We implement two critical sound manipulations that challenge intelligibility: addition of background noises and changes in speech rhythm. Using data-driven computational analysis we contribute to a more detailed understanding of those cortical mechanisms that allow vision to enhance hearing. We test specific questions relating to the overall hypothesis that vision facilities hearing by restoring the tracking and encoding of acoustic features by specific rhythmic auditory processes that are affected by our realistic manipulations of sound intelligibility. Using high-density MEG we localize these processes to specific areas and study their functional interactions and relation to perception. Hearing difficulties are a frequent disability in the UK and a global burden to society. The proposed work provides basic knowledge about the physiology of hearing, the neural processes relevant for speech intelligibility, and the mechanisms of multisensory perception. Our work will provide knowledge to better understand hearing in challenging environments, the mechanisms involved in hearing deficits or to improve the function of auditory prosthesis and speech-based human-computer interfaces. In addition we enhance computational tools and analysis methods for data-driven neuroimaging studies.</gtr:technicalSummary><gtr:potentialImpactText>Academic impact:
The proposed work is fundamental neuroscience research and will produce key insights into the functional organisation of sensory pathways and the mechanisms underlying auditory perception and multisensory integration. While the proposed research addresses basic issues with little immediate potential for direct commercial exploitation, it offers critical knowledge relevant for academic researchers in neuroscience and neurophysiology, psychology, systems and clinical neuroscience and researchers and engineers focusing on the development of auditory prosthetic devices or human-computer interfaces. Specifically, we see the following academic impact:
1. Enhancing the knowledge economy by dissemination of results at national and international meetings and publishing papers in peer-reviewed journals. We target the relevant subject areas in neuroscience and engineering and we have a track record in high impact publications and review journals, making this impact very likely.
2. Enhancing collaborations for multidisciplinary research. This project will yield a huge amount of data that can be used for additional analysis and to inspire and inform computational and theoretical neuroscience. We are continuously collaborating with computational neuroscientists to refine our analysis and to share data. Both data and algorithms will be made available on request to others.
3. Delivering a highly skilled researcher. This project will require both advanced experimental and data handling skills. By training a research assistant at the postdoc level, this project will deliver a highly skilled researcher for future neuroscience, clinical research or bioinformatics.
4. Delivering tools for bioinformatics and computational approaches. Systems and data driven approaches to neuroscience provide considerable challenges for data mining and interpretation. Our work continuously refines existing and develops novel methods for data analysis in computational neuroscience. These will be described in open access publication (where possible), shared with others on request or may be posted on our laboratory website.

Economic and Societal Impact:
1. Engaging with industry and clinical science
This project can create links to markets of neurotechnology, especially the development of neuroprosthetic implants for hearing deficits. We have already engaged with the newly formed 'Scottish Hearing Research Network' and we are envisaging a closer engagement with clinically-oriented scientists at the Scottish branch of the MRC Hearing Research Institute. This provides multiple points of contact for the exchange of knowledge with clinically and engineering oriented scientists and potential industrial partners.
2. Enhancing public awareness of hearing 
Because this project and other ongoing projects in our laboratory contribute to a better understanding of hearing, we will contribute to enhancing public awareness of the importance of hearing and hearing disorders. Our university is regularly participating in events on public engagement (e.g. Brain Awareness Week by the Dana Foundation, visits to Schools). In addition the PI has a track record of publications in general peer review journals (Nature Reviews Neuroscience, Trends in Neuroscience) and public science journals (Scientific American), which further aids to this impact. Together with media relation officers of Glasgow University we seek the best strategies to disseminate our results to the press and general public.
3. Contributing to 3Rs
Combining this work based on non-invasive neuroimaging in humans in combination with our other work using animal models can contribute to the 'reduction' of the 3Rs. A better validation and understanding of non-invasive tools can further reduce the use of animals in the long term.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2014-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>369826</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Birmingham</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Prof. Uta Noppeney, Birmingham, UK</gtr:description><gtr:id>9F3955EC-EDC8-4DE5-90FA-A6914F9B2FD1</gtr:id><gtr:impact>Invitation to submit a review article to the journal 'Trends in Cognitive Sciences'</gtr:impact><gtr:outcomeId>58a6c9872f4f27.12695490-1</gtr:outcomeId><gtr:partnerContribution>Intellectual contributions to experimental design.</gtr:partnerContribution><gtr:piContribution>We have provided analytical tools and ideas for experimental design to plan a collaborative study. We are also considering to submit a joint review on the current research in the field.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Explorathon - Glasgow Science Centre</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>CE4BD9D2-11E8-4EE1-8598-ECCDEB915674</gtr:id><gtr:impact>Full day booth at the Glasgow Science Centre during the Eplorathon Scotland event in September 2015. We exhibited facts and hands on experiences on brain function and the senses, and how the senses interact. The audience were families, kids and all visitors at the Science Centre. A team of 6 six of us manned the stand full day, and attendance was quantified by stickers handed over to each participant using our hands on experiences.</gtr:impact><gtr:outcomeId>56a21a9c1c71b1.26997515</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Participation in the Explorathon Scotland 2016</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>F83C245A-C05B-414D-A122-23563AA6AA8B</gtr:id><gtr:impact>Exhibit at the Glasgow Science Centre displaying facts about our research, insights and quizz for kids. The exhibit was part of the explorathon scotland, a full day showcase for science.</gtr:impact><gtr:outcomeId>58a6c79ed2aba8.40991669</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have shown that the benefit of seeing a speakers face for hearing arises not only from changes in local brain activity, i.e. within a brain region, but also from changes in functional connectivity.</gtr:description><gtr:exploitationPathways>Can be used to perform causal tests, such as using brain stimulation.</gtr:exploitationPathways><gtr:id>63C45AEE-924A-4A3E-9857-3EF13D71C3E8</gtr:id><gtr:outcomeId>56d5548425fce4.96412169</gtr:outcomeId><gtr:sectors><gtr:sector>Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.biorxiv.org/content/early/2016/12/30/097493</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>980FF169-1426-4A2A-A899-0F6E1C10D084</gtr:id><gtr:title>Sounds facilitate visual motion discrimination via the enhancement of late occipital visual representations.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c23fc44c582fe1d6c8c3b4f51d3e6058"><gtr:id>c23fc44c582fe1d6c8c3b4f51d3e6058</gtr:id><gtr:otherNames>Kayser SJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>588b638e757ae4.80436132</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C4625753-07C4-4A41-91A5-EDECE025A237</gtr:id><gtr:title>Conference Abstract: Interactions between oscillatory power in fronto-parietal regions and prosodic entrainment in auditory areas</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/825d9371df12099064c473086faff490"><gtr:id>825d9371df12099064c473086faff490</gtr:id><gtr:otherNames>Keitel A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a70b7c674192.76132949</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B74C43C-E502-4452-B92F-F1474860ADF0</gtr:id><gtr:title>Lip movements entrain the observers' low-frequency brain oscillations to facilitate speech intelligibility.</gtr:title><gtr:parentPublicationTitle>eLife</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f8387ac92aa3d4f7852a5a27a3990fda"><gtr:id>f8387ac92aa3d4f7852a5a27a3990fda</gtr:id><gtr:otherNames>Park H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2050-084X</gtr:issn><gtr:outcomeId>585d414451c372.88409708</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>188DD1E3-AEFD-485E-81A3-3E0C3593D97C</gtr:id><gtr:title>Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/825d9371df12099064c473086faff490"><gtr:id>825d9371df12099064c473086faff490</gtr:id><gtr:otherNames>Keitel A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>588b634755f910.59107491</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7857448E-580F-4B86-83D4-83D18C65861C</gtr:id><gtr:title>Contributions of local speech encoding and functional connectivity to audio-visual speech perception.</gtr:title><gtr:parentPublicationTitle>eLife</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/847137823aa4a5a3be74fd867ce3e64d"><gtr:id>847137823aa4a5a3be74fd867ce3e64d</gtr:id><gtr:otherNames>Giordano BL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2050-084X</gtr:issn><gtr:outcomeId>5a2fcc35403347.68787497</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9672C233-C38B-4E4C-956C-81697B0BD821</gtr:id><gtr:title>Neural correlates of multisensory reliability and perceptual weights emerge at early latencies during audio-visual integration.</gtr:title><gtr:parentPublicationTitle>The European journal of neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3b64bc86890756f4b26b73323138414d"><gtr:id>3b64bc86890756f4b26b73323138414d</gtr:id><gtr:otherNames>Boyle SC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0953-816X</gtr:issn><gtr:outcomeId>5a661705b08102.20364511</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>032BD248-A216-42B2-8920-6BB14BEE86D1</gtr:id><gtr:title>Who is That? Brain Networks and Mechanisms for Identifying Individuals.</gtr:title><gtr:parentPublicationTitle>Trends in cognitive sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1df8b9415cb39e61351f2f77b90f99ea"><gtr:id>1df8b9415cb39e61351f2f77b90f99ea</gtr:id><gtr:otherNames>Perrodin C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1364-6613</gtr:issn><gtr:outcomeId>585d74993ca6d7.94684470</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E90E102-5854-4E5F-9E04-CC032B102D46</gtr:id><gtr:title>Conference Abstract A novel mutual information estimator for analysing M/EEG data and quantifying representational interactions between signals</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/92f2fa96bbb904c053d83b33cd29fc73"><gtr:id>92f2fa96bbb904c053d83b33cd29fc73</gtr:id><gtr:otherNames>Ince</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a70b0269b882.39569302</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A13D6BB-B4DD-4F66-82A7-EA7761E1EE34</gtr:id><gtr:title>Prestimulus influences on auditory perception from sensory representations and decision processes.</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Academy of Sciences of the United States of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c23fc44c582fe1d6c8c3b4f51d3e6058"><gtr:id>c23fc44c582fe1d6c8c3b4f51d3e6058</gtr:id><gtr:otherNames>Kayser SJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0027-8424</gtr:issn><gtr:outcomeId>585d3923626cb5.16130707</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>82B4943E-D2CF-4D72-9895-436707A4B210</gtr:id><gtr:title>Irregular Speech Rate Dissociates Auditory Cortical Entrainment, Evoked Responses, and Frontal Alpha.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c23fc44c582fe1d6c8c3b4f51d3e6058"><gtr:id>c23fc44c582fe1d6c8c3b4f51d3e6058</gtr:id><gtr:otherNames>Kayser SJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>56a218a51f26b0.28792695</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>48EA7CBC-CE47-4D36-A8FB-DB787AAC14FE</gtr:id><gtr:title>Neural population coding: combining insights from microscopic and mass signals.</gtr:title><gtr:parentPublicationTitle>Trends in cognitive sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5b67d54d3731baa54742112ac1760a64"><gtr:id>5b67d54d3731baa54742112ac1760a64</gtr:id><gtr:otherNames>Panzeri S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1364-6613</gtr:issn><gtr:outcomeId>5675df9fd9593</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26ECC425-A7F8-4290-B060-F9120F817759</gtr:id><gtr:title>Individual Human Brain Areas Can Be Identified from Their Characteristic Spectral Activation Fingerprints.</gtr:title><gtr:parentPublicationTitle>PLoS biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/825d9371df12099064c473086faff490"><gtr:id>825d9371df12099064c473086faff490</gtr:id><gtr:otherNames>Keitel A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1544-9173</gtr:issn><gtr:outcomeId>585d3f50a8a4f3.67084168</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>41CFAC67-5939-4086-AE93-43F64302F344</gtr:id><gtr:title>Natural asynchronies in audiovisual communication signals regulate neuronal multisensory interactions in voice-sensitive cortex.</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Academy of Sciences of the United States of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1df8b9415cb39e61351f2f77b90f99ea"><gtr:id>1df8b9415cb39e61351f2f77b90f99ea</gtr:id><gtr:otherNames>Perrodin C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0027-8424</gtr:issn><gtr:outcomeId>5675de1d5910a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>74453320-C02C-40BE-BA46-0C1A386B098B</gtr:id><gtr:title>Multisensory causal inference in the brain.</gtr:title><gtr:parentPublicationTitle>PLoS biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/77b750ce36120b73e26e32ad712b6d7f"><gtr:id>77b750ce36120b73e26e32ad712b6d7f</gtr:id><gtr:otherNames>Kayser C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1544-9173</gtr:issn><gtr:outcomeId>585d764fa385f6.27832553</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>33F4140E-4877-4A9F-9A01-3BE4F513521A</gtr:id><gtr:title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula.</gtr:title><gtr:parentPublicationTitle>Human brain mapping</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/339bb3b877cac9698849f38a0abddae3"><gtr:id>339bb3b877cac9698849f38a0abddae3</gtr:id><gtr:otherNames>Ince RA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1065-9471</gtr:issn><gtr:outcomeId>585d33084a2e41.32521803</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6CFFFC79-23E0-459E-902D-31DCB9FEA8BE</gtr:id><gtr:title>Conference Abstract: Two opposite mechanisms for tracking speech in noise in the right frontal cortex in the presence or absence of visual speech information.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4356cfb27c4092926116c71e87f5e171"><gtr:id>4356cfb27c4092926116c71e87f5e171</gtr:id><gtr:otherNames>Giordano B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a70b3d4a69f8.20143528</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/L027534/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E793F7FE-614C-4A45-83A0-BE79B172092C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal &amp; human physiology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F439A20B-A9B0-4A68-B703-7F6AE7570E39</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems neuroscience</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>