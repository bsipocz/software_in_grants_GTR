<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/5EB8468E-0236-46A2-88A2-6994C073C814"><gtr:id>5EB8468E-0236-46A2-88A2-6994C073C814</gtr:id><gtr:name>Continental AG</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5EB8468E-0236-46A2-88A2-6994C073C814"><gtr:id>5EB8468E-0236-46A2-88A2-6994C073C814</gtr:id><gtr:name>Continental AG</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4EC6D004-D9AF-4C41-B135-015CFA50F5C9"><gtr:id>4EC6D004-D9AF-4C41-B135-015CFA50F5C9</gtr:id><gtr:name>Intelligent Ultrasound</gtr:name><gtr:address><gtr:line1>Innovation Centre</gtr:line1><gtr:line2>99 Milton Park</gtr:line2><gtr:postCode>OX14 4RY</gtr:postCode><gtr:region>South East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE6EAB1A-CC2F-4FDB-A57F-41E76D78ACAB"><gtr:id>AE6EAB1A-CC2F-4FDB-A57F-41E76D78ACAB</gtr:id><gtr:name>Oxford University Hospitals NHS Trust</gtr:name><gtr:address><gtr:line1>Churchill Site</gtr:line1><gtr:line2>Old Road</gtr:line2><gtr:line3>Headington</gtr:line3><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX3 7LJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/DDD14154-81FB-417E-81AA-AAEA066ECA04"><gtr:id>DDD14154-81FB-417E-81AA-AAEA066ECA04</gtr:id><gtr:name>The Wellcome Trust  Sanger Institute</gtr:name><gtr:address><gtr:line1>Wellcome Trust Genome Campus</gtr:line1><gtr:line2>Hinxton</gtr:line2><gtr:postCode>CB10 1SA</gtr:postCode><gtr:region>East of England</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FFD80473-A5D1-43FF-B423-C25E30E16BD4"><gtr:id>FFD80473-A5D1-43FF-B423-C25E30E16BD4</gtr:id><gtr:name>Max Planck</gtr:name><gtr:address><gtr:line1>Stuhlsatzenhausweg 85</gtr:line1><gtr:line4>Saarbruecken</gtr:line4><gtr:line5>D-66123</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0D45E022-BFAA-417F-AE38-B0F5BA451930"><gtr:id>0D45E022-BFAA-417F-AE38-B0F5BA451930</gtr:id><gtr:name>Skolkovo Inst of Sci and Tech (Skoltech)</gtr:name><gtr:address><gtr:line1>ul. Novaya</gtr:line1><gtr:line2>d.100, Karakoum Building</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/74CC97D8-B6D9-4F7A-B506-58B8EE318B69"><gtr:id>74CC97D8-B6D9-4F7A-B506-58B8EE318B69</gtr:id><gtr:name>MirriAd</gtr:name><gtr:address><gtr:line1>21 Broadwall</gtr:line1><gtr:postCode>SE1 9PL</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/63416F60-AF0C-4EF6-AA94-892EBC127714"><gtr:id>63416F60-AF0C-4EF6-AA94-892EBC127714</gtr:id><gtr:name>Yotta Ltd</gtr:name><gtr:address><gtr:line1>Yotta House</gtr:line1><gtr:line2>8 Hamilton Terrace</gtr:line2><gtr:postCode>CV32 4LY</gtr:postCode><gtr:region>West Midlands</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FAE9EDEC-6E79-43CE-B0DF-A1ABC5473AB0"><gtr:id>FAE9EDEC-6E79-43CE-B0DF-A1ABC5473AB0</gtr:id><gtr:name>GE Global Research</gtr:name><gtr:address><gtr:line1>Freisinger Landstrasse 50</gtr:line1><gtr:line4>Garching bei Muenchen</gtr:line4><gtr:line5>D-85748</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Germany</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/93EEC196-3499-4B58-AAEB-AF17754C67CF"><gtr:id>93EEC196-3499-4B58-AAEB-AF17754C67CF</gtr:id><gtr:name>Mirada Medical UK</gtr:name><gtr:address><gtr:line1>Oxford Centre for Innovation</gtr:line1><gtr:line2>New Road</gtr:line2><gtr:postCode>OX1 1BY</gtr:postCode><gtr:region>South East</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9934CE61-CA77-42D8-8BAD-58A33FA3CA38"><gtr:id>9934CE61-CA77-42D8-8BAD-58A33FA3CA38</gtr:id><gtr:name>BP British Petroleum</gtr:name><gtr:address><gtr:line1>Headquarters</gtr:line1><gtr:line2>1 St James's Square</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW1Y 4PD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B0A996FD-E1FE-4F29-AF95-D820F0CB9F95"><gtr:id>B0A996FD-E1FE-4F29-AF95-D820F0CB9F95</gtr:id><gtr:name>Qualcomm Incorporated</gtr:name><gtr:address><gtr:line1>5775 Morehouse Drive</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9"><gtr:id>F262B9E0-E9D1-4BFF-B9BF-B4C81D12D8B9</gtr:id><gtr:name>Microsoft Research Ltd</gtr:name><gtr:address><gtr:line1>21 Station Road</gtr:line1><gtr:postCode>CB1 2FB</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/04B6A2AB-0C99-425B-98F3-5F2CDFF2D434"><gtr:id>04B6A2AB-0C99-425B-98F3-5F2CDFF2D434</gtr:id><gtr:firstName>Alison</gtr:firstName><gtr:surname>Noble</gtr:surname><gtr:orcidId>0000-0002-3060-3772</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A5C0A983-76C5-48F0-81C1-0B62723C7EE9"><gtr:id>A5C0A983-76C5-48F0-81C1-0B62723C7EE9</gtr:id><gtr:firstName>Andrea</gtr:firstName><gtr:surname>Vedaldi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6D419C9B-1ACA-435A-B8B1-36FA3461BF52"><gtr:id>6D419C9B-1ACA-435A-B8B1-36FA3461BF52</gtr:id><gtr:firstName>Philip</gtr:firstName><gtr:otherNames>Hilaire</gtr:otherNames><gtr:surname>Torr</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/AE1D8AF3-9EA1-401B-976F-AE44CC6E424D"><gtr:id>AE1D8AF3-9EA1-401B-976F-AE44CC6E424D</gtr:id><gtr:firstName>Jens</gtr:firstName><gtr:surname>Rittscher</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6DB33BF0-3EA1-420D-9875-85B72E852F6A"><gtr:id>6DB33BF0-3EA1-420D-9875-85B72E852F6A</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Zisserman</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM013774%2F1"><gtr:id>A74189F1-89A9-4799-A40C-D879EC1240E2</gtr:id><gtr:title>Seebibyte: Visual Search for the Era of Big Data</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M013774/1</gtr:grantReference><gtr:abstractText>The Programme is organised into two themes. 

Research theme one will develop new computer vision algorithms to enable efficient search and description of vast image and video datasets - for example of the entire video archive of the BBC. Our vision is that anything visual should be searchable for, in the manner of a Google search of the web: by specifying a query, and having results returned immediately, irrespective of the size of the data. Such enabling capabilities will have widespread application both for general image/video search - consider how Google's web search has opened up new areas - and also for designing customized solutions for searching.
A second aspect of theme 1 is to automatically extract detailed descriptions of the visual content. The aim here is to achieve human like performance and beyond, for example in recognizing configurations of parts and spatial layout, counting and delineating objects, or recognizing human actions and inter-actions in videos, significantly superseding the current limitations of computer vision systems, and enabling new and far reaching applications. The new algorithms will learn automatically, building on recent breakthroughs in large scale discriminative and deep machine learning. They will be capable of weakly-supervised learning, for example from images and videos downloaded from the internet, and require very little human supervision.

The second theme addresses transfer and translation. This also has two aspects. The first is to apply the new computer vision methodologies to `non-natural' sensors and devices, such as ultrasound imaging and X-ray, which have different characteristics (noise, dimension, invariances) to the standard RGB channels of data captured by `natural' cameras (iphones, TV cameras). The second aspect of this theme is to seek impact in a variety of other disciplines and industry which today greatly under-utilise the power of the latest computer vision ideas. We will target these disciplines to enable them to leapfrog the divide between what they use (or do not use) today which is dominated by manual review and highly interactive analysis frame-by-frame, to a new era where automated efficient sorting, detection and mensuration of very large datasets becomes the norm. In short, our goal is to ensure that the newly developed methods are used by academic researchers in other areas, and turned into products for societal and economic benefit. To this end open source software, datasets, and demonstrators will be disseminated on the project website.

The ubiquity of digital imaging means that every UK citizen may potentially benefit from the Programme research in different ways. One example is an enhanced iplayer that can search for where particular characters appear in a programme, or intelligently fast forward to the next `hugging' sequence. A second is wider deployment of lower cost imaging solutions in healthcare delivery. A third, also motivated by healthcare, is through the employment of new machine learning methods for validating targets for drug discovery based on microscopy images</gtr:abstractText><gtr:potentialImpactText>The proposed programme encompasses new methodology and applied research in computer vision that will impact not only the imaging field, but other non-imaging disciplines, and it will encourage end-user uptake of imaging technologies and commercial interest in embedding imaging technologies in products. These are the main beneficiaries of programme research.

We have carefully chosen members of our Programme Advisory Board (PAB) and User Group to represent a comprehensive and diverse range of academic and industry interests and expect them to challenge us to ensure that the impact of the Programme is realised. We will ensure that both the PAB and the User Group are constantly refreshed with appropriate representatives.

The Programme will have Economic and Societal impact by 
1. Developing new and improved computer vision technologies for commercialisation by a wide range of companies;
2. Enhancing the Big Data capabilities and knowledge base of UK industries.
3. Enhancing quality of life by improving, for instance, healthcare capabilities, surveillance, environmental monitoring of roads, and new means of enjoying digital media in the home. Other engineering advances will aim to make a large impact &amp;quot;behind the scenes&amp;quot;, for instance to underpin better understanding of biological effects at the individual cell level and characterisation of advanced materials. 
4. Training the next generation of computer vision researchers who will be equipped to support the imaging needs of science, technology and wider society for the future; 

Impact on Knowledge includes 
1. Realisation of new approaches to essential computer vision technology, and the dissemination of research findings through publications and conference presentations and the distribution of open source software and image databases.
2. Sharing knowledge with collaborators via Transfer and Application Projects (TAPs) and other activities leading to adoption of advanced computer vision methods across many disciplines of science, engineering and medicine that currently do no use them.
3. Communication of advances to a public audience through website articles and other co-ordinated public understanding activities.</gtr:potentialImpactText><gtr:fund><gtr:end>2020-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>4466184</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Continental AG</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>Video Recognition from the Dashboard</gtr:description><gtr:id>8034FCB2-1FE4-455D-9774-82AAB53FD31E</gtr:id><gtr:impact>N/A</gtr:impact><gtr:outcomeId>58c80252d63612.44045641-1</gtr:outcomeId><gtr:partnerContribution>Supplying data.</gtr:partnerContribution><gtr:piContribution>Working with research engineers to develop recognition in road scenes and human gestures.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Metal Crystal Counting</gtr:description><gtr:id>A3A06639-F25E-420B-B142-E2FB19A0A734</gtr:id><gtr:impact>Software given to collaborator. Project paper is in progress.</gtr:impact><gtr:outcomeId>58c689f0534cf4.74993256-1</gtr:outcomeId><gtr:partnerContribution>The partner provide dataset and interpretation of the data.</gtr:partnerContribution><gtr:piContribution>We provide software algorithm.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Graphene Defect Detection</gtr:description><gtr:id>EEE82FEB-2DCF-4D3A-896B-FB3931437B5C</gtr:id><gtr:impact>Project paper is in Progress.</gtr:impact><gtr:outcomeId>58c68a983c3b09.34138690-1</gtr:outcomeId><gtr:partnerContribution>The partner provide dataset and interpretation of the computer analysis.</gtr:partnerContribution><gtr:piContribution>We provide software algorithm.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Penguin Counting</gtr:description><gtr:id>15DD1A08-0ED1-49E8-8020-8525CA25AAB6</gtr:id><gtr:impact>Paper: counting in the Wild, by Carlos Arteta, Victor Lempitsky and Andrew Zisserman.

This collaboration is between Information Engineering and Zoology disciplines.</gtr:impact><gtr:outcomeId>58c688d0a39863.60092119-1</gtr:outcomeId><gtr:partnerContribution>The collaboration provide dataset and specialised analysing methods.</gtr:partnerContribution><gtr:piContribution>We provide software algorithm.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Micrograph Defect Detection</gtr:description><gtr:id>F97796D5-F6B1-4494-A110-B9F85B6F48D3</gtr:id><gtr:impact>Software has been given to the collaborator.</gtr:impact><gtr:outcomeId>58c68b573782c6.90724923-1</gtr:outcomeId><gtr:partnerContribution>The partner provide dataset and interpretation of the computer analysis.</gtr:partnerContribution><gtr:piContribution>We provide software algorithm.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Show and Tell Event - Computer Vision Software - 14 June 2016 (Oxford)</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>9EB4B139-C0E0-4338-B2C3-DF8C51861FB5</gtr:id><gtr:impact>A main aim of the Seebibyte Project is to transfer the latest computer vision methods into other disciplines and industry. We want the software developed in this project to be taken up and used widely by people working in industry and other academic disciplines, and are organizing regular Show and Tell events to demonstrate new software developed by project researchers. A main outcome from these events will be new inter-disciplinary collaborations. As a first step, Transfer and Application Projects (TAPs) are developed with new collaborators.

This first Show and Tell event was restricted to participants from the University of Oxford only, in particular researchers from the Department of Engineering Science, the Department of Earth Sciences and the Department of Materials. Future events will also target external participants, including from industry. The June 14 event focused on four topics: 1) Counting; 2) Landmark Detection (KeyPoint Detection); 3) Segmentation (Region Labelling); and 4) Text Spotting. Further information for each of the topics - including the event presentations and new software demos - is available on the event webpage (www.seebibyte.org/June14.html). The event received a positive feedback from participants and has resulted in several new TAPs being completed. It is anticipated that some of these will lead to new collaborations.</gtr:impact><gtr:outcomeId>576a602cdd6f77.49262313</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.seebibyte.org/June14.html</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Samsung Satellitte Symposium, European Congress in Radiology</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>7F02DE1A-06B5-471F-B233-8B811C0A49B8</gtr:id><gtr:impact>Talk was part of a lunch symposium presenting latest research in AI applied to radiology</gtr:impact><gtr:outcomeId>58c2c9fee3ad78.19120098</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>MISS Summer School</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B490F413-E578-450B-9BA6-1C0A5C45A24C</gtr:id><gtr:impact>Invited lecturer at international summer school.</gtr:impact><gtr:outcomeId>58c2ca999291c0.41286227</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Teaching in Summer School iV&amp;L</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>58929836-86E6-4F0E-A461-C99157EA7054</gtr:id><gtr:impact>The iV&amp;amp;L Training School aims at bringing together Vision and Language researchers and to provide the opportunity for cross-disciplinary teaching and learning. Over 80 students attended the summer school and received training in deep learning across two disciplines, Computer Vision and Natural Language Processing. Students expressed interest in future research in the area.</gtr:impact><gtr:outcomeId>58c8082dbc03a9.38171660</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://ivl-net.eu/ivl-net-training-school-2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Microsoft Postgraduate Summer School</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>8C989868-2457-412E-B74D-76D379CA2798</gtr:id><gtr:impact>Invited talk at Microsoft Summer school</gtr:impact><gtr:outcomeId>58c2cb33c4cf58.45430717</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Teaching in Summer School MISS</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>888C371D-3C8D-404D-82C2-D960834FD6B7</gtr:id><gtr:impact>The Medical Imaging Summer School is the largest summer school in its field. Around 200 students attended the school and received training in the science and technology of medical imaging. Students expressed interest in future research in the area.</gtr:impact><gtr:outcomeId>58c806b2260260.02999058</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://iplab.dmi.unict.it/miss/index.html</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Teaching in Summer School ICVSS</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>7714B11A-42FE-41A3-A9CF-54E21AC3EE99</gtr:id><gtr:impact>This International Computer Vision Summer School aims to provide both an objective and clear overview and an in-depth analysis of the state-of-the-art research in Computer Vision and Machine Learning. The participants benefited from direct interaction and discussions with world leaders in Computer Vision.</gtr:impact><gtr:outcomeId>58c80a78b5f9c6.39618436</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://iplab.dmi.unict.it/icvss2015/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Keynote speaker</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>DAFCEA11-7919-49C9-974B-8838034B204A</gtr:id><gtr:impact>Over 230 Academics and Industry Experts attended MEIbioeng 16 to meet, share, debate and learn from their peers.

The annual conference supported the discussion of newly developing Biomedical Engineering research areas alongside established work that contribute towards the common goal of improving human health and well-being via development of new healthcare technologies.</gtr:impact><gtr:outcomeId>58c691cb252da6.54742047</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.ibme.ox.ac.uk/news-events/events/meibioeng-16</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2500000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>ERC Advanced Grant</gtr:description><gtr:end>2021-10-02</gtr:end><gtr:fundingOrg>European Research Council (ERC)</gtr:fundingOrg><gtr:id>47604FE4-2072-4B4A-B515-50EED8A58407</gtr:id><gtr:outcomeId>58c2c412a436a7.25143604</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>55000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Oxford Martin School</gtr:department><gtr:description>Big Data Science in Medicine and Healthcare</gtr:description><gtr:end>2020-03-02</gtr:end><gtr:fundingOrg>University of Oxford</gtr:fundingOrg><gtr:id>4558D09E-63A1-4799-AE1E-F472D565D0A9</gtr:id><gtr:outcomeId>58c6904183ec92.76394233</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>308823</gtr:amountPounds><gtr:country>Germany, Federal Republic of</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Visual Recognition</gtr:description><gtr:end>2019-04-02</gtr:end><gtr:fundingOrg>Continental AG</gtr:fundingOrg><gtr:id>6B14F686-BD0F-4763-BE66-CBC805649048</gtr:id><gtr:outcomeId>58c68f1d3340f3.17417281</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2016-11-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our two-stream approach of basic research and dissemination seems working. On the first, we are publishing our research in the principal conferences and winning prizes. We conduct dissemination by engaging other communities through our Show-and-Tell events and Transfer Application Projects. We also make available our software and publications that have emerged from our research.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>9469C009-0E61-4CC0-B4FE-6CD5066A1EA7</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d6be52725892.95629001</gtr:outcomeId><gtr:sector>Healthcare,Culture, Heritage, Museums and Collections,Retail,Transport</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset contains over 38k images of celebrities in different types of scenes. There are 4611 celebrities and 16 places involved. The images were obtained using Google Image Search and verified by human annotation.</gtr:description><gtr:id>39ADF24E-F536-446D-80D5-FE77EE03FBD6</gtr:id><gtr:impact>Publications have resulted from this research based on this dataset.
Y. Zhong, R. Arandjelovic, A. Zisserman
Faces in Places: Compound Query Retrieval 
British Machine Vision Conference, 2016</gtr:impact><gtr:outcomeId>58c7aebe255c69.26293486</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Celebrity in Places Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/data/celebrity_in_places/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>This is a synthetically generated dataset, in which word instances are placed in natural scene images, while taking into account the scene layout.
The dataset consists of 800 thousand images with approximately 8 million synthetic word instances. Each text instance is annotated with its text-string, word-level and character-level bounding-boxes</gtr:description><gtr:id>59128662-85C2-4F87-9814-3680B010DF05</gtr:id><gtr:impact>A publication has resulted from this research:

A. Gupta, A. Vedaldi, A. Zisserman
Synthetic Data for Text Localisation in Natural Images 
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016</gtr:impact><gtr:outcomeId>58c7af27f24304.04464570</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Text Localisation Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/data/scenetext</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset consists of up to 1000 utterances of 500 different words, spoken by hundreds of different speakers. All videos are 29 frames (1.16 seconds) in length, and the word occurs in the middle of the video.</gtr:description><gtr:id>E5F118AD-58E6-44F8-ACC4-E0C5D7CCB6F9</gtr:id><gtr:impact>Publications have resulted form this research and an award has been won:
[1] J. S. Chung, A. Zisserman
Lip Reading in the Wild - Best Student Paper Award
Asian Conference on Computer Vision, 2016

[2] J. S. Chung, A. Zisserman
Out of time: automated lip sync in the wild 
Workshop on Multi-view Lip-reading, ACCV, 2016</gtr:impact><gtr:outcomeId>58c7afb87703c7.60418937</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>BBC-Oxford Lip Reading Dataset</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/data/lip_reading/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This is an sudio-to-video synchronisation network which can be used for audio-visual synchronisation tasks including: 
(1) removing temporal lags between the audio and visual streams in a video, and
(2) determining who is speaking amongst multiple faces in a video.</gtr:description><gtr:id>E2F49B78-F75D-44EE-BF6B-71332D8B7908</gtr:id><gtr:impact>A publication has resulted from this research:
J. S. Chung, A. Zisserman
Out of time: automated lip sync in the wild 
Workshop on Multi-view Lip-reading, ACCV, 2016</gtr:impact><gtr:outcomeId>58c7b9166a5728.36663569</gtr:outcomeId><gtr:title>Lip Synchronisation</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/lipsync/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>It is a model based on convolution neural network to automatically detect keypoints (like head, elbow, ankle, etc.) in a photograph of a human body.</gtr:description><gtr:id>38912897-2EB0-42B4-B6D0-FD6456C9D266</gtr:id><gtr:impact>A paper has resulted from this research:

V. Belagiannis, A. Zisserman
Recurrent Human Pose Estimation 
arXiv:1605.02914</gtr:impact><gtr:outcomeId>58c7b570ca5e42.93608001</gtr:outcomeId><gtr:title>Convnet Keypoint Detection</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/keypoint_detection/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>VGG Image Annotator is a standalone application, with which you can define regions in an image and create a textual description of those regions. Such image regions and descriptions are useful for supervised training of learning algorithms.</gtr:description><gtr:id>7921B63C-9345-4300-A209-CAAD87E10AD6</gtr:id><gtr:impact>The VIA tool has been employed to annotate large volume of scanned images of 15th Century books in the Faculty of Medieval and Modern Languages in the University of Oxford for the 15th Century Booktrade project (http://15cbooktrade.ox.ac.uk/).</gtr:impact><gtr:outcomeId>58c7b7b7d6f2d9.18410881</gtr:outcomeId><gtr:title>VGG Image Annotator</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/via/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>MatConvNet is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs. Many pre-trained CNNs for image classification, segmentation, face recognition, and text detection are available.</gtr:description><gtr:id>306B2034-77BD-41AE-993C-D1A7C8F6699E</gtr:id><gtr:impact>The MatConvNet toolbox is widely employed in the researches conducted by researchers in the Visual Geometry Group in the University of Oxford including Text Spotting, Penguin Counting and Human Action Recognition.

The researcher Andrea Vedaldi has taught this software in following Summer Schools:
Medical Imaging Summer School (MISS), Favignana (Sicily), 2016:
(Somewhat) Advanced Convolutional Neural Networks [slides];
Understanding CNNs using visualisation and transformation analysis [slides];
All video lectures from the summer school.
iV&amp;amp;L Net Training School 2016. Malta.</gtr:impact><gtr:outcomeId>58c7baf4c88380.76681786</gtr:outcomeId><gtr:title>MatConvNet</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.vlfeat.org/matconvnet/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This is a model based on convolution neural network to automatically detect English text in a natural images</gtr:description><gtr:id>C1C12C65-8DD5-4FD6-84E1-88C103DB124A</gtr:id><gtr:impact>A publication has resulted from this research:

A. Gupta, A. Vedaldi, A. Zisserman
Synthetic Data for Text Localisation in Natural Images 
IEEE Conference on Computer Vision and Pattern Recognition, 2016</gtr:impact><gtr:outcomeId>58c7b4d1b9d2f7.30981958</gtr:outcomeId><gtr:title>Convnet text spotting</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/textspot/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>This is a model to recognize human actions in video.</gtr:description><gtr:id>7089978A-77FD-42C5-8F58-A26EED642632</gtr:id><gtr:impact>A publication has resulted from this research:

Convolutional Two-Stream Network Fusion for Video Action Recognition. 
C. Feichtenhofer, A. Pinz, A. Zisserman, CVPR, 2016.</gtr:impact><gtr:outcomeId>58c7b62805f086.93248360</gtr:outcomeId><gtr:title>Convnet Human Action Recognitio</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/~vgg/software/two_stream_action/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2E4DBE3C-2C92-4D3A-A5CA-BEA97FD0EFB2</gtr:id><gtr:title>Convolutional Two-Stream Network Fusion for Video Action Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ab932b707301bccd6ede2f15b297db4d"><gtr:id>ab932b707301bccd6ede2f15b297db4d</gtr:id><gtr:otherNames>Feichtenhofer C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>573c3360e2f090.39637069</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FEDB1816-C7B3-4082-96DC-0772B0DBB32B</gtr:id><gtr:title>Weakly Supervised Deep Detection Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6281cea112e6c2e99b26ef42f36ffffd"><gtr:id>6281cea112e6c2e99b26ef42f36ffffd</gtr:id><gtr:otherNames>Bilen, H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd8daca31e96.26488732</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3348C26-65A6-4CA7-9F42-D7DC64E12D0E</gtr:id><gtr:title>Human pose search using deep networks</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a1bb371945e96131313cf8063e55ca3e"><gtr:id>a1bb371945e96131313cf8063e55ca3e</gtr:id><gtr:otherNames>Jammalamadaka N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe45bf2e553.34371231</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F5842BDB-EF10-4FDB-8626-C53764A5F537</gtr:id><gtr:title>Fully-automated alignment of 3D fetal brain ultrasound to a canonical reference space using multi-task learning.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d5625d7cea04cfc52a4857b93193bad0"><gtr:id>d5625d7cea04cfc52a4857b93193bad0</gtr:id><gtr:otherNames>Namburete AIL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>5a9bf39f7aa381.84349697</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>04439593-B78F-46D3-A905-7F92DE877CB5</gtr:id><gtr:title>A framework for analysis of linear ultrasound videos to detect fetal presentation and heartbeat.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aafe7b2e3227e8456e5e1650b9b73a45"><gtr:id>aafe7b2e3227e8456e5e1650b9b73a45</gtr:id><gtr:otherNames>Maraci MA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>58c2d45e49bec2.53709363</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CAA161B9-EFA1-418F-A080-45A13ED3F86C</gtr:id><gtr:title>I Have Seen Enough: Transferring Parts Across Categories</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f807007e32572d613ac51d95ffe25757"><gtr:id>f807007e32572d613ac51d95ffe25757</gtr:id><gtr:otherNames>Novotny D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd9254d1fb73.68225537</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FCB01C8-FBA2-4B14-8231-4881F7397864</gtr:id><gtr:title>Synthetic Data for Text Localisation in Natural Images</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2647108e79ca174352f02bf675cba5e9"><gtr:id>2647108e79ca174352f02bf675cba5e9</gtr:id><gtr:otherNames>Gupta A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd91d891fed4.20691265</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25D44012-F33C-4B88-93FA-A2290EF18AEC</gtr:id><gtr:title>Recurrent Human Pose Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e914a1debcc62b4602b5e16bd06d03f"><gtr:id>6e914a1debcc62b4602b5e16bd06d03f</gtr:id><gtr:otherNames>Belagiannis V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58be7ffa7f5051.88075845</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>300BA8E0-2E27-4C5B-8AEA-6D9E00CB3520</gtr:id><gtr:title>Multi-task Convolutional Neural Network for Patient Detection and Skin Segmentation in Continuous Non-contact Vital Sign Monitoring</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d7e12942dafe2ceee6abf33286f896b"><gtr:id>5d7e12942dafe2ceee6abf33286f896b</gtr:id><gtr:otherNames>Chaichulee, S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58be807bd299b5.25970729</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C62FD389-5497-48EB-8CD8-F66383F0C164</gtr:id><gtr:title>Synthetic Data for Text Localisation in Natural Images</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2647108e79ca174352f02bf675cba5e9"><gtr:id>2647108e79ca174352f02bf675cba5e9</gtr:id><gtr:otherNames>Gupta A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>573c33fa519068.73253748</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6DF661D2-4016-4801-92C0-B6E63CAA7D8B</gtr:id><gtr:title>SpineNet: Automated classification and evidence visualization in spinal MRIs.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9d1dfbadd1a4dce5e3bc30871d758fc4"><gtr:id>9d1dfbadd1a4dce5e3bc30871d758fc4</gtr:id><gtr:otherNames>Jamaludin A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>5a2fe32a67e367.90795024</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CA7BC401-BF07-4EFD-8E64-6F2FAB41AE29</gtr:id><gtr:title>Microscopy cell counting and detection with fully convolutional regression networks</gtr:title><gtr:parentPublicationTitle>Computer Methods in Biomechanics and Biomedical Engineering: Imaging &amp; Visualization</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2822ecb2ba9c0b6217bfd001aa570c4b"><gtr:id>2822ecb2ba9c0b6217bfd001aa570c4b</gtr:id><gtr:otherNames>Xie W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d3413e2aa04.72465493</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6E003EA-27D7-447E-8CEA-ABDE0115A04A</gtr:id><gtr:title>Automated annotation and quantitative description of ultrasound videos of the fetal heart.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7df074e696f6e7034079c3fa38ac16f3"><gtr:id>7df074e696f6e7034079c3fa38ac16f3</gtr:id><gtr:otherNames>Bridge CP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>58c2d45e1dadb9.36999688</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7C258345-96B0-4EA2-B855-66ED0D1B7161</gtr:id><gtr:title>Counting in The Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cf94ee81ecb0cf461eb7d2e240704a4d"><gtr:id>cf94ee81ecb0cf461eb7d2e240704a4d</gtr:id><gtr:otherNames>Arteta, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b59512690b63.82959694</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>807AD6B8-8C5C-44DE-B844-B895F4C9A3C0</gtr:id><gtr:title>Lip Reading in the Wild</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d80ddce33601108e0f0f3076a6603a6a"><gtr:id>d80ddce33601108e0f0f3076a6603a6a</gtr:id><gtr:otherNames>Chung, J.S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd8e1508cbc8.23021398</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3C631AD9-D098-496D-BFA7-01F2847B2937</gtr:id><gtr:title>Reflections on ultrasound image analysis.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/82dffc6eb9b52b876d98349f4a23c3b7"><gtr:id>82dffc6eb9b52b876d98349f4a23c3b7</gtr:id><gtr:otherNames>Alison Noble J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>58c2d265b45f46.20347317</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1BB296E0-A89F-4F55-9E4C-C21D852D1A0B</gtr:id><gtr:title>Microscopy Cell Counting with Fully Convolutional Regression Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/45ffb461c28d78d63a18ba3450e3bc25"><gtr:id>45ffb461c28d78d63a18ba3450e3bc25</gtr:id><gtr:otherNames>Xie, W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cc27ba7f5012.00804810</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21C6223B-6710-4E77-AFCA-29FC09F1F265</gtr:id><gtr:title>SpineNet: Automatically Pinpointing Classification Evidence in Spinal MRIs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b6150d591748b3d6c309205a7229fe6"><gtr:id>7b6150d591748b3d6c309205a7229fe6</gtr:id><gtr:otherNames>Jamaludin, A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>577a7e9f9ac702.71610237</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>41B963CC-16D4-4504-9EB0-27A9E7E30CFC</gtr:id><gtr:title>Analysis of live cell images: Methods, tools and opportunities.</gtr:title><gtr:parentPublicationTitle>Methods (San Diego, Calif.)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/55d7cc1b19d764013773ece717c7fe9d"><gtr:id>55d7cc1b19d764013773ece717c7fe9d</gtr:id><gtr:otherNames>Nketia TA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1046-2023</gtr:issn><gtr:outcomeId>58cabc6e74fc51.62520207</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D7B698CF-B219-4660-89C1-7FC3D197FF25</gtr:id><gtr:title>Efficient minimization of higher order submodular functions using monotonic Boolean functions</gtr:title><gtr:parentPublicationTitle>Discrete Applied Mathematics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eb36c3cd67f98536e9819f1d6798f7e9"><gtr:id>eb36c3cd67f98536e9819f1d6798f7e9</gtr:id><gtr:otherNames>Ramalingam S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe4d02ef8d5.26559621</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>49EA91AD-8134-4423-9652-5B7E868C5133</gtr:id><gtr:title>Clinical and molecular consequences of disease-associated de novo mutations in SATB2.</gtr:title><gtr:parentPublicationTitle>Genetics in medicine : official journal of the American College of Medical Genetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05839e074e7c4e7051cc77c710f184f4"><gtr:id>05839e074e7c4e7051cc77c710f184f4</gtr:id><gtr:otherNames>Bengani H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1098-3600</gtr:issn><gtr:outcomeId>5a35e35e7baf70.81154405</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F964B249-111F-43F7-BEE0-3F420F048D00</gtr:id><gtr:title>3D Shape Attributes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ba2bd458aa0599d88d04cad61d7a1fda"><gtr:id>ba2bd458aa0599d88d04cad61d7a1fda</gtr:id><gtr:otherNames>Fouhey D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>573c33b0428497.77428478</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>079CFA83-F3EE-4527-854B-B23681B47100</gtr:id><gtr:title>ISSLS PRIZE IN BIOENGINEERING SCIENCE 2017: Automation of reading of radiological features from magnetic resonance images (MRIs) of the lumbar spine without human intervention is comparable with an expert radiologist.</gtr:title><gtr:parentPublicationTitle>European spine journal : official publication of the European Spine Society, the European Spinal Deformity Society, and the European Section of the Cervical Spine Research Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9d1dfbadd1a4dce5e3bc30871d758fc4"><gtr:id>9d1dfbadd1a4dce5e3bc30871d758fc4</gtr:id><gtr:otherNames>Jamaludin A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0940-6719</gtr:issn><gtr:outcomeId>58be8415b73109.70003928</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25CA870B-9DF2-43DF-9D49-E68E7B5B48C2</gtr:id><gtr:title>Faces in Places: Compound Query Retrieval</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0bb0deeeec4bee40b70824ee22d01400"><gtr:id>0bb0deeeec4bee40b70824ee22d01400</gtr:id><gtr:otherNames>Zhong Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58bd92db311802.10836190</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M013774/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A759BB04-AFFE-4780-BD31-9A2707BC44BA</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Medical Imaging</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>