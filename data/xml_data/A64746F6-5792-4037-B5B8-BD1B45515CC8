<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/63E77D4B-1BBF-4E12-B781-BB8D0C93BA1A"><gtr:id>63E77D4B-1BBF-4E12-B781-BB8D0C93BA1A</gtr:id><gtr:name>Catalonia Institute for Energy Research (IREC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A392302C-02C7-4EB9-B279-3B3436877EFA"><gtr:id>A392302C-02C7-4EB9-B279-3B3436877EFA</gtr:id><gtr:name>National Gallery, London</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3B1E5961-8677-4A32-B15C-12FB968FA163"><gtr:id>3B1E5961-8677-4A32-B15C-12FB968FA163</gtr:id><gtr:name>University of Pennsylvania</gtr:name><gtr:address><gtr:line1>3451 Walnut Street,</gtr:line1><gtr:line2>Suite P-221</gtr:line2><gtr:line3>Franklin Building</gtr:line3><gtr:line4>Philadelphia</gtr:line4><gtr:postCode>PA 19104</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/5E2B04DD-4A03-45ED-9892-61C5CCB8AC68"><gtr:id>5E2B04DD-4A03-45ED-9892-61C5CCB8AC68</gtr:id><gtr:name>Newcastle University</gtr:name><gtr:department>Institute of Neuroscience</gtr:department><gtr:address><gtr:line1>1 Park Terrace</gtr:line1><gtr:line4>Newcastle Upon Tyne</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>NE1 7RU</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5E2B04DD-4A03-45ED-9892-61C5CCB8AC68"><gtr:id>5E2B04DD-4A03-45ED-9892-61C5CCB8AC68</gtr:id><gtr:name>Newcastle University</gtr:name><gtr:address><gtr:line1>1 Park Terrace</gtr:line1><gtr:line4>Newcastle Upon Tyne</gtr:line4><gtr:line5>Tyne and Wear</gtr:line5><gtr:postCode>NE1 7RU</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/63E77D4B-1BBF-4E12-B781-BB8D0C93BA1A"><gtr:id>63E77D4B-1BBF-4E12-B781-BB8D0C93BA1A</gtr:id><gtr:name>Catalonia Institute for Energy Research (IREC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A392302C-02C7-4EB9-B279-3B3436877EFA"><gtr:id>A392302C-02C7-4EB9-B279-3B3436877EFA</gtr:id><gtr:name>National Gallery, London</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3B1E5961-8677-4A32-B15C-12FB968FA163"><gtr:id>3B1E5961-8677-4A32-B15C-12FB968FA163</gtr:id><gtr:name>University of Pennsylvania</gtr:name><gtr:address><gtr:line1>3451 Walnut Street,</gtr:line1><gtr:line2>Suite P-221</gtr:line2><gtr:line3>Franklin Building</gtr:line3><gtr:line4>Philadelphia</gtr:line4><gtr:postCode>PA 19104</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E07FDFB3-EE00-4C11-B570-773B13A3EDCD"><gtr:id>E07FDFB3-EE00-4C11-B570-773B13A3EDCD</gtr:id><gtr:name>International Commission on Illumination</gtr:name><gtr:address><gtr:line1>CIE Central Bureau</gtr:line1><gtr:line2>Kegelgasse 27</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Austria</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/72AE482E-97F3-4497-AF43-5D48736CC8B4"><gtr:id>72AE482E-97F3-4497-AF43-5D48736CC8B4</gtr:id><gtr:name>Imsense Ltd</gtr:name><gtr:address><gtr:line1>Norwich Bio-Incubator</gtr:line1><gtr:line2>John Innes Centre</gtr:line2><gtr:line3>Colney Lane</gtr:line3><gtr:postCode>NR4 7UH</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B5B63714-9B3A-45B0-A1CD-92304D1ABBFB"><gtr:id>B5B63714-9B3A-45B0-A1CD-92304D1ABBFB</gtr:id><gtr:firstName>Anya</gtr:firstName><gtr:surname>Hurlbert</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH022325%2F1"><gtr:id>A64746F6-5792-4037-B5B8-BD1B45515CC8</gtr:id><gtr:title>Illuminating Colour Constancy: from Physics to Photography</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H022325/1</gtr:grantReference><gtr:abstractText>In daily life, we depend on colour images which represent the real world, from photographs of key personal events to pictures of possible purchases. In general, these are poor approximations of the real thing. Our aim is to understand better how we perceive colours in the real world, and how to recreate that perception with images. Central to these aims is colour constancy, a fundamental phenomenon which keeps object colours stable even under large changes in the colour of the illumination - we see an apple as red whether it is under bluish daylight or yellowish tungsten light. Camera sensors, which faithfully record the changing light signals, do not naturally possess colour constancy. But digital cameras are often equipped with special colour balancing modules to cope with changes in lighting, and the photographs they produce may be further processed to remove colour casts. In computer vision, such 'color correction' algorithms are necessary to enable machines to use colour as a reliable cue - for example, in automated grading of manufactured goods such as tiles. Human vision and computer vision are typically studied in isolation from each other: the first aims to understand why colours appear as they do to humans, and the other to make them as useful as possible to machines, regardless of how they appear. These two goals are generally not identical, because neither human nor computer colour constancy is perfect.To bridge colour constancy from humans to machines we will perform an innovative set of experiments. First, we will systematically study illuminant metamerism. Metamerism is what makes all image reproduction work: two stimuli with vastly different colour spectra can induce the same colour percept. The light invoking a white percept on a TV has a highly spiky spectrum compared to the flat spectral reflectance of a piece of white paper in daylight. Yet, illuminants which look the same when shining on white paper can sometimes make other surfaces change appearance. We experience this phenomenon when we buy clothes which look good under the artificial shop lights but less satisfactory when we take them outdoors. We will quantify this effect for real scenes under real lights using a new 'tuneable' spectral illuminator with which we can generate any light spectrum. Our second innovation is to make use of newly available High-Dynamic-Range (HDR) displays. In contradistinction to the real world where the brightest point in the scene may be a 100000 times as bright as the darkest point, most displays struggle to produce a dynamic range of even 1000:1 and printed photographs are at most 100:1. Yet we know that colour perception depends on the overall dynamic range of the scene. The new HDR displays can output contrast ratios of 100000:1 and we will use them to measure constancy in lab conditions but with real world brightnesses. A third challenge that we face in making colour photographs match our perception of the real world is the inaccuracy of colour memory. Typically, when we view a photograph, we do not have the real thing to compare it with, but must recall the original scene from memory. The imperfections of our memory then may taint our judgment. It is well known that our memory colours for familiar objects such as sky, grass, and skin tend to be 'over-saturated' -- grass may be remembered as greener and the sky as bluer than they actually are. Thus, when we test colour correction algorithms by asking people which image they prefer, we might find that they do not prefer the one that most accurately reproduces the original scene, but instead matches their imperfect memory. We will quantify these effects of memory and preference. Finally, our research will, at all stages, consider how measured percepts of colour might be predicted by mathematical models. Ultimately, we will design algorithms to automatically see colours as we do, making for better photographs and more useful vision machines.</gtr:abstractText><gtr:fund><gtr:end>2014-11-17</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-05-17</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>136093</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Pennsylvania</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>School of Arts and Sciences</gtr:department><gtr:description>Collaboration with Brainard Lab</gtr:description><gtr:id>B6C7CDE2-FD6C-4635-B741-D00134EDF060</gtr:id><gtr:impact>Radonjic&amp;acute;, A., Pearce, B., Aston, S., Krieger, A., Dubin, H., Cottaris, N. P., Brainard, D. H., &amp;amp; Hurlbert, A. C. (2016). Illumination discrimination in real and simulated scenes. Journal of Vision, 16(11):2, 1-18, doi:10.1167/16.11.2.

Brainard, David H and Hurlbert, Anya C. (2015) Colour Vision: Understanding #TheDress. Current Biology 25 (13):R551-4.</gtr:impact><gtr:outcomeId>58c85a2044f3f1.86467208-1</gtr:outcomeId><gtr:partnerContribution>The collaboration involves exchange of research ideas, techniques, and analysis, from both sides.</gtr:partnerContribution><gtr:piContribution>The collaboration involves exchange of research ideas, techniques, and analysis, from both sides.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>National Gallery, London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>National Gallery</gtr:description><gtr:id>1FD67A70-B6FF-43AA-B803-6C6BA57E58FC</gtr:id><gtr:impact>Outputs are in preparation.</gtr:impact><gtr:outcomeId>5464cb7087b1d5.86086888-1</gtr:outcomeId><gtr:partnerContribution>Collaboration in design, installation and running of interactive film and experiment as part of Making Colour exhibition.</gtr:partnerContribution><gtr:piContribution>Collaboration in creating and delivering public exhibition at the National Gallery.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Catalonia Institute for Energy Research (IREC)</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:description>EU FP7 HI-LED (ICT STREP Award) Consortium</gtr:description><gtr:id>3ED2F65A-7606-4AA1-84F6-99ED671B1575</gtr:id><gtr:impact>Outputs include (1) Interactive installation and exhibit as part of Making Colour, the summer 2014 exhibition at the National Gallery; (2) conference presentations and proceedings papers on the visual and non-visual effects of spectral variations in light; (3) other publications in preparation.

The collaboration is multi-disciplinary, involving: photonics and optics; horticulture; conservation science; lighting technology; neuroscience, biomedicine, and visual perception.</gtr:impact><gtr:outcomeId>58beff28ad59b1.00031313-1</gtr:outcomeId><gtr:partnerContribution>HI-LED partners and their contributions include: 
IREC (Barcelona) - development of multi-channel LED spectrally tuneable light engine
University of Pannonia - assessment of spectrally tuneable illumination for museum lighting
University of Wageningen - improving plant growth and nutrient content by spectrally tuning the illumination
IRTA - improving plant growth and nutrient content by spectrally tuning the illumination
LPI - optical design of lamps
Fraunhofer - design of light engines
Hortilux - improving plant growth and nutrient content by spectrally tuning the illumination</gtr:partnerContribution><gtr:piContribution>Contributions to the application of smart lighting for human health and performance. Demonstration of effects of varying spectral power distribution of illumination on mood, cognition, alertness, perception and physiological function. 
Expertise in behavioural and psychophysical studies; in spectroradiometry, colorimetry and visual perception; neuroscience and biomedicine.
Contributions included training of postdoctoral staff, access to behavioural and psychophysical testing facilities, and collaborations with other partners including the National Gallery.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Richard Gregory Memorial Lecture</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>AF133C98-05FE-403E-9F87-EB9BF1832C38</gtr:id><gtr:impact>Public lecture on colour perception. Demonstration of effects of spectrally tuneable lighting.</gtr:impact><gtr:outcomeId>58c85b78a0b211.47559231</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bristol.ac.uk/news/2015/october/richard-gregory-memorial-lecture.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Centre for Life Public Lecture</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>61118169-E706-4859-A606-9128C64D18FE</gtr:id><gtr:impact>Public lecture on colour perception</gtr:impact><gtr:outcomeId>58c85c612c7464.82181476</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.life.org.uk/news/renowned-neuroscience-professor-at-life</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Making Colour Exhibition</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D5A0F590-A7BA-4E97-9783-2B81C4733261</gtr:id><gtr:impact>The summer exhibition was attended by over 60,000 individuals from the UK and abroad. Most of these will have visited our particular exhibit in the exhibition. This exhibit presented a short film on colour perception and on the novel use of tuneable LED illumination. It included interactive experiments designed to examine aspects of colour perception. Approx 11,000 individuals contributed complete data sets to the experiments, which we are now analysing and will publish both on the website and in journals. Also associated with this exhibition were several talks including one to ~300 individuals.

Embedded in the interactive experiment were questions which assessed knowledge and interest in the topic; the data from these are being analysed.
Media coverage of the exhibit was extensive. 
http://www.theguardian.com/artanddesign/2014/jun/17/making-colour-review-national-gallery
http://www.standard.co.uk/goingout/exhibitions/making-colour-national-gallery--exhibition-review-9555847.html
http://www.economist.com/blogs/prospero/2014/07/making-colour-national-gallery
http://kidsinmuseums.org.uk/2014/07/17/making-colour-national-gallery/
http://www.bbc.co.uk/programmes/b046kybtInternet, etc 

http://www.bbc.co.uk/news/entertainment-arts-27884975
http://www.ft.com/cms/s/2/cbbefc5a-fc37-11e3-98b8-00144feab7de.html#axzz3GssJ6X1m
http://www.luxreview.com/news/365/national-gallery-tests-how-leds-affect-colour-perception
http://www.commercial-lamps.co.uk/news/news/national-gallery-tests-how-leds-affect-colour-perception/
http://light-bulbs-direct.co.uk/2014/07/21/the-national-gallery-tests-how-light-affects-art/
http://www.lrb.co.uk/v36/n14/charles-hope/at-the-national-gallery
http://www.telegraph.co.uk/culture/art/art-reviews/10908737/Making-Colour-National-Gallery-review-a-welcome-refresher.html
http://www.ft.com/cms/s/2/02c01722-f542-11e3-afd3-00144feabdc0.html#axzz3GssJ6X1m</gtr:impact><gtr:outcomeId>5464cf9d54d1e3.73395924</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.nationalgallery.org.uk/making-colour</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Spectrum of Science</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>00CDF3BA-B3BA-4DCC-9EC9-6463CA258E7B</gtr:id><gtr:impact>Explanation and demonstrations of colour constancy, in episode 3 of &amp;quot;Colour: The Spectrum of Science&amp;quot;, shown on BBC Four, 2015 (and repeated in 2016).</gtr:impact><gtr:outcomeId>58c85d2fce48c8.64848489</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/articles/dcGXtCmtyklClYgYgJHKG9/is-the-colour-you-see-the-same-as-the-colour-i-see-how-i-explored-the-hidden-stories-of-our-planet-s-colours</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The new behavioural paradigm and methods for generating changing illuminations in real-time with tuneable multi-channel LED technology formed the basis of and were directly deployed in a multi-media interactive installation on human colour perception, shown as an integral part of the National Gallery's 2014 exhibition 'Making Colour', the first in Britain to chart the 700-year material history of colour.

The multi-media installation took the visiting public beyond the scientific and historical examination of materials used by artists to create colour and involved them in an understanding of how the human brain itself creates colour through visual perception. An interactive exhibit introduced visitors to the complex mechanisms by which our brains perceive and interpret colour, via a film drawing its content from human biology, neuroscience, psychology and physiology. The audience further contributed to a live interactive perceptual experiment, which not only illustrated the scientific concepts but also contributed to continuing academic research in this important field.

The installation at the National Gallery was additionally funded by a People award from the Wellcome Trust.

Further details:A custom installation was successfully constructed within the National Gallery Sainsbury Wing small cinema as the final room of the 2014 Making Colour exhibition. This installation was composed of a specially produced short film, two custom produced art works and a set of computer controlled spectrally tuneable light sources. The short film introduced the fundamental principles of how humans perceive colour, from the eye to brain. Interlaced within the film were a set of perceptual experiments, using the pair of displayed artworks illuminated by spectrally tuneable light sources (LEDMOTIVE/HI-LED). The experiments enabled the audience to partake in experimental measurements of colour perception, in particular colour constancy and illumination preference. These experimental paradgims were first developed and tested in the laboratory in the initial phases of the EPSRC-funded project &amp;quot;Illuminating Colour Constancy&amp;quot;. They required precisely timed pre-programmed changes in illumination delivered by the tuneable light sources, which also provided the sole illumination in the cinema. A voice-over instructed the audience to input their responses via electronic clickers attached to the cinema seats. The audience responded to questions including &amp;quot;What colour is the object?&amp;quot; and &amp;quot;Which illumination do you prefer?&amp;quot; in reference to two versions of Gauguin's Bowl of Fruit and Tankard before a Window, one a contemporary painted copy and the other a 12-ink print. 

Of the 65,000 visitors to Making Colour, at least 1 in 6 took part in the interactive exhibit, recording their responses to 5 main experimental questions. The results from this massive data collection, together with the original film and a recording of the actual experiments, will be posted on the National Gallery website in 2016. The exhibition received extensive media coverage, in print, online and on radio, with the interactive installation called &amp;quot;clever&amp;quot; (The Economist), &amp;quot;one to watch&amp;quot; (artlyst), and &amp;quot;cutting-edge&amp;quot; (the BBC).

The impact of this interactive demonstration to an international audience in a highly public venue is both economic and cultural. The economic impact is via the demonstration of energy-efficient LED technology for use in museums and other public venues. The cultural impact is via the demonstration of technological innovation to achieve new perspectives on cultural artifacts.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>3336ADAF-E8F3-4EA1-A1E3-F525762DDB29</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56e18b7ea56554.79312894</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project, Illuminating Colour Constancy: from Physics to Photography, examined colour constancy from the joined perspectives of human and computer vision. 
Colour constancy is the phenomenon of human vision which enables people to perceive stable surface colours despite changes in the prevailing light colour. Colour constancy is a fundamental cornerstone of perception, and hence is of deep interest to a large research community, from visual neuroscience and psychology to graphics and computer vision. But it poses an important engineering problem for the design of devices that produce colour images -- in many domains, from digital cameras, television, remote sensing, displays, printing and industrial inspection - and for the design of lighting in many settings, from museum and retail displays to the home and office. 
Given the ubiquity of colour images in everyday life, used for professional, commercial, and personal purposes to convey information on a variety of levels, it is clear that colour constancy is also a vital feature of everyday life. The enormous interest in #thedress, the internet phenomenon in which people disagreed over the colour of a dress photographed under ambiguous lighting, demonstrates the impact of colour constancy - and its failure. 
Our aims in this project were first, to gain a better understanding of the extent of human colour constancy in real-world conditions, or, in other words, to understand the ways in which human colour constancy is imperfect; and second, to make better algorithms for colour constancy, by better matching human perception.
Our main achievements and developments in this project are:
1. The design and deployment of a novel method for measuring human colour constancy in real-world conditions, using real scenes and real illuminations. We have shown that this method - the illumination discrimination task - yields robust results in both laboratory experiments (with small numbers of participants) and mass public settings (with thousands of participants). 
2. The discovery that colour constancy in humans is better for certain illuminations than others. In particular, the human visual system is optimised for colour constancy under illumination changes that occur along the &amp;quot;daylight locus&amp;quot;, the set of chromaticities that describe the natural variation of daylight. Specifically, colour constancy in humans is best for illuminations that vary in the &amp;quot;blue&amp;quot; direction.
3. The discovery that people show a &amp;quot;blue bias&amp;quot; in assessing subjective &amp;quot;white-points&amp;quot; in real scenes, both indoors in relatively low-power lighting conditions, and outdoors in high-power natural daylight. 
4. The technical innovation of using tuneable multi-channel LED technology to probe human visual perception and to tailor illuminations for different settings in real-world applications. In particular we developed an accurate method for generating illumination spectra produced by tuneable multi-channel LED lamps, which enables the optimisation of the spectra to match different desired characteristics, for example, either the chromaticity or shape of the spectral power distribution, including its smoothness or deviation from natural daylight. This method of &amp;quot;illumination metamer generation&amp;quot; now underlies several new programmes of research.</gtr:description><gtr:exploitationPathways>To improve the design and generation of illumination spectral power distributions for different settings, in particular for optimising the colour appearance of objects such as artworks in museums or food products in retail displays. 

To enhance the use of tuneable LED technology in different settings, from museums and shops to the home and office.

To improve automatic colour correction algorithms for digital images.</gtr:exploitationPathways><gtr:id>8CB8F365-9D97-43C0-B919-1AFBBF2D7B34</gtr:id><gtr:outcomeId>56e1893ca93cd2.37901760</gtr:outcomeId><gtr:sectors><gtr:sector>Agriculture, Food and Drink,Creative Economy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://www.ncl.ac.uk/ion/staff/profile/anyahurlbert.html#research</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>BD7459BB-2B62-467B-863E-6431692289FF</gtr:id><gtr:title>Chromatic illumination discrimination ability reveals that human colour constancy is optimised for blue daylight illuminations.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91e8064bd6dbd1db56b1eaa537ab1509"><gtr:id>91e8064bd6dbd1db56b1eaa537ab1509</gtr:id><gtr:otherNames>Pearce B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5464c480326601.92083815</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>732CBD49-EC85-4F94-833C-9B47D7C72E8D</gtr:id><gtr:title>Handbook of Experimental Phenomenology - Visual Perception of Shape, Space and Appearance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3370c71b3f2f0a9f996fb0aadbeef8c"><gtr:id>d3370c71b3f2f0a9f996fb0aadbeef8c</gtr:id><gtr:otherNames>Hurlbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>9781119954682</gtr:isbn><gtr:outcomeId>5464c7636dc848.10082246</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4BE1DEA9-1B13-4489-9E96-DC8349DA24C7</gtr:id><gtr:title>Colour Constancy in Immersive Viewing</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3370c71b3f2f0a9f996fb0aadbeef8c"><gtr:id>d3370c71b3f2f0a9f996fb0aadbeef8c</gtr:id><gtr:otherNames>Hurlbert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e18e954bdb55.78617228</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21845B16-CCA6-4F16-9413-C02CB7C4CB96</gtr:id><gtr:title>Illumination discrimination in real and simulated scenes.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c26d5707184b3ff9f9e2fa9f62d7ec7"><gtr:id>9c26d5707184b3ff9f9e2fa9f62d7ec7</gtr:id><gtr:otherNames>Radonjic A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>58bef32ac85332.03633042</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D6757391-9D64-4BFC-A7EA-676DD7DAD615</gtr:id><gtr:title>Skin chromaticity gamuts for illumination recovery</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a59354823a7fbe530187a8c3452370c"><gtr:id>5a59354823a7fbe530187a8c3452370c</gtr:id><gtr:otherNames>Stuart Crichton (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_723464778914078d70</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5F40504D-70D9-48AA-820F-886761F5E128</gtr:id><gtr:title>Color correction using root-polynomial regression.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>585d415b3a58c8.63555065</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5B58A80E-F066-4EB3-A95A-0FB45396842E</gtr:id><gtr:title>Colour Constancy by Illumination Matching in Real World Scenes</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/91e8064bd6dbd1db56b1eaa537ab1509"><gtr:id>91e8064bd6dbd1db56b1eaa537ab1509</gtr:id><gtr:otherNames>Pearce B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464ca154b31d9.84384005</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7DCBB979-5FC0-4D9C-AC91-AA5437A39242</gtr:id><gtr:title>Spectrally tunable LED illuminator for vision research</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/daa86953232111800ee72fa53fd11a92"><gtr:id>daa86953232111800ee72fa53fd11a92</gtr:id><gtr:otherNames>Michal Mackiewicz (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_853046177313f08936</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>703253A6-A540-4D56-B9A0-53C86EF8D343</gtr:id><gtr:title>On calculating metamer sets for spectrally tunable LED illuminators.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>5464c5efaccb25.55724584</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>71B0CB7A-5003-4BA6-9268-42AFB1F04FA2</gtr:id><gtr:title>Performance on the Farnsworth-Munsell 100-Hue Test Is Significantly Related to Nonverbal IQ.</gtr:title><gtr:parentPublicationTitle>Investigative ophthalmology &amp; visual science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1283e52fe361211b1e151efb17f337e5"><gtr:id>1283e52fe361211b1e151efb17f337e5</gtr:id><gtr:otherNames>Cranwell MB</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0146-0404</gtr:issn><gtr:outcomeId>58bef1feb7c375.34831992</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BEA7A64D-DE6F-48A6-84A6-B41C88A071EE</gtr:id><gtr:title>On calculating metamer sets for spectrally tunable LED illuminators.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8879ef5121d8d4a4ff256c71412b8da0"><gtr:id>8879ef5121d8d4a4ff256c71412b8da0</gtr:id><gtr:otherNames>Finlayson G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn><gtr:outcomeId>58bef1ff1301c5.20381915</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FADEDE3F-219C-4695-9545-7852059BFBB6</gtr:id><gtr:title>Colour Vision: Understanding #TheDress.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f047fa8e117c04544a9846d819884765"><gtr:id>f047fa8e117c04544a9846d819884765</gtr:id><gtr:otherNames>Brainard DH</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>56e18e95703594.82330748</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55DE06D8-F729-4AE3-BC19-75B318406E8E</gtr:id><gtr:title>Root-polynomial colour correction</gtr:title><gtr:parentPublicationTitle>Final Program and Proceedings - IS and T/SID Color Imaging Conference</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/14056728d02f5b5bd30f72de6317911d"><gtr:id>14056728d02f5b5bd30f72de6317911d</gtr:id><gtr:otherNames>Finlayson G.D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>10831304</gtr:issn><gtr:outcomeId>5464c763933347.12312010</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H022325/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>