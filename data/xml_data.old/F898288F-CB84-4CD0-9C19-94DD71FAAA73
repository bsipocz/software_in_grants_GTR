<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5F812F7E-6C9C-4BB8-8BAD-DD2ED86AF76B"><gtr:id>5F812F7E-6C9C-4BB8-8BAD-DD2ED86AF76B</gtr:id><gtr:firstName>Julie</gtr:firstName><gtr:surname>Harris</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD002281%2F1"><gtr:id>F898288F-CB84-4CD0-9C19-94DD71FAAA73</gtr:id><gtr:title>The information used to perceive binocular motion in depth</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D002281/1</gtr:grantReference><gtr:abstractText>Because our eyes are positioned side by side in our heads, there is a large portion of our visual field that can be seen through both eyes at the same time. But each eye gets a slightly different view of the world, because it is in a slightly different location, and hence each eye's image is slightly different. The small differences between the eyes are called binocular disparities and it been known for about 150 years that our brains use disparity to help us see depth and shape in the world. In fact, our brains are exquisitely sensitive to disparity: we can detect depth differences as small as the thickness of a sheet of paper at twice arms reach. Binocular disparity is also potentially useful for perceiving how objects move in depth. During object motion in three dimensions (3-D), the binocular disparity of the object will change. However, there will also be direct differences in the size of motion signals in each eye. For example, if an object move directly towards the nose, that object's image will move in one direction in one eye, and the opposite direction in the other eye. If we want to understand how the brain sees 3-D object motion, we must first find out whether it uses binocular disparity, motion differences between the eyes, or both. Such knowledge is critically important for furthering our understanding of basic brain processes. It also has potential application to the enhancement of virtual reality (VR) technology: VR systems can be designed to exploit what the visual system is most sensitive to.Over the last 10 years, several research groups have attempted to test what visual information is used by the brain, using simple visual tests that attempt to isolate the two sources of information. The results have been controversial. One problem is that complex tricks have to be employed to eliminate one of the sources of information (in the real world, both are always present). These manipulations result in visual stimuli that are noisy and that may not have completely eliminated all the expected information. In this project we propose to not just test human vision with such stimuli. Additionally, we will design models called 'ideal observers'. These are mathematical models designed to use all the information available in a stimulus. For example an ideal observer for detecting changing disparity will use all disparity information within a visual stimulus. Indeed, applying the model to a stimulus is a way of testing what information that stimulus contains. By using these models, we can design visual stimuli that contain both sources of information and calculate how well an ideal observer could use the information to see 3-D motion. We then compare human performance with the model, rather than simply testing whether people can see the motion or not. For the first time, we will be able to test the relative importance of binocular disparity and motion information for seeing 3-D object motion.</gtr:abstractText><gtr:fund><gtr:end>2009-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>257959</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Binocular vision exhibit, Fife Science Festival: ST. Andrews Open Day</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>B648484C-C2AF-4B49-AA49-1CFE7DD580D4</gtr:id><gtr:impact>Demos on 3D vision sparked conversation about why we have 2 eyes and how they work. The activities were popular with all age-groups, from small children, to parents, and older adults.

Several people commented that they didn't know vision was linked to neuroscience and psychology, as well as optometry and physics.</gtr:impact><gtr:outcomeId>545cee0f752321.47394181</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2010,2011,2012</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project explored the importance of changes in binocular disparity (CD), inter-ocular velocity differences (IOVD) and eye-movement information (EM) in the perception of motion in three dimensions.We used two-frame motion and random dot noise to deliverequivalent strengths of CD and IOVD information to determine which source of information would be dominant under such sparse visual conditions. Whilst CD information could be used precisely only one participant was able to consistently perceive</gtr:description><gtr:id>CBCA0C7D-D0BB-4414-B463-0B815D28ACCD</gtr:id><gtr:outcomeId>r-3524978518.0436257798993e</gtr:outcomeId><gtr:sectors/></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>35B3D3B0-01AC-4422-9B57-4740B185C5E4</gtr:id><gtr:title>Two independent mechanisms for motion-in-depth perception: evidence from individual differences.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0e9cd387689f72e723bbf2a8f7423522"><gtr:id>0e9cd387689f72e723bbf2a8f7423522</gtr:id><gtr:otherNames>Nefs HT</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>doi_53d087087c7c2205</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9AE3DDBF-B233-4696-A3C5-625BA6FF0CB1</gtr:id><gtr:title>The interaction of eye movements and retinal signals during the perception of 3-D motion direction.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be7e877515155727d7107bd411ee5c9e"><gtr:id>be7e877515155727d7107bd411ee5c9e</gtr:id><gtr:otherNames>Harris JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2006-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d07707750b4914</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9DB41E3E-557C-4861-BA2E-4612D958B73F</gtr:id><gtr:title>Extra-retinal signals support the estimation of 3D motion.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce7be495436fb06a6611c5c02765713e"><gtr:id>ce7be495436fb06a6611c5c02765713e</gtr:id><gtr:otherNames>Welchman AE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00e00eff0f5fd</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A355A576-276C-49AF-A923-D8A6E473A7B1</gtr:id><gtr:title>Comparing motion induction in lateral motion and motion in depth.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be7e877515155727d7107bd411ee5c9e"><gtr:id>be7e877515155727d7107bd411ee5c9e</gtr:id><gtr:otherNames>Harris JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00e00ef5673bd</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F745CE98-C5B8-4B3B-A178-41E1F10A27A2</gtr:id><gtr:title>Vision in 3D Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/721232b044663811c843d6e79c3e07eb"><gtr:id>721232b044663811c843d6e79c3e07eb</gtr:id><gtr:otherNames>Harris J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9780511736261</gtr:isbn><gtr:outcomeId>doi_53d01101136bfafc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>17D2AF52-3EA6-4549-92D4-922E8F041B13</gtr:id><gtr:title>Vergence effects on the perception of motion-in-depth.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0e9cd387689f72e723bbf2a8f7423522"><gtr:id>0e9cd387689f72e723bbf2a8f7423522</gtr:id><gtr:otherNames>Nefs HT</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>doi_55f94e94ee278757</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E567991E-0085-4724-9EB4-08DB18404D11</gtr:id><gtr:title>What visual information is used for stereoscopic depth displacement discrimination?</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0e9cd387689f72e723bbf2a8f7423522"><gtr:id>0e9cd387689f72e723bbf2a8f7423522</gtr:id><gtr:otherNames>Nefs HT</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>doi_53d0390395eac969</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>00645250-B5CF-4CD2-9DC7-6C22B211B312</gtr:id><gtr:title>Binocular vision and motion-in-depth.</gtr:title><gtr:parentPublicationTitle>Spatial vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be7e877515155727d7107bd411ee5c9e"><gtr:id>be7e877515155727d7107bd411ee5c9e</gtr:id><gtr:otherNames>Harris JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0169-1015</gtr:issn><gtr:outcomeId>doi_53d0770770b05c6d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D714AE10-6886-4AA1-A788-A535BBDB839F</gtr:id><gtr:title>Induced motion in depth and the effects of vergence eye movements.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0e9cd387689f72e723bbf2a8f7423522"><gtr:id>0e9cd387689f72e723bbf2a8f7423522</gtr:id><gtr:otherNames>Nefs HT</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d07707758dd967</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D002281/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>85</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EFFEC6B1-6BC8-4C9D-9D77-02CEF5E4E301</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Biomedical neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>85</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>