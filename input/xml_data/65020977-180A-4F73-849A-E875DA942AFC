<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/B0DA50CA-D11E-4251-9678-4AA2F93DB545"><gtr:id>B0DA50CA-D11E-4251-9678-4AA2F93DB545</gtr:id><gtr:name>Polytechnic University of Catalonia</gtr:name><gtr:address><gtr:line1>Polytechnic University of Catalonia</gtr:line1><gtr:line2>C/ Jordi Girona, 31</gtr:line2><gtr:line4>Barcelona</gtr:line4><gtr:line5>E-08034</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Spain</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/22748C58-39CF-4110-8F6C-B0D1D5DF85F3"><gtr:id>22748C58-39CF-4110-8F6C-B0D1D5DF85F3</gtr:id><gtr:name>?cole normale sup?rieure de Lyon (ENS Lyon)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B0DA50CA-D11E-4251-9678-4AA2F93DB545"><gtr:id>B0DA50CA-D11E-4251-9678-4AA2F93DB545</gtr:id><gtr:name>Polytechnic University of Catalonia</gtr:name><gtr:address><gtr:line1>Polytechnic University of Catalonia</gtr:line1><gtr:line2>C/ Jordi Girona, 31</gtr:line2><gtr:line4>Barcelona</gtr:line4><gtr:line5>E-08034</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Spain</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/22748C58-39CF-4110-8F6C-B0D1D5DF85F3"><gtr:id>22748C58-39CF-4110-8F6C-B0D1D5DF85F3</gtr:id><gtr:name>?cole normale sup?rieure de Lyon (ENS Lyon)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FA6E1CBA-992B-4060-8F8F-513D55A737C6"><gtr:id>FA6E1CBA-992B-4060-8F8F-513D55A737C6</gtr:id><gtr:firstName>Krystian</gtr:firstName><gtr:surname>Mikolajczyk</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK01904X%2F1"><gtr:id>65020977-180A-4F73-849A-E875DA942AFC</gtr:id><gtr:title>Visual Sense. Tagging visual data with semantic descriptions</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K01904X/1</gtr:grantReference><gtr:abstractText>Recent years have witnessed an unprecedented growth in the number of image and video collections, partially due to the increased popularity of photo and video sharing websites. One such website alone (Flickr) stores billions of images. And this is not the only way in which visual content is present on the Web: in fact most web pages contain some form of visual content. However, while most traditional tools for search and retrieval can successfully handle textual content, they are not prepared to handle heterogeneous documents. This new type of content demands the development of new efficient tools for search and retrieval.


The large number of readily accessible multi-media data-collections pose both an opportunity and a challenge. The opportunity lies in the potential to mine this data to automatically discover mappings between visual and textual content. The challenge is to develop tools to classify, filter, browse and search such heterogeneous data. In brief, the data is available, but the tools to make sense of it are missing.

The Visual Sense project aims to automatically mine the semantic content of visual data to enable &amp;quot;machine reading&amp;quot; of images. In recent years, we have witnessed significant advances in the automatic recognition of visual concepts. These advances allowed for the creation of systems that can automatically generate keyword-based image annotations. However, these annotations, e.g. &amp;quot;man&amp;quot; and &amp;quot;pot&amp;quot;, fall far short of the sort of more meaningful descriptive captions necessary for indexing and retrieval of images, for example,&amp;quot;Man cooking in kitchen&amp;quot;. The goal of this project is to move a step forward and predict semantic image representations that can be used to generate more informative sentence-based image annotations, thus facilitating search and browsing of large multi-modal collections. It will address the following key open research challenges:

1) Develop methods that can derive a semantic representation of visual content. Such representations must go beyond the detection of objects and scenes and also include a wide range of object relations.
2) Extend state-of-the-art natural language techniques to the tasks of mining large collections of multi-modal documents and generating image captions using both semantic representations of visual content and object/scene type models derived from semantic representations of the textual component of multi-modal documents.
3) Develop learning algorithms that can exploit available multi-modal data to discover mappings between visual and textual content. These algorithms should be able to leverage 'weakly' annotated data and be robust to large amounts of noise.

Thus, the main focus of the Visual Sense project is the development of machine learning methods for knowledge and information extraction from large collections of visual and textual content and for the fusion of this information across modalities. The tools and techniques developed in this project will have a variety of applications. To demonstrate them, we will address three case studies: 1) evaluation of generated descriptive image captions in established international image annotation benchmarks, 2) re-ranking for improved image search and 3) automatic illustration of articles with images.

To address these broad challenges, the project will build on expertise from multiple disciplines, including computer vision, machine learning and natural language processing (NLP). It brings together four research groups from University of Surrey (Surrey, UK), Institut de Robotica i Informatica Industrial (IRI, Spain), Ecole Centrale de Lyon (ECL, France), and University of Sheffield (Sheffield, UK) having each well established and complementary expertise in their respective areas of research.</gtr:abstractText><gtr:potentialImpactText>Innovations in rich annotation of visual data directly impact the European ICT, Digital Media and Creative industries as well as having broader societal impact for any individual seeking to make sense of the huge amount of valuable, yet unstructured, image and video published online. Specifically, the project will:

Reinforce the position of the European ICT and Digital Media research, widening marketing opportunities especially for technology-providing SMEs who consume, produce or have need to search or aggregate visual data. 
Stimulate greater creativity through technologies and tools to search professional and user-generated digital media content. 
Provide digital media/service search engines with innovative offers for interactive and personalised digital media. 
Enhance opportunities for education through illustrative images and videos complementing text descriptions.
This project technologies focus on visual data search, and metadata is the key to improving image and video search capability. The development of metadata and associated tools is in its infancy, and here lies an opportunity within both the ICT and Digital Media sectors. Although metadata frequently accompanies image and video sources, it is often patchy and limited to technical data on media capture rather than on content itself. When editorial metadata is present, this is often interest-specific and in a non-standardised form. As the NEM note within their 2009 Strategic Research Agenda (NEM-SRA), &amp;quot;without metadata [visual data] content is almost valueless&amp;quot;. Our core innovations include the automated annotation of visual data to enrich source-supplied metadata with additional metadata derived automatically from visual and text content, so enabling search and aggregation. NEM-SRA notes this topic of &amp;quot;automatic video indexing&amp;quot; as one of five key promising topics for further research; another being the fusion of visual, aural and meta-data in multimedia search. These topics are explicitly addressed by WP2-WP5. Not only will enhanced techniques for semantic metadata annotation add value to Digital Media, but visual recognition also benefit other areas of ICT e.g. for surveillance, robotics, sports and medical analysis, automated manufacturing, and assisted living.

ViSen is timely in that the opportunities to exploit its outcomes are just beginning to appear. Professional customers are starting to use textual metadata tools for searching archives and managing stored content; they are already acutely aware of the need for similar and more advanced tools for live content. Consumers are becoming accustomed to searching social network sites for visual content. Other professional users
have expressed interest in the ability to search live feeds for specified content or contexts. Market conditions are favourable and will be actively monitored during the final year of ViSen through a living website reflecting routes to market and exploitation opportunities.
Project partners are in close collaboration with the BBC and SMEs, e.g. Omniperception UK, which offers multimedia search tools. We will also investigate commercial exploitation possibilities arising from ViSen with the assistance of the University's Commercialisation of IP Team and via its agreement with Fusion IP plc (http://www.fusionip.co.uk), whose mission is to seek commercial avenues to exploit IP generated in research projects by the University of Sheffield.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-02-08</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>331317</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>?cole normale sup?rieure de Lyon (ENS Lyon)</gtr:collaboratingOrganisation><gtr:country>France, French Republic</gtr:country><gtr:description>ECL</gtr:description><gtr:id>632B26CA-7C3A-4D9E-9876-F862DD2E16EB</gtr:id><gtr:impact>Software took=ls for image annotations and ImageCLEF benchmark dataset.</gtr:impact><gtr:outcomeId>56e0a5526569b5.18367488-1</gtr:outcomeId><gtr:partnerContribution>Development of image feature extractors and object detectors for image annotation and retrieval.</gtr:partnerContribution><gtr:piContribution>Development of machine learning methods for image annotations</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Polytechnic University of Catalonia</gtr:collaboratingOrganisation><gtr:country>Spain, Kingdom of</gtr:country><gtr:description>IRI</gtr:description><gtr:id>4FCDCE3D-7119-4455-8327-2AA456AF7A66</gtr:id><gtr:impact>Publications, datasets, evaluation benchmarks.</gtr:impact><gtr:outcomeId>56e0a065b86e49.07115784-1</gtr:outcomeId><gtr:partnerContribution>Collection of a large news dataset for development and evaluation of computer vision and natural language processing methods.</gtr:partnerContribution><gtr:piContribution>Development of new methods for text and image analysis in news articles.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Sheffield</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Sheffield</gtr:description><gtr:id>FDB0E57A-45C7-471C-89E0-4F440D4B7A4A</gtr:id><gtr:impact>Software and datasets such as Deep Canonical Correlation, ImageCLEF benchmark.</gtr:impact><gtr:outcomeId>56e0a492360de0.75450953-1</gtr:outcomeId><gtr:partnerContribution>Development of natural language processing methods for generating natural image captions.</gtr:partnerContribution><gtr:piContribution>Development of image annotation methods for vision and language</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings , software and datasets were used in the scientific community but may also have impact on the creative industries.</gtr:description><gtr:id>15174381-F0AE-492B-AE26-8B9AAC15E8E6</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545bb93f899c75.06352421</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>ImageCLEF 2015, 2016 benchmark data for image annotation tasks.</gtr:description><gtr:id>C51EB324-DCC8-4A5C-B04D-208048F4E50B</gtr:id><gtr:impact>Used by many internationally recognised research labs for evaluation of their computer vision approaches.</gtr:impact><gtr:outcomeId>56e0a3a868ecd1.98974953</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>ImageCLEF</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.imageclef.org/2015/annotation</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>BreakingNews, a novel dataset with approximately 100K news articles including images, text
and captions, and enriched with heterogeneous meta-data (such as GPS coordinates and user comments). We show this dataset
to be appropriate to explore intersection of computer vision and
natural language processing have achieved unprecedented breakthroughs in tasks like automatic captioning or image retrieval.</gtr:description><gtr:id>A1A9567B-CFA0-422F-A9D3-C17A65A483F7</gtr:id><gtr:impact>This is the largest existing database for analysing visual and natural language content in form of news articles rather than short captions. The database has already been used and referenced by other research labs.</gtr:impact><gtr:outcomeId>58c7f4b4369052.62189231</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>BreakingNews: Article Annotation by Image and Text Processing</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.iri.upc.edu/people/aramisa/BreakingNews/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Low level image descriptors for large scale matching and retrieval</gtr:description><gtr:id>147CE295-D469-4E0E-8314-5E1CE46280FD</gtr:id><gtr:impact>Has been used by other researchers in machining and retrieval applications</gtr:impact><gtr:outcomeId>56e0a2dc510b67.21029657</gtr:outcomeId><gtr:title>BOLD</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/vbalnt/bold</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>980C644A-7905-4B62-A131-EE371D1D2827</gtr:id><gtr:title>Improving Object Tracking with Voting from False Positive Detections</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62700cf1ac424352a942168a3f7254b8"><gtr:id>62700cf1ac424352a942168a3f7254b8</gtr:id><gtr:otherNames>Balntas V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e09d771e88d1.37385965</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>622E2E97-0B88-46EA-A247-6996EEBA0610</gtr:id><gtr:title>A Robust and Scalable Visual Category and Action Recognition System Using Kernel Discriminant Analysis With Spectral Regression</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fce242c6484754ca439b336688f80b07"><gtr:id>fce242c6484754ca439b336688f80b07</gtr:id><gtr:otherNames>Tahir M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d081a211ff8.90226280</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6C9D9482-DFE8-4729-A418-7C3BE5FE4823</gtr:id><gtr:title>Performance evaluation of image filtering for classification and retrieval</gtr:title><gtr:parentPublicationTitle>ICPRAM 2013 - Proceedings of the 2nd International Conference on Pattern Recognition Applications and Methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/231f7118d1d62fc71a45f69af21b4247"><gtr:id>231f7118d1d62fc71a45f69af21b4247</gtr:id><gtr:otherNames>Schubert F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d08199daa00.64043468</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>65262345-C06B-479A-BC7A-DC09BF8A5560</gtr:id><gtr:title>Online Learning and Detection with Part-Based, Circulant Structure</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/12ed1e36ecb5427e3785aa4c051ebca0"><gtr:id>12ed1e36ecb5427e3785aa4c051ebca0</gtr:id><gtr:otherNames>Akin O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e09d773f8570.55222008</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CCA07490-E293-4C9F-A82D-85B25B58D2A2</gtr:id><gtr:title>Automatic annotation of tennis games: An integration of audio, vision, and learning</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8bb95557325dc92e149eaa32cb7a250"><gtr:id>a8bb95557325dc92e149eaa32cb7a250</gtr:id><gtr:otherNames>Yan F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675fd51837d5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4724DBE1-C54C-49BE-BB0B-1EA7E529CE57</gtr:id><gtr:title>Comparison of mid-level feature coding approaches and pooling strategies in visual concept detection</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/496b72b2cb5bb2cb524e10db471777b2"><gtr:id>496b72b2cb5bb2cb524e10db471777b2</gtr:id><gtr:otherNames>Koniusz P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545d0819c38dd0.80502805</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E41E6A53-05BC-4B0A-A4B8-68CA2948CDB3</gtr:id><gtr:title>Full ranking as local descriptor for visual recognition: A comparison of distance metrics on sn</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd0cd8cbc0f4907c2ee85c98c1972869"><gtr:id>fd0cd8cbc0f4907c2ee85c98c1972869</gtr:id><gtr:otherNames>Chan C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e09d77e4cd47.17034840</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0F12A8C6-9F08-4318-BB61-709E536210F0</gtr:id><gtr:title>Robust Registration and Filtering for Moving Object Detection in Aerial Videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9e04307631976a48a46831ab21ee3004"><gtr:id>9e04307631976a48a46831ab21ee3004</gtr:id><gtr:otherNames>Schubert F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e09d77757176.89877461</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46F2468E-32AE-467C-873A-6F17E86E83FE</gtr:id><gtr:title>BOLD - Binary online learned descriptor for efficient image matching</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/62700cf1ac424352a942168a3f7254b8"><gtr:id>62700cf1ac424352a942168a3f7254b8</gtr:id><gtr:otherNames>Balntas V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e09d77c7b3f5.09875260</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43821068-A96E-4B18-8F11-9BE58090660E</gtr:id><gtr:title>Ranking Images Based on Aesthetic Qualities</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6512b1bffffee4012b1c2188c28e14d5"><gtr:id>6512b1bffffee4012b1c2188c28e14d5</gtr:id><gtr:otherNames>Gaur A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e09d770199c3.34760435</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61D830E2-B984-4B16-ABF9-3008774593C7</gtr:id><gtr:title>Deep correlation for matching images and text</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8bb95557325dc92e149eaa32cb7a250"><gtr:id>a8bb95557325dc92e149eaa32cb7a250</gtr:id><gtr:otherNames>Yan F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e09d77a79f17.60161367</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K01904X/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>65020977-180A-4F73-849A-E875DA942AFC</gtr:id><gtr:grantRef>EP/K01904X/1</gtr:grantRef><gtr:amount>331317.28</gtr:amount><gtr:start>2013-02-08</gtr:start><gtr:end>2015-08-31</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>C60C72EA-53D1-4FB7-909B-A78BBB51D6BB</gtr:id><gtr:grantRef>EP/K01904X/2</gtr:grantRef><gtr:amount>58562.08</gtr:amount><gtr:start>2015-12-01</gtr:start><gtr:end>2016-09-30</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>