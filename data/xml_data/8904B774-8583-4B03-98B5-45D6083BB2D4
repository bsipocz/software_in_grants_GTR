<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E5A82D2C-5AD4-488A-ACFF-566345A5D6DA"><gtr:id>E5A82D2C-5AD4-488A-ACFF-566345A5D6DA</gtr:id><gtr:name>Heriot-Watt University</gtr:name><gtr:department>Sch of Engineering and Physical Science</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Riccarton</gtr:line2><gtr:line3>Ricarton</gtr:line3><gtr:line4>Currie</gtr:line4><gtr:postCode>EH14 4AS</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E5A82D2C-5AD4-488A-ACFF-566345A5D6DA"><gtr:id>E5A82D2C-5AD4-488A-ACFF-566345A5D6DA</gtr:id><gtr:name>Heriot-Watt University</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Riccarton</gtr:line2><gtr:line3>Ricarton</gtr:line3><gtr:line4>Currie</gtr:line4><gtr:postCode>EH14 4AS</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2C7C1A46-3E61-4551-AF6C-30F8BC9BAF00"><gtr:id>2C7C1A46-3E61-4551-AF6C-30F8BC9BAF00</gtr:id><gtr:name>Jaguar Cars Ltd</gtr:name><gtr:address><gtr:line1>Engineering Centre</gtr:line1><gtr:line2>Abbey Road</gtr:line2><gtr:line3>Whitley</gtr:line3><gtr:line4>Coventry</gtr:line4><gtr:line5>Warwickshire</gtr:line5><gtr:postCode>CV3 4LF</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A56F5C34-0655-4DCA-87D2-7D7C2228DA5C"><gtr:id>A56F5C34-0655-4DCA-87D2-7D7C2228DA5C</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Wallace</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN012402%2F1"><gtr:id>8904B774-8583-4B03-98B5-45D6083BB2D4</gtr:id><gtr:title>TASCC: Pervasive low-TeraHz and Video Sensing for Car Autonomy and Driver Assistance (PATH CAD)</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N012402/1</gtr:grantReference><gtr:abstractText>This project combines novel low-THz (LTHz) sensor development with advanced video analysis, fusion and cross learning. Using the two streams integrated within the sensing, information and control systems of a modern automobile, we aim to map terrain and identify hazards such as potholes and surface texture changes in all weathers, and to detect and classify other road users (pedestrians, car, cyclists etc.). 

The coming era of autonomous and assisted driving necessitates new all-weather technology. Advanced concepts of interaction between the sensed and processed data, the control systems and the driver can lead to autonomy in decision and control, securing all the needed information for the driver to intervene in critical situations. The aims are to improve road safety through increased situational awareness, and increase energy efficiency by reducing the emission of pollutants caused by poor control and resource use in both on and off-road vehicles. 

Video cameras remain at the heart of our system: there are many reasons for this: low cost, availability, high resolution, a large legacy of processing algorithms to interpret the data and driver/passenger familiarity with the output. However it is widely recognized that video and/or other optical sensors such as LIDAR (c.f. Google car) are not sufficient. The same conditions that challenge human drivers such as heavy rain, fog, spray, snow and dust limit the capability of electro-optical sensors. We require a new approach.

The key second sensor modality is a low-THz radar system operating within the 0.3-1 THz frequency spectrum. By its very nature radar is robust to the conditions that limit video. However it is the relatively short wavelength and wide bandwidth of this LTHz radar with respect to existing automotive radar systems that can bring key additional capabilities. This radar has the potential to provide: (i) imagery that is closer to familiar video than those provided by a conventional radar, and hence can begin to exploit the vast legacy of image processing algorithms; (ii) significantly improved across-road image resolution leading to correspondingly significant improvements in vehicle, pedestrian and other 'actor' (cyclists, animals etc.) detection and classification; (iii) 3D images that can highlight objects and act as an input to the guidance and control system; (iv) analysis of the radar image features, such as shadows and image texture that will contribute to both classification and control. 

The project is a collaboration between three academic institutions - the University of Birmingham with its long standing excellence in automotive radar research and radar technologies, the University of Edinburgh with world class expertise in signal processing and radar imaging and Heriot-Watt University with equivalent skill in video analytics, LiDAR and accelerated algorithms. The novel approach will be based on a fusion of video and radar images in a cross-learning cognitive process to improve the reliability and quality of information acquired by an external sensing system operating in all-weather, all-terrain road conditions without dependency on navigation assisting systems.</gtr:abstractText><gtr:fund><gtr:end>2019-12-09</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-12-10</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>427500</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In this work, three academic institutions - the University of Birmingham with its long standing excellence in automotive radar research and radar technologies, the University of Edinburgh with world class expertise in signal processing and radar imaging and Heriot-Watt University with equivalent skill in video analytics, LiDAR and accelerated algorithms are carrying out fundamental and applied research on novel multifunction multimodal sensing technology by combining, for the first time, low-THz radar and video imagery for a new generation of automotive sensors intended for both driver assistance and autonomous driving. The approach will be based on a fusion of video and radar images in a cross-learning cognitive process to improve the reliability and quality of information acquired by an external sensing system operating in all-weather, all-terrain road conditions without dependency on navigation assisting systems.

The project is at an early stage. To date, the key work at Heriot-Watt University has been in video analytics, that is the processing and interpretation of video data for the purposes of fusion, scene mapping and actor recognition. First, we have resolved many issues associated with calibration and registration using these different sensor modes. We have also conducted (and are continuing) several trials using radar, video and LiDAR data to give ourselves a dataset for future work in both scene mapping and recognition. Second, we have evaluated a number of current techniques for actor (car, pedestrian etc.) detection from images. Deep Neural Networks (DNNS) have shown very promising results in applications where large, labelled datasets are available, and could potentially be used for video streams at least. However, there is little work on their application to forward looking RADAR data and it remains to be seen whether these are effective in our context. We have also evaluated examples of object recognition using &amp;quot;hand-crafted&amp;quot; feature sets. Finally, we have evaluated results on our tracking and optical flow software on these video sequences. These include simple application of the traditional Kalman filter, through to more complex multi-target tracking algorithms using particle filters and Random Finite Set methods.</gtr:description><gtr:exploitationPathways>Driver assistance or full vehicle autonomy depends on fast and accurate interpretation of sensor data. If successful, the project outcomes should be taken forward by our partners, Jaguar Land Rover, in developing premium cars. However, results should also be more widely available through publication and other dissemination.</gtr:exploitationPathways><gtr:id>47241D4F-56A1-4DB7-954D-2D79D52F3FF3</gtr:id><gtr:outcomeId>58b3e762184ed2.59581143</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Environment,Transport</gtr:sector></gtr:sectors><gtr:url>https://portal.axillium.com/TASCC/programme</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs><gtr:policyInfluenceOutput><gtr:areas/><gtr:description>Presentation to EPSRC Theme Meeting on Robotics and Autonomous Systems (January)</gtr:description><gtr:geographicReach>Europe</gtr:geographicReach><gtr:id>C0B8E5C0-4A94-4F07-83D6-4770E4EF7AD2</gtr:id><gtr:outcomeId>58b3f487d9c5c5.55167667</gtr:outcomeId><gtr:type>Gave evidence to a government review</gtr:type><gtr:url>https://www.epsrc.ac.uk/newsevents/pubs/epsrc-delivery-plan-2016-17-2019-20/</gtr:url></gtr:policyInfluenceOutput></gtr:policyInfluenceOutputs><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/N012402/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>F78E4567-DD59-4364-9D1F-0A778996E941</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Instrumentation Eng. &amp; Dev.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>