<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/B86BF310-2812-4FAC-8860-A72CF97C07C8"><gtr:id>B86BF310-2812-4FAC-8860-A72CF97C07C8</gtr:id><gtr:name>Cork Institute of Technology</gtr:name><gtr:address><gtr:line1>Cork Institute of Technology</gtr:line1><gtr:line2>Rossa Avenue</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E"><gtr:id>598EC0B6-AEB9-4323-95F7-5B4EFCDDF33E</gtr:id><gtr:name>Friedrich-Alexander University</gtr:name><gtr:address><gtr:line1>Schlossplatz 4</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C01F36D5-8E9C-47AF-AA5E-6267C814CF59"><gtr:id>C01F36D5-8E9C-47AF-AA5E-6267C814CF59</gtr:id><gtr:name>INRIA</gtr:name><gtr:address><gtr:line1>Domaine de Voluceau</gtr:line1><gtr:line2>Rocquencourt BP-105</gtr:line2><gtr:postCode>F-78153</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FE1EC3EB-3D38-4ABB-94B2-8429AC475C8F"><gtr:id>FE1EC3EB-3D38-4ABB-94B2-8429AC475C8F</gtr:id><gtr:name>Northwestern University</gtr:name><gtr:address><gtr:line1>633 Clark Street</gtr:line1><gtr:postCode>60208</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9587265A-30C0-45F0-B974-BDB3DAED8E8D"><gtr:id>9587265A-30C0-45F0-B974-BDB3DAED8E8D</gtr:id><gtr:name>Telecom ParisTech</gtr:name><gtr:address><gtr:line1>46 Rue Barrault</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A3C215EE-5645-4543-ACD7-878BFB368AC7"><gtr:id>A3C215EE-5645-4543-ACD7-878BFB368AC7</gtr:id><gtr:firstName>Panos</gtr:firstName><gtr:surname>Kudumakis</gtr:surname><gtr:orcidId>0000-0003-0518-4198</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DAD387A7-797C-4DC5-B27F-A52A9FCDC601"><gtr:id>DAD387A7-797C-4DC5-B27F-A52A9FCDC601</gtr:id><gtr:firstName>Philip JB</gtr:firstName><gtr:surname>Jackson</gtr:surname><gtr:orcidId>0000-0001-7933-5935</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795"><gtr:id>2D1D7ADD-45F8-4B1A-89C5-9D33EAA1E795</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Sandler</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/67CAA1DB-31CD-4307-A2B8-B3A2A41082E0"><gtr:id>67CAA1DB-31CD-4307-A2B8-B3A2A41082E0</gtr:id><gtr:firstName>Sebastian</gtr:firstName><gtr:surname>Ewert</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/889107E6-0975-411A-B38E-66CC4B682180"><gtr:id>889107E6-0975-411A-B38E-66CC4B682180</gtr:id><gtr:firstName>Martin</gtr:firstName><gtr:otherNames>Clifford</gtr:otherNames><gtr:surname>Dewhirst</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0C941CCA-2B6E-4ADC-B915-3543BAEF5164"><gtr:id>0C941CCA-2B6E-4ADC-B915-3543BAEF5164</gtr:id><gtr:firstName>Chris</gtr:firstName><gtr:surname>Cannam</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6578CA75-F7D3-4493-BE4E-6F7BB672B593"><gtr:id>6578CA75-F7D3-4493-BE4E-6F7BB672B593</gtr:id><gtr:firstName>Christopher</gtr:firstName><gtr:surname>Hummersone</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/C0A7F478-E188-4D82-AE3F-342E1BD5C29A"><gtr:id>C0A7F478-E188-4D82-AE3F-342E1BD5C29A</gtr:id><gtr:firstName>Philip</gtr:firstName><gtr:surname>Coleman</gtr:surname><gtr:orcidId>0000-0002-3266-7358</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/CAF908AB-A1A1-4BFD-9848-22BC53CDC3DA"><gtr:id>CAF908AB-A1A1-4BFD-9848-22BC53CDC3DA</gtr:id><gtr:firstName>Russell</gtr:firstName><gtr:surname>Mason</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2E9DAD26-CC75-4B99-8C38-CDE0C5397477"><gtr:id>2E9DAD26-CC75-4B99-8C38-CDE0C5397477</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>Dixon</gtr:surname><gtr:orcidId>0000-0002-6098-481X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/058B7945-94E9-450F-A970-E88BA39F33D9"><gtr:id>058B7945-94E9-450F-A970-E88BA39F33D9</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Johnathan</gtr:otherNames><gtr:surname>Bryan-Kinns</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL027119%2F2"><gtr:id>1CEC4D98-AAD2-41EE-A306-4EF7F281D016</gtr:id><gtr:title>Musical Audio Repurposing using Source Separation</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/L027119/2</gtr:grantReference><gtr:abstractText>Delivery of audio has become increasingly complex: originally in single channel (mono) or 2-channel stereo format, now surround sound in &amp;quot;5.1&amp;quot; format (5 main speakers plus one low frequency effects channel) is available in many home cinema systems, and many other multichannel audio formats are available (e.g. 6.1, 7.1, 10.2 and 22.2). In addition, new interactive apps allow users to remix musical audio, changing instrument volumes, and music games allow players to control individual instruments. Content creators therefore have to develop new ways to create and distribute their audio content to allow their content to be played back on these multichannel systems, or remixed by users to suit their own tastes.
However, much audio content is still in legacy formats, mainly 2-channel stereo. We therefore need ways to &amp;quot;repurpose&amp;quot; this legacy audio content, converting these into surround sound or to the separate &amp;quot;stems&amp;quot; needed for remixable audio.
The aim of this project is to develop a new approach to high quality audio repurposing, based on high quality musical audio source separation. To achieve this we will combine new high resolution separation techniques with information such as musical scores, instrument recognition, onset detection, and pitch tracking. Instead of aiming at generic source separation, we will develop algorithms designed to match the separation performance to the final target (upmixing or remixing). In parallel, we will investigate perceptual evaluation measures for source separation, remixing and upmixing, and develop new diagnostic evaluation techniques tailored to measure different aspects of the repurposed outcome.
The outcomes of this project will allow music consumers to enjoy their favourite songs in interactive remixing apps and games, even where the original separate &amp;quot;stems&amp;quot; are not available. It will also allow music companies, broadcasters and sound archive holders to provide high quality upmixed versions of their large archive content, for an increasing generation of listeners with surround sound systems in the home.</gtr:abstractText><gtr:potentialImpactText>(Non-academic beneficiaries are outlined here and in &amp;quot;Pathways to Impact&amp;quot;. For more on academic impact, see &amp;quot;Academic Beneficiaries&amp;quot; and the &amp;quot;Academic Impact&amp;quot; section in the Case for Support.)
Audio researchers in industry will benefit from new methods for upmixing and remixing emerging from the project. 
Manufacturers of audio upmixing equipment and plugins, and broadcasters wishing to upmix legacy 2-channel stereo content, will benefit from our new high-quality upmixing methods. Manufacturers of other musical audio effects boxes will benefit from new methods for remixing allowing repurposing of legacy audio content. 
Other holders of legacy audio and audiovisual archives, such as the British Library, BFI and regional sound archives, will benefit from the ability to upmix their content for modern audiences becoming increasingly used to surround sound audio. 
There is a strong interest amongst both professional and high-end consumer audio users in new methods for unmixing 2-channel stereo content to 5.1 surround sound, leading to a range of upmix (or &amp;quot;unwrap&amp;quot;) plugins for systems such as ProTools. These users will benefit from new upmix approaches emerging from this project, either through direct use of research prototypes, or through enhanced software or tools from audio equipment or plugin manufacturers. 
Sound artists and composers will benefit from our remixing methods, allowing them to use sounds from mixed audio signals as part of compositions. 
Remixing apps are becoming available for mobile devices, allowing users to remix and share audio tracks. Currently these are limited to use tracks where the separated sources are available from the original music label. These remix users and companies would benefit from the ability to remix from the stereo content that they already own. 
The staff employed on the project, including postdoctoral research assistants undertaking the research, will gain skills applicable to industrial problems such as advanced digital signal processing, research software development, and evaluation methodologies.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>856793</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2980000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>H2020-ICT-2015 Audio Commons</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>688382</gtr:fundingRef><gtr:id>10D6C8D4-4036-4684-9010-5CB3F97279F1</gtr:id><gtr:outcomeId>568bdebdb40db2.04379356</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>TwoWayMixer is a demonstration prototype that showcases the utilization of Deep Neural Networks to audio remixing through source separation, implemented using web audio technology.</gtr:description><gtr:id>397534D4-89DA-4E84-A769-DC0E00356E5C</gtr:id><gtr:impact>The demo allows researchers and practitioners to understand the potential of source separation algorithms for audio remixing, and contributes to the emerging field of Web Audio. 
It was presented in the Second Web Audio Conference (WAC 2016).</gtr:impact><gtr:outcomeId>58c81424765343.46167186</gtr:outcomeId><gtr:title>Two-way mixer</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/g-roma/twowaymixer</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Untwist is python library for audio source separation. It provides a self-contained object-oriented framework including common source separation algorithms as well as input/output functions, data management utilities and time-frequency transforms.</gtr:description><gtr:id>4F08B7E7-E913-405B-A3CE-AA1258753971</gtr:id><gtr:impact>The development of library has enabled research on audio source separation within the Musical Audio Repurposing using Source Separation project. 
By releasing as open source, we help researchers in the field of signal processing and source separation to transition to the Python programming language, and encourage good practices in research software engineering. The library was presented at the 9th European Conference on Python in Science (EuroScipy 2016).</gtr:impact><gtr:outcomeId>58c81280d6b3c6.55150917</gtr:outcomeId><gtr:title>untwist</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/IoSR-Surrey/untwist</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>AC3FA4AC-F816-4D03-B532-6C931C6F97D6</gtr:id><gtr:title>Single-channel audio source separation using deep neural network ensembles</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6991d8abd65eefb1f983eb14593b94ad"><gtr:id>6991d8abd65eefb1f983eb14593b94ad</gtr:id><gtr:otherNames>Grais EM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b994e5da6951.36732602</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>414BBA1A-7ABD-42D6-8C3C-5A3312567D36</gtr:id><gtr:title>Evaluation of audio source separation models using hypothesis-driven non-parametric statistical methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3fff870985131b893b7254f573f4031a"><gtr:id>3fff870985131b893b7254f573f4031a</gtr:id><gtr:otherNames>Simpson A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b995f28f1178.85372688</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52C053DC-1EE5-4B8E-934B-4BCA6B255C8D</gtr:id><gtr:title>Polyphonic Sound Event Tracking Using Linear Dynamical Systems</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fdf37cffef1.51411585</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>32B59F63-6D8D-4551-9819-688B045AEB4B</gtr:id><gtr:title>Non-negative matrix factorisation incorporating greedy hellinger sparse coding applied to polyphonic music transcription</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845955cb409b459776fd0fe8bfe01bf7"><gtr:id>845955cb409b459776fd0fe8bfe01bf7</gtr:id><gtr:otherNames>O'Hanlon K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b8ef4d34f57.79536908</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>743F0570-3332-4DB3-9E55-F46F780046C8</gtr:id><gtr:title>Multivariate iterative hard thresholding for sparse decomposition with flexible sparsity patterns</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24d1e16799534c8f8a61c4921cb54efa"><gtr:id>24d1e16799534c8f8a61c4921cb54efa</gtr:id><gtr:otherNames>Rencker L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8847c40b2302.16037119</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BC868CB4-8DAA-46F1-AF7B-890B1003DB19</gtr:id><gtr:title>Untwist: A new toolbox for audio source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99a7c63f380.63542356</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D180E2D-FA55-4506-A34C-3C9B55485A4F</gtr:id><gtr:title>Single channel audio source separation using deep neural network ensembles</gtr:title><gtr:parentPublicationTitle>140th Audio Engineering Society International Convention 2016, AES 2016</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f02e802bf08504ad958be72d9e98238"><gtr:id>3f02e802bf08504ad958be72d9e98238</gtr:id><gtr:otherNames>Grais E.M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a8847bf6e1933.45189333</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0BF7E963-A382-4626-B350-19E1EDE5470A</gtr:id><gtr:title>Assessment of musical noise using localization of isolated peaks in time-frequency domain</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b4f831f8f52df57819ab64f7018546e"><gtr:id>8b4f831f8f52df57819ab64f7018546e</gtr:id><gtr:otherNames>Hamon R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a876ea99f1495.26118336</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E61BAD5-A25A-4DF7-9920-0030CCD8EA55</gtr:id><gtr:title>Singing Voice Separation Using Deep Neural Networks and F0 Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c701913fdd98.38264396</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C0C8CE4-05CF-44B4-834E-21ED29DEBF3D</gtr:id><gtr:title>Music remixing and upmixing using source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99a0fbade35.65162612</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>45DB4D50-6BCB-42AB-ABB7-20592C8A13B6</gtr:id><gtr:title>Learning soft mask with DNN and DNN-SVM for multi-speaker DOA estimation using an acoustic vector sensor</gtr:title><gtr:parentPublicationTitle>Journal of the Franklin Institute</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a0385ddf822030de0fee889762bbc89e"><gtr:id>a0385ddf822030de0fee889762bbc89e</gtr:id><gtr:otherNames>Wang D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2fdf2d0152f6.06635573</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A6C5A148-7EB0-452B-B814-BA960D9361BA</gtr:id><gtr:title>Remixing musical audio on the web using source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3cc0a06c5cb065eaa734bd19963705df"><gtr:id>3cc0a06c5cb065eaa734bd19963705df</gtr:id><gtr:otherNames>Roma G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99b0a668674.76797431</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4ED32EE5-7A8B-44FF-968C-9065C4AC71DB</gtr:id><gtr:title>A greedy algorithm with learned statistics for sparse signal reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24d1e16799534c8f8a61c4921cb54efa"><gtr:id>24d1e16799534c8f8a61c4921cb54efa</gtr:id><gtr:otherNames>Rencker L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a876ea942ac97.39052904</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9FB14B93-2828-442A-A0E3-DDA2D1A21DF0</gtr:id><gtr:title>Two-Stage Single-Channel Audio Source Separation Using Deep Neural Networks</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/228946c0aeaa4e48b73a820d80916550"><gtr:id>228946c0aeaa4e48b73a820d80916550</gtr:id><gtr:otherNames>Grais E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe09b6da7a2.31812638</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C9B1AD1E-63B7-47B6-8183-7B323FD2BBB6</gtr:id><gtr:title>Binaural and log-power spectra features with deep neural networks for speech-noise separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7be224528d3770a9b3d5cb5505b4cdac"><gtr:id>7be224528d3770a9b3d5cb5505b4cdac</gtr:id><gtr:otherNames>Zermini A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a89c6b1e42b64.25444737</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L027119/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>C7D66F18-A65B-47AD-A75E-3AF70A71E673</gtr:id><gtr:grantRef>EP/L027119/1</gtr:grantRef><gtr:amount>887606.65</gtr:amount><gtr:start>2014-11-01</gtr:start><gtr:end>2014-12-31</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>1CEC4D98-AAD2-41EE-A306-4EF7F281D016</gtr:id><gtr:grantRef>EP/L027119/2</gtr:grantRef><gtr:amount>856793.48</gtr:amount><gtr:start>2015-04-01</gtr:start><gtr:end>2018-07-31</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>