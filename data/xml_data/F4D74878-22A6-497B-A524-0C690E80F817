<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/BF7F36F8-2AC3-4660-BDB9-F61AD740E8C0"><gtr:id>BF7F36F8-2AC3-4660-BDB9-F61AD740E8C0</gtr:id><gtr:name>British Computer Association of the Blind (BCAB)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/576D938C-97F1-417A-A1B1-553CF4A78A9D"><gtr:id>576D938C-97F1-417A-A1B1-553CF4A78A9D</gtr:id><gtr:name>Royal National Institute for Blind People</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BF7F36F8-2AC3-4660-BDB9-F61AD740E8C0"><gtr:id>BF7F36F8-2AC3-4660-BDB9-F61AD740E8C0</gtr:id><gtr:name>British Computer Association of the Blind (BCAB)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/576D938C-97F1-417A-A1B1-553CF4A78A9D"><gtr:id>576D938C-97F1-417A-A1B1-553CF4A78A9D</gtr:id><gtr:name>Royal National Institute for Blind People</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B3F53984-542F-44AD-9E65-8C46703F522E"><gtr:id>B3F53984-542F-44AD-9E65-8C46703F522E</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:surname>Curzon</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/371109BC-9E2F-4D9D-AD34-98602F5B2B8B"><gtr:id>371109BC-9E2F-4D9D-AD34-98602F5B2B8B</gtr:id><gtr:firstName>Michael</gtr:firstName><gtr:surname>Proulx</gtr:surname><gtr:orcidId>0000-0003-4066-3645</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F08D5405-BBCD-42A2-BBD4-0021935C68C4"><gtr:id>F08D5405-BBCD-42A2-BBD4-0021935C68C4</gtr:id><gtr:firstName>Tony</gtr:firstName><gtr:surname>Stockman</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/058B7945-94E9-450F-A970-E88BA39F33D9"><gtr:id>058B7945-94E9-450F-A970-E88BA39F33D9</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Johnathan</gtr:otherNames><gtr:surname>Bryan-Kinns</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/80FA2A44-109A-4161-8522-8078A7249E9B"><gtr:id>80FA2A44-109A-4161-8522-8078A7249E9B</gtr:id><gtr:firstName>Oussama</gtr:firstName><gtr:surname>Metatla</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ017205%2F1"><gtr:id>F4D74878-22A6-497B-A524-0C690E80F817</gtr:id><gtr:title>Design Patterns for Inclusive Collaboration (DePIC)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J017205/1</gtr:grantReference><gtr:abstractText>Our interaction with the world around us relies on perception which exploits combinations of the senses we have available to us. Enabling people to use combinations of senses becomes critical in situations where people who have different senses available to them interact with each other. These differences can arise because of temporary or permanent sensory impairment, or due to the technology they are using. However, very little research has examined how people combine and map information from one sense to another, particularly for individuals with sensory impairments, and then used such mappings to inform the design of technology to make collaboration easier. The aim of this multi-disciplinary project is to develop new ways for people to interact with each other using different combinations of senses. This will reduce barriers to collaboration caused by sensory impairment, and improve social and workplace inclusion by optimising the use of available senses. We will combine empirical studies of mappings between senses with participatory design techniques to develop new ideas for inclusive design grounded in Cognitive Psychology. We will capture these design ideas and mappings in the form of Design Patterns and demonstrate their usefulness through the development of interactive systems to support assisted work, living, and leisure.</gtr:abstractText><gtr:potentialImpactText>Potential beneficiaries of this project outside the research community include anyone who could benefit from reducing barriers to collaboration due to sensory impairment. Examples from various sectors are given below.

Wider public:
* Visually impaired individuals, through reduced barriers to work, living, and leisure.
* Sighted individuals, through raising awareness of barriers to collaboration, ways to reduce these barriers, and increased opportunities for collaboration.
* Teachers in schools with visually impaired students, through increasing opportunities for collaborative learning.

Commercial private sector:
* Commercial companies in general, through increased inclusion of skilled workers.
* Accessible software developers, through examples of best practice in cross-modal design, and access to the open-source platform for development.
* Assistive hardware developers, through identification of new hardware requirements and new areas for assistive hardware development.
* Telehealth providers through examples of best practice in cross-modal collaboration design.
* HCI, Interaction Design, and Accessible Design practitioners, through Design Patterns and examples of best practice to inform their accessible design practice.
* New company development through possible spin-out consulting on cross-modal interaction and the open-source platform.

Policy-makers, charities, and governmental agencies:
* Charities such as the British Computer Association of the Blind, and Royal National Institute of Blind People, and policy-makers through examples of best practice in support for inclusive collaboration.
* Government agencies through the establishment of the UK as an international leader in the field of accessible systems.

Public sector, and others:
* Healthcare workers working with visually impaired patients through support for shared interaction with medical data.
* Visually impaired healthcare workers through being able to access and collaborate with medical data
* Museums and galleries, through examples of best practice in cross-modal design particularly for interactive exhibits and guides.

Researchers employed on the project:
* Improved skills in accessible software development and study methodologies, which may be transferable e.g. the commercial private sector on completion of the project.
* Improved public engagement skills through exhibitions and outreach work.

Others undertaking direct interaction with the project:
* Participants in Participatory Design, study participants, and steering committee, through exposure to new assistive interfaces, and contribution to design direction and refinement.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>787237</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Computer Association of the Blind (BCAB)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with BCAB</gtr:description><gtr:id>51A28247-65C7-408B-ABE8-670C1B9696BA</gtr:id><gtr:outcomeId>b972b33eb972b352-1</gtr:outcomeId><gtr:piContribution>The British Computer Association of the Blind (BCAB ) served on our steering committee and directly contributed to the project through hands-on feedback on the design in workshops , identification of suitable places to raise awareness of our tool (e.g. mailing lists, newsletters, etc.), and identification of people to take part in workplace studies of the use of the tool.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Royal National Institute for Blind People</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Partnership with RNIB</gtr:description><gtr:id>64C125B8-867A-42D4-8051-78DA9D112D0D</gtr:id><gtr:outcomeId>b972b460b972b474-1</gtr:outcomeId><gtr:piContribution>Through the CCmI project we have developed and strengthened our partnerships with user advocates: RNIB, BCAB, and New College Worcester - in fact, RNIB and BCAB are supporting our new EPSRC funding proposal (DePIC). The RNIB and BCAB have contributed their time and knowledge to the CCmI project through the steering committee, and New College Worcester has contributed time and effort to the project through organising and taking part in studies of our tool with pupils at New College Worcester.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>NESTA SoundLab Play Space Event</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>93F965AA-C35D-4F73-8539-8163AC57F401</gtr:id><gtr:impact>NESTA SoundLab Play Space Event demonstrations of ShapeTones which was developed in the DePIC project as an accessible game</gtr:impact><gtr:outcomeId>56cf2fbaae0b65.47889273</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>participatory design workshops</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>A6419452-602E-4C7C-82DC-2406571EBD31</gtr:id><gtr:impact>A series of participatory design workshops with visually impaired musicians and audio production specialists</gtr:impact><gtr:outcomeId>56cf2db6df9204.62738963</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Study participants or study members</gtr:primaryAudience><gtr:year>2013,2014,2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Sensory Support Services Technology Day</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>4593A240-34D7-445C-8B67-8593D168811F</gtr:id><gtr:impact>Sensory Support Services Technology Day demonstrations of audio-haptic technology with school children with sensory impairments and their carers.</gtr:impact><gtr:outcomeId>56cf2f29788872.20341553</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We have presented our work in a number of non-academic venues to engage with general public and people living with visual impairments:
A series of participatory design workshop with visually impaired musicians and audio production specialists
AccessNIME workshop at NIME'2014
Sonification for Sports &amp;amp; Performances workshop at ICAD'2014
AccessNIME hackathon
Demosntration of accessible technology at the Visually Impaired Musicians Lives conference
Sensory Support Services Technology Day demonstrations of audio-haptic technology NESTA SoundLab Play Space Event demonstrations of ShapeTones
Latitude Music Festival, Wellcome Trust Hub
Bath Alumni Weekend &amp;quot;Ideas Hub&amp;quot;
Science Museum &amp;quot;You have been upgraded&amp;quot; Festival
General University Lecture Programme (in association with the University for the Third Age), University of Bath, United Kingdom,
TEDx, Bath, United Kingdom
Wiltshire Science Festival, United Kingdom

Peak level meters, along with other components which rely on the sense of sight for their use, are inherently inaccessible to people living visual impairments. Our AccessiblePeakMeter is the first plug-in which makes these previously inaccessible meters completely accessible. It uses real-time sonification to deliver information to the user about audio levels and peaks in audio signals, and so supports core activities in audio production.

The AccessiblePeakMeter comes as a VST, AU or AAX plug-in, two of the main industry standards for the deployment of digital audio effects into professional DAWs (e.g. Cakewalk Sonar, Cockos Reaper, Ableton Live). The plug-in can be run on both Windows (32/64) and Mac platforms and it is free for download and completely open source.

AccessiblePeakMeter received the Best Solution by a Large Organization award in the AT&amp;amp;T and New York University Connect Ability Challenge, a global software competition for software technologies that improve the lives of people living with disabilities. The winner were announced on Sunday July 26 2015, the day of the 25th anniversary of the Americans with Disabilities Act.</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>7089C1D5-5258-4421-9B6F-60AADDAE61E5</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56cf1e179ccb94.45731676</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have found new ways to combine sensory output (sound, sight, and touch) to allow people with different sensory abilities to use interactive systems. We developed design patterns and guidelines for how to combine different modalities, and developed new methods to engage people with different sensory abilities in design processes, and then tested these with real world users.

A key example of the use of the guidelines is our Accessible Digital Audio Workstation and Accessible Peak Meter which allows visually-impaired Audio Engineers to undertake complex work which was previously extremely difficult due to the visual nature of the user interfaces. We have also used the guidelines to develop an accessible game which can be played by sighted, blind, and deaf participants at the same time, and is available for public download.</gtr:description><gtr:exploitationPathways>Design of more inclusive and accessible technologies across all domains of technology use.</gtr:exploitationPathways><gtr:id>2C6D2968-8B21-42B0-95C6-17B01D2B05C1</gtr:id><gtr:outcomeId>56cf202fe58d19.53802863</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Education</gtr:sector></gtr:sectors><gtr:url>http://depic.eecs.qmul.ac.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The JHapticGUI library allows to embed force feedback devices in the development of Java GUI based programs
 
Force feedback devices are widely used for a diverse range of application including 3D modeling, medical simulations, games and virtual sculpture. Normally these applications are based on a 3D vector graphics (e.g. OpenGL) scene which is &amp;quot;augmented&amp;quot; with sense of touch, namely the possibility to touch the virtual objects that are rendered on the screen.
 
However, a relatively unexplored field is the use of such devices to provide a haptic modality of interaction with everyday programs featuring a 2D graphical user interface made of buttons, lists, menus etc. Such a modality, besides opening up new ways of interacting with graphical user interfaces, would have an impact on the access to common software by people with visual impairments.
 
The JHapticGUI library helps building such software in Java. It provides an abstract and clean interface to the haptic device from the Java code allowing at the same time full freedom as to how to design the haptic scene. It takes care of the synchronization between the Java event dispatching thread and the thread that handles the device. Furthermore it provides a communication protocol based on general purpose messages for connecting haptic and graphic events. Details of communication implementation are hidden to the client code but the content of the message can be defined by the application, for maximum customization of the program logic</gtr:description><gtr:id>35C272ED-9B31-47D2-AD9F-20FD5FEA16E7</gtr:id><gtr:impact>Provides a new way to make accessible interfaces</gtr:impact><gtr:outcomeId>56cf2664b0a905.70824109</gtr:outcomeId><gtr:title>jHapticGUI</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/?q=jhapticgui</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>ShapeTones is an audiovisual memory game for iOS (iPhone/iPad). A sequence of 3 shapes and tones (&amp;quot;ShapeTones&amp;quot;) is played, and the player tries to reproduce it. Tapping different areas of the screen trigger different ShapeTones. The game starts with 3 ShapeTones. As the game evolves, more ShapeTones become available. When a new ShapeTone is added, a trial screen is shown to demonstrate where each ShapeTone is triggered. Some surprises happen along the way. As a one player game, the sequence is created automatically. As a two player game, one player creates the sequence, passes the device to the other player, who tries to repeat it. They then swap the roles.</gtr:description><gtr:id>D6751F8D-92DF-4088-BA97-CFC9CFBB75AA</gtr:id><gtr:impact>Public release on Apple iTunes</gtr:impact><gtr:outcomeId>56cf25f33a85d0.19649983</gtr:outcomeId><gtr:title>ShapeTones</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/?q=shapetones</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>A simple DAW prototype that can be operated with the usual graphical user interface or with its peculiar audio-haptic interface. 
The latter allows visually impaired users to interact with the program without having to look at the screen, by using haptics (through the Geomagic&amp;reg; Touch? Haptic Device) and sound (speech as well as non-speech).

It supports cut/copy/paste of audio clips and automation graphs of gain and panning. It is not really meant to be used for real tasks but rather to test out cross-modal approaches of interacting with DAW's. It features several sonifications and &amp;quot;haptifications&amp;quot; of different aspects of DAW's that are often inaccessible to visually impaired users.

Waveform selection
Waveform peaks
Automation envelopes
Its sonification of peaks underlies the sonification techniques used in the Accessible Peak Meter audio plug-in.</gtr:description><gtr:id>07A90ABD-44D5-467C-BC04-A4145C8FC51B</gtr:id><gtr:impact>First truly accessible Digital Audio Workstation prototype.</gtr:impact><gtr:outcomeId>56cf26cf0cf465.28766634</gtr:outcomeId><gtr:title>Cross-Modal DAW Prototype</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/?q=dawprototype</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>ofxEnveloper is an openFrameworks addon that converts gestures into an envelope type of graphical information, by recognising inflection points from the gesture. Thresholds for horizontal and vertical lines can also be defined. It is particularly aimed for animation and sound applications, and for use with drawing surfaces (such as Wacom tablets or touchscreen interfaces).
 
Possible uses:
audio effect or EQ envelopes;
animation curves;
creation/manipulation of audio and/or visual sequences;
arpeggiators.</gtr:description><gtr:id>B00D0298-8AAB-4A62-BF33-C6C8D196BFF5</gtr:id><gtr:impact>Potential use in sound editing applications</gtr:impact><gtr:outcomeId>56cf27418ef192.97199072</gtr:outcomeId><gtr:title>ofxEnveloper</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/?q=ofxenveloper</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Peak level meters, along with other components which rely on the sense of sight for their use, are inherently inaccessible to people living visual impairments. Our AccessiblePeakMeter is the first plug-in which makes these previously inaccessible meters completely accessible. It uses real-time sonification to deliver information to the user about audio levels and peaks in audio signals, and so supports core activities in audio production.

The AccessiblePeakMeter comes as a VST, AU or AAX plug-in, two of the main industry standards for the deployment of digital audio effects into professional DAWs (e.g. Cakewalk Sonar, Cockos Reaper, Ableton Live). The plug-in can be run on both Windows (32/64) and Mac platforms and it is free for download and completely open source!</gtr:description><gtr:id>B6FF30FC-D8B0-4A58-A17D-49BC6F8793BA</gtr:id><gtr:impact>AccessiblePeakMeter received the Best Solution by a Large Organization award in the AT&amp;amp;T and New York University Connect Ability Challenge, a global software competition for software technologies that improve the lives of people living with disabilities. The winner were announced on Sunday July 26 2015, the day of the 25th anniversary of the Americans with Disabilities Act.</gtr:impact><gtr:outcomeId>56cf255abaa791.68070440</gtr:outcomeId><gtr:title>Accessible Peak Meter</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://depic.eecs.qmul.ac.uk/apm/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>96E5A1DF-2EBE-49B0-A66D-621E695BE599</gtr:id><gtr:title>Auditory scene analysis and sonified visual images. Does consonance negatively impact on object formation when using complex sonified stimuli?</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/769a0d7b29c95a179ce5bc84f994f1d6"><gtr:id>769a0d7b29c95a179ce5bc84f994f1d6</gtr:id><gtr:otherNames>Brown DJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>585d530d4ab6a3.65795769</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A616338-EA3C-4998-B6DC-9362F16F25A9</gtr:id><gtr:title>Accessible Collaborative Interaction with Diagrams</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e77dac7a6c74d7d3a33e15faee434237"><gtr:id>e77dac7a6c74d7d3a33e15faee434237</gtr:id><gtr:otherNames>Metatla, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>546229a4e5cf83.22237617</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D6E10B3-ADD4-4590-A27B-EBE57C285637</gtr:id><gtr:title>Sonification of reference markers for auditory graphs: effects on non-visual point estimation tasks</gtr:title><gtr:parentPublicationTitle>PeerJ Computer Science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442f06f2b2938179e170c93d972bbf37"><gtr:id>442f06f2b2938179e170c93d972bbf37</gtr:id><gtr:otherNames>Metatla O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4e1bdc1878.79725588</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19A83E17-709E-4E40-AA64-AB822DFF7304</gtr:id><gtr:title>Two-Dimensional Rubber-Hand Illusion: The&amp;nbsp;Dorian&amp;nbsp;Gray Hand Illusion</gtr:title><gtr:parentPublicationTitle>Multisensory Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cff2b15b5a12ee2e7d7fb46f7c32ffd9"><gtr:id>cff2b15b5a12ee2e7d7fb46f7c32ffd9</gtr:id><gtr:otherNames>Pasqualotto A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>doi_55f982982bcbaba0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB61104C-0D5C-43B0-BDFD-9201B0546A73</gtr:id><gtr:title>Non-Visual Menu Navigation: the Effect of an Audio-Tactile Display</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e77dac7a6c74d7d3a33e15faee434237"><gtr:id>e77dac7a6c74d7d3a33e15faee434237</gtr:id><gtr:otherNames>Metatla, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54622ac361b033.19879890</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EC6708AA-E7C6-461F-961C-D580D7990D5E</gtr:id><gtr:title>Multisensory perceptual learning and sensory substitution.</gtr:title><gtr:parentPublicationTitle>Neuroscience and biobehavioral reviews</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/923da7fbe9b1a26130f6baad1baa3755"><gtr:id>923da7fbe9b1a26130f6baad1baa3755</gtr:id><gtr:otherNames>Proulx MJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0149-7634</gtr:issn><gtr:outcomeId>doi_55f974974de19347</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>80A7A422-D096-4587-A2F3-3BDBED1DB4F8</gtr:id><gtr:title>Using community engagement to drive co-creation in rural China</gtr:title><gtr:parentPublicationTitle>International Journal of Design</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fbc349d2fc465d67b54b7c17905208d7"><gtr:id>fbc349d2fc465d67b54b7c17905208d7</gtr:id><gtr:otherNames>Wang W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b81d96b648f9.72008883</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87B7D6E1-0292-4965-B780-0EEEF025AEE8</gtr:id><gtr:title>An Activity Patterns Approach to Cross-modal Interface Design</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e77dac7a6c74d7d3a33e15faee434237"><gtr:id>e77dac7a6c74d7d3a33e15faee434237</gtr:id><gtr:otherNames>Metatla, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54622a85a6ab97.39384500</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DEAEC73C-37B4-4C4F-8F0B-D69F8E16B4B3</gtr:id><gtr:title>Multisensory integration, sensory substitution and visual rehabilitation.</gtr:title><gtr:parentPublicationTitle>Neuroscience and biobehavioral reviews</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/923da7fbe9b1a26130f6baad1baa3755"><gtr:id>923da7fbe9b1a26130f6baad1baa3755</gtr:id><gtr:otherNames>Proulx MJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0149-7634</gtr:issn><gtr:outcomeId>5675da789dccf</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>68412385-7712-40E6-8A1D-3839903039DC</gtr:id><gtr:title>Supporting Cross-Modal Collaboration in the Workplace</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e77dac7a6c74d7d3a33e15faee434237"><gtr:id>e77dac7a6c74d7d3a33e15faee434237</gtr:id><gtr:otherNames>Metatla, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54622955750405.13573972</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5A2ED1A2-771D-43FA-8806-317F3939E3D2</gtr:id><gtr:title>Other ways of seeing: From behavior to&amp;nbsp;neural mechanisms in the online &amp;quot;visual&amp;quot;&amp;nbsp;control of action with sensory&amp;nbsp;substitution.</gtr:title><gtr:parentPublicationTitle>Restorative neurology and neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/923da7fbe9b1a26130f6baad1baa3755"><gtr:id>923da7fbe9b1a26130f6baad1baa3755</gtr:id><gtr:otherNames>Proulx MJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0922-6028</gtr:issn><gtr:outcomeId>585d53011daae5.44857539</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0DFBAAEF-54D3-474B-9EF4-60C019D308A8</gtr:id><gtr:title>Increased signal complexity improves the breadth of generalization in auditory perceptual learning.</gtr:title><gtr:parentPublicationTitle>Neural plasticity</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/769a0d7b29c95a179ce5bc84f994f1d6"><gtr:id>769a0d7b29c95a179ce5bc84f994f1d6</gtr:id><gtr:otherNames>Brown DJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1687-5443</gtr:issn><gtr:outcomeId>doi_55f944944010989e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C82278E-BA1B-4867-86A4-D6213C8AC729</gtr:id><gtr:title>Audio-haptic mockups, audio diaries and participatory prototyping: Designing with and for people living with visual impairments</gtr:title><gtr:parentPublicationTitle>CoDesign: International Journal of CoCreation in Design and the Arts.</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442f06f2b2938179e170c93d972bbf37"><gtr:id>442f06f2b2938179e170c93d972bbf37</gtr:id><gtr:otherNames>Metatla O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58b81e37654744.97444057</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7B99076C-384C-404F-97D8-384E8E12B68E</gtr:id><gtr:title>Visual Objects in the Auditory System in Sensory Substitution: How Much Information Do We Need?</gtr:title><gtr:parentPublicationTitle>Multisensory Research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57ce83deb2398a489b9544f192ed396b"><gtr:id>57ce83deb2398a489b9544f192ed396b</gtr:id><gtr:otherNames>Brown D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f94494401a856f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EDDED5FC-01B4-490F-8142-348ED432220A</gtr:id><gtr:title>Anticlockwise or clockwise? A dynamic Perception-Action-Laterality model for directionality bias in visuospatial functioning.</gtr:title><gtr:parentPublicationTitle>Neuroscience and biobehavioral reviews</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/384a3691b5a44deee0b34a98a2b52935"><gtr:id>384a3691b5a44deee0b34a98a2b52935</gtr:id><gtr:otherNames>Karim AKMR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0149-7634</gtr:issn><gtr:outcomeId>585d52fe148ec8.76369300</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C456D07-1829-44A5-B8F9-24F4A0CB0893</gtr:id><gtr:title>Neural Stimulation Has a Long-Term Effect on Foreign Vocabulary Acquisition.</gtr:title><gtr:parentPublicationTitle>Neural plasticity</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cff2b15b5a12ee2e7d7fb46f7c32ffd9"><gtr:id>cff2b15b5a12ee2e7d7fb46f7c32ffd9</gtr:id><gtr:otherNames>Pasqualotto A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1687-5443</gtr:issn><gtr:outcomeId>5675e6889f4ed</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5654DD8F-A8D0-41F1-9A29-16B4E54D2FF5</gtr:id><gtr:title>Parietal transcranial random noise stimulation improves long-term visual acquisition of foreign vocabulary</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a6e4b814854e4603c4e92e92cde05d5"><gtr:id>5a6e4b814854e4603c4e92e92cde05d5</gtr:id><gtr:otherNames>Pasqualotto A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>5a34ede7c6ac94.45870921</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3CA7C74A-7F2D-4DB5-A2F8-FD965C7B60FE</gtr:id><gtr:title>Audio-haptic interfaces for digital audio workstations</gtr:title><gtr:parentPublicationTitle>Journal on Multimodal User Interfaces</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442f06f2b2938179e170c93d972bbf37"><gtr:id>442f06f2b2938179e170c93d972bbf37</gtr:id><gtr:otherNames>Metatla O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b81cc6568bc5.21367061</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A569A9A-B6AB-4382-B6BA-5433EAED6A93</gtr:id><gtr:title>Audio&amp;acirc;??Vision Substitution for Blind Individuals: Addressing Human Information Processing Capacity&amp;Acirc;&amp;nbsp;Limitations</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57ce83deb2398a489b9544f192ed396b"><gtr:id>57ce83deb2398a489b9544f192ed396b</gtr:id><gtr:otherNames>Brown D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d3f5492dea2.11100025</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1419D128-804A-41B2-857B-425264357F1C</gtr:id><gtr:title>Designing with and for people living with visual impairments: audio-tactile mock-ups, audio diaries and participatory prototyping</gtr:title><gtr:parentPublicationTitle>CoDesign</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/442f06f2b2938179e170c93d972bbf37"><gtr:id>442f06f2b2938179e170c93d972bbf37</gtr:id><gtr:otherNames>Metatla O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cf15cfc1a4e1.07619769</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21FDC14F-2330-4C0D-88A9-3500CECAB64F</gtr:id><gtr:title>Where am I? Who am I? The Relation Between Spatial Cognition, Social Cognition and Individual Differences in the Built Environment.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/923da7fbe9b1a26130f6baad1baa3755"><gtr:id>923da7fbe9b1a26130f6baad1baa3755</gtr:id><gtr:otherNames>Proulx MJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>585d53109f55b7.92856767</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A474576-0936-43A6-9411-00523E2F6A8D</gtr:id><gtr:title>Developing Methods and Techniques for the Design of Cross-modal Displays</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e77dac7a6c74d7d3a33e15faee434237"><gtr:id>e77dac7a6c74d7d3a33e15faee434237</gtr:id><gtr:otherNames>Metatla, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54622a43f231b7.92072159</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J017205/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>90</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>