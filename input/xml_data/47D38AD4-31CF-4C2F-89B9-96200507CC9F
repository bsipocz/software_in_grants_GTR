<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/F21F29B2-0B99-4924-8161-FE5AB5A5C90D"><gtr:id>F21F29B2-0B99-4924-8161-FE5AB5A5C90D</gtr:id><gtr:name>Silicon Graphics Inc</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/CCFE4AA7-0CE6-4E88-92AA-311632597966"><gtr:id>CCFE4AA7-0CE6-4E88-92AA-311632597966</gtr:id><gtr:name>Electrosonic</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/7ED107EA-B191-4F38-9DB4-5C13AC4E9AB2"><gtr:id>7ED107EA-B191-4F38-9DB4-5C13AC4E9AB2</gtr:id><gtr:name>Visual Acuity Ltd</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/102DDBE2-D0C6-4980-B1AE-6C5A15949672"><gtr:id>102DDBE2-D0C6-4980-B1AE-6C5A15949672</gtr:id><gtr:name>Avanti Communications</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/0107329B-F086-4C36-8652-5AF6752EC6DC"><gtr:id>0107329B-F086-4C36-8652-5AF6752EC6DC</gtr:id><gtr:name>Roehampton University</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line2>Roehampton Lane</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW15 5PU</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0107329B-F086-4C36-8652-5AF6752EC6DC"><gtr:id>0107329B-F086-4C36-8652-5AF6752EC6DC</gtr:id><gtr:name>Roehampton University</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line2>Roehampton Lane</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW15 5PU</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F21F29B2-0B99-4924-8161-FE5AB5A5C90D"><gtr:id>F21F29B2-0B99-4924-8161-FE5AB5A5C90D</gtr:id><gtr:name>Silicon Graphics Inc</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CCFE4AA7-0CE6-4E88-92AA-311632597966"><gtr:id>CCFE4AA7-0CE6-4E88-92AA-311632597966</gtr:id><gtr:name>Electrosonic</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7ED107EA-B191-4F38-9DB4-5C13AC4E9AB2"><gtr:id>7ED107EA-B191-4F38-9DB4-5C13AC4E9AB2</gtr:id><gtr:name>Visual Acuity Ltd</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/102DDBE2-D0C6-4980-B1AE-6C5A15949672"><gtr:id>102DDBE2-D0C6-4980-B1AE-6C5A15949672</gtr:id><gtr:name>Avanti Communications</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1E267E0C-FC58-46FF-99CC-0AC5FDF5F588"><gtr:id>1E267E0C-FC58-46FF-99CC-0AC5FDF5F588</gtr:id><gtr:name>Silicon Graphics International Corp SGI</gtr:name><gtr:address><gtr:line1>200 Berkshire Place</gtr:line1><gtr:line2>Wharfedale Road</gtr:line2><gtr:line3>Winnersh</gtr:line3><gtr:line4>Wokingham</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG41 5RD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A66281EB-0EDE-495D-AD55-D81A4E060EC6"><gtr:id>A66281EB-0EDE-495D-AD55-D81A4E060EC6</gtr:id><gtr:name>Electrosonic Ltd</gtr:name><gtr:address><gtr:line1>Hawley Mill</gtr:line1><gtr:line2>Hawley Road</gtr:line2><gtr:line4>Dartford</gtr:line4><gtr:postCode>DA2 7SY</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/77E367B6-A19C-47CB-A7C0-20F8B35D1217"><gtr:id>77E367B6-A19C-47CB-A7C0-20F8B35D1217</gtr:id><gtr:name>Avanti Communications Limited</gtr:name><gtr:address><gtr:line1>20 Blackfriars Lane</gtr:line1><gtr:postCode>EC4V 6EB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/281178AD-E637-43C4-9943-0B22423A69AF"><gtr:id>281178AD-E637-43C4-9943-0B22423A69AF</gtr:id><gtr:name>VISUAL ACUITY LIMITED</gtr:name><gtr:address><gtr:line1>8 Brighton Office Campus</gtr:line1><gtr:line2>Hunns Mere Way</gtr:line2><gtr:line4>Brighton</gtr:line4><gtr:postCode>BN2 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/DEB7523A-12F0-46D2-9CF5-A33204EFFABA"><gtr:id>DEB7523A-12F0-46D2-9CF5-A33204EFFABA</gtr:id><gtr:firstName>John</gtr:firstName><gtr:surname>Rae</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F2B5CE55-9452-4119-BB1E-4BBA840F8B97"><gtr:id>F2B5CE55-9452-4119-BB1E-4BBA840F8B97</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:surname>Dickerson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE007570%2F1"><gtr:id>47D38AD4-31CF-4C2F-89B9-96200507CC9F</gtr:id><gtr:title>Eye Catching: Supporting tele-communicational eye-gaze in Collaborative Virtual Environments</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E007570/1</gtr:grantReference><gtr:abstractText>Driven by the potentials and demands of an increasing global market and fed by advances in information and communication technology, one of the trends in the modern workplace is for more distributed team working. Distributed working has long been a major topic in computer science, but despite excellent work and the development of highly sophisticated computer-supported cooperative work (CSCW) systems, in many situations there is no substitute for a face-to-face meeting. The consequent demands for travel to meetings have immediate short term quality of life and productivity impacts on individuals. There may also be more far-reaching and profound implications of our current reliance on long distance travel. Thus it is still very relevant to try to determine why some CSCW fails and to research possible technologies for expanding the situations for which face to face meetings can be avoided. There are numerous common collaborative scenarios that require a more natural way of interacting across a distance. Specifically we have identified conscious and subconscious communication of attention and emotion as common critical elements that make many such scenarios hard to support without eye-gaze. Eye-gaze is a key interactional resource in collaboration but it is not well supported in today's communication technology. Indeed many have claimed that lack of ability to faithfully represent eye-gaze is the key failing of current CSCW systems. Within today's video based systems eye-gaze can be maintained in some limited way if the user is willing to look directly at a camera, but this is unnecessarily constraining in a social situation, especially during object or environment focussed collaboration.We propose to evaluate the role of eye-gaze in tele-communication so as to better design future communication technologies. To do this we will build the world's first tele-collaboration system that supports two and three way communicational eye-gaze without restricting the gaze direction of participants. We will integrate eye-tracking technologies into Immersive Projection Technology (IPT) displays, and develop the software necessary to build a consistent collaborative virtual environment where each participant can see the other and accurately track their eye-gaze. To prove the utility of this system we will compare it to AccessGrid technology which provides state-of-the-art video conferencing on large wall displays. Although unable to support communicational eye-gaze between moving participants, AccessGrid does offer advantages in terms of placement within working environments and realism of representation. Comparison between the two approaches will provide valuable insight into future development of each. Through a series of experiments we will establish what conditions are necessary and sufficient to support communicational eye-gaze in a tele-communication system; validate the support of eye-gaze in tele-communication by measuring its impact on collaboration; measure the impact of technology approaches and variables; establish when eye-gaze is important; and establish situations where eye-gaze is critical for successful collaboration at a distance.</gtr:abstractText><gtr:fund><gtr:end>2008-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>84253</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Visual Acuity Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>VISUAL ACUITY LIMITED</gtr:description><gtr:id>3A42EB2A-9220-4399-B8C5-15F9157DCFDB</gtr:id><gtr:outcomeId>b9c6a200b9c6a214-1</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2006-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Silicon Graphics Inc</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Silicon Graphics Inc</gtr:description><gtr:id>62A79E2F-944A-4C0C-AE77-EBFE2AC45AA8</gtr:id><gtr:outcomeId>b9c4ce76b9c4ce8a-1</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2006-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Avanti Communications</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Avanti Communications Limited</gtr:description><gtr:id>9DA9E182-45CC-4E29-95A7-6FFA0C87983B</gtr:id><gtr:outcomeId>b9c598a6b9c598c4-1</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2006-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Electrosonic</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Electrosonic Ltd</gtr:description><gtr:id>36CFE2B9-C81E-40B9-B10B-79A78A322DF7</gtr:id><gtr:outcomeId>b9c5bf48b9c5bf5c-1</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2006-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>&quot;Passing stuff: How do humans accomplish manual object transfers?&quot;</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:id>754D2E9E-5F5D-4D3D-9AE8-C5FCC253E7F1</gtr:id><gtr:impact>Departmental Research Presentation, Department of Psychology, University of Staffordshire.</gtr:impact><gtr:outcomeId>r-9445250443.5975570ba7b928</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2010</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Vocal and visible displays of stance in object-centred interactions.</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FAA3108C-E26F-4A8C-BA33-CF03CD7F512D</gtr:id><gtr:impact>(Extracted from the Abstract:)

One fundamental locus for language use is in the context of handling objects. But the manipulation of objects, is rarely value-free, rather it is saturated with matters of taste. Consequently displays of stance and affect often occur in the course of projects that involve making or moving things and participants can be seen to use a range of linguistics resource to express such things as approval/disapproval, enthusiasm/reticence (Ochs &amp;amp; Schieffelin (1989). The present study aims to contribute to such studies by further examination of both family and peer-interactions in which participants are concerned with tasks that involve the handling of objects. The aim is to answer the following questions: How, and in what ways, do people working together on the transformation of materials, display stance and affect as part of organizing themselves to get that work done? This paper uses Conversation Analysis (CA) to examine videorecordings of interactions in which participants are concerned with doing things with material objects. The data involve interactions involving objects that are being made by the participants (e.g. cooking or assembly furniture). The analysis shows how participants'values and asymmetries in the participants'knowledge come to the surface focusing on (1) How advice- and instructiongiving can occur as situated practices and how their delivery is interactionally organized, (2) How stance and affect are manifested in these activities.The analysis shows how multimodal resources are implicated in these activities, in particular how the positioning of objects and persons can be such that participants analysis of something can be literally a matter of their stance and other visible conduct. In conclusion, we discuss the relevance of Conversation Analysis for understanding human sociality in particular how sensitivities, values and methods are passed on, shared or managed.

This is a presentation at an academic conference.</gtr:impact><gtr:outcomeId>r-2383617518.8269630bc95362</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2011</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Transformations of gazing practices in the trajectory of humorous talk in a co-present, object-focused, multiparty task.</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>96B05BAB-BCD7-49E9-BE9A-1E22162D8F1A</gtr:id><gtr:impact>Abbreviated Abstract: The organization of talk with respect to concurrent activities undertaken by co-present participants is little understood. Whilst the business of some settings is fundamentally accomplished through talk, in other settings it is the manipulation of objects that comprises the business at hand. Although talk may become relevant - indeed necessary - on occasion in such settings (e.g. in order to secure the coordination of actions), non task-related talk may be a possibility. Such talk appears to be highly important for the character of the setting and is evidently important in the management of the relationship between participants. This talk is often of a humorous nature. In this paper we examine the conduct of three people engaged in the assembly of an item of flat-pack furniture. 

We show that a recurrent type of humorous talk involves:

(a) the occurrence of a task-related event

(b) humorous talk that retrospectively targets that event

(c) responses to the humorous talk that display an understanding of the humorous talk. 



Consequently then, this type of talk, though not directly task-related, is connected with the concurrent task in two ways. Firstly it is rooted in task-related events; secondly it thematizes features of the current situation. 

Such talk allows for transformations of the local participation framework, for example providing opportunities for participants who may be engaged in individual work on sub-tasks to engage or interact with other participants. Producers of humorous remarks regularly monitor their recipients by gazing at them. We discuss the importance of humorous talk within such settings and how it creates opportunities for affiliation through nonverbal displays of engagement and shared understanding. 
.

This is a presentation at an academic conference.</gtr:impact><gtr:outcomeId>r-6564977892.2693610b9d8408</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2009</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Passing stuff: How do humans accomplish manual object transfers?</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B4E0A54B-7DC3-4733-94E1-5E3B26C5BB29</gtr:id><gtr:impact>Presentation to the University of Wisconson Madison

Social Psychology and Microsociology (SPAM) group.</gtr:impact><gtr:outcomeId>r-4642926579.9400640bd294a4</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2011</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Data Analysis Workshop</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>703D3E5F-6EC5-4F46-BABC-C195515FDF48</gtr:id><gtr:impact>Data Analysis Worshop.</gtr:impact><gtr:outcomeId>r-2302438954.17306470bd28ab8</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2011</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Showing how the work is done in art studio instructional interactions</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>63EE43EC-04D8-435C-85AB-AACB8C515FC2</gtr:id><gtr:impact>Instruction in craft practice makes extensive use of gesture in order to demonstrate the practical and aesthetic possibilities of hands and minds, tools and materials. The present report focuses on instruction concerning a specific a feature of manual work namely the progressive transformation of materials. Ingold has referred to &amp;quot;the processional character of tool use&amp;quot; (Ingold, 2006) and in an account that reflects on his own practice, articulates how the conduct of a basic action (sawing a plank) involves and requires different sawing action at different phases of the job as the cut progresses. The sites where craft skills are developed characteristically involve learning through observing or co-participating with expert practitioners (Marchand, 2008; Sennet, 2008) or through forms of guided participation in activities (Ekstr&amp;ouml;m, Lindwall, &amp;amp; S&amp;auml;lj&amp;ouml;, 2009). Drawing on Conversation Analysis to examine the sequential organization of actions in videorecordings of 16 classes in a printmaking studio and a metalworking studio, this paper shows how instructors use gesture in coordination with other resources (such as talk and the manipulation of tools) to display the progressive character of different jobs. In particular, the analysis shows how bodily conduct and facial expressions are used to show effort and care during different phases of procedures. Instructors reveal a concern with getting students to appreciate what should be done, the manner in which it should be done (e.g. what degree force or care is required); what should be looked for (or otherwise sensed) and what might be found. The paper concludes be discussing how instruction in this configuration of mindful action which can accommodate and control progressively unfolding work is fundamentally dependent on the coordination of gestural resources.
.

This is a research presentation at an academic conference.</gtr:impact><gtr:outcomeId>r-4354182113.2918650c070a4a</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Passing stuff: How do humans accomplish manual object transfers?</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D0D1F6B0-E5B1-40CA-AF95-858276E8F596</gtr:id><gtr:impact>The transfer of objects from one party to another is a routine human activity; the understanding of core aspects of it appear early in life (Lerner and Zimmerman, 2003, Wootton 1994). In addition to the perceptual and kinaesthetic skills involved, the passing of objects presents a coordination problem such that parties' bodies are brought into appropriate positions with respect to each other, and objects released and secured such that they are not dropped or their contents spilled.



This paper uses Conversation Analysis to examine the social organization of manual object transfers in video-recordings of several hours of naturally-occurring domestic interactions. These mainly involve adult participants engaged in the assembly of flat-pack furniture and include some food-preparation and mealtime interactions.



We show that manual object transfers have a distinctive phase-structure that participants orient to (Sacks and Schegloff 2002, Lerner and Raymond, 2008) involving preparation for transfer, the transfer itself and a post transfer phase. Whilst transfer itself involves the non-verbal actions of releasing and taking the object being passed, it routinely occurs with concurrent talk. We examine how talk from the recipient of the object can play a crucial role in the accomplishment of the transfer.
.

This is a presentation at an academic conference.</gtr:impact><gtr:outcomeId>r-6874763534.2877030ba80824</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2010</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Tele-mediated social interaction in Videoconferencing and in an Immersive Collaborative Virtual Environment.</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>99BB7535-4F75-49F4-91BE-ED93325EB5C9</gtr:id><gtr:impact>The very first words reputed to have been heard over a telephone system concern spoke to the participants' physical separation between the participants. (Bell to his assistant: &amp;quot;Mr. Watson come here, I want you.&amp;quot;) The development of telecommunication systems that enable participants to see, as well as hear, each other seeks to advance on the telephone system. However the well-known problems of spatial alignment in videoconferencing can have the effect of making participants' spatial separation apparent. Whilst telephone talk can constitute and take place within a shared virtual aural space, in videoconferencing participants usually look out from their own -and gaze into each others' - separate spaces. An alternative means of supporting tele-mediated interaction is the use of virtual reality to create a shared virtual space in which participants are represented by avatars. This paper reports on the Eye-Catching project where participants interact in an Immersive Collaborative Virtual Environments (ICVE) in which, for the first time, their eye-gaze is captured by eye-trackers mounted in the stereo-glasses that are worn in the ICVE - and displayed by their avatars. We compare three way interaction within this environment with interactions in an AccessGrid videoconferencing environment. We examine video-recordings from head-mounted cameras showing each participant's view and which uses eye-tracking to show their point of gaze within that view. Using conversation analysis we examine the management of turn-taking and gaze distribution in these environments. Firstly we discuss the analytical impact of working with eye-tracking data. Secondly we assess the possible limitations and/or advantages of these different technologies for supporting tele-mediated social interaction. Thirdly we discuss how participants'understanding of space becomes evident in their conduct.

This is a presentation at an academic conference.</gtr:impact><gtr:outcomeId>r-2475245913.93326040b946030</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2008</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>&quot;Interacting with things&quot;</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:id>37D0AC7A-8F99-43D8-9D82-E7249D580FB0</gtr:id><gtr:impact>Research Seminar, Department of Education, Presentation at the University of Roehampton.</gtr:impact><gtr:outcomeId>r-4821866885.5930370b9d5a14</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:year>2009</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Hands, objects and courses of action: How handshape in handling objects can be interactionally relevant</gtr:description><gtr:form>Scientific meeting (conference/symposium etc.)</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>4F769297-492C-40F3-948C-210D59F75B73</gtr:id><gtr:impact>Abstract. In road traffic, a vehicle's position and orientation are major cues to the driver's intention (or lack of intention). As such, what someone is doing with an object can inform others about how they might participate in the unfolding course of action that that person is engaged in. Unlike the control of a vehicle, in the case of objects that are literally handled (that is objects that are manipulated manually, by the hands), properties of the hands themselves, such as finger positions, can be become relevant (Streeck, 2009). In such settings, it is not just that hands reflect key aspects of what a speaker is doing (McNeill, 1992) but that the properties of an object (e.g. its shape or position) can become a field within which the

hand(s) operate (Goodwin, 2000). Whilst previous research has examined how participants' talk, gaze and gestures can all be relevant for co-participants (e.g. Bolden, 2003) the focus of the present analysis is on manual configurations specifically.

This present paper aims to make a contribution to our understanding of how the position of the hands, in handling objects, can make available to others what an agent is doing, more specifically, how features of the course of action that an agent is engaged in can become available to others. We use Conversation Analysis to examine the social organization of manual object handling in video-recordings of several hours of naturally-occurring domestic interactions, in particular involving adult human participants engaged in the assembly of flat-pack furniture. We show how configurations of the hand, such as finger positions, are oriented to by co-participants, in particular how they can make available for them the course of action that an agent is engaged in. We discuss the the relevance of the visibility of hand-object configurations in telecommunication and virtual reality applications.
.

This is a presentation at an academic conference.</gtr:impact><gtr:outcomeId>r-529597811.894116760ba80518</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2010</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Despite and the development of highly sophisticated computer-supported cooperative work (CSCW) systems, which aim to allow physically distant participants to interact via telecommunication systems, in many situations there is no substitute for meeting face-to-face. However the demands of travelling to meetings have a profound impact on people's quality of life and productivity and our current reliance on long-distance travel may have serious environmental implications. Consequently, it is important to determine why some CSCW systems fail and to research technologies that can reduce the need for face-to-face meetings. 

A key issue is that seeing other people's eye-gaze is an important interactional resource, it is involved in the communication of focus of attention and of emotion states, but is not well-supported by current telecommunication technology. Current video-based systems can support eye-gaze if users are positioned such that they look directly at a camera; this can be highly restrictive, especially in interactions involving moving around an object.



This project evaluated the potential of tele-communication systems using Immersive Projection Technology (IPT) displays in which, rather than seeing video images of each other, participants meet avatars (graphical representations) of one another in a shared virtual space, or Immersive Collaborative Virtual Environment (iCVE). Working closely with computer scientists, we contributed psychological and social scientific expertise to the building and evaluation of an IPT system that integrated eye-tracking technologies to capture each participant's eye-gaze behaviour and reproduce it on an avatar of that person. This is the world's first tele-collaboration system that supports two and three way eye-gaze without restricting participants' gaze direction.

Initial tests established that observers of an avatar with fixed-eyes found it very hard to judge which of a number of objects the person represented by the avatar was looking at. However when eye-tracking was added, the person's gaze could be judged very accurately. We used another task in which viewers made judgments about the gaze-direction of a remote participant to evaluate our IPT-based approach by comparing it with state-of-the-art video conferencing (with screens and high definition cameras arranged to provide optimal alignment across the remote spaces). We further tested the performance of the iCVE in an object-manipulation task (in which participants had to use eye-gaze) by comparing different avatar gaze conditions (eye-tracking; eye-gaze modelled based on previously collected parameters, and eyes-fixed straight ahead). 

In addition to analyzing the data arising from these studies, in particular examining the organization of interactions in these experiments and comparing it to previously identified practices, we also collected further samples of face-to-face interactional behaviour from a variety of settings in order to better understand how eye-gaze is used.



Key findings were:

-Video conferencing allows gaze-direction to be judged accurately when the position of participants is constrained; the iCVE system enables this without this constraint.

-Video-technology and iCVE systems have complementary strengths and weaknesses; the former faithfully communicates a person's appearance, the latter faithfully represents where they are looking.

-Eye-tracking in an iCVE improves people's ability to judge gaze-direction in some but not all circumstances; in some situations head-orientation is sufficient.

-Avatar appearance, including accurate representation of eye movement, is an important determinant of success of these systems.

-In the iCVE, when difficulties in judging other's gaze-direction arise, parties use change of position as a resource, exploiting being in a shared virtual space.

-In the iCVE, certain interactional gazing practices cannot be simulated but need to be captured and reproduced.</gtr:description><gtr:exploitationPathways>The idea of using graphic representations of remote participants appearance that show the participant's eye gaze have a number of applications (telecommunication, training) and could be developed commercially. This research contributes to our understanding of human interaction in face-to-face settings and in tele-mediated settings. The main implications are for the design of telecommunication systems. The research demonstrates the value of immersive projective technologies in telecommunication settings. It also suggests that providing participants at remote sites with visible access to how a participants are moving with respect to objects within their site can be important.</gtr:exploitationPathways><gtr:id>BABD013C-2AB8-4037-A065-40767E38AFE5</gtr:id><gtr:outcomeId>r-6135261334.6048277774a3d0</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9EBF905D-6493-46D2-A708-4F6ABDF5C0BE</gtr:id><gtr:title>withyou-An Experimental End-to-End Telepresence System Using Video-Based Reconstruction</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/89bbd8bbb11d7df2e8abe26e3a4ea234"><gtr:id>89bbd8bbb11d7df2e8abe26e3a4ea234</gtr:id><gtr:otherNames>Roberts D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>doi_55f950950e47693d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61A1F09B-3FF6-43A6-94A3-D6E35386A3FA</gtr:id><gtr:title>An assessment of eye-gaze potential within immersive virtual environments</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Multimedia Computing, Communications, and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96e8ea87bba1be812a92169b4bea38a9"><gtr:id>96e8ea87bba1be812a92169b4bea38a9</gtr:id><gtr:otherNames>Murray N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53d07407413db853</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D8A5CE91-3672-4005-A29B-84E21DEDF6AF</gtr:id><gtr:title>Simulation versus Reproduction for Avatar Eye-Gaze in Immersive Collaborative Virtual Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/230c63452a31ceb4e1c5e6da48bf939a"><gtr:id>230c63452a31ceb4e1c5e6da48bf939a</gtr:id><gtr:otherNames>J Rae</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_326355083113e43708</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2CFFFBA-2866-4FE3-A6E1-8B132F90BDB9</gtr:id><gtr:title>Communicating Eye-gaze Across a Distance: Comparing an Eye-gaze enabled Immersive Collaborative Virtual Environment, Aligned Video Conferencing, and B</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/748bb76d513300e1f62f1543b0b306f1"><gtr:id>748bb76d513300e1f62f1543b0b306f1</gtr:id><gtr:otherNames>D Roberts</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_490455983513dcb406</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06E6294D-1D51-48F1-B9E8-7772166CD9D4</gtr:id><gtr:title>Eye gaze in virtual environments: evaluating the need and initial work on implementation</gtr:title><gtr:parentPublicationTitle>Concurrency and Computation: Practice and Experience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96e8ea87bba1be812a92169b4bea38a9"><gtr:id>96e8ea87bba1be812a92169b4bea38a9</gtr:id><gtr:otherNames>Murray N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53cfc3fc36a8a021</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0EE87E56-160C-4874-B60D-E72D81D16A1C</gtr:id><gtr:title>Some Implications of Eye Gaze Behavior and Perception for the Design of Immersive Telecommunication Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/662c41f4317d8817ae87aa88c1832721"><gtr:id>662c41f4317d8817ae87aa88c1832721</gtr:id><gtr:otherNames>Rae J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1643-0</gtr:isbn><gtr:outcomeId>doi_53d05705762e35bd</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E007570/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>