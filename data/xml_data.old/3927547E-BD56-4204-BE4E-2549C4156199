<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Human Communication Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/99686D00-BBAC-4A5E-9938-7927C0ACC48D"><gtr:id>99686D00-BBAC-4A5E-9938-7927C0ACC48D</gtr:id><gtr:firstName>Bencie</gtr:firstName><gtr:surname>Woll</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=RES-185-31-0001"><gtr:id>3927547E-BD56-4204-BE4E-2549C4156199</gtr:id><gtr:title>Business Engagement Award</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>RES-185-31-0001</gtr:grantReference><gtr:abstractText>&lt;p>This project builds on DCAL?s research portfolio. Studies 1 and 2 establish partnerships with businesses working in educational multimedia; Study 3 transfers the findings of our research on lipreading to forensic work.&lt;/p>

&lt;p>&lt;strong>Study 1. e-profiling of children?s development&lt;/strong>&lt;br />Applies DCAL?s research expertise in children?s acquisition of BSL to computer-based assessments (e-profiling).&amp;nbsp; Expertise in the development of language and cognitive assessments&amp;nbsp; enables the evaluation of whether adaptations of nationally normed assessments are appropriate and valid; in turn DCAL benefits from the partners? expertise to assist DCAL to convert existing assessments to on-line administration.&lt;br />&lt;br />&lt;strong>Study 2. Signing avatars&lt;/strong>&lt;br />Involves a partnership in the use of signing avatar technology. The technology requires significantly improved linguistic underpinning since the existing grammatical descriptions are limited and often incorrect. DCAL expertise in the linguistic description of BSL will be applied to the creation of grammars to enable automatic rule-based translations from English to BSL.&lt;br />&lt;br />&lt;strong>Study 3 Speechreading&lt;/strong>&lt;br />The growth of CCTV recording offers opportunities for the interpretation of visual speech for use by public and private agencies (such as legal and judicial services, MI5, etc.). In collaboration with Deafworks, a series of seminars will provide training in forensic speechreading.&lt;/p></gtr:abstractText><gtr:fund><gtr:end>2008-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2007-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>55572</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The speechreading part of the project has assisted the development of the profession of forensic speechreaders; the avatar research has contributed to new developments in automated sign language generation</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>7B49EDDE-0B13-47C7-9268-466DEABDA13F</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>54620412c92505.55076886</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Government, Democracy and Justice</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have learned more about the demands and possibilities of using expert speechreaders in legal settings and to the establishment of criteria and tools for assessing speechreading skills.
In relation to signing avatars, the research has contributed to commercial and academic projects seeking to develop automated translation from written English to British Sign Language</gtr:description><gtr:exploitationPathways>The signing avatar findings are already in use by broadcasters seeking to develop better provision for deaf TV and web viewers; and the speechreading findings are being used in legal settings where speechreaders are asked to serve as expert witnesses</gtr:exploitationPathways><gtr:id>6644C9D3-AA13-432E-9792-440D0E3D8815</gtr:id><gtr:outcomeId>546204e5666310.28480366</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Government, Democracy and Justice</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>52C8F3EA-5326-49B9-B849-E7844ADAED01</gtr:id><gtr:title>What are the challenges in developing automated sign language translations?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/68760b458971e64b09953d8df9ed69a0"><gtr:id>68760b458971e64b09953d8df9ed69a0</gtr:id><gtr:otherNames>Woll B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54510fb9bb0be7.10029566</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>936BD160-0747-4800-9152-113188ABE4DC</gtr:id><gtr:title>Reading time data for evaluating broad-coverage models of English sentence processing.</gtr:title><gtr:parentPublicationTitle>Behavior research methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ff7ff026e8ee9082406961b3c0fe739"><gtr:id>2ff7ff026e8ee9082406961b3c0fe739</gtr:id><gtr:otherNames>Frank SL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1554-351X</gtr:issn><gtr:outcomeId>545bf42795ddc7.48667468</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D36943B-9674-4C84-8D47-41F0E142A200</gtr:id><gtr:title>How do typically developing deaf children and deaf children with autism spectrum disorder use the face when comprehending emotional facial expressions in British sign language?</gtr:title><gtr:parentPublicationTitle>Journal of autism and developmental disorders</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a0ac2021ff2690f1b651997c876a14e2"><gtr:id>a0ac2021ff2690f1b651997c876a14e2</gtr:id><gtr:otherNames>Denmark T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0162-3257</gtr:issn><gtr:outcomeId>545ce676e208e2.71400155</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>86F9879B-216D-4FDB-8B57-6DB9FD5D14F1</gtr:id><gtr:title>Multimodal Signals: Cost Action 2102 and Eucognition International School Vietri Sul Mare, Italy, April 21-26, 2008, Revised Selected and Invited Papers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1a5fe4cf51dc8d5f05f5bcc2a92a78fe"><gtr:id>1a5fe4cf51dc8d5f05f5bcc2a92a78fe</gtr:id><gtr:otherNames>Esposito, Anna; Hussain, Amir; Marinaro, Maria; Martone, Raffaele</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>9783642005244</gtr:isbn><gtr:outcomeId>i-1743435879.93032843bb503de</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RES">RES-185-31-0001</gtr:identifier><gtr:identifier type="RCUK">RES-185-31-0001</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>28710E94-8B47-404A-BD40-7BCB8C709774</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Linguistics (General)</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>