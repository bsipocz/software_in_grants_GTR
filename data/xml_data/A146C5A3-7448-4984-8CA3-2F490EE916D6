<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/0A73F6B3-EE27-4627-B3A4-6F6BFA7772AA"><gtr:id>0A73F6B3-EE27-4627-B3A4-6F6BFA7772AA</gtr:id><gtr:firstName>Walterio</gtr:firstName><gtr:otherNames>Wolfgang</gtr:otherNames><gtr:surname>Mayol-Cuevas</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/126D662F-FECC-4EA1-8BEA-F6EE25D36EAE"><gtr:id>126D662F-FECC-4EA1-8BEA-F6EE25D36EAE</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:otherNames>David</gtr:otherNames><gtr:surname>Calway</gtr:surname><gtr:orcidId>0000-0002-3488-5842</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/60A0C2A9-43B9-4BF6-A00A-3B2EADC1B14B"><gtr:id>60A0C2A9-43B9-4BF6-A00A-3B2EADC1B14B</gtr:id><gtr:firstName>Angela</gtr:firstName><gtr:surname>Doufexi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/BD657D8D-44F1-4C16-A1B9-8272420F71BC"><gtr:id>BD657D8D-44F1-4C16-A1B9-8272420F71BC</gtr:id><gtr:firstName>Henk</gtr:firstName><gtr:surname>Muller</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/04781A3B-0267-4D6B-B742-ADD641230E87"><gtr:id>04781A3B-0267-4D6B-B742-ADD641230E87</gtr:id><gtr:firstName>Christopher</gtr:firstName><gtr:surname>Williams</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/228395AF-65B2-4ED9-824F-F10C11FE44A9"><gtr:id>228395AF-65B2-4ED9-824F-F10C11FE44A9</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Beach</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=DT%2FF006489%2F1"><gtr:id>A146C5A3-7448-4984-8CA3-2F490EE916D6</gtr:id><gtr:title>ViewNet - Context enhanced networked services by fusing mobile vision and location</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>DT/F006489/1</gtr:grantReference><gtr:abstractText>The aim of the project is two fold to develop a flexible adhoc open communications network archetecture allowing plug-in of different carriers such as TETRA, WI-FI, GSM etc and novel sensor technology for personal localisation which provides greater contextual detail than existing systems. Localisation data, from GPS for example, only becomes useful when it is placed in context. Currently this is done mainly through integration with geographical data, locating users with respect to landmarks such as roads and buildings. Although effective for some applications, such as vehicle navigation, this is not sufficiently dynamic nor detailed enough for potential applications in personal localisation, which need to operate at far higher resolutions, e.g. in terms of rooms, objects, walkways, etc. The innovation in this project will to address this by fusing localisation with real-time vision based mapping. The latter will give a 'live' 3-D representation of space, enabling users to be simultaneously localised in absolute terms and relative to their immediate physical surroundings, e.g. within a room or with respect to an object. The use of vision based mapping also means that the derived map will be directly related to a users' perceived sense of location, allowing easy visualisation of the detailed context being provided and direct reference to the 3-D map. The implementation will be based within a scalable network architecture, with integrated communications and supporting multiple users. This will facilitate distributed mapping, allowing operational teams to share visual and position data relative to a common map. The approach will provide a powerful framework for personal localisation, particularly for applications in security or disaster incident management, in which rapid surveying and visualisation of sites is of critical importance.</gtr:abstractText><gtr:fund><gtr:end>2010-08-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-06-02</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>437643</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The project has successfully demonstrated the integration of visual mapping, absolute positioning and virtual annotation to create a system for efficiently creating and retrieving information in its spatial context. This is the first time this has been done in a real-time practical setting and user trials have demonstrated the unique potential of the technology. The trials were motivated by scenarios relating to operations of emergency services, hence demonstrating the potential of the technology in the target application area. 
Several of the individual components within the project have also significantly advanced the state of the art, notably the work on advanced visual mapping and virtual annotation, SWE message passing, multiple wireless operation and the use of location information for improved wireless performance. These advances enhance the potential for exploiting ViewNet technology and are likely to have significant potential in their own right.

There are several additional outcomes from the project which are likely to prove significant:

1. The usability studies carried out as part of the demonstrator evaluation represent a unique insight into the viability of this type of technology and provide a valuable benchmark for assessing usability requirements.
2. The use of the Sensor Web Enablement (SWE) standard for message passing within ViewNet is probably the first use of the standard within a real-time application and hence represents an important contribution to the development of standards in the area.
3. The relevance of the ViewNet technology to the Computer Games Industry was established early in the project and this was pursued as a further potential exploitation route. Towards this, a proof of principle game application was developed based on ViewNet technology in collaboration with Mobile Pie, a Bristol-based game development company. This successfully demonstrated the potential for exploitation in this area.
4. The technology is also of significant relevance to eye tracking technology, providing the potential for systems to become more flexible and hence useful. As a direct result of the Viewnet project, the main technical partner - University of Bristol - enhanced its collaboration with US market leader in Eye Tracking - Applied Science Laboratories (ASL) - and have subsequently played a key role in developing new applications for its Mobile Eye technology.

In addition to the above, the Viewnet has impacted on the UK expertise in vision based localisation and mapping, through enhanced knowledge acquired by post-doctorate research assistants and the use of the technology as demonstrators within undergraduate and postgraduate degree programmes within the University of Bristol.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>86A35AF9-F4E3-4CC8-90FD-E4DD6D132587</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5465d1c3845b72.61474835</gtr:outcomeId><gtr:sector>Creative Economy,Education,Leisure Activities, including Sports, Recreation and Tourism,Retail</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The key findings from this research project relate to how mobile augmented reality systems can be deployed in real world scenarios. Important discoveries and developments were made in the following areas:

1. Advanced visual mapping and annotation - the ability to build local 3-D models within real environments and enabling virtual annotation, tagging physical attributes with virtual data.

2. Development of a mobile wireless infrastructure, over secure low bandwidth technology, to support the above mapping and annotation.

3. The fusion of multiple localisation technologies to enable robust position finding and tracking in both GPS and GPS-denied environments.

4. Development of techniques and trials to enable full evaluation of an integrated prototype system based on the above in the context of recording and analysing crime scenes.</gtr:description><gtr:exploitationPathways>The project was developed and evaluated in the context of crime scene capture and analysis. However the principles behind the system have far wider implications. There are many applications in which mobile augmented reality can and will play an important role. But to be effective such systems need got be robust and operate in a seamless and scalable manner. This project demonstrated one way of doing that. Future applications include computer gaming, disaster relief operations (e.g. earthquakes), and health, notably assisted living and stroke rehabilitation for example.</gtr:exploitationPathways><gtr:id>C6371F17-5B40-47A3-A215-25EDBB785E24</gtr:id><gtr:outcomeId>5465cf8cad57f0.80179689</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Construction,Creative Economy,Environment,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Manufacturing, including Industrial Biotechology,Culture, Heritage, Museums and Collections,Retail,Security and Diplomacy</gtr:sector></gtr:sectors><gtr:url>http://www.viewnet.org.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>E36EBAE1-88B6-40B5-BFA3-9177CC05865E</gtr:id><gtr:title>Visual Mapping and Multi-modal Localisation 2 for Anywhere AR Authoring</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/00b1aa4a0ae640bccc57df6213f016fa"><gtr:id>00b1aa4a0ae640bccc57df6213f016fa</gtr:id><gtr:otherNames>A Gee</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_770159112513cc1394</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19C94F07-9DC2-49B1-AC96-A0147F9A4617</gtr:id><gtr:title>Application of multiple-wireless to a visual localisation system for emergency services</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6d39b91e99598c1fbe10facd499f6262"><gtr:id>6d39b91e99598c1fbe10facd499f6262</gtr:id><gtr:otherNames>Efthymiou C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-8017-3</gtr:isbn><gtr:outcomeId>5465eac22d4938.20802553</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D27FF5B9-FE2B-4821-9B1D-B2DD0AD4EA46</gtr:id><gtr:title>Wireless Schedulers with Future Sight via Real-Time 3D Environment Mapping</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d906c34eecaf86705d30c2b783c23c6f"><gtr:id>d906c34eecaf86705d30c2b783c23c6f</gtr:id><gtr:otherNames>Webb M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-3573-9</gtr:isbn><gtr:outcomeId>doi_53d06006071f3da6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C16080B-A655-4FD2-93BD-13909718B20C</gtr:id><gtr:title>Low-Feedback Multiple-Access and Scheduling via Location and Geometry Information</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a107288efe833a6a2ee86b9222076d19"><gtr:id>a107288efe833a6a2ee86b9222076d19</gtr:id><gtr:otherNames>Han C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-3573-9</gtr:isbn><gtr:outcomeId>doi_53d0600606f3dada</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E4374EB-3A62-4C76-803D-2498F583D6E3</gtr:id><gtr:title>A topometric system for wide area augmented reality</gtr:title><gtr:parentPublicationTitle>Computers &amp; Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f7994c99ba7ab372788c405738a1faa1"><gtr:id>f7994c99ba7ab372788c405738a1faa1</gtr:id><gtr:otherNames>Gee A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5465ea74c8e070.18667130</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA21824F-44ED-4E6C-8096-98283E7B1DA5</gtr:id><gtr:title>Enhanced Wireless Multiple Access and Scheduling Techniques Using Positional and Environment Information</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a63eac14c70f0d15fc73327315e98bf"><gtr:id>5a63eac14c70f0d15fc73327315e98bf</gtr:id><gtr:otherNames>C Han</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_273581894313da3546</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">DT/F006489/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>13B6D44B-6EAF-464B-A4CC-1C08F8DDE5A0</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Mobile Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>34B6BDD6-DA02-4CA0-A969-29D50394A953</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Networks &amp; Distributed Systems</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>