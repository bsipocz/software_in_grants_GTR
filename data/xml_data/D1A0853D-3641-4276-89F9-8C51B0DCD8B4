<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name><gtr:address><gtr:line1>BBSRC</gtr:line1><gtr:line2>Polaris House</gtr:line2><gtr:line3>North Star Avenue</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:postCode>SN2 1UH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/8468C481-F977-4EDF-90F6-4E834956AECE"><gtr:id>8468C481-F977-4EDF-90F6-4E834956AECE</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>McOwan</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF037384%2F1"><gtr:id>D1A0853D-3641-4276-89F9-8C51B0DCD8B4</gtr:id><gtr:title>Analysing Dynamic Change in Faces</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F037384/1</gtr:grantReference><gtr:abstractText>Humans are very good at understanding and interpreting the motion of others peoples faces. We can effortlessly recognise emotions and interpret subtle facial behaviors such as sardonic smiles, thoughtful frowns or questioning looks, but the question remains, how do we do this? We need new computer based tools to be able to explore this fascinating area of psychology. In this project we will develop a new form of three-dimensional camera system that will allow us to record the movements of people's faces and then process this video information to discover the components of movements that go to make them up. Once we are able to discover the parts of movements that add together to make familiar facial expression we can use this to be able to create new faces; in much the same way as a music mixing desk allows you to blend together different sounds, we will have software that allows us to mix new faces with whatever expressions we select. Using this new tool we can then carry out experiments to look at how we process faces and imitate other people's facial movement. We will examine how observing the movement in one persons face can be translated into movements of our own face to imitate the action. Because the faces we use are created in the computer we can manipulate them in any way we like. This new technology will allow us to address a large set of basic questions. Can we imitate a person if the face seen only from the side or if it is shown upside down? Do we do better when we imitate our self, a friend or a stranger? We can even create caricatures of faces, where we exaggerate particular movements, to evaluate how these facial gestures are represented in the human face processing system. A better understanding of how imitation works will help us understand social behaviors and their development, and also help in developing computer systems that can both recognise and react to our facial expressions. The new face mixing software will also have commercial applications, for example it can be of use in the computer games and entertainment industry. Movements from one persons face can be used as the instructions to be transferred to create another persons face making the same movement. This will allow for example a voice actor to control the movements of a characters face in addition to simply providing the expressive dialogue, the generation of high quality realistic synthetic actors or faster more efficient ways to video conference over your mobile phone.</gtr:abstractText><gtr:fund><gtr:end>2011-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>287391</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>7500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Society of London</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>IE110952</gtr:fundingRef><gtr:id>21824B77-7F70-4FEE-8706-519638C265E7</gtr:id><gtr:outcomeId>5ec404985ec404ac</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>7500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Society of London</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>IE110952</gtr:fundingRef><gtr:id>CDB25201-5AA0-4EDE-9376-59697E37FB5A</gtr:id><gtr:outcomeId>r-2710932821.697493068e7344</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Ideas first developed in this project for systems tracking facial feature expressions are currently under development via a licensing deal with a company who are interested in this face and emotion tracking method
The system has undergone user tests with a number of companies who have provided feedback to help enhance the user interface 
Further funding from Epsrc impact acceleration fund has allowed development of a secondment of an ra to the company to develop the commercial version of the system</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>4BA5DE65-ECA7-48A0-8D22-D634C2E5AA97</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545b8d22170d14.81717656</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Retail</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The objectives were; To develop new tools for 3D facial motion capture and analysis. To characterise modes of variation in meaningful segments of facial motion.To utilise artificially generated stimuli and novel measurement techniques to allow psychophysical experiments exploring the perception of facial action and the imitation of human facial motion.To deliver interdisciplinary science based educational outreach to UK schools.

Key findings: To allow a working system we developed a new approach to image motion analysis that characterised the bright-dark, yellow-blue and red-green opponent channels of the human colour system as chromatic derivatives. We incorporated chromatic derivatives into our existing spatio-temporal brightness derivative method for motion and binocular disparity calculation and demonstrated improved performance. 

We built PCA spaces across individuals rather than across expressions to investigate &amp;quot;family resemblance&amp;quot; between different classes. We used a novel technique of mapping a vector representing a deviation of a male face from the male mean into a female face space. This resulted in a female &amp;quot;sibling&amp;quot;. We showed that the &amp;quot;sibling pairs&amp;quot; looked more alike than a random pairing indicating &amp;quot;family resemblance&amp;quot; may be encoded by similar vectors referenced to the average of classes of faces. 

The same technology can be used to visualise our prejudices. We found that average Conservative and Labour MP's faces were indistinguishable. However average faces rated as strongly labour or strongly conservative did look distinctively different and were correctly matched to their stereotypical category by participants in a follow-up experiment. 

We tested the ability of participants to identify a facial action projected onto a computer graphic avatar as being generated by themselves, a friend or another person. If based on experience they should find it easiest to recognise a friend. However we found participants could recognise themselves and their friends from the motion alone when upright but only themselves for upside down faces. Disrupting the timing of the motion showed self-recognition was based on rhythmic cues we have about our own facial motion.</gtr:description><gtr:exploitationPathways>we have developed links with sme Black swan data and have licences an element of the above research tor development into a product. A secondment of an RA was made possible with support form EPSRC impact acceleration account award to college.

'Building the emotion machine's' technological background is based on the EPSRC funded research 'Analysing Dynamic Change in Faces - EP/F037384/1' by McOwan, using the biologically inspired methods and computer vision techniques to develop systems capable of tracking faces, recognising facial expressions and so supporting affective computing applications. This work was extended to extract contextual cues and visual features of a user allowing the detection of the level of his/her engagement within a human-machine interaction system.
Built on a 3D distribution of facial features extracted we can track a face and recover the valence of emotion and contextual interest to infer the users' emotional state and appropriately modify the contextual behaviour to enhance the interaction quality. The feeling index fuses data on the facial expression, head direction and the contextual cues from the interaction to accurately and robustly predict the engagement of the user. The system has been successfully tested in long-term interactions in real social scenarios as a part of the Framework 7 project LIREC (Living with Robots &amp;amp; Interactive Companions) where an extended empirical evaluation was undertaken in real social settings to validate the theory and technology framework and outputs developed.

Proof Of Concept App
? An iOS iPad application
? Face detection/tracking with discrete and continuous affective states detection.
? Measure effectiveness and accuracy of the face detection solution to detect defined AU (Action Units) 
? Build companies understanding of the background technology to explore potential methodologies and commercial proof of concept demos
? Developed application to demonstrate the technology to potential clients
? The POC was also used as demonstration purposes in science outreach events.
? Accuracy and reliability of the face detection was enhanced
the Affective Sensing &amp;amp; Visualiser POC work delivered
? iOS iPad application
? Proof of concept app demonstrating: GUI interface enabling detection of users affective states whilst watching video content, Collection and aggregation of discrete and continuous state data, Export of affective data to backend platform, Import of affective data from backend platform to secondary application, Creative design and visualisation of affective data
Demo POC to potential commercial brands to gather feedback
? Internal evaluation of the POC and Implementation of new face detection solution (Visage Technologies) and updated AU detection.
Outcome
? Developed the POC applications to demonstrate the technology with demonstration to film company and high street retail chains. 
? developed a better understanding of potential routes to market and strategic approaches to develop further POC apps
application

3. Immediate impact
This section is to describe the immediate impact of the project on users and stakeholders compared to how things were 

The project has increased the awareness within Black Swan (including clients) of face detection and affective sensing technology as an important area of innovative. 

development and collaboration is ongoing.</gtr:exploitationPathways><gtr:id>F114BD3B-3866-4D25-9E37-215EC5BECC75</gtr:id><gtr:outcomeId>r-4242001049.4459434776007b8</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism,Manufacturing, including Industrial Biotechology,Culture, Heritage, Museums and Collections,Retail,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/F037384/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>