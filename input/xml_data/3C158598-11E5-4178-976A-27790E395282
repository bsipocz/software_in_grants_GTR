<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C6E66080-879D-4C24-9063-88E56AF55614"><gtr:id>C6E66080-879D-4C24-9063-88E56AF55614</gtr:id><gtr:name>Sony Broadcast and Professional Europe</gtr:name><gtr:address><gtr:line1>The Heights</gtr:line1><gtr:line2>Brooklands</gtr:line2><gtr:postCode>KT13 0XW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/ED2930A1-730E-4DAA-8A6C-D8B0764AB68A"><gtr:id>ED2930A1-730E-4DAA-8A6C-D8B0764AB68A</gtr:id><gtr:name>Bang &amp; Olufsen</gtr:name><gtr:address><gtr:line1>Peter Bangsvej 15</gtr:line1><gtr:line2>Struer</gtr:line2><gtr:line4>Dk - 7600</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C6E66080-879D-4C24-9063-88E56AF55614"><gtr:id>C6E66080-879D-4C24-9063-88E56AF55614</gtr:id><gtr:name>Sony Broadcast and Professional Europe</gtr:name><gtr:address><gtr:line1>The Heights</gtr:line1><gtr:line2>Brooklands</gtr:line2><gtr:postCode>KT13 0XW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/ED2930A1-730E-4DAA-8A6C-D8B0764AB68A"><gtr:id>ED2930A1-730E-4DAA-8A6C-D8B0764AB68A</gtr:id><gtr:name>Bang &amp; Olufsen</gtr:name><gtr:address><gtr:line1>Peter Bangsvej 15</gtr:line1><gtr:line2>Struer</gtr:line2><gtr:line4>Dk - 7600</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Denmark</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/60D7327A-1435-4707-801E-2F711691ACB6"><gtr:id>60D7327A-1435-4707-801E-2F711691ACB6</gtr:id><gtr:name>NHK Science &amp; Technology Research Labs</gtr:name><gtr:address><gtr:line1>1-10-11 Kinuta</gtr:line1><gtr:line2>Setagaya-KU</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D9E8EFC3-ED31-4709-9226-5493DA66469E"><gtr:id>D9E8EFC3-ED31-4709-9226-5493DA66469E</gtr:id><gtr:name>Orbitsound Limited</gtr:name><gtr:address><gtr:line1>1st Floor</gtr:line1><gtr:line2>1 Rosoman Place</gtr:line2><gtr:postCode>EC1R 0JY</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3E23D348-8257-4D82-9B7E-84D10C0C5B96"><gtr:id>3E23D348-8257-4D82-9B7E-84D10C0C5B96</gtr:id><gtr:name>KEF Audio (UK) Ltd</gtr:name><gtr:address><gtr:line1>Global House</gtr:line1><gtr:line2>High Street</gtr:line2><gtr:postCode>RH10 1DL</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CEC8F498-9BD0-4E94-82AE-F9A788F390CF"><gtr:id>CEC8F498-9BD0-4E94-82AE-F9A788F390CF</gtr:id><gtr:name>Codemasters</gtr:name><gtr:address><gtr:line1>Codemasters Campus Stoneythorpe</gtr:line1><gtr:line4>Southam</gtr:line4><gtr:line5>Warwickshire</gtr:line5><gtr:postCode>CV47 2DL</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/33DE3D33-8720-4362-94EC-5F77E5456421"><gtr:id>33DE3D33-8720-4362-94EC-5F77E5456421</gtr:id><gtr:name>DTS Inc</gtr:name><gtr:address><gtr:line1>100 Enterprise Way</gtr:line1><gtr:line2>Suite C-120</gtr:line2><gtr:postCode>95066</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CB73AAEC-F0B9-44F9-A417-2B70F2A5055C"><gtr:id>CB73AAEC-F0B9-44F9-A417-2B70F2A5055C</gtr:id><gtr:name>Electronic Arts</gtr:name><gtr:address><gtr:line1>Onslow House</gtr:line1><gtr:line2>Onslow Road</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU1 4HU</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/0BF66707-4B41-495D-8B85-9873A32A5FC1"><gtr:id>0BF66707-4B41-495D-8B85-9873A32A5FC1</gtr:id><gtr:firstName>Philip Arthur</gtr:firstName><gtr:surname>Nelson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/2D0F82D5-F19E-4EA8-A0C7-C7F120102A69"><gtr:id>2D0F82D5-F19E-4EA8-A0C7-C7F120102A69</gtr:id><gtr:firstName>Adrian</gtr:firstName><gtr:surname>Hilton</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/CCC13D5A-EFEC-4FC2-A6E2-5C1570F0BAC5"><gtr:id>CCC13D5A-EFEC-4FC2-A6E2-5C1570F0BAC5</gtr:id><gtr:firstName>Filippo</gtr:firstName><gtr:otherNames>Maria</gtr:otherNames><gtr:surname>Fazi</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8A520840-7095-4A65-8161-CAD850606765"><gtr:id>8A520840-7095-4A65-8161-CAD850606765</gtr:id><gtr:firstName>Trevor</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Cox</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL000539%2F1"><gtr:id>3C158598-11E5-4178-976A-27790E395282</gtr:id><gtr:title>S3A: Future Spatial Audio for an Immersive Listener Experience at Home</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/L000539/1</gtr:grantReference><gtr:abstractText>3D sound can offer listeners the experience of &amp;quot;being there&amp;quot; at a live event, such as the Proms or Olympic 100m, but
currently requires highly controlled listening spaces and loudspeaker setups. The goal of S3A is to realise practical 
3D audio for the general public to enable immersive experiences at home or on the move.

Virtually the whole of the UK population consume audio. S3A aims to unlock the creative potential of 3D sound and deliver to listeners a step change in immersive experiences. This requires a radical new listener centred approach to audio enabling 3D sound production to dynamically adapt to the listeners' environment. Achieving immersive audio experiences in uncontrolled living spaces presents a significant research challenge. This requires major advances in our understanding of the perception of spatial audio together with new representations of audio and the signal processing that allows content creation and perceptually accurate reproduction. Existing audio production formats (stereo, 5.1) and those proposed for future cinema spatial audio (24,128) are channel-based requiring specific controlled loudspeaker arrangements that are simply not practical for the majority of home listeners. S3A will pioneer a novel object-based methodology for audio signal processing that allows flexible production and reproduction in real spaces. The reproduction will be adaptive to loudspeaker configuration, room acoustics and listener locations. The fields of audio and visual 3D scene understanding will be brought together to identify and model audio-visual objects in complex real scenes. Audio-visual objects are sound sources or events with known spatial properties of shape and location over time, e.g. a football being kicked, a musical instrument being played or the crowd chanting at a football match. Object based representation will transform audio production from existing channel based signal mixing (stereo, 5.1, 22.2) to spatial control of isolated sound sources and events. This will realise the creative potential of 3D sound enabling intelligent user-centred content production, transmission and reproduction of 3D audio content in platform independent formats. Object-based audio will allow flexible delivery (broadcast, IP and mobile) and adaptive reproduction of 3D sound to existing and new digital devices.</gtr:abstractText><gtr:potentialImpactText>Virtually the whole UK population are consumers of audio content. S3A will deliver to listeners a step change in the quality
of perceived sound, and provide new opportunities for UK creative industries to generate wealth through the artistic
exploitation of new audio-visual technology.

S3A's scientific and engineering advances will ensure that UK research remains at the forefront of spatial audio and pioneers new integrated audio-visual signal processing methodologies. This research will enable UK creative industries (broadcast, film, games, interactive media) to develop and exploit the best future spatial audio production and delivery technologies that add value to listeners' experience. The UK is a world-leader in audio-visual content production which is a growth sector contributing 12% (&amp;pound;120B) to the economy with over 2.5M employees. Consequently the UK is extremely well placed to exploit S3A research through creative/technology SME's(KEF,DTS,Orbitsound), TV (BBC), film (DNeg, Framestore, MPC), games (EA, Sony, Codemasters). S3A will enable UK creative industries to lead future technologies and standards for spatial audio and object-based audio-visual production.

Pathways to impact include:
(1) collaboration with the BBC to realise S3A technology in the next generation of spatial audio for broadcast and IP networks;
(2) working with games/film/web companies to address their requirements for spatial audio production &amp;amp; reproduction;
(3) leading international open standards for spatial audio through the BBC who are actively engaged in ISO/MPEG standards for audio and visual content;
(4) licensing of S3A technology to UK SME's for integration in mobile and home platforms;
(5) engagement of representative bodies to ensure needs of the hearing impaired are addressed;
(6) collaboration with creatives on showcasing the potential of spatial audio to deliver new listener experience;
(7) engaging the public in S3A research through spatial audio test broadcasts and web-baed interactive media with the BBC
(8) public engagement with the science behind S3A spatial audio through pilots for feature documentaries on BBC TV/radio with involvement of co-investigator Prof.Trevor Cox who is a regular science commentator;
(9) open source tools for amateurs &amp;amp; professionals to edit/stream 3D sound to support active engagement in creative use of spatial audio;
(10) workshops to foster an audio-visual research community bringing together audio and visual researchers from both academia and industry.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-06-12</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-12-12</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>5415204</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Sony Broadcast and Professional Europe</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Sony Broadcast and Professional Europe</gtr:description><gtr:id>F41B028A-76BE-4AFA-8D36-1DAFEF9D2134</gtr:id><gtr:outcomeId>b9973088b997309c-1</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2004-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>BBC Research and Development</gtr:description><gtr:id>FC1C7988-B5EE-402B-A7DB-B7411E031828</gtr:id><gtr:impact>Multi-disciplinary collaboration involves Computer Vision, Video Analysis, Psychoacoustics, Signal Processing and Spatial Audio</gtr:impact><gtr:outcomeId>b9c5b39ab9c5b3ae-1</gtr:outcomeId><gtr:partnerContribution>In kind contribution (members of Steering/Advisory Boards) Use of the BBC lab and research/development facilities. 
Studentships (industrial case) funding and co-supervision of PhD students.</gtr:partnerContribution><gtr:piContribution>Research in Computer Vision for broadcast production and Audio.
Technologies for 3D production, free-view point video in sports, stereo production from monocular cameras, video annotation
Member of the BBC Audio Research Partnership - developing the next generation of broadcast technology.</gtr:piContribution><gtr:sector>Public</gtr:sector></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Bang &amp; Olufsen</gtr:collaboratingOrganisation><gtr:country>Denmark, Kingdom of</gtr:country><gtr:description>B&amp;amp;O</gtr:description><gtr:id>A2E40998-E424-4DA2-AD97-1805A2C77AB1</gtr:id><gtr:impact>Publications listed on http://iosr.uk/projects/POSZ/
Multi-disciplinary Collaboration: Signal Processing, Psychoacoustics and Spatial audio</gtr:impact><gtr:outcomeId>56e0a2830d1b63.02844572-1</gtr:outcomeId><gtr:partnerContribution>Scholarships (fees and bursaries) for EU/Home students.
In-kind contribution by members of B&amp;amp;O Research department (Soren Bech, member of Steering /Advisory Boards and co-supervisor of funded students). Use of research facilities at their labs in Denmark</gtr:partnerContribution><gtr:piContribution>Spatial audio research (POSZ and S3A EPSRC funded projects)</gtr:piContribution><gtr:sector>Private</gtr:sector></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Aura Satz: The Trembling line exhibition</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>96C91538-7173-4F0A-B56A-067C240391C0</gtr:id><gtr:impact>The Trembling Line is an exhibition by Aura Satz exploring acoustics, vibration, sound visualisation and musical gesture with an aim to wrest the space between sound and image to see how far these can be stretched apart before they fold back into one another. .
The centrepiece of the show is the film and sound installation The Trembling Line, which explores visual and acoustic echoes between decipherable musical gestures and abstract patterning, orchestral swells and extreme slow-motion close-ups of strings and percussion. It features a score by Leo Grant and an innovative multichannel audio system by the Institute of Sound and Vibration Research (ISVR), University of Southampton, as part of the S3A research project on immersive listening.</gtr:impact><gtr:outcomeId>56e0b47d130873.07352234</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.hansardgallery.org.uk/event-detail/199-aura-satz-the-trembling-line/</gtr:url><gtr:year>2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>AES - Good vibrations bringing Radio Drama to life - Eloise Whitmore</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D832F410-29E5-4726-9F05-FFB33765324C</gtr:id><gtr:impact>AES in Cambridge on Wed 16th Dec. Talk by Eloise Whitmore about radio drama and S3A cutting edge production methods such as object based audio and 3D sound design.</gtr:impact><gtr:outcomeId>58c92cf3e675a9.96361176</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.aes-uk.org/forthcoming-meetings/good-vibrations-bringing-radio-drama-to-life/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>S3A visit to Parma University - Casa della Musica</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5FCF73E7-ABEE-48A6-A9F4-36B16070BAD8</gtr:id><gtr:impact>As part of the collaboration with Parma University and Casa della Musica, S3A hosted a classical concert open to the general public. The aim of the event was to record the concert using different microphone arrays and S3A technology as well as 360 video. The concert was organised in collaboration with Parma University and the Conservatorio Arrigo Boito. The recording is being used for further research.</gtr:impact><gtr:outcomeId>58c921f1441981.63248392</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.comune.parma.it/notizie/news/CULTURA/2017-01-12/Progetto-S3A-Audio-spaziale-il-meeting-internazionale-a-Parma-1.aspx</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TAUNTON Stem Festival</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>3EB925DD-4134-4A76-9BE8-A06176B6973D</gtr:id><gtr:impact>S3A presented an object-based sound system with S3A technology and interactive content at a major STEM festival in Taunton. The event targeted primary and secondary school pupils. The event was covered by local press.</gtr:impact><gtr:outcomeId>58c91d694fdf18.16307677</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Soundbar Technology market research</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BF170EE3-7C0B-4D9F-966E-1493493EB0B6</gtr:id><gtr:impact>As part of the ICURe programme, Dr Marcos Simon Galvez has had the opportunity to discuss his research on soundbars and further use of the technology with industry (nationally and internationally).</gtr:impact><gtr:outcomeId>58c91a55d7a817.86220389</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Sound now and next</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>50DFA4BD-105E-47E3-99BF-AEBCC2039B50</gtr:id><gtr:impact>The S3A Programme Grant was represented at the Technology Fair with Demos (ar per list below) and the Radio Drama production (James Woodcock) which was showcased in the BBC Demo Room.
- Towards perceptual metering for an object-based audio system [Dr Jon Francombe, University of Surrey and Yan Tang, University of Salford]
- 3D Head tracking for Spatial Audio and Audio-Visual Speaker tracking [Dr Teo de Campos, University of Surrey and Dr Marcos Simon Galvez, University of Southampton]
- Headphone simulation of 3D spatial audio systems in different listening environments&amp;quot;. [Dr Rick Hughes, University of Salford and Chris Pike, BBC] 
The demos and radio drama generated significant attention from other attendees/external organisations and universities. The S3A Advisory Steering Board commended S3A for the rapid progress and the impact of the demos.</gtr:impact><gtr:outcomeId>56e00095739182.06347595</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/rd/blog/2015-06-sound-now-next-watch-talks-online</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Winchester Cathedral Primary Science Festival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>2401BF69-5267-4D5E-944C-D30575CD75B5</gtr:id><gtr:impact>An acoustic workshop lasting over 50 minutes was undertaken with 6 groups of 16 primary school children aged 9-11, as part of a Science Festival at Winchester Cathedral (Nov 16). The workshop was carried out by Steve Elliott and Marcos Simon Galvez and it covered activities to do with how sound travels and its speed, length and pitch in musical instruments and reverberation and localisation. The last activity involved live recordings from a dummy head to multiple headphones that the students listened to, in order to demonstrate binaural sound localisation. The feedback received from teachers was that this event had helped increase the students' knowledge of acoustic and their perception of science and engineering.</gtr:impact><gtr:outcomeId>58c91c51df2e31.42759999</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>107560</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC I-case studentship</gtr:description><gtr:end>2020-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>51F4DEF1-391B-4ECE-A388-446086C86825</gtr:id><gtr:outcomeId>56dffc9f3780f8.46823571</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>108580</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC i-case studentship</gtr:description><gtr:end>2021-09-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>5ECA9D75-4A94-45FF-B72F-469283C63C48</gtr:id><gtr:outcomeId>56dffd1438a501.74306701</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>35000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>ICURE - Support for Junior Researcher, Dr J Francombe for 3 months, able to claim up to &amp;pound;35,000 of travel and expenditure, to carry out market validation of research-based business ideas and to receive intensive support in developing those.</gtr:description><gtr:end>2016-07-02</gtr:end><gtr:fundingOrg>SETsquared Partnership</gtr:fundingOrg><gtr:id>D58DAFB0-11C5-429A-8341-8C13236581A7</gtr:id><gtr:outcomeId>58c9110b0a3dd3.52670524</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2016-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Listener centred spatial audio reproduction for immersive spatial audio experience at home and improve content accessibility for the hearing impaired.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>3C2CBB36-FD24-4D8A-96ED-6BEEDE22BE64</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545cf7f7f42042.20558268</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>S3A is pioneering methods for creating immersive spatial audio experiences for the listener at home or on the move. Research is investigating all aspects of the production from recording and editing through to delivery and practical reproduction at home. 
S3A has delivered advances in the following areas:
- understanding and modelling listener perception of spatial audio in real spaces
- perceptual metering of spatial audio
- end-to-end production of spatial audio from recording to reproduction
- object-based audio recording, editing and manipulation
- object-based spatial audio reproduction
- listenter centered reproduction
- room modelling and adaption in spatial audio reproduction
- audio-visual localisation of sound sources
- source separation for multiple object sources
- perceptual modelling of intelligibility of sound sources
- methods to control and improve intelligibility of content
- audio-visual room modelling 
- creation of new spatial audio experiences for listeners at home

These technologies have been integrated to demonstrate enhanced and new listening experiences. Technologies developed in S3A are contributing to international standards and new consumer technologies.</gtr:description><gtr:exploitationPathways>S3A has contributed new technologies for creation of immersive audio and audio-visual content which can be experienced by the listener at home. Further exploitation is expected through:

- commercial exploitation novel methods for production of audio and audio-visual content in the creative industries (TV, film, games, internet)
- novel methods for listeners to experience spatial audio a home (consumer electronics, TV, film, games)
- novel devices for audio and visual content 
- technologies for perceptual metering of audio in production and reproduction
- new creative tools and media experiences</gtr:exploitationPathways><gtr:id>0827C42A-A04A-48D9-B848-B756CD147F40</gtr:id><gtr:outcomeId>56e0ae6711c086.32049853</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://spatial-audio.org/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Person tracking using audio and depth cues
Identity association using PHD filters in multiple head tracking with depth sensors</gtr:description><gtr:id>F3E13E1E-7CCB-4197-9F22-5ACDDFE7B197</gtr:id><gtr:impact>Permission is granted to use the S3A Room Impulse Response dataset for academic purposes only, provided that it is suitably referenced in publications related to its use</gtr:impact><gtr:outcomeId>58ca7a68b2f703.65544175</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>S3A speaker tracking with Kinect2</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://cvssp.org/data/s3a/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>RIR datasets captured as part of the S3A project and supplementary material.</gtr:description><gtr:id>46F374D5-898B-4CAF-AA8C-7D3140D8CAF9</gtr:id><gtr:impact>Permission is granted to use the S3A Room Impulse Response dataset for academic purposes only, provided that it is suitably referenced in publications related to its use</gtr:impact><gtr:outcomeId>58ca7a08666b58.04523920</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Room Impulse Responses (RIRs) and Visualisation</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://cvssp.org/data/s3a/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Data created for the S3A Radio Drama</gtr:description><gtr:id>C596ECD4-1D38-47B9-9366-4D28094C5EA1</gtr:id><gtr:impact>The Radio Drama was used for the creation of the VR The Turning Forest - award winning BBC's first VR production</gtr:impact><gtr:outcomeId>58ca7af3161a54.21681875</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>S3A radio drama scenes</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://cvssp.org/data/s3a/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>70819C3C-232A-4E53-8664-D25610D4CE50</gtr:id><gtr:title>An Audio-Visual System for Object-Based Audio: From Recording to Listening</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a675f37bd28b2.68154044</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9C8F497F-753E-4CD9-A5B9-7501505F7378</gtr:id><gtr:title>Investigation into the role of the nonnegativity constraint in sound field reproduction problems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/68b21e08d88e6bd18351b4088865316b"><gtr:id>68b21e08d88e6bd18351b4088865316b</gtr:id><gtr:otherNames>Fazi FM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2c9be452025.42419317</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>67BBC065-EC34-4942-9008-B87E39465C64</gtr:id><gtr:title>Visualization of compact microphone array room impulse responses</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e02ae7868496.90447902</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0446D572-0281-4B83-AC03-0E2600E5EC9B</gtr:id><gtr:title>Estimation of room reflection parameters for a reverberant spatial audio object</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e03325a21ae0.05696043</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>20C74293-54C4-4BDF-B3E8-D3849C39F186</gtr:id><gtr:title>Speech reaction time measurements for the evaluation of audio-visual spatial coherence</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0ca9ab1fd7b25c1f6d6c93778b282984"><gtr:id>0ca9ab1fd7b25c1f6d6c93778b282984</gtr:id><gtr:otherNames>Stenzel H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c6481bb7d1.50788575</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A785E0C1-BE96-4ADF-A56D-599B43C22F23</gtr:id><gtr:title>The Relationship Between Target Quality and Interference in Sound Zone</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/469eb5f0fe5db35f68e8653bfeb71670"><gtr:id>469eb5f0fe5db35f68e8653bfeb71670</gtr:id><gtr:otherNames>Baykaner K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e0193eaaaf28.80007691</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BD67C9A-AB24-4D99-B459-93C31554413D</gtr:id><gtr:title>Production and reproduction of program material for a variety of spatial audio formats</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e03525ecc832.84208307</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1969C5D6-07DF-425E-81BF-496B62EB2908</gtr:id><gtr:title>A glimpse-based approach for predicting binaural intelligibility with single and multiple maskers in anechoic chambers</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2b73e2c30a6.30105096</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1D9E726A-0D87-4B79-A11E-05274E1256E3</gtr:id><gtr:title>Automatic Speech-to-Background Ratio Selection to Maintain Speech Intelligibility in Broadcasts Using an Objective Intelligibility Metric</gtr:title><gtr:parentPublicationTitle>Applied Sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5aa7c648c5aaf7.09963231</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C67F8DB2-A536-4620-B3BC-1DFDE11154BE</gtr:id><gtr:title>Determining and labeling the preference dimensions of spatial audio replay</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca7465bca6b4.10957538</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13D395D9-F5C0-4690-90AB-9AC06011B2C8</gtr:id><gtr:title>A metric for predicting binaural speech intelligibility in stationary noise and competing speech maskers.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>58c182f7d61ce2.56095510</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53D9EC5E-7A40-445D-B39F-B53D1B96EE2D</gtr:id><gtr:title>Estimation of the stability of a virtual sound source using a microphone array</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/68b21e08d88e6bd18351b4088865316b"><gtr:id>68b21e08d88e6bd18351b4088865316b</gtr:id><gtr:otherNames>Fazi FM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2d1fcc012c1.97972886</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C1608F3F-7043-4B73-9562-35ED0164A0D6</gtr:id><gtr:title>Media Device Orchestration for Immersive Spatial Audio Reproduction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c647763964.93913496</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B6AA4807-10E3-4A03-B438-DFD6D02EAEF6</gtr:id><gtr:title>Elicitation of Expert Knowledge to Inform Object-Based Audio Rendering to Different Systems</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd39fdea0e343efef2f0c0546582ea8d"><gtr:id>dd39fdea0e343efef2f0c0546582ea8d</gtr:id><gtr:otherNames>Woodcock J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5aa7c648e6e849.24061836</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15AAACE9-8A4C-45A2-A2F4-60DF5FDC83C4</gtr:id><gtr:title>Evaluation of Spatial Audio Reproduction Methods (Part 1): Elicitation of Perceptual Differences</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ca7466507fa7.63931915</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C97ABE9-1ED3-4926-899C-B39FD287F283</gtr:id><gtr:title>The Effect of Early Impulse Response Length and Visual Environment on Externalization of Binaural Virtual Sources</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/31a677ed70db05b2ed706c64cbb623ee"><gtr:id>31a677ed70db05b2ed706c64cbb623ee</gtr:id><gtr:otherNames>Sinker J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca7915a79b61.52394721</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3A221ED2-7701-4F18-B05E-A8AC727DE8FB</gtr:id><gtr:title>Evaluation of Spatial Audio Reproduction Methods (Part 2): Analysis of Listener Preference</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ca74662565b3.89798175</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C389FB62-4C21-4D99-BE76-59672B56B746</gtr:id><gtr:title>A perceptual approach to object-based room correction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27495c54e4925ba7801906eb42a66550"><gtr:id>27495c54e4925ba7801906eb42a66550</gtr:id><gtr:otherNames>Menzies D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2bdd408ee11.34304861</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BBD9324A-EA19-4D8E-B048-4051D85D2CB6</gtr:id><gtr:title>Presenting the S3A Object-Based Audio Drama Dataset</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd39fdea0e343efef2f0c0546582ea8d"><gtr:id>dd39fdea0e343efef2f0c0546582ea8d</gtr:id><gtr:otherNames>Woodcock J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca76d13cb2a8.70951353</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D6D7F6F-238D-4265-909E-E1C27AED630B</gtr:id><gtr:title>A Theoretical Analysis of Sound Localization, with Application to Amplitude Panning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27495c54e4925ba7801906eb42a66550"><gtr:id>27495c54e4925ba7801906eb42a66550</gtr:id><gtr:otherNames>Menzies D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2d393979749.47676468</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>655C65B3-28E0-4E47-97CC-C7CD0D263AC5</gtr:id><gtr:title>A cognitive framework for the categorisation of auditory objects in urban soundscapes</gtr:title><gtr:parentPublicationTitle>Applied Acoustics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd39fdea0e343efef2f0c0546582ea8d"><gtr:id>dd39fdea0e343efef2f0c0546582ea8d</gtr:id><gtr:otherNames>Woodcock J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c1852586f2e9.93472246</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>47277BB4-5E03-490D-BBA5-27CE2179AE4C</gtr:id><gtr:title>Optimization-based reproduction of diffuse audio objects</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d28cc8c0f0ff32922c1fcc6dc3b3f12c"><gtr:id>d28cc8c0f0ff32922c1fcc6dc3b3f12c</gtr:id><gtr:otherNames>Franck A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2cf6cd90c22.37111363</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>480A32F5-C04B-4FFE-99DD-A90A4E2EF8A4</gtr:id><gtr:title>Numerical optimization of loudspeaker configuration for sound zone reproduction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e1644811e366.40130078</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>34F92FDC-7D94-48C5-89CE-F14EF0CD7E67</gtr:id><gtr:title>Stereophonic personal audio reproduction using planarity control optimization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e163c04c43f8.52573416</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCEA0DB2-CA99-4E17-A420-EC24F4C25A70</gtr:id><gtr:title>Robust Acoustic Contrast Control with Reduced In-situ Measurement by Acoustic Modeling</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a85a1d677894e96aeec2aa5be9524fd"><gtr:id>5a85a1d677894e96aeec2aa5be9524fd</gtr:id><gtr:otherNames>Zhu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c64872eaf0.43446717</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EFE9EBB0-FD1E-40A6-9DF2-835B7711D7DE</gtr:id><gtr:title>Visualization of Compact Microphone Array Room Impulse Responses</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58ca7742b649e0.25345111</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D5B0660B-34CE-4DAD-BE5B-3B8103E43890</gtr:id><gtr:title>Perceptual evaluation of spatial audio: where next?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e03d5a630a67.66094109</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BC9D22D2-E6D4-4C9E-BE3C-DB22AA25F321</gtr:id><gtr:title>Person tracking using audio and depth cues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e02963039ea3.38617345</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CF226FE5-426B-43D6-A86B-5FE84B6EFAB5</gtr:id><gtr:title>Listener tracking stereo for object based audio reproduction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e8cef19dea70ace88dcf1f87d31ddcd"><gtr:id>3e8cef19dea70ace88dcf1f87d31ddcd</gtr:id><gtr:otherNames>Simon Galvez MF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2d1363b7bd6.54126021</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCBDD020-24A2-404B-904C-8D76CFA3899F</gtr:id><gtr:title>Sparse $\ell _{1}$-Optimal Multiloudspeaker Panning and Its Relation to Vector Base Amplitude Panning</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d28cc8c0f0ff32922c1fcc6dc3b3f12c"><gtr:id>d28cc8c0f0ff32922c1fcc6dc3b3f12c</gtr:id><gtr:otherNames>Franck A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c2dde864d091.05587876</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B0483496-BF62-451B-8D2D-7B61F126CD52</gtr:id><gtr:title>Personalized Object-Based Audio for Hearing Impaired TV Viewers</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8bb8407fdb844a4000f66da555247709"><gtr:id>8bb8407fdb844a4000f66da555247709</gtr:id><gtr:otherNames>Shirley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c6489cba02.10951509</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0273DF88-4156-47BC-91AA-C3D3897AA173</gtr:id><gtr:title>Object-Based Reverberation for Spatial Audio</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c184bcb8c782.36212375</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD628E55-911A-4D9D-A441-CC1B11AB8073</gtr:id><gtr:title>A 3D model for room boundary estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e036cc37c124.09086696</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C9AC8102-D54B-46CA-90AC-015B1C434E9A</gtr:id><gtr:title>Audio object separation using microphone array beamforming</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e033f5a0d127.42227971</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7B8E8458-F43C-4D5A-AEB0-EA993D93886D</gtr:id><gtr:title>A Listener Position Adaptive Stereo System for Object-Based Reproduction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e8cef19dea70ace88dcf1f87d31ddcd"><gtr:id>3e8cef19dea70ace88dcf1f87d31ddcd</gtr:id><gtr:otherNames>Simon Galvez MF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2d43f9f2839.65066829</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7047D0C9-69F4-4367-8EA7-08EE8B5C155B</gtr:id><gtr:title>Decoding and Compression of Channel and Scene Objects for Spatial Audio</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27495c54e4925ba7801906eb42a66550"><gtr:id>27495c54e4925ba7801906eb42a66550</gtr:id><gtr:otherNames>Menzies D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c64951eaf4.73736236</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CEA37551-8FBA-4C11-984F-A5C5CEA73444</gtr:id><gtr:title>Robust reproduction of sound zones with local sound orientation.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a85a1d677894e96aeec2aa5be9524fd"><gtr:id>5a85a1d677894e96aeec2aa5be9524fd</gtr:id><gtr:otherNames>Zhu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>5aa7eb91bc8065.81946302</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FC404153-5157-4EBA-B16C-8939372DA561</gtr:id><gtr:title>Object-Based Audio Reproduction using a Listener-Position Adaptive Stereo System</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3b7ea90cc37df5eab9092d7feed4d0b4"><gtr:id>3b7ea90cc37df5eab9092d7feed4d0b4</gtr:id><gtr:otherNames>Sim?n Galvez M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c184bc87cfd1.74293310</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D572AAFC-4C4B-407E-A9E0-5118D4F40860</gtr:id><gtr:title>Evaluating a distortion-weighted glimpsing metric for predicting binaural speech intelligibility in rooms</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d5d6b711184.77038716</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1981AE91-EE00-4F5F-947A-C1492AF74C18</gtr:id><gtr:title>On object-based audio with reverberation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e028566374b0.74368149</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1782BD77-647D-4A86-8D63-C3CAD041518F</gtr:id><gtr:title>Spatial reproduction of near sources at low frequency using adaptive panning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27495c54e4925ba7801906eb42a66550"><gtr:id>27495c54e4925ba7801906eb42a66550</gtr:id><gtr:otherNames>Menzies D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2d064936ac9.31773742</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6A1867FF-7717-42FA-96CE-493ADEBDC75C</gtr:id><gtr:title>Mean-Shift and Sparse Sampling-Based SMC-PHD Filtering for Audio Informed Visual Speaker Tracking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/704fc7816ca45ac4c40425b2b2aa38d5"><gtr:id>704fc7816ca45ac4c40425b2b2aa38d5</gtr:id><gtr:otherNames>Kilic V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d41b0f34278.14279918</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>056F4F24-607D-4E13-AF19-378B154668A2</gtr:id><gtr:title>Installation of a Flexible 3D Audio Reproduction System into a Standardized Listening Room</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a797822f66f8e729326cd955478730d6"><gtr:id>a797822f66f8e729326cd955478730d6</gtr:id><gtr:otherNames>Mason R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca7654176b23.83806685</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9975A160-3868-4706-AC5D-264C51FB3669</gtr:id><gtr:title>A Listener Adaptive Optimal Source Distribution System for Virtual Sound Imaging</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e8cef19dea70ace88dcf1f87d31ddcd"><gtr:id>3e8cef19dea70ace88dcf1f87d31ddcd</gtr:id><gtr:otherNames>Simon Galvez MF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca77a35a1d32.81164649</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A0AB846C-8D78-4DBA-AB44-C25751B0791D</gtr:id><gtr:title>Planarity panning for listener-centered spatial audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e03ed4c0a570.45599086</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>72222867-B0F3-41EF-96E9-49A8C68BD969</gtr:id><gtr:title>Loudspeaker arrays for transaural reproduction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e8cef19dea70ace88dcf1f87d31ddcd"><gtr:id>3e8cef19dea70ace88dcf1f87d31ddcd</gtr:id><gtr:otherNames>Simon Galvez MF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca7809d181b2.20890228</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>89F270E0-623C-4044-948D-849A65B7B759</gtr:id><gtr:title>Object-Based 3D Audio Production for Virtual Reality Using the Audio Definition Model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3fc14b42b009afdb43d984a329c635a"><gtr:id>e3fc14b42b009afdb43d984a329c635a</gtr:id><gtr:otherNames>Pike C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58ca78c099f895.79922394</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A82E1130-D25A-4044-9A34-FA3861CCE8A2</gtr:id><gtr:title>A non-intrusive method for estimating binaural speech intelligibility from noise-corrupted signals captured by a pair of microphones</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b37d1950beadde9137ccd8e4f48ed375"><gtr:id>b37d1950beadde9137ccd8e4f48ed375</gtr:id><gtr:otherNames>Tang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5aa7c64919d224.00304190</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9DA9B377-5E58-4A59-9584-D8F789CE641B</gtr:id><gtr:title>Low-Complexity, Listener's Position-Adaptive Binaural Reproduction Over a Loudspeaker Array</gtr:title><gtr:parentPublicationTitle>Acta Acustica united with Acustica</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/31fa44f5240dc0b87859464b084b16cf"><gtr:id>31fa44f5240dc0b87859464b084b16cf</gtr:id><gtr:otherNames>Sim?n G?lvez M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c6483dc2b2.79356728</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>041939F7-8AB2-453F-BD99-A0758AC391C8</gtr:id><gtr:title>Source, sensor and reflector position estimation from acoustical room impulse responses</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e032507aaff9.81919787</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7A617918-5FB6-480C-BD88-5F0AE4055A8E</gtr:id><gtr:title>A Low-Frequency Panning Method With Compensation for Head Rotation</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27495c54e4925ba7801906eb42a66550"><gtr:id>27495c54e4925ba7801906eb42a66550</gtr:id><gtr:otherNames>Menzies D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a352f49174b85.93666181</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3EDC4D7C-36F7-410B-8FEF-D0AF104C2622</gtr:id><gtr:title>Joint Mixing Vector and Binaural Model Based Stereo Source Separation</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af463c0336f79ef0dbc212494ac75443"><gtr:id>af463c0336f79ef0dbc212494ac75443</gtr:id><gtr:otherNames>Alinaghi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675fd646bc0a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B54A202-2185-4B50-852D-7F629E91E96F</gtr:id><gtr:title>Comparison of listener-centric sound field reproduction methods in a convex optimization framework</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d28cc8c0f0ff32922c1fcc6dc3b3f12c"><gtr:id>d28cc8c0f0ff32922c1fcc6dc3b3f12c</gtr:id><gtr:otherNames>Franck A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2ccc1dd11d7.00274852</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A3861A8-1221-4B74-A790-974008A30619</gtr:id><gtr:title>Elicitation of the differences between real and reproduced Audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e154327a4f39.11220756</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>63FA6BC7-AC37-4935-A1AA-946D6653FC32</gtr:id><gtr:title>Production and Reproduction of Program Material for a Variety of Spatial Audio Formats</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f33bc24caede38f95e31111e511c2d2d"><gtr:id>f33bc24caede38f95e31111e511c2d2d</gtr:id><gtr:otherNames>Francombe J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58ca75821d4c30.27087270</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E6D997A9-C2CA-442F-B259-58D8BA70F366</gtr:id><gtr:title>The room-in-room effect and its influence on perceived room size in spatial audio reproduction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b44a1b1b98001b6a6fadfef2dc43a85"><gtr:id>7b44a1b1b98001b6a6fadfef2dc43a85</gtr:id><gtr:otherNames>Hughes RJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2b82d460c99.01302561</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C409C4A9-3B9F-4BCC-AFF2-F4CE84CD8DA2</gtr:id><gtr:title>Audio Assisted Robust Visual Tracking With Adaptive Particle Filtering</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/704fc7816ca45ac4c40425b2b2aa38d5"><gtr:id>704fc7816ca45ac4c40425b2b2aa38d5</gtr:id><gtr:otherNames>Kilic V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d41a45dd008.72328100</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9171A44D-9309-4138-9B0C-23C48B06AF46</gtr:id><gtr:title>Room boundary estimation from acoustic room impulse responses</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e03aff801ef7.39727581</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46276E60-ED9F-4613-85A4-EED9758AFEDF</gtr:id><gtr:title>Acoustic Reflector Localization: Novel Image Source Reversion and Direct Localization Methods</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be093f60c30d04a17c9ace9f3f103e5a"><gtr:id>be093f60c30d04a17c9ace9f3f103e5a</gtr:id><gtr:otherNames>Remaggi L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58ca7465ee2f99.67739700</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D0428941-EF90-4EDA-B7F1-275879B15B5D</gtr:id><gtr:title>Identity association using PHD filters in multiple head tracking with depth sensors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2ba6a774f73.57055862</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D900B080-BB2A-4500-A08C-8CAFC29DEB7B</gtr:id><gtr:title>A perceptually-weighted deep neural network for monaural speech enhancement in various background noise conditions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5aa7c647b7dba8.73358348</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>20645E19-A802-414E-B0EE-071A53AA5531</gtr:id><gtr:title>Multiple Speaker Tracking in Spatial Audio via PHD Filtering and Depth-Audio Fusion</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2ff0853d1331.18240070</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>43F0D8AD-7D52-4108-98F5-E1E5D710CD02</gtr:id><gtr:title>A source separation evaluation method in object-based spatial audio</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e02fdb2e3d09.68602011</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C84896FB-3735-46B2-B9B9-3606C19CA851</gtr:id><gtr:title>Personal audio with a planar bright zone.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05677f2dc5eb02d4021413b7e1d64358"><gtr:id>05677f2dc5eb02d4021413b7e1d64358</gtr:id><gtr:otherNames>Coleman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>56e0193ece4202.86703962</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B8995B5-39DE-4C17-A401-EDCE2CE99B94</gtr:id><gtr:title>Implementation Of Dynamic Panning Reproduction With Adaptation For Head Rotation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27495c54e4925ba7801906eb42a66550"><gtr:id>27495c54e4925ba7801906eb42a66550</gtr:id><gtr:otherNames>Menzies D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>58c2cef3913663.64940495</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2FD189C3-0927-4357-8A59-307DBF81A295</gtr:id><gtr:title>Room Layout Estimation with Object and Material Attributes Information Using a Spherical Camera</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/30090b6ac8e7835f0dcaf37d05926960"><gtr:id>30090b6ac8e7835f0dcaf37d05926960</gtr:id><gtr:otherNames>Kim H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c2b9c845c3a3.52370656</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>73576694-23A2-45D7-A2F7-C323B4836B65</gtr:id><gtr:title>Interference Reduction in Reverberant Speech Separation With Visual Voice Activity Detection</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e162b35712e7.83432801</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A3E66A9B-C954-462D-8F70-F6F826EB13F1</gtr:id><gtr:title>Categorization of Broadcast Audio Objects in Complex Auditory Scenes</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd39fdea0e343efef2f0c0546582ea8d"><gtr:id>dd39fdea0e343efef2f0c0546582ea8d</gtr:id><gtr:otherNames>Woodcock J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d5d6f978f04.12378522</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L000539/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>