<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:department>Computer Laboratory</gtr:department><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B3980ADD-58CF-4A26-B922-E005960588A6"><gtr:id>B3980ADD-58CF-4A26-B922-E005960588A6</gtr:id><gtr:firstName>Per Ola</gtr:firstName><gtr:surname>Kristensson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH027408%2F1"><gtr:id>AEC62E13-CCF3-4039-A153-2E97DB66398F</gtr:id><gtr:title>Text Entry by Inference: Eye Typing, Stenography, and Understanding Context of Use</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/H027408/1</gtr:grantReference><gtr:abstractText>My research is based on the observation that our daily interaction with computers is highly redundant. Some of these redundancies can be modelled and exploited by intelligent user interfaces. Intelligent text entry methods use AI techniques such as machine learning to exploit redundancies in our languages. They enable users to write quickly and accurately, without the need for a key press for every single intended letter.In this programme I propose to develop two new intelligent text entry methods. The first is a system that enables disabled users to communicate efficiently using an eye-tracker. The second system is a novel intelligent text entry method that is inspired by stenography.In addition, I propose to explore text entry methods' broader context. The research literature has concentrated on inventing text entry methods that promise high entry rates and low error rates. Now that we have text entry methods that have reasonably high entry rates it is time to complement this objective function by discovering other aspects of text entry. I propose to use social-science techniques, such as diary and field-studies, to understand how users would prefer to use text entry methods in the wild. System 1: Eye-typing by inferenceThis is a system that will potentially increase the entry rate in eye-typing systems. Current eye-typing systems are inherently slow (due to the dwell timeouts), and users perceive them as frustrating. I propose to build a system that enables users to eye-type without the need for a dwell timeout at all. Potentially, my method will be faster than any other eye-tracker based method in the world.With my proposed system users write words by directing their gaze at the intended letter keys, in sequence. Users' intended words are transcribed when they look at a result area positioned above the keyboard. Users can write more than one word. They can also write sequences or words, or even stop short within a word. They may go to the spacebar key between words but this is not strictly necessary for the system to be able to correctly infer users' intended words.System 2: Stenography by inferenceThis system will be a stenography system for pen or single-finger input. The primary application is mobile text entry. However, I strive to create a system that to some extent can replace the desktop keyboard, should users so desire. Potentially it will be faster than any other pen-based text entry method.The idea behind this method is to enable users to write words quickly by gesturing patterns they have previously learned. Such open-loop recall from muscle-memory is much faster than the closed-loop visually-guided motions users are required to perform when they tap on, for example, an on-screen keyboard. My proposed system will enable users to quickly and accurately articulate gestures for individual words. These gestures will be fixed for a particular word. That is, each word is associated with a single (prototypical) unique gestural pattern. A user's input gesture is recognised by a pattern recognizer. The word whose closest pattern best match the user's input gesture will be outputted by the system as the user's intended word.Understanding the broader context of text entryThe last component of my proposed programme serves to contribute new perspectives to the text entry research field. As previously discussed, context of use is largely unexplored in text entry. I intend to explore this topic using a range of qualitative methods. I intend to perform interviews, conduct field studies (e.g. studying participants trying a prototype mobile speech recognizer at a caf), and diary-studies. The latter will be conducted with a system that provides users of a choice of a few text entry methods that I hypothesize will be useful for different situations. I also intend to read literature on design and architecture to further my understanding of the complete design space of text entry.</gtr:abstractText><gtr:potentialImpactText>Research dissemination The research results will be submitted for publications in the best relevant fora. I will first aim for the top-tier multi-disciplinary journals because both of the proposed systems have a high chance of breaking new ground and interest the general research community. System 1 (eye-typing by inference) have a high probability to break record-speeds of writing with the eyes only (currently held by Dasher), and System 2 (stenography by inference) has a high probability to break the record speeds of pen-based and single-finger based input (currently held by ShapeWriter). For justifications behind my estimations see the system descriptions outlined earlier. I also aim to continually publish intermediate research results in the human-computer interaction literature: CHI, IUI and UIST, and/or the HCI-journals Human-Computer Interaction, ACM Transactions on Computer-Human Interaction, and others. Transfer of knowledge to the general public My experience with previous press coverage of my research is that despite all the previous press articles, what contributed the most to reaching the general public was the release of ShapeWriter on Apple's AppStore for the iPhone. Learning from this experience I propose make software available for download as soon as it is practical to do so. I will start a website with a blog that explains how the systems work and provides users with the possibility to download the software. Technology transfer I will investigate the opportunity to patent isolated or entire parts of systems 1 and 2. I am an inventor or co-inventor of six patent applications and have so far been granted two. I have also co-founded a technology start-up based on my previous research results (ShapeWriter, Inc.), and I have experience in attracting capital, talking to enterprise customers, and all the other (extreme) challenges in actually making a technical start-up company work. I have also some experience of the technology-transfer services available within the University of Cambridge. I was an Executive Committee Member of Cambridge University Entrepreneurs 2008 - 2009, and I have attended seminars organised by Cambridge Enterprises (technology transfer body at Cambridge). It should be remembered that there are many ways to make technology transfer work. I want to have an academic career and I am not interested in starting another company unless an extraordinary opportunity arises. I do think there is a huge possibility to improve society by providing users with more efficient text entry methods, particularly for users who prefer to avoid the desktop computer. However, impact on society (number of customers) and business opportunity (market size) are not the same things. I know from my work on ShapeWriter, Inc. that it is extremely hard to make money from licensing to mobile phone manufacturers. At the same time, while enthusiasts are willing to pay for better text entry, the majority will not pay retail for a new text entry method. A possible exception is the market for accessibility software. However, I think it is morally objectionable to sell tax-funded research outcomes at high prices for disabled users. Therefore, I am probably going to release the software under an open source license, if this ends up being feasible. I predict two problems with the open source release scheme. First, there needs to be an active developer community that maintains the software. I will try to solve this by either attracting third-party developers, or by merging my project into a larger project that has an established developer community. Second, users need to be aware that my software exists. I will tackle this by putting up a website, uploading demonstration videos to websites, participating in university outreach efforts, and by demonstrating my systems at scientific conferences.</gtr:potentialImpactText><gtr:fund><gtr:end>2011-03-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>246359</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In this project we have investigated how to use machine learning and other AI approaches to improve text entry and control of computer systems. We have investigated dwell-free eye-typing and found that is has the potential to be twice as fast as regular dwell-based eye-typing. (Proc. ETRA 2012; Best Paper Honourable Mention). We have also investigated how to create error correction interfaces for speech recognition.We have also devised a new way to perform voice-only correction we call 'one-step' correction. Using our method one can correct speech recognition errors by merely speak the correction interface and there is no need for the user to first 'select' erroneous text. The system automatically infers the location of incorrect text and replaces it with a spoken correction (Proc. SLT 2010). We have used the same algorithm to create a system that fuses gesture and speech interaction. It enables users to enter text by speaking, gesturing, or a combination of both. Our system automatically fuses both input modalities and generates the most likely result (Proc. Interspeech 2011). Another issue in intelligent text entry is the underlying language model. A language model assigns probabilities to word sequences. For a language model to be useful it needs to be trained on appropriate text data. A predictive text entry system is limited by the predictive power of the underlying language model. This problem of creating appropriate language models is particularly acute for predictive Augmentative and Alternative Communication (AAC) devices that predict text that motor-disabled non-speaking individuals want to communicate. This lack of efficient language models due to lack of representative data has been a long-standing problem in the AAC field for over 25 years. We invented a new method to create efficient language models for AAC using a combination of crowdsourcing and intelligent mining of social media (Twitter and blog data). Using our new method we could create efficient language models for AAC that outperform existing models (Proc. EMNLP 2011; article in New Scientist in February 2012). We later used our language model to enable an illiterate AAC user to communicate on her own for the first time (Proc. SLPAT 2012; Proc. ASSETS 2012; ACM SIGACCESS Best Student Paper Award). Our new method and models are now actively used in the AAC field.The language models were also used (in combination with an error correction model) in a paper that investigated how to support efficient text entry on touchscreen tablets (Proc. CHI 2013). Two surveys on text entry and intelligent interaction have also been produced (one 'Research Highlight' in Communications of the ACM and one survey article in Foundations and Trends in Human-Computer Interaction.Understanding gesture interaction is often crucial for efficient intelligent interaction. We have investigated the memorability of gestures and found that in a series of three experiments self-defined gestures were significantly easier to remember than pre-designed gestures, even when one takes controls for training time (Proc. CHI 2013).Finally, we have explored how to leverage proxemics to design new intelligent interactive systems (Proc. IUI 2013; Pervasive and Mobile Computing 2013; Ext. Abstracts CHI 2013).To help the text entry field progress we have also acted as the lead organiser for the text entry workshops at CHI 2012 and CHI 2013.</gtr:description><gtr:exploitationPathways>The empirical and technical work has lead to implications for design that can be used as solution principles for developing similar user interfaces in industry. Researchers can build on these design implications to further study potential additional principles of design.</gtr:exploitationPathways><gtr:id>9D444E87-D917-4AFF-8000-C225B97BED4A</gtr:id><gtr:outcomeId>r-239083627.46153364779656e2</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>1EDC9454-9D25-4EAB-8136-8A97A4DEADCD</gtr:id><gtr:title>The word-gesture keyboard</gtr:title><gtr:parentPublicationTitle>Communications of the ACM</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f77a4d194f90c5207bd0e7a903f2741"><gtr:id>5f77a4d194f90c5207bd0e7a903f2741</gtr:id><gtr:otherNames>Zhai S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d0750756c63669</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C0A2EF7-49E7-46B6-BCC1-8626EA36982B</gtr:id><gtr:title>A versatile dataset for text entry evaluations based on genuine mobile emails</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7223c65f6056953fb5693adafb9f7b53"><gtr:id>7223c65f6056953fb5693adafb9f7b53</gtr:id><gtr:otherNames>Keith Vertanen (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_91846123586431c4b0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9F9F7E4E-D3CA-4DE1-96AD-CE1BDCC99C82</gtr:id><gtr:title>Multi-touch rotation gestures: performance and ergonomics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e8959a4df1fb727dc5c05a5cd34ffc70"><gtr:id>e8959a4df1fb727dc5c05a5cd34ffc70</gtr:id><gtr:otherNames>Eve Hoggan (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>r_122455612864311876</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>80DF0880-77AA-49FA-AEDA-ED0037CB3DED</gtr:id><gtr:title>Complementing text entry evaluations with a composition task</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Computer-Human Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f8649914a2705eef000a6cdaf303dde4"><gtr:id>f8649914a2705eef000a6cdaf303dde4</gtr:id><gtr:otherNames>Vertanen K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5675fe6b29f29</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4BAD1BD3-3985-4279-A2C9-159631B3FB8C</gtr:id><gtr:title>Grand challenges in text entry</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9c343a98cd11ba27ee4d6ed9abda96d"><gtr:id>c9c343a98cd11ba27ee4d6ed9abda96d</gtr:id><gtr:otherNames>Per Ola Kristensson (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>r_53335629356431c85c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BE86120-F464-4016-BE6A-8A51D1DBD190</gtr:id><gtr:title>The imagination of crowds: Conversational AAC language modeling using crowdsourcing and large data sources</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7223c65f6056953fb5693adafb9f7b53"><gtr:id>7223c65f6056953fb5693adafb9f7b53</gtr:id><gtr:otherNames>Keith Vertanen (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_471983690864258c0e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6AF42996-96CB-4DA4-867D-B6DE15C4E8D4</gtr:id><gtr:title>Getting it right the second time: Recognition of spoken corrections</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f8649914a2705eef000a6cdaf303dde4"><gtr:id>f8649914a2705eef000a6cdaf303dde4</gtr:id><gtr:otherNames>Vertanen K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7904-7</gtr:isbn><gtr:outcomeId>doi_53d05c05c5fea7c3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BEA16C08-DED0-4CE4-8ED1-B34329788753</gtr:id><gtr:title>Spelling as a complementary strategy for speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7223c65f6056953fb5693adafb9f7b53"><gtr:id>7223c65f6056953fb5693adafb9f7b53</gtr:id><gtr:otherNames>Keith Vertanen (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>r_3999707143643116e6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4828EF65-F25F-44C0-8610-01325063F4FB</gtr:id><gtr:title>Estimating and using absolute and relative viewing distance in interactive systems</gtr:title><gtr:parentPublicationTitle>Pervasive and Mobile Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5732350034a429073cb1495b53e711a2"><gtr:id>5732350034a429073cb1495b53e711a2</gtr:id><gtr:otherNames>Dostal J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_53d0050058e8bcd8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06B6A711-0DF1-4594-BCA2-ED1052EE2DC0</gtr:id><gtr:title>Subtle gaze-dependent techniques for visualising display changes in multi-display environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0e0e94d735f12a6c4a4d652f06cd0b37"><gtr:id>0e0e94d735f12a6c4a4d652f06cd0b37</gtr:id><gtr:otherNames>Jakub Dostal (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>r_694463042364311ba0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E5A6FF3-E1CC-4A21-87CC-9235FE23A292</gtr:id><gtr:title>Asynchronous multimodal text entry using speech and gesture keyboards</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9c343a98cd11ba27ee4d6ed9abda96d"><gtr:id>c9c343a98cd11ba27ee4d6ed9abda96d</gtr:id><gtr:otherNames>Per Ola Kristensson (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_55229389566431cac8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8D631BA3-B91F-4249-AB59-07572ECA4B60</gtr:id><gtr:title>Applying prediction techniques to phoneme-based AAC systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a3eee40769985a82ebd2e06ef26371b2"><gtr:id>a3eee40769985a82ebd2e06ef26371b2</gtr:id><gtr:otherNames>Ha Trinh (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>r_412365976564311a10</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0A70E45A-3B9E-4178-86C2-A3D2061EF6AC</gtr:id><gtr:title>Design dimensions of intelligent text entry tutors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9c343a98cd11ba27ee4d6ed9abda96d"><gtr:id>c9c343a98cd11ba27ee4d6ed9abda96d</gtr:id><gtr:otherNames>Per Ola Kristensson (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_535997916464312640</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C0242F5D-3638-47D2-8CBB-AF0C445069CD</gtr:id><gtr:title>Continuous recognition of one-handed and two-handed gestures using 3D full-body motion tracking sensors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9c343a98cd11ba27ee4d6ed9abda96d"><gtr:id>c9c343a98cd11ba27ee4d6ed9abda96d</gtr:id><gtr:otherNames>Per Ola Kristensson (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>r_28683066756431c5e6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0FC94779-2203-4C33-8B55-82A76E8F7862</gtr:id><gtr:title>Foundational Issues in Touch-Surface Stroke Gesture Design - An Integrative Review</gtr:title><gtr:parentPublicationTitle>Foundations and Trends? in Human-Computer Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5f77a4d194f90c5207bd0e7a903f2741"><gtr:id>5f77a4d194f90c5207bd0e7a903f2741</gtr:id><gtr:otherNames>Zhai S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d0830831246e17</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H027408/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>AEC62E13-CCF3-4039-A153-2E97DB66398F</gtr:id><gtr:grantRef>EP/H027408/1</gtr:grantRef><gtr:amount>246359.63</gtr:amount><gtr:start>2010-06-01</gtr:start><gtr:end>2011-03-28</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>F81DC7AC-169A-4146-AEAE-3C5B4182C019</gtr:id><gtr:grantRef>EP/H027408/2</gtr:grantRef><gtr:amount>183157.23</gtr:amount><gtr:start>2011-03-28</gtr:start><gtr:end>2013-05-27</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>