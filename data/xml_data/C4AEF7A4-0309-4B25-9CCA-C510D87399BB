<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/326EF7F4-5944-472D-BC94-2CA4484AEE74"><gtr:id>326EF7F4-5944-472D-BC94-2CA4484AEE74</gtr:id><gtr:name>University of Groningen</gtr:name><gtr:address><gtr:line1>Postbus 72</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2ED56452-FCBA-47B8-9F8B-DAE60779D29E"><gtr:id>2ED56452-FCBA-47B8-9F8B-DAE60779D29E</gtr:id><gtr:name>Audio Analytic Ltd</gtr:name><gtr:address><gtr:line1>50 St Andrews Street</gtr:line1><gtr:postCode>CB2 3AS</gtr:postCode><gtr:region>East of England</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/22C9FF8A-DD74-4CE6-B9AC-88CC35DAD198"><gtr:id>22C9FF8A-DD74-4CE6-B9AC-88CC35DAD198</gtr:id><gtr:name>Pompeu Fabra University</gtr:name><gtr:address><gtr:line1>Placa de la Merce, 10</gtr:line1><gtr:line2>Edifici Ta'nger</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/CA90CEC6-C62E-46B4-87FD-EB3BBDFE1509"><gtr:id>CA90CEC6-C62E-46B4-87FD-EB3BBDFE1509</gtr:id><gtr:firstName>William</gtr:firstName><gtr:surname>Davies</gtr:surname><gtr:orcidId>0000-0002-5835-7489</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/47654C46-517A-4E02-AD51-FEA33EA39D50"><gtr:id>47654C46-517A-4E02-AD51-FEA33EA39D50</gtr:id><gtr:firstName>David Mark</gtr:firstName><gtr:surname>Frohlich</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DAD387A7-797C-4DC5-B27F-A52A9FCDC601"><gtr:id>DAD387A7-797C-4DC5-B27F-A52A9FCDC601</gtr:id><gtr:firstName>Philip JB</gtr:firstName><gtr:surname>Jackson</gtr:surname><gtr:orcidId>0000-0001-7933-5935</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/FA6E1CBA-992B-4060-8F8F-513D55A737C6"><gtr:id>FA6E1CBA-992B-4060-8F8F-513D55A737C6</gtr:id><gtr:firstName>Krystian</gtr:firstName><gtr:surname>Mikolajczyk</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B1CDD839-FF89-4EBA-8F72-453446B6FE49"><gtr:id>B1CDD839-FF89-4EBA-8F72-453446B6FE49</gtr:id><gtr:firstName>Wenwu</gtr:firstName><gtr:surname>Wang</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DB9B029E-6575-4ABC-A08F-ED21A698645B"><gtr:id>DB9B029E-6575-4ABC-A08F-ED21A698645B</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Plumbley</gtr:surname><gtr:orcidId>0000-0002-9708-1075</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8A520840-7095-4A65-8161-CAD850606765"><gtr:id>8A520840-7095-4A65-8161-CAD850606765</gtr:id><gtr:firstName>Trevor</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Cox</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN014111%2F1"><gtr:id>C4AEF7A4-0309-4B25-9CCA-C510D87399BB</gtr:id><gtr:title>Making Sense of Sounds</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N014111/1</gtr:grantReference><gtr:abstractText>In this project we will investigate how to make sense from sound data, focussing on how to convert these recordings into understandable and actionable information: specifically how to allow people to search, browse and interact with sounds.

Increasing quantities of sound data are now being gathered in archives such as sound and audiovisual archives, through sound sensors such as city soundscape monitoring and as soundtracks on user-generated content. For example, the British Library (BL) Sound Archive has over a million discs and thousands of tapes; the BBC has some 1 million hours of digitized content; smart cities such as Santander (Spain) and Assen (Netherlands) are beginning to wire themselves up with a large number of distributed sensors; and 100 hours of video (with sound) are uploaded you YouTube every minute.

However, the ability to understand and interact with all this sound data is hampered by a lack of tools allowing people to &amp;quot;make sense of sounds&amp;quot; based on the audio content. For example, in a sound map, users may be able to search for sound clips by geographical location, but not by &amp;quot;similar sounds&amp;quot;. In broadcast archives, users must typically know which programme to look for, and listen through to find the section they need. Manually-entered textual metadata may allow text-based searching, but these typically only refer to the entire clip or programme, can often be ambiguous, and are hard to scale to large datasets. In addition, browsing sound data collections is a time-consuming process: without the help of e.g. key frame images available from video clips, each sound clip has to be &amp;quot;auditioned&amp;quot; (listened to) to find what is needed, and where the point of interest can be found. Radio programme producers currently have to train themselves to listen to audio clips at up to double speed to save time in the production process. Clearly better tools are needed.

To do this, we will investigate and develop new signal processing methods to analyse sound and audiovisual files, new interaction methods to search and browse through sets of sound files, and new methods to explore and understand the criteria searchers use when searching, selecting and interacting with sounds. The perceptual aspect will also investigate people's emotional response to sounds and soundscapes, assisting sound designers or producers to find audio samples with the effect they want to create, and informing the development of public policy on urban soundscapes and their impact on people.

There are a wide range of potential beneficiaries for the research and tools that will be produced in this project, including both professional users and the general public. Archivists who are digitizing content into sound and audiovisual archives will benefit from new ways to visualize and tag archive material. Radio or television programme makers will benefit from new ways to search through recorded programme material and databases of sound effects to reuse, and new tools to visualize and repurpose archive material once identified. Sound artists and musicians will benefit from new ways to find interesting sound objects, or collections of sounds, for them to use as part of compositions or installations. Educators will benefit from new ways to find material on particular topics (machines, wildlife) based on their sound properties rather than metadata. Urban planners and policy makers will benefit from new tools to understand the urban sound environment, and people living in those urban environments will benefit through improved city sound policies and better designed soundscapes, making the urban environment more pleasant. For the general public, many people are now building their own archives of recordings, in the form of videos with soundtracks, and may in future include photographs with associated sounds (audiophotographs). This research will help people make sense of the sounds that surround us, and the associations and memories that they bring.</gtr:abstractText><gtr:potentialImpactText>Potential beneficiaries of this project outside of the academic research community include anyone who could benefit from new ways to explore sound and audiovisual data, or could benefit from access to the sounds that would be enabled by the research. Examples from different sectors are given below.

Commercial private sector:
* Commercial companies designing audio equipment, through easier access to new audio research;
* Musicians, composers and sound artists, through ways to find and explore new sounds as part of their creative output;
* Computer games companies, through new ways to reuse sound datasets creatively for new game sounds;
* Audio archiving companies, through access to the latest algorithms and methods for annotating and exploring sound archives;
* Television and radio companies, through ability to use sound data exploration technologies in the creation, editing and re-use of audio and audiovisual programmes.
* Acoustic consultants, through access to new ways of mapping and understanding soundscapes, which will help drive new design possibilities for the built environment.
* Internet-of-things companies who supply smart cities with networked sensor systems, through access to novel acoustic algorithms for more sophisticated mapping.
Policy-makers and others in government and government agencies:
* Urban planning authorities, through new insights into the impact of sounds and how to visualize and understand these impacts;
* Research funders, through establishment of a network of researchers in sound data research, opening up new opportunities for valuable research, and new demonstrators showing the value of research.

Public sector, third sector and others:
* Museums and other organizations with sound archives, through new software methods to allow people to explore and use their archives;
* Smart cities, through better ways to make sense of acoustic data from urban microphone arrays;
* Science promotion organizations, in particular through outputs from the projects on how people perceive and navigate sounds.

Wider public:
* People interested in exploring audio recordings at home, school, college or university, either for educational or general interest purposes;
* People recording sounds on mobiles and other portable devices, including those capturing audio as soundtracks to videos;
* Teachers in schools, colleges or universities who want to use sound examples for teaching audio or music;
* People living in urban environments, through improved city sound policies and better designed soundscapes, making the urban environment more pleasant;
* Audiences of creative output involving audio and music, through availability of new creative outputs facilitated by creative access to new sounds.

Researchers employed on the project:
* Improved skills in research methodologies, which may be transferred into e.g. the commercial private sector on completion of the project.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-03-13</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-03-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1275401</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>DCASE 2016 Workshop on Detection and Classification of Acoustic Scenes and Events</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C76813E5-B4AF-4066-AA26-B04DD92C5347</gtr:id><gtr:impact>One-day workshop for researchers working on computational analysis of sound events and scene analysis to present and discuss their results. Resulted in increased interest in the topic area, and a new workshop is planned for 2017</gtr:impact><gtr:outcomeId>58b9a6b75fec14.80656865</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Postgraduate students</gtr:primaryAudience><gtr:url>http://www.cs.tut.fi/sgn/arg/dcase2016/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>87C30E92-4653-48A1-90E6-8D6FDF191C54</gtr:id><gtr:title>Toward an evidence-based taxonomy of everyday sounds</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bbd5231abb730a11ea2a3b10e15f2937"><gtr:id>bbd5231abb730a11ea2a3b10e15f2937</gtr:id><gtr:otherNames>Bones O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b9930cb2cd58.32563535</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C393A5B2-CD66-4ECC-9F69-CB131AAC3C72</gtr:id><gtr:title>Detection of overlapping acoustic events using a temporally-constrained probabilistic model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b98cc5c6adc7.35571051</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D486A48A-3A91-4BFC-8279-7CE29B4A465D</gtr:id><gtr:title>Deep neural network baseline for DCASE Challenge 2016</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb49b9a2a9cc4702b2094725c6f2a626"><gtr:id>cb49b9a2a9cc4702b2094725c6f2a626</gtr:id><gtr:otherNames>Kong Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b9991f858e59.87908346</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>50D8D572-4D37-4295-8878-5583E6FE2018</gtr:id><gtr:title>Polyphonic Sound Event Tracking Using Linear Dynamical Systems</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fdf37cffef1.51411585</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ED30883D-C28F-4B5B-B572-9BE38541AEC6</gtr:id><gtr:title>Fully DNN-based multi-label regression for audio tagging</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0cd56c5d6f324ffe01776b7518642ba"><gtr:id>d0cd56c5d6f324ffe01776b7518642ba</gtr:id><gtr:otherNames>Xu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99cc236b576.29334289</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A553B8D1-B1B3-4341-959B-32912F7D2FAF</gtr:id><gtr:title>Hierarchical learning for DNN-based acoustic scene classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0cd56c5d6f324ffe01776b7518642ba"><gtr:id>d0cd56c5d6f324ffe01776b7518642ba</gtr:id><gtr:otherNames>Xu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58b99bfb85fa03.50085499</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>63CC025A-039F-41D6-9736-03D3F3BDCA8C</gtr:id><gtr:title>Computational Analysis of Sound Scenes and Events</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a8847c0711ee7.16700153</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>298CAC1A-816A-4EF7-B60E-D0DE2A030731</gtr:id><gtr:title>Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0cd56c5d6f324ffe01776b7518642ba"><gtr:id>d0cd56c5d6f324ffe01776b7518642ba</gtr:id><gtr:otherNames>Xu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe006533cb2.98949152</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>645830B2-2201-492F-90AF-C8CC4375BE4F</gtr:id><gtr:title>Clang, chitter, crunch: Perceptual organisation of onomatopoeia</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bbd5231abb730a11ea2a3b10e15f2937"><gtr:id>bbd5231abb730a11ea2a3b10e15f2937</gtr:id><gtr:otherNames>Bones O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a992989582651.50405734</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61C40011-0B40-49A2-93DC-3674A601A8D5</gtr:id><gtr:title>Computational Analysis of Sound Scenes and Events</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c7135498fa65e36083afac95404e123d"><gtr:id>c7135498fa65e36083afac95404e123d</gtr:id><gtr:otherNames>Ellis D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a8847c0d51414.77135017</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0CC90B18-1A61-4CDE-A68F-875BFF028059</gtr:id><gtr:title>Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/66c832ff971f47adad988753010d9ac3"><gtr:id>66c832ff971f47adad988753010d9ac3</gtr:id><gtr:otherNames>Mesaros A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a352dd2f0e060.81530735</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>26FCEDBE-0773-4C55-A091-4B1E8110062B</gtr:id><gtr:title>Joint detection and classification convolutional neural network on weakly labelled bird audio detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb49b9a2a9cc4702b2094725c6f2a626"><gtr:id>cb49b9a2a9cc4702b2094725c6f2a626</gtr:id><gtr:otherNames>Kong Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8847c3b24df2.01278795</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>41317383-A5AE-4BC0-B7D5-BE1F846AC811</gtr:id><gtr:title>A joint detection-classification model for audio tagging of weakly labelled data</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb49b9a2a9cc4702b2094725c6f2a626"><gtr:id>cb49b9a2a9cc4702b2094725c6f2a626</gtr:id><gtr:otherNames>Kong Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a876ea98019e9.03179322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCE83037-8595-40AC-91A7-206C2E2DEEC5</gtr:id><gtr:title>Fast tagging of natural sounds using marginal co-regularization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5df6878d7c6b2adb436dfac36c2d2735"><gtr:id>5df6878d7c6b2adb436dfac36c2d2735</gtr:id><gtr:otherNames>Huang Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a876ea9bd5389.01586587</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>513F1A8B-381D-47EC-BD0A-2215AC79AA9A</gtr:id><gtr:title>Automatic Environmental Sound Recognition: Performance Versus Computational Cost</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f7b1878e971a0aeaa3ca61089c272023"><gtr:id>f7b1878e971a0aeaa3ca61089c272023</gtr:id><gtr:otherNames>Sigtia S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4014851de1.42737062</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B3679B38-2C0F-4994-9482-C7A94A90D993</gtr:id><gtr:title>Computational Analysis of Sound Scenes and Events</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d124682233a45a8cc9794b83b7c3482"><gtr:id>5d124682233a45a8cc9794b83b7c3482</gtr:id><gtr:otherNames>Virtanen T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a8847c14b61b0.47461189</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>591D6F8F-D179-41E9-A3B3-86F57BE573E8</gtr:id><gtr:title>Convolutional gated recurrent neural network incorporating spatial features for audio tagging</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0cd56c5d6f324ffe01776b7518642ba"><gtr:id>d0cd56c5d6f324ffe01776b7518642ba</gtr:id><gtr:otherNames>Xu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a876ea9262676.17809309</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N014111/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>