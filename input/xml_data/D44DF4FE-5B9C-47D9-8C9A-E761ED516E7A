<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46B41008-0EB4-4E28-BBFB-E98366999EC5"><gtr:id>46B41008-0EB4-4E28-BBFB-E98366999EC5</gtr:id><gtr:name>Durham University</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>Old Shire Hall</gtr:line1><gtr:line2>Old Elvet</gtr:line2><gtr:line4>Durham</gtr:line4><gtr:line5>County Durham</gtr:line5><gtr:postCode>DH1 3HP</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46B41008-0EB4-4E28-BBFB-E98366999EC5"><gtr:id>46B41008-0EB4-4E28-BBFB-E98366999EC5</gtr:id><gtr:name>Durham University</gtr:name><gtr:address><gtr:line1>Old Shire Hall</gtr:line1><gtr:line2>Old Elvet</gtr:line2><gtr:line4>Durham</gtr:line4><gtr:line5>County Durham</gtr:line5><gtr:postCode>DH1 3HP</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/618A8D43-0EF6-4279-8B8E-18194606E9EE"><gtr:id>618A8D43-0EF6-4279-8B8E-18194606E9EE</gtr:id><gtr:firstName>John</gtr:firstName><gtr:surname>Mollon</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/906B3C74-EB41-4EA7-BBFF-198B78A35820"><gtr:id>906B3C74-EB41-4EA7-BBFF-198B78A35820</gtr:id><gtr:firstName>Hannah</gtr:firstName><gtr:otherNames>Elizabeth</gtr:otherNames><gtr:surname>Smithson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FE019986%2F1"><gtr:id>D44DF4FE-5B9C-47D9-8C9A-E761ED516E7A</gtr:id><gtr:title>Sensory storage of spatio-temporal objects: sequences, signs and facial expressions</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/E019986/1</gtr:grantReference><gtr:abstractText>&lt;p>Many of the objects of visual perception are intrinsically spatiotemporal. Examples are facial expressions, manual gestures, and the trajectories of oncoming vehicles. The aim of this project is to better understand how such spatiotemporal objects are represented and stored in visual perception. There is well-accepted evidence for an auditory store that holds successive speech sounds in sequence, but visual sensory memory has been likened to a snapshot, with each successive input terminating or overwriting the previous one. Owing to the widespread acceptance of the &amp;quot;snapshot&amp;quot; model, the brief storage of dynamic visual stimuli remains largely unexplored. Yet many important real-world visual stimuli are dynamic. &lt;/p> 
&lt;p>This project asks whether the brain has the capacity to store a sensory representation of such stimuli, and specifically whether a late stimulus can allow retrospective analysis of earlier input. The planned experiments use &amp;quot;performance measures&amp;quot;: they do not rely on subjects? introspections, but on the number and nature of errors made in a controlled memory task. From the subjects? objective performance, the properties of storage systems within the visual part of the brain can be inferred. Understanding the brain's capacity to buffer sensory information is important in understanding human interaction with a constantly changing visual environment. &lt;/p></gtr:abstractText><gtr:fund><gtr:end>2011-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2007-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>250965</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>At any one moment, many more stimuli are falling on our senses than we can attend to and act upon. Each of our different senses briefly stores these incoming stimuli, so that stimuli that require a response can be processed even if their significance becomes apparent only after the physical event has passed. There is well-accepted evidence for an auditory store that holds successive speech sounds in sequence, but visual sensory memory has been likened to a snapshot, with each successive input terminating or overwriting the previous one. Owing to the widespread acceptance of the 'snapshot' model, the brief storage of dynamic visual stimuli remains largely unexplored. Yet many important real-world visual stimuli are dynamic. Examples are facial expressions, manual gestures, and the trajectories of oncoming vehicles. The aim of this project was to better understand how such spatiotemporal objects are represented and stored in visual perception. Our experimental results show that successive visual inputs are stored in sequence for a fraction of a second and specifically that a later stimulus can allow retrospective analysis of earlier inputs. We show this for alphanumeric materials, for objects that move through complex trajectories and for stimuli that change in colour but not in spatial position. This accumulated evidence, for different categories of information, shows that the brain has a general capacity to buffer sensory information that is changing rapidly over time. This leads to a reassessment of how human beings interact with a constantly changing visual environment.</gtr:description><gtr:exploitationPathways>1. Almost every current textbook of cognitive psychology introduces the classical model of iconic storage, in which successive inputs are superposed in a decaying two-dimensional representation. Tens of thousands of undergraduates are offered this account each year. We expect - certainly we warmly hope - that our results will lead to the replacement of this textbook account by a more subtle model of the visual sensory stores, a model that acknowledges that the store can hold successive items, as well as spatiotemporal objects such as gestures and trajectories. 
 
2. In scores of experiments in the field of psycholinguistics, a pattern mask is used to 'terminate the time for which a stimulus is available for processing'. In such experiments, the target stimulus is typically of high contrast and is clearly visible, but is followed after 50 or 100 ms by a high-contrast mask. For example, in a very widely-cited study, Rayner, Inhoff, Morrison, Slowiaczek and Bertera (1981) used a gaze-contingent masking paradigm to infer the time required to extract from each fixation the visual information necessary for reading. With mask delays as short as 50 ms, reading became possible. Similarly, in another classical study, Van Orden (1987) used a masking experiment to argue that skilled readers use phonological features in the visual identification of words: when target words were followed by a pattern mask, participants continued to show false positives to phonologically similar foils but not to orthographically similar foils, which Van Orden interpreted to mean that the most rapidly activated codes in reading were phonological ones.

Our results suggest that all such experiments depend on an invalid assumption - the assumption that the representation of the target is overwritten or displaced by the onset of the pattern mask. We have shown that a cue that follows a pattern mask can direct processing within an earlier target array: thus a visual representation of the target must survive the mask and remain available within the sensory buffers. 

3. In many sports, the competitor must dynamically store trajectories. These include the trajectories of other players (e.g., in football), of an opponent's limbs (e.g., in fencing or boxing) or of a ball (e.g., in cricket). Thus our findings may prove of importance in the applied field of sports psychology: they offer a more detailed understanding of how a sportsperson holds a trajectory or a spatiotemporal object in his or her sensory memory - whether or not attention is directed to the trajectory. Our experiments show large individual differences in the capacity of the visual sensory store: our tests may offer a means to select those who will excel in sports where multiple trajectories must be concurrently held in mind.</gtr:exploitationPathways><gtr:id>3022DF62-AFC1-406E-9018-E536FD881A70</gtr:id><gtr:outcomeId>545fb0084ddad6.97748733</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>In testing our participants' memory for spatiotemporal objects, such as trajectories, we developed a motion-tracking procedure to capture the participant's response, and new algorithms to analyse the relationship between presented and reported objects. We plan further experiments using techniques developed in the present project.</gtr:description><gtr:id>E5856CF2-2F40-4754-8922-B33FD3BC9847</gtr:id><gtr:impact>Not yet published.</gtr:impact><gtr:outcomeId>545fb8343d5bf0.58418862</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Motion analysis</gtr:title><gtr:type>Model of mechanisms or symptoms - human</gtr:type></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>AD73191D-8D6A-4996-8FD4-E75E4D824E92</gtr:id><gtr:title>Part-report for successive visual inputs</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/98849871c3f172b8d9eb017ace59533a"><gtr:id>98849871c3f172b8d9eb017ace59533a</gtr:id><gtr:otherNames> Wayne Smith (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>r_7820907452cb4d2958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A7C2A95-EBA8-4A17-9DCB-31ACA4220892</gtr:id><gtr:title>Target and mask interaction at different cortical levels in sensory storage</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/29b3612c25c935316b232a6ee31d49c2"><gtr:id>29b3612c25c935316b232a6ee31d49c2</gtr:id><gtr:otherNames> Rishi Bhardwaj (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_3867898192cb4d2444</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60669F95-C7C1-4FDF-99FD-4E4EE3E13AD9</gtr:id><gtr:title>Is there brief temporal buffering of successive visual inputs?</gtr:title><gtr:parentPublicationTitle>Quarterly journal of experimental psychology (2006)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3548516ae6be34cf530584f6a457ee4a"><gtr:id>3548516ae6be34cf530584f6a457ee4a</gtr:id><gtr:otherNames>Smith WS</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1747-0218</gtr:issn><gtr:outcomeId>541d7c14bb3434.19401683</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A81EF74-09B2-43F3-B1E3-073140479D39</gtr:id><gtr:title>Sensory memory for motion trajectories</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/29b3612c25c935316b232a6ee31d49c2"><gtr:id>29b3612c25c935316b232a6ee31d49c2</gtr:id><gtr:otherNames> Rishi Bhardwaj (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_5884920216cb4d2584</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9FA9C580-CE8C-452F-929D-AE49C0322565</gtr:id><gtr:title>Compatible and incompatible representations in visual sensory storage.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6438b8f63e0ba08da34d6631fd19a5a7"><gtr:id>6438b8f63e0ba08da34d6631fd19a5a7</gtr:id><gtr:otherNames>Bhardwaj R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>541d7c151bbbe6.59653173</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RES">RES-062-23-0530</gtr:identifier><gtr:identifier type="RCUK">ES/E019986/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>