<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>School of Public Health</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E74B2C2E-8863-4EC6-9BF0-15A7F4A87547"><gtr:id>E74B2C2E-8863-4EC6-9BF0-15A7F4A87547</gtr:id><gtr:firstName>Esra</gtr:firstName><gtr:surname>Suel</gtr:surname><gtr:orcidId>0000-0001-9246-3966</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=MR%2FS003983%2F1"><gtr:id>C6658097-F45E-495E-85EF-AB3736D479C8</gtr:id><gtr:title>Application of deep learning to heterogeneous open data for measuring urban environment and health</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>MR/S003983/1</gtr:grantReference><gtr:abstractText>In today's world, there is a great deal of information from sources like Google Street View and mobile phone usages about our environment and movement. The promise of big data, in many research fields, business, policy-making alike, that it will transform and improve the way we do things. Researchers in environmental health are also increasingly interested to use of information generated by mobile phones, satellites, wearable devices, social media, etc. The main challenge, however, is that making sense of diverse data at such big scales is not easy. Further, these new datasets are characteristically very different from conventional survey based data, hence their analysis require new methods. To this end, promising advances in deep learning over the past five years have achieved close-to-human performance on visual tasks such as recognizing objects and classifying them (e.g., telling a chair apart from a bird). In this project, I aim to apply advanced machine learning techniques to answer important questions in environmental health. Specifically, the focus will be on analysing images captured by satellites and street view cameras, and data generated from mobile phones, and investigating what they can reveal about health outcomes and its environmental and social determinants.</gtr:abstractText><gtr:technicalSummary>Recent advances in deep learning methods have achieved unprecedented improvements in accuracy on multiple visual tasks such as object recognition and classification. While previously unused large-scale data is gaining importance in public health research, recent technical developments mostly focused on advanced spatial machine learning and statistics methods while integration of deep learning techniques have been largely unexplored. The overall goal of my fellowship research is to leverage these advanced methods to answer important questions in environment and health research. Specifically, convolutional neural networks will be trained using satellite and street view imagery to extract outcomes of health and its environmental/social determinants. Transfer learning and class specific saliency maps will be used for post-hoc model visualisation. A combined analysis of models trained for multiple outcomes will be conducted to study overlaps and deviations. Secondly, joint use of mobile phone and image data as predictors will be explored for improving model performance. New methods of data integration will be developed to enable the use of multiple big data sources as predictors in a unified modelling framework. Finally, transferability of deep learning models trained on data from one city to other geographies will be evaluated; adaptation techniques to facilitate transferability will be explored.</gtr:technicalSummary><gtr:fund><gtr:end>2021-02-13</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/C008C651-F5B0-4859-A334-5F574AB6B57C"><gtr:id>C008C651-F5B0-4859-A334-5F574AB6B57C</gtr:id><gtr:name>MRC</gtr:name></gtr:funder><gtr:start>2018-02-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>331573</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">MR/S003983/1</gtr:identifier></gtr:identifiers><gtr:healthCategories><gtr:healthCategory><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:healthCategory></gtr:healthCategories><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics/><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>