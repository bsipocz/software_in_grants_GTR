<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2"><gtr:id>E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2</gtr:id><gtr:firstName>Barry-John</gtr:firstName><gtr:surname>Theobald</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0F5A33F9-B3F0-4899-AE00-DC8500D1FB9C"><gtr:id>0F5A33F9-B3F0-4899-AE00-DC8500D1FB9C</gtr:id><gtr:firstName>Ben</gtr:firstName><gtr:surname>Milner</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM014053%2F1"><gtr:id>6A1DA34A-DB4C-4386-9DB3-CE62E445B4F2</gtr:id><gtr:title>Speech Animation using Dynamic Visemes</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M014053/1</gtr:grantReference><gtr:abstractText>This project will investigate new methods for automatically producing speech animation. For animators in the movie industry this is typically a tedious, iterative process that involves key-framing static lip-poses and then handcrafting a blending function to transition from one key pose to another. In is not uncommon for an animator to spend several hours producing animation for just a few seconds of speech. 

We have previously worked on identifying a new dynamic unit for speech animation, termed dynamic visemes, and have shown that these units produce better animation than more traditional phoneme-based units. In this project we will integrate dynamic visemes into state of the art approaches to further improve upon the quality of automated animation that is currently possible. Furthermore, we will investigate how dynamic visemes relate to speech acoustics so that animation can be generated directly from the voice of an actor.

We will build tools that can be implemented in commercial animation pipelines so animation studios can use our tools as a basis for animating any speech on their own models. This will leave their artists free to focus on the overall performance of the character.

The proposed project is ambitious in its aims, proposing a new approaches for producing better speech animation. However, the impact of the work is wide reaching and has the potential to influence the production of speech content in all animated movies and computer games.</gtr:abstractText><gtr:potentialImpactText>During the course of this project we will develop new techniques for producing better quality speech animation than is available using the current state of the art. These techniques will be implemented in easy to use tools that will remove the burden from professional artists in the production of animated speech content. The artist will need only provide the audio assets from the actor and our tools will automatically generate the lip motion synchronised with the spoken words. The artist can then focus on the character performance, e.g. adding the expression and head pose variation to bring the character to life.

The manual effort required to create production-quality animated speech cannot be overstated. It is not unusual for even very skilled artists to spend many hours lip-syncing a character for only a very short scene. We will work with our industrial partner and industrial advisors to develop the research ideas into tools that work with industry standard software so that they can easily fit within animation pipelines currently used by the various studios and VFX houses. These new approaches will allow production-quality animation to be created in real-time, and so will provide significant savings to studios. Furthermore, we have demonstrated that our approach is not dependent on a particular model or rig and so can be applied to all animation, from cartoons to videorealistic characters. This will also mean that high quality speech animation is available to all, and not just the largest studios producing the biggest budget content.

More broadly, this work will impact research areas that involve the analysis of the (visible) speech articulators. For example, our approaches could find application in speech therapy, whereby a patient is shown speech-related exercises on a model of their own face. Our tools could automatically analyse the patients motions and compare them against the expect motion and show errors in their production. This form of assessment might be beneficial to stroke victims, whereby they need to re-train their facial muscles to properly articulate their speech. A face-to-face virtual speech therapist is always at hand to provide useful and immediate feedback, and our analysis tools can be used to log progress and provide progress related information to a real speech therapist.

There has been work showing the effectiveness of computer generated characters as learning aids, and our tools could be developed into a language tutoring system. Speech-related movements can be tracked on the face of a student and the virtual tutor can compare the observed motion with the expected motion of a native speaker. A range of face models, from cartoon-like to video real, will make foreign language learning more 'fun' and hopefully re-engage school children in learning foreign languages.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-07-21</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-07-22</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>343515</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Presentation at NorDev conference</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>FC368F88-7499-41C4-AC86-4BAD174091CC</gtr:id><gtr:impact>Presented my research at a booth to participants at a regional developers' conference.</gtr:impact><gtr:outcomeId>58c6bf845d2956.38959644</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>School visit to Fakenham Academy</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>6A81CB18-7025-4C3D-8295-2646E37CDD1D</gtr:id><gtr:impact>Talk to High School girls about my research, UEA and higher education.</gtr:impact><gtr:outcomeId>58c6be53bf3cd2.17722112</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Research-led teaching</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>16B6040E-C4FC-4B92-B1C5-14CB812E394A</gtr:id><gtr:impact>Embedded results of latest research into Level 6 module on Audio-visual Processing module. This took form of lectures and additional to laboratory classes and general awareness to students of latest work in this area.</gtr:impact><gtr:outcomeId>58c6c1c2973815.01400288</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Undergraduate students</gtr:primaryAudience><gtr:year>2015,2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>14000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Proof of Concept Funding</gtr:description><gtr:end>2017-08-02</gtr:end><gtr:fundingOrg>University of East Anglia</gtr:fundingOrg><gtr:id>C6C001CE-2603-482F-AB9C-D5C5E77203F6</gtr:id><gtr:outcomeId>58c6c245988295.53855754</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-05-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>It is possible to estimate visual speech features from audio speech features using deep neural networks.</gtr:description><gtr:exploitationPathways>Automatic animation of face in film/TV industry.</gtr:exploitationPathways><gtr:id>566C57E2-1889-478D-BCD4-1E6F478D68D3</gtr:id><gtr:outcomeId>56e01734d51c49.31151846</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A large AV speech dataset derived from YouTube video that has been face tracked and processed, and contains many thousands of hours of data.</gtr:description><gtr:id>0769361D-3FDE-4855-A66E-1F6727005E80</gtr:id><gtr:impact>Tool developed for tracking facial features and assessing audio-visual speech synchrony and we plan to release this to the speech processing community.</gtr:impact><gtr:outcomeId>58c6c0b7047259.56815654</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>YouTube AV Speech database</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F5ABD435-12D5-4183-9D0D-9A7D186589E2</gtr:id><gtr:title>A deep learning approach for generalized speech animation</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/06b8bfbbf7c5f9349931d4f38f3cbde2"><gtr:id>06b8bfbbf7c5f9349931d4f38f3cbde2</gtr:id><gtr:otherNames>Taylor S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe4613cfae4.62970294</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4B62CB57-ECFD-4DA3-8B24-746CFDE8C0E5</gtr:id><gtr:title>HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles for Time Series Classification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fbc7da6cd639b6ba3c34641706009382"><gtr:id>fbc7da6cd639b6ba3c34641706009382</gtr:id><gtr:otherNames>Lines J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6bbb40e92f8.22113516</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55436F06-B4A6-4F61-878A-2AD4584A398C</gtr:id><gtr:title>Generating Intelligible Audio Speech From Visual Speech</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/65e60e2942570ecd270b34029d2eedbc"><gtr:id>65e60e2942570ecd270b34029d2eedbc</gtr:id><gtr:otherNames>Le Cornu T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>23299290</gtr:issn><gtr:outcomeId>5aa8dff9eb68d9.35892426</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M014053/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>