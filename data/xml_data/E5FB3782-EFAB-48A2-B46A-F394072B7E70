<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3AF70AC4-19B0-4B79-8A06-D63F374FC0E7"><gtr:id>3AF70AC4-19B0-4B79-8A06-D63F374FC0E7</gtr:id><gtr:name>Queen Margaret University Edinburgh</gtr:name><gtr:address><gtr:line1>Queen Margaret University Drive</gtr:line1><gtr:line2>Musselburgh</gtr:line2><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH21 6UU</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Philosophy Psychology &amp; Language</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3AF70AC4-19B0-4B79-8A06-D63F374FC0E7"><gtr:id>3AF70AC4-19B0-4B79-8A06-D63F374FC0E7</gtr:id><gtr:name>Queen Margaret University Edinburgh</gtr:name><gtr:address><gtr:line1>Queen Margaret University Drive</gtr:line1><gtr:line2>Musselburgh</gtr:line2><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH21 6UU</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6E9EAE62-9ADC-4E84-B950-56966D646A8F"><gtr:id>6E9EAE62-9ADC-4E84-B950-56966D646A8F</gtr:id><gtr:firstName>Simon</gtr:firstName><gtr:surname>King</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/06CF538A-1BA2-4675-B7CA-66870C8DDDAF"><gtr:id>06CF538A-1BA2-4675-B7CA-66870C8DDDAF</gtr:id><gtr:firstName>Alice</gtr:firstName><gtr:surname>Turk</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/5D7F708A-5218-40FC-BD20-E55CBA1B14F6"><gtr:id>5D7F708A-5218-40FC-BD20-E55CBA1B14F6</gtr:id><gtr:firstName>Steve</gtr:firstName><gtr:surname>Renals</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE01609X%2F1"><gtr:id>E5FB3782-EFAB-48A2-B46A-F394072B7E70</gtr:id><gtr:title>An Edinburgh Speech Production Facility</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E01609X/1</gtr:grantReference><gtr:abstractText>The proposal is for a facility designed to record and analyse the movements of the lips, tongue, and jaw during spoken dialogue. This facility will be the first of its kind in the UK, and will be useful for applications in speech recognition and speech synthesis, as well as for developing theories of the cognitive representations and processes involved in normal and impaired speech production. The first output of the facility will be a database of recorded dialogue that will be useful for researchers interested in the relationships between speech movement and acoustics (important for speech technology applications), as well as in the particular types of pronunciations that speakers use during spontaneous dialogue.</gtr:abstractText><gtr:fund><gtr:end>2010-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>619576</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Queen Margaret University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Queen Margaret University</gtr:description><gtr:id>CEEDD03D-E207-44C1-A643-B1FCF5EF6920</gtr:id><gtr:impact>Geng et al. 2013
Scobbie et al. 2013</gtr:impact><gtr:outcomeId>546395df227690.76321770-1</gtr:outcomeId><gtr:partnerContribution>Joint collaboration on our EPSRC An Edinburgh Speech Production Facility project (EP/E016359/1 to QMU; EP/E01609X/1 to U. of Edinburgh)</gtr:partnerContribution><gtr:piContribution>Joint collaboration on our EPSRC An Edinburgh Speech Production Facility project (EP/E016359/1 to QMU; EP/E01609X/1 to U. of Edinburgh)</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2007-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We created a publicly available corpus of speech recordings that includes synchronized articulatory and acoustic records of speech in dialogue. Our facility is available for funded use; we offer calibration, gluing, recording, and data post-processing services. We commissioned the development of data analysis software, available through Articulate Instruments.</gtr:description><gtr:id>2BB418B8-F98D-490F-8723-65399FED3E16</gtr:id><gtr:outcomeId>r-3154464460.12197795d15e</gtr:outcomeId><gtr:sectors/><gtr:url>http://www.lel.ed.ac.uk/projects/ema/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The DoubleTalk articulatory speech corpus includes synchronised audio and articulatory trajectories for 12 speakers of English. The corpus was collected at the Edinburgh Speech Production Facility (ESPF) using two synchronized Carstens AG500 electromagnetic articulometers. The first release of the corpus comprises orthographic transcriptions aligned at phrasal level to EMA and audio data for each of 6 mixed-dialect speaker pairs. It is available from the ESPF online archive (http://espf.ppls.ed.ac.uk/frontend.php/project/espf-doubletalk). A variety of tasks were used to elicit a wide range of speech styles, including monologue (a modified Comma Gets a Cure and spontaneous story-telling), structured spontaneous dialogue (Map Task and Diapix), a wordlist task, a memory-recall task, and a shadowing task.</gtr:description><gtr:id>DBD87702-C30E-4007-83BF-B1F10A4C3ED9</gtr:id><gtr:impact>This is the first corpus of its kind that is publicly available to researchers wishing to study the nature of speech articulation in spontaneous dialogue.</gtr:impact><gtr:outcomeId>546219e3969dc0.73585807</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>The Edinburgh Speech Production Facility DoubleTalk Corpus</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://espf.ppls.ed.ac.uk/frontend.php/project/espf-doubletalk</gtr:url><gtr:yearFirstProvided>2010</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs><gtr:researchMaterialOutput><gtr:description>The Edinburgh Speech Production is unique worldwide in its capability of synchronously recording speech movement and acoustic data simultaneously from two participants in dialogue. This facility is built around two Carstens' AG500 electromagnetic articulographs, and acoustic recording equipment. The articulographs record the movements of sensors attached to the lips, tongue, jaw, and head at a sampling rate sufficiently high to obtain precise timing data about these movements (200 Hz). The facility is new; EPSRC funded its development from 2007-2010. As part of the EPSRC project, a U. of Edinburgh/ Queen Margaret University team recorded a corpus of 8 dialogue sessions. The dialogue corpus has been partially annotated, and will be available to the research community in 2010. We anticipate that it will be exploited for many purposes: For example, by speech technologists interested in the relationship between articulation and acoustics, and by linguists and psycholinguists interested in the relationship between grammar, pragmatics, and speech in an interactive</gtr:description><gtr:id>265A5898-0CE1-48C5-8E9A-DA8364CC1CB6</gtr:id><gtr:impact>The Edinburgh Speech Production Facility DoubleTalk corpus which is publicly available.</gtr:impact><gtr:outcomeId>54639b39e54139.03730823</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Double Electromagnetic Articulometry at the Edinburgh Speech Production Facility</gtr:title><gtr:type>Improvements to research infrastructure</gtr:type><gtr:url>http://www.ppls.ed.ac.uk/staff/resources/articulography.php</gtr:url></gtr:researchMaterialOutput></gtr:researchMaterialOutputs><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>To enable wider use of EMA data from the ESPF facility, Articulate Instruments Ltd produced an extra component of the AAA software programme specifically to handle EMA data. This commerically-available software, designed for articulatory speech analysis, therefore provides the opportunity for users familiar with other articulatory data to access and analyse EMA data without having to learn new software. In addition, the company's contribution to the design of the facility enables synchronised collection of EPG (electropalatography) data.</gtr:description><gtr:id>C0A68C16-EBAC-4556-A4AC-AC5DE6F264CE</gtr:id><gtr:impact>Sales of software, contributing to the economic success of the company (cf. REF Impact case study at QMU).</gtr:impact><gtr:outcomeId>5463bc24ab35f1.23114982</gtr:outcomeId><gtr:title>AAA software: EMA component</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.articulateinstruments.com/</gtr:url><gtr:yearFirstProvided>2010</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>84945E59-878A-4D45-8165-696CC1E9CC31</gtr:id><gtr:title>Timing in talking: what is it used for, and how is it controlled?</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/84f9c6460ffece20c36e2ac713c4913e"><gtr:id>84f9c6460ffece20c36e2ac713c4913e</gtr:id><gtr:otherNames>Turk A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>54621372aa6aa9.83176663</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B16A8855-B69A-4340-9C45-D3EAB3DA7E26</gtr:id><gtr:title>Between the Regular and the Particular in Speech and Language</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8072ef2b77aa3e529996c0257f8a4a4e"><gtr:id>8072ef2b77aa3e529996c0257f8a4a4e</gtr:id><gtr:otherNames>Geng, C., Mooshammer, C., Nam, H., Hoole, P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_709161368263ddb1e0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0272C7CE-1D74-45E9-BB86-1F345917CE63</gtr:id><gtr:title>The Edinburgh Speech Production Facility DoubleTalk Corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8a70eb95db64e6987e1006e6ec01af73"><gtr:id>8a70eb95db64e6987e1006e6ec01af73</gtr:id><gtr:otherNames>Scobbie, J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546216ba7d8899.71759022</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DCFAF5A0-381E-491B-8AF6-F8636B682990</gtr:id><gtr:title>Foreign Accent Conversion Through Concatenative Synthesis in the Articulatory Domain</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0fbb3ce1131ab01fcfe9342a197b9ff4"><gtr:id>0fbb3ce1131ab01fcfe9342a197b9ff4</gtr:id><gtr:otherNames>Felps D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f950950f0a7123</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EF8D772C-0EBC-43D7-B526-1E036790413D</gtr:id><gtr:title>Announcing the Electromagnetic Articulography (Day 1) Subset of the mngu0 Articulatory Corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/99adcd6969dae59d6d3a07f1cdeb2b90"><gtr:id>99adcd6969dae59d6d3a07f1cdeb2b90</gtr:id><gtr:otherNames>Richmond, K.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_66701424236427c898</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4231FB0B-AEB2-462F-9E7B-BE7171AA2AE7</gtr:id><gtr:title>Relying on critical articulators to estimate vocal tract spectra in an articulatory-acoustic database</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/357cab0e102e20e9ecca162e422f41a9"><gtr:id>357cab0e102e20e9ecca162e422f41a9</gtr:id><gtr:otherNames>Felps, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>r_7468567485cac710d4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EE89D05C-4B8F-4419-BC91-2844D8400188</gtr:id><gtr:title>A common co-ordinate system for mid-sagittal articulatory measurement</gtr:title><gtr:parentPublicationTitle>QMU CASL Working papers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d2cb4105450db721efcb411007666df0"><gtr:id>d2cb4105450db721efcb411007666df0</gtr:id><gtr:otherNames>Scobbie, JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>546398b522d983.08134666</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2844799C-B8D4-46EE-986B-8CE004174AC7</gtr:id><gtr:title>Recording speech articulation in dialogue: Evaluating a synchronized double electromagnetic articulography setup</gtr:title><gtr:parentPublicationTitle>Journal of Phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24179b49774713b48feec4cbdb6f059c"><gtr:id>24179b49774713b48feec4cbdb6f059c</gtr:id><gtr:otherNames>Geng C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54621516715119.33961752</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E01609X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>