<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:department>Sch of Computing</gtr:department><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FB417857-D248-4239-8923-5BAAC7E89114"><gtr:id>FB417857-D248-4239-8923-5BAAC7E89114</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Everingham</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DE0CEBB6-9C5F-4FEC-8714-B13526DCB21B"><gtr:id>DE0CEBB6-9C5F-4FEC-8714-B13526DCB21B</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Hogg</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B2D977BB-2CB6-41C4-9648-348170733814"><gtr:id>B2D977BB-2CB6-41C4-9648-348170733814</gtr:id><gtr:firstName>Derek</gtr:firstName><gtr:surname>Magee</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI01229X%2F1"><gtr:id>E11C3E42-ED9E-4495-A208-4ADDAFDC5B86</gtr:id><gtr:title>Learning to Recognise Dynamic Visual Content from Broadcast Footage</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I01229X/1</gtr:grantReference><gtr:abstractText>This research is in the area of computer vision - making computers which can understand what is happening in photographs and video. As humans we are fascinated by other humans, and capture endless images of their activities, for example home movies of our family on holiday, video of sports events or CCTV footage of people in a town center. A computer capable of understanding what people are doing in such images would be able to do many jobs for us, for example finding clips of our children waving, fast forwarding to a goal in a football game, or spotting when someone starts a fight in the street. For Deaf people, who use a language combining hand gestures with facial expression and body language, a computer which could visually understand their actions would allow them to communicate in their native language. While humans are very good at understanding what people are doing (and can learn to understand special actions such as sign language), this has proved extremely challenging for computers.Much work has tried to solve this problem, and works well in particular settings for example the computer can tell if a person is walking so long as they do it clearly and face to the side, or can understand a few sign language gestures as long as the signer cooperates and signs slowly. We will investigate better models for recognising activities by teaching the computer by showing it many example videos. To make sure our method works well for all kinds of setting we will use real world video from movies and TV. For each video we have to tell the computer what it represents, for example throwing a ball or a man hugging a woman . It would be expensive to collect and label lots of videos in this way, so instead we will extract approximate labels automatically from subtitle text and scripts which are available for TV. Our new methods will combine learning from lots of approximately labelled video (cheap because we get the labels automatically), use of contextual information such as which actions people do at the same time, or how one action leads to another ( he hits the man, who falls to the floor ), and computer vision methods for understanding the pose of a person (how they are standing), how they are moving, and the objects which they are using.By having lots of video to learn from, and methods for making use of approximate labels, we will be able to make stronger and more flexible models of human activities. This will lead to recognition methods which work better in the real world and contribute to applications such as interpreting sign language and automatically tagging video with its content.</gtr:abstractText><gtr:fund><gtr:end>2016-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>439457</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Oxford</gtr:description><gtr:id>CBFB29DF-619B-413A-B962-55F03EDF0DDF</gtr:id><gtr:impact>See joint publications:
1) Charles, J., Pfister, T., Magee, D., Hogg D. and Zisserman A. Upper Body Pose Estimation with Temporal Sequential Forests. in British Machine Vision Conference (BMVC), 2014.

2) Pfister, T., Simonyan, K., Charles, J. and Zisserman A. Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos. in Asian Conference on Computer Vision (ACCV), 2014.

3) Pfister, T., Charles, J. and Zisserman A. Domain-adaptive Discriminative One-shot Learning of Gestures. in European Conference on Computer Vision (ECCV), 2014. 

4) Charles, J., Pfister, T, Everingham, M. and Zisserman A. Automatic and Efficient Human Pose Estimation for Sign Language Videos. International Journal of Computer Vision (IJCV)

5) Charles, J., Pfister, T., Magee, D., Hogg D. and Zisserman A. Domain Adaptation for Upper Body Pose Tracking in Signed TV Broadcasts. in British Machine Vision Conference (BMVC), 2013.

6) Pfister, T., Charles, J. and Zisserman A. Large-scale Learning of Sign Language by Watching TV (Using Co-occurrences). in British Machine Vision Conference (BMVC), 2013

7) Pfister, T., Charles, J., Everingham, M. and Zisserman A. Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts. in British Machine Vision Conference (BMVC), 2012.</gtr:impact><gtr:outcomeId>545ce27169b7a3.76589825-1</gtr:outcomeId><gtr:partnerContribution>Code and Ideas</gtr:partnerContribution><gtr:piContribution>Joint research</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The project has been working with providors of sign language augmentation to the BBC in order to determine applicability of developed technologies to real world application.</gtr:description><gtr:id>7215C195-24C8-423F-8E86-45AA5A196B28</gtr:id><gtr:impactTypes/><gtr:outcomeId>56df2495783f86.63807622</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Education</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This grant (in part) relates to sign language recognition using large corpus's of video data. One major Leeds aspect (this is a collaboration with Oxford and Surrey) is tracking of upper body pose. Our efforts have led to personalised models that can be generated automatically for new video data sets. This means large corpus's can be annotated automatically and 'deep learning' used on this massive volume of data to generate a state of the art pose detector. 

Additionally, we have collated a number of large video data sets within the project which are available to other researchers. The Methods developed are also openly available.

http://www.robots.ox.ac.uk/~vgg/data/pose/</gtr:description><gtr:exploitationPathways>Generic pose detection is useful in the Chosen domain (sign language interpretation) and is being used by our project partners in Oxford. However, pose detection from a single monocular camera has wider applicability in areas such as security monitoring, entertainment, assisted living etc. Conventionally special equipment (e.g. Microsoft Kinekt) are required for such applications.</gtr:exploitationPathways><gtr:id>CDA522D5-9E98-4C13-89EE-12E5D4D487F3</gtr:id><gtr:outcomeId>56df23c6bca1a3.22744010</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.robots.ox.ac.uk/~vgg/research/pose_track/index.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>2C8CB93C-A286-48B3-8A6C-AF609089F314</gtr:id><gtr:title>Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos</gtr:title><gtr:parentPublicationTitle>N/A</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b19f49784ad002b6968f9010116856f5"><gtr:id>b19f49784ad002b6968f9010116856f5</gtr:id><gtr:otherNames>Pfister, T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545cdc5d99f695.76974070</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CBE31078-0DAA-4824-93E5-ADF4D25778A2</gtr:id><gtr:title>Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign Language TV Broadcasts.</gtr:title><gtr:parentPublicationTitle>n/a</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/794946e7dbca6021acaeea472626370a"><gtr:id>794946e7dbca6021acaeea472626370a</gtr:id><gtr:otherNames>Pfister, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545cdf6f9fcab8.30605049</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98C97500-2246-4D7A-BB58-335F365D88EE</gtr:id><gtr:title>Large-scale Learning of Sign Language by Watching TV (Using Co-occurrences).</gtr:title><gtr:parentPublicationTitle>n/a</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/229b2a2ed47beec720217b12bd19ed95"><gtr:id>229b2a2ed47beec720217b12bd19ed95</gtr:id><gtr:otherNames>Pfister, T.,</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>545cdef9057b20.07015323</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1CE01EFA-B2EA-4A36-84DF-9143AC4480AD</gtr:id><gtr:title>Large-scale Learning of Sign Language by Watching TV (Using Co-occurrences)</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b19f49784ad002b6968f9010116856f5"><gtr:id>b19f49784ad002b6968f9010116856f5</gtr:id><gtr:otherNames>Pfister, T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56cd8d162a9681.32820876</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E535A81-EF91-421D-87F3-81142D567035</gtr:id><gtr:title>Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/794946e7dbca6021acaeea472626370a"><gtr:id>794946e7dbca6021acaeea472626370a</gtr:id><gtr:otherNames>Pfister, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56cd90884109c9.18288084</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D4E53B7-A41C-4AA3-A5EA-931C798A7E54</gtr:id><gtr:title>Upper body pose estimation with temporal sequential forests</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57e70663bc644ed52b371f14840efd8a"><gtr:id>57e70663bc644ed52b371f14840efd8a</gtr:id><gtr:otherNames>Charles, J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56cd91dd8be321.86344746</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCC57B61-6C97-4FC5-A6B0-3A5E9986EAA9</gtr:id><gtr:title>Personalizing Human Video Pose Estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/250b919baa4364e0192abc775244ce94"><gtr:id>250b919baa4364e0192abc775244ce94</gtr:id><gtr:otherNames>Charles J.</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56df211a8078d9.78228102</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DEF7E224-1DD9-46F9-8C2F-B1E22724BD55</gtr:id><gtr:title>Flowing ConvNets for Human Pose Estimation in Videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/794946e7dbca6021acaeea472626370a"><gtr:id>794946e7dbca6021acaeea472626370a</gtr:id><gtr:otherNames>Pfister, T.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56cd925975d849.26143846</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>84B7AEF2-F035-412C-BD2F-EDC1AEEBC703</gtr:id><gtr:title>Upper Body Pose Estimation with Temporal Sequential Forests.</gtr:title><gtr:parentPublicationTitle>[]</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/57e70663bc644ed52b371f14840efd8a"><gtr:id>57e70663bc644ed52b371f14840efd8a</gtr:id><gtr:otherNames>Charles, J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545cdba1f28019.25177060</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AE4CE458-A89A-4C26-9A81-C28D65B39791</gtr:id><gtr:title>Automatic and Efficient Human Pose Estimation for Sign Language Videos</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/215a3501ac12c0957a121af339f6bea5"><gtr:id>215a3501ac12c0957a121af339f6bea5</gtr:id><gtr:otherNames>Charles J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56cd8fb0b62e33.79240243</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FCCA5572-48CD-42C0-B45D-249D9C91B488</gtr:id><gtr:title>Domain Adaptation for Upper Body Pose Tracking in Signed TV Broadcasts</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a70aed6bcae09be3277e25ee0856dbb"><gtr:id>5a70aed6bcae09be3277e25ee0856dbb</gtr:id><gtr:otherNames>Charles, J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56cd8ddf4ecc59.67977805</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I01229X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>