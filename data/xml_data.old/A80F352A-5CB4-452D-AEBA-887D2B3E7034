<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/7F75BAD0-CD11-40DC-BF44-43B483A316F8"><gtr:id>7F75BAD0-CD11-40DC-BF44-43B483A316F8</gtr:id><gtr:firstName>Tao</gtr:firstName><gtr:surname>Xiang</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG063974%2F1"><gtr:id>A80F352A-5CB4-452D-AEBA-887D2B3E7034</gtr:id><gtr:title>Multi-Object Video Behaviour Modelling for Abnormality Detection and Differentiation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G063974/1</gtr:grantReference><gtr:abstractText>There are over 4.2 million closed-circuit television (CCTV) surveillance cameras operational in the UK and many more worldwide, collecting a colossal amount of video data for security, safety, and infrastructure and facility management purposes. A typical existing CCTV system relies on a handful of human operators at a centralised control room for monitoring video inputs from hundreds of cameras. Too many cameras and too few operators leave the system ill equipped to fulfil the task of detecting events and anomalies that require immediate and appropriate response. Consequently, the use of the existing CCTV surveillance systems is limited predominately to post-mortem analysis. There is thus an increasing demand for automated intelligent systems for analysing the content of the vast quantities of surveillance videos and triggering alarms in a timely and robust fashion. One of the most critical components and functionalities of such a system is to monitor object behaviour captured in the videos and detect/predict any suspicious and abnormal behaviour that could pose a threat to public safety and security. This project aims to develop underpinning capabilities for an innovative intelligent video analytics system for detecting abnormal video behaviour in public spaces. More specifically, the project will address three open problems:1.To develop a new model for spatio-temporal visual context for abnormal behaviour detection. Behaviours are inherently context-aware, exhibited through constraints imposed by scene layout and the temporal nature of activities in a given scene. Consequently, the same behaviour can be deemed as either normal or abnormal depending on where and when it occurs. We aim to go beyond the state-of-the-art semantic scene modelling approaches, most of which are focused solely on modelling scene layout such as entry and exit points, by developing a more comprehensive spatio-temporal model of dynamic visual context. 2.To develop a novel multi-object behaviour model for real-time detection and differentiation of abnormalities in complex video behaviours that involve multiple objects interacting with each other (e.g. a group of people meet in front of a ticket office at a train station and then go to different platforms). 3.To develop a novel online adaptive learning algorithm for estimating the parameters of the behaviour model to be developed. Although video abnormality detection tools are already available in many existing CCTV control systems, human operators are often reluctant to use them because there are too many parameters to tune and re-tune for different scenarios given changing visual context. With the incremental and adaptive learning algorithm our behaviour model can be used for different surveillance scenarios over a long period of time with minimal human intervention. More importantly, using the algorithm, our behaviour model will become adaptive to both changes of visual context (therefore the definition of normality/abnormality), and valuable feedbacks from human operators on the abnormality detection output of the model.</gtr:abstractText><gtr:fund><gtr:end>2012-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>350506</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>11200</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Royal Society International Exchange Programme</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>The Royal Society</gtr:fundingOrg><gtr:fundingRef>IE110976</gtr:fundingRef><gtr:id>693D00ED-86A9-477C-9B9B-9472F4039A94</gtr:id><gtr:outcomeId>56d43976e736b4.72860764</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project has the following key findings:

(1) We have developed a machine learning algorithm that can detect abnormal object behaviour in video with very few examples. For example, if a CCTV (closed circuit television) operator watches a short clip of CCTV footage and notices something that is out of norm, he/she only needs to give an indication that there is an abnormality in the clip. The developed algorithm can then automatically identify what the abnormality is and build a model to detect it if it occurs again in the future. This is achieved by developing a novel topic model with fast learning and inference algorithm. Such an algorithm is very useful in practice where asking human to locate and describe a behaviour anomaly in video is hard, but giving a binary indicator on whether there is an anomaly in the video is easier.

(2) Human feedback can be prompted at the right moment and exploited subsequently to improve the performance of a classifier, as well as discover more new classes as quick as possible. This is achieved by formulating a novel active learning criterion which needs no parameter to tune. With this approach we can address many real world problems involving rare class discovery and classification, including video anomaly detection, financial fraud detection, and computer network intrusion detection. For example, this approach can be used together with the anomaly detection algorithm mentioned above. More specifically, if a human operator is asked to give feedback on every single video clip, he or she will be overwhelmed. However, if our model can automatically identify the most important video clips for improving the model , only feedback for those clips is needed, reducing the work load of human significantly.

(3) For detecting objects from images or actions from video, it is possible to learn a model with weak supervision from human, that is, only whether or not the image/video contain the object/action of interest is required to be annotated, rather than their precise locations. This makes learning a detector a much easier task. For example, one can search the keyword &amp;quot;cat&amp;quot; on Google Image and get thousands of images containing cats. Without needing to locate exactly where the cat is in each image, our approach can build a model for cat and use it to locate each cat not only in the retrieved images, but also in any unseen images.</gtr:description><gtr:exploitationPathways>The research community in the areas of computer vision and machine learning can benefit directly from the research publications generated from the project (18 of them). Many of the project publications have high citation numbers. 


The outcomes of the project can be used in non-academic context in a number of ways:

(1) General public's safety and security can be improved by applying the abnormal behaviour detection techniques developed in the project. In particular, by detecting anomalies as they occur or even before they are about to happen, law enforcement agencies can act promptly to protect the lives and properties of citizens.

(2) Operators of CCTV surveillance system can benefit greatly from the outcomes of the project. In particular, the operators can be better focused on the automatically identified suspicious behaviour rather than spread their attention across dozens or even hundreds of video feeds.

(3) Social media sharing website users can benefit from the project outcomes by having more efficiently image and video search tools.

(4) Visually impaired individuals can also benefit via improved methods for describing and summarising image and video data. The developed techniques can potentially be integrated into a system that can automatically annotate the content of images and videos. The generated annotation can be read out by a voice synthesiser.</gtr:exploitationPathways><gtr:id>EFD7C10F-2CBE-46D8-A184-EA89D0B925E9</gtr:id><gtr:outcomeId>r-8349343750.951200577608bca</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Security and Diplomacy,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>dpgmm: (Apache 2.0) A Dirichlet process Gaussian mixture model, implemented using the mean field variational technique. Its about as good as a general purpose density estimator can get, though it suffers from a heavy computational and memory burden. Code is in pure python, depends on the gcp module and it is very neat and reasonably well commented - speed is reasonable for the method as the code vectorises well. Unlike some other implementations handles incremental learning correctly - both adding extra sticks to the model and adding extra data after convergence.</gtr:description><gtr:id>B7409BD3-7CDE-4C2A-BD1B-54504C753845</gtr:id><gtr:impact>The dynamic background model has been very popular among the research community and many other researchers benefit from using the code released by us.</gtr:impact><gtr:outcomeId>56d436760ce696.46932736</gtr:outcomeId><gtr:title>Dynamic background model</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/thaines/helit</gtr:url><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B34E0294-D3FF-45FB-834F-A6C1C5D78B50</gtr:id><gtr:title>Transfer Learning by Ranking for Weakly Supervised Object Annotation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/15147acd0b188a35bf76cd83cd3b2035"><gtr:id>15147acd0b188a35bf76cd83cd3b2035</gtr:id><gtr:otherNames>Zhiyuan Shi (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_2163796062140bddd0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6209069D-3297-40B2-8825-1328E0DB6D31</gtr:id><gtr:title>Weakly Supervised Action Detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b696b89de5cd610c64688635134e9854"><gtr:id>b696b89de5cd610c64688635134e9854</gtr:id><gtr:otherNames>Tao Xiang (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_47868999491408521e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2051BBCD-D823-43C5-8218-107B02F1612E</gtr:id><gtr:title>Time-Delayed Correlation Analysis for Multi-Camera Activity Understanding</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/90e7a3a71faf9859dbc2f8194dd9b6f5"><gtr:id>90e7a3a71faf9859dbc2f8194dd9b6f5</gtr:id><gtr:otherNames>Loy C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53cfdefdef6ec8cb</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F27AC9EB-95E4-490F-ADE7-46D30D21D26D</gtr:id><gtr:title>Incremental activity modeling in multiple disjoint cameras.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/164ae623860b6c9e1a568fab94e4af3e"><gtr:id>164ae623860b6c9e1a568fab94e4af3e</gtr:id><gtr:otherNames>Loy CC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05efb66a00</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98A4BDF7-1B86-46DD-8D52-225583D16AA8</gtr:id><gtr:title>Delta-Dual Hierarchical Dirichlet Processes: A pragmatic abnormal behaviour detector</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c492bb39ff2308f7f364eb19212b22b"><gtr:id>9c492bb39ff2308f7f364eb19212b22b</gtr:id><gtr:otherNames>Haines T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1101-5</gtr:isbn><gtr:outcomeId>doi_53d0580586aa2756</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C20774F-8CF9-413E-BB21-D86B3ED54E88</gtr:id><gtr:title>Detecting and discriminating behavioural anomalies</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/90e7a3a71faf9859dbc2f8194dd9b6f5"><gtr:id>90e7a3a71faf9859dbc2f8194dd9b6f5</gtr:id><gtr:otherNames>Loy C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d0040044d19257</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8B32AC23-AA4F-4141-A99C-591093B650BD</gtr:id><gtr:title>Stream-based joint exploration-exploitation active learning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/90e7a3a71faf9859dbc2f8194dd9b6f5"><gtr:id>90e7a3a71faf9859dbc2f8194dd9b6f5</gtr:id><gtr:otherNames>Loy C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1226-4</gtr:isbn><gtr:outcomeId>doi_53d0570574194f3b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8F4AA0E9-2B3D-421A-B17C-221305C9AEB1</gtr:id><gtr:title>Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/10ab0788c5cce787e81cf821070df42f"><gtr:id>10ab0788c5cce787e81cf821070df42f</gtr:id><gtr:otherNames>Siva P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f93d93d38b084f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1DD4B445-82FD-480D-9BC0-A499644C0D3F</gtr:id><gtr:title>Background Subtraction with DirichletProcess Mixture Models.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/270cc0fa0ae8ba65b7e2a48739e82679"><gtr:id>270cc0fa0ae8ba65b7e2a48739e82679</gtr:id><gtr:otherNames>Haines TS</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_55f93d93d39583e3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>235418C1-C961-4B22-9454-9E3E8ADE98B0</gtr:id><gtr:title>Weakly supervised object detector learning with model drift detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/10ab0788c5cce787e81cf821070df42f"><gtr:id>10ab0788c5cce787e81cf821070df42f</gtr:id><gtr:otherNames>Siva P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1101-5</gtr:isbn><gtr:outcomeId>doi_53d0580586982a72</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G063974/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>