<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3E18DACB-05AA-4C71-88FB-C909A8311BB5"><gtr:id>3E18DACB-05AA-4C71-88FB-C909A8311BB5</gtr:id><gtr:name>Birkbeck College</gtr:name><gtr:department>Psychological Sciences</gtr:department><gtr:address><gtr:line1>Malet Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 7HX</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3E18DACB-05AA-4C71-88FB-C909A8311BB5"><gtr:id>3E18DACB-05AA-4C71-88FB-C909A8311BB5</gtr:id><gtr:name>Birkbeck College</gtr:name><gtr:address><gtr:line1>Malet Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 7HX</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/EDB55F61-EF46-40F4-814A-AE2577213A94"><gtr:id>EDB55F61-EF46-40F4-814A-AE2577213A94</gtr:id><gtr:firstName>Martin</gtr:firstName><gtr:surname>Eimer</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FL016400%2F1"><gtr:id>1D749C1B-22D0-4BD2-8AAB-8D001CCA57F4</gtr:id><gtr:title>Tracing the template: Investigating the representation of perceptual relevance</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/L016400/1</gtr:grantReference><gtr:abstractText>Adaptive perception requires the prioritization of relevant over irrelevant information. When we are looking for a specific book of which we only remember the color of its cover, we can limit our search to mainly that color. The mental representation of what we are looking for is called the attentional template (also target template, search template, attentional set; e.g., Folk et al., 1992). An attentional template is a flexible representation reflecting current selection preferences, as derived from continuously changing task demands and prior selection history. Even though attentional templates are essential for shaping and controlling perception and action in everyday life, surprisingly little is known about their nature. For example, when you look for your car keys, do you look for their shape, their color, or both? In case of the latter, are shape and color integrated in a single representation, or are they independently represented? Can you look for your wallet at the same time, without affecting your &amp;quot;key&amp;quot; template? Furthermore, it is often assumed that visual attention is guided by visual templates, but it is perfectly possible that non-visual types of representation (e.g., semantic codes) are also involved. Finally, the nature of the template may change fundamentally in the course of learning, as a result of selection history. The aim of this collaborative project is to answer some of these fundamental questions.</gtr:abstractText><gtr:potentialImpactText>This proposal features primarily basic psychological and neuroscientific research into the mental representation which drives our vision, and fundamental constraints on top-down attentional control. Primary beneficiaries are cognitive and neuroscientists with interests in perception, memory, and action in healthy as well as clinical populations. People suffering from ADHD, ADD, schizophrenia, Parkinson, or Alzheimer are known to suffer from attention-related problems, and our work will put important theoretical constraints on mapping out those deficits. Our research also has practical implications for many situations where selective visual processing is involved - from driving to airline baggage screening, from interpreting X-rays to human search for roadside bombs in lesser peaceful parts of the world, and from web browsing to marketing.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2014-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>276749</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This is an ongoing research grant, funded by the ESRC as part of the ORA initiative, with partner institutions in the Netherlands and Germany.
The research conducted in this project to date was exceptionally successful, and has already resulted in 11 publications. Most of them were in leading high-impact journals in the field, and many of our recent findings have already received substantial attention from other researchers world-wide (see publications section for a detailed list of all publications to date).</gtr:description><gtr:exploitationPathways>This is basic cognitive neuroscience research with possible applications in the field of ergonomics.</gtr:exploitationPathways><gtr:id>81CA928E-E2F9-4B2D-933B-23F4F042CC04</gtr:id><gtr:outcomeId>56af8dbc00de23.65088865</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Education</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>477246EA-D4A7-434F-AB59-5EA98E898941</gtr:id><gtr:title>The guidance of spatial attention during visual search for color combinations and color configurations.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b83653b6c4393eba43b1971f1f71de5"><gtr:id>7b83653b6c4393eba43b1971f1f71de5</gtr:id><gtr:otherNames>Berggren N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5829d4a7e76f64.54528566</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13C52D3E-FD6D-4162-9640-60EC2B44F838</gtr:id><gtr:title>The Role of Color in Search Templates for Real-world Target Objects.</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f08cbc36bc515ad6e21b0c161c28d65"><gtr:id>9f08cbc36bc515ad6e21b0c161c28d65</gtr:id><gtr:otherNames>Nako R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>5829d4a6ee3ba5.51622929</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E558090-D35F-44CC-8FB9-3A9FD3EE8E0F</gtr:id><gtr:title>Why the item will remain the unit of attentional selection in visual search.</gtr:title><gtr:parentPublicationTitle>The Behavioral and brain sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cae372acc1c9002d24cd87115d277a93"><gtr:id>cae372acc1c9002d24cd87115d277a93</gtr:id><gtr:otherNames>Eimer M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0140-525X</gtr:issn><gtr:outcomeId>5a788379631a23.87453079</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6EC5C833-D3AD-46F1-A6EB-D20E01751822</gtr:id><gtr:title>Target objects defined by a conjunction of colour and shape can be selected independently and in parallel.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df5dab2d58f5d48407448135c9c4da20"><gtr:id>df5dab2d58f5d48407448135c9c4da20</gtr:id><gtr:otherNames>Jenkins M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>5a1d383e5e24c4.43447384</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>361CCE19-3752-429F-8AB9-2175CB6283F6</gtr:id><gtr:title>Multivariate EEG analyses support high-resolution tracking of feature-based attentional selection.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b4368d03144b5d4d5d7997f5caced15b"><gtr:id>b4368d03144b5d4d5d7997f5caced15b</gtr:id><gtr:otherNames>Fahrenfort JJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn><gtr:outcomeId>5a1d38826765a9.47384183</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5CFD4C3D-ADE2-4DA9-A44B-6454E0101335</gtr:id><gtr:title>The control of attentional target selection in a colour/colour conjunction task.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b83653b6c4393eba43b1971f1f71de5"><gtr:id>7b83653b6c4393eba43b1971f1f71de5</gtr:id><gtr:otherNames>Berggren N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>5829d4a94f1da9.52544264</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BDDC8353-24D0-4398-8B03-BF6CAC5D705C</gtr:id><gtr:title>The spatially global control of attentional target selection in visual search</gtr:title><gtr:parentPublicationTitle>Visual Cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b83653b6c4393eba43b1971f1f71de5"><gtr:id>7b83653b6c4393eba43b1971f1f71de5</gtr:id><gtr:otherNames>Berggren N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fca75eb0221.65087781</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9ECA7B91-D943-4B9B-A26C-4C387BF1A0E2</gtr:id><gtr:title>Multiple foci of spatial attention in multimodal working memory.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1ce947e1da708714fcf7119d6ad1293"><gtr:id>d1ce947e1da708714fcf7119d6ad1293</gtr:id><gtr:otherNames>Katus T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>5829d4a930b428.32283357</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2B7FF1BD-0F37-44FF-8FA4-9516A3973AB6</gtr:id><gtr:title>Does Contralateral Delay Activity Reflect Working Memory Storage or the Current Focus of Spatial Attention within Visual Working Memory?</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7b83653b6c4393eba43b1971f1f71de5"><gtr:id>7b83653b6c4393eba43b1971f1f71de5</gtr:id><gtr:otherNames>Berggren N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>5829d4a7399bb2.68990083</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C9D9891B-3D8B-405F-8721-4AF5A2F94959</gtr:id><gtr:title>Sustained maintenance of somatotopic information in brain regions recruited by tactile working memory.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1ce947e1da708714fcf7119d6ad1293"><gtr:id>d1ce947e1da708714fcf7119d6ad1293</gtr:id><gtr:otherNames>Katus T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>5603caf75ed689.14728743</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A743C29E-AD1E-48BD-B8C6-A188B757C32B</gtr:id><gtr:title>Rapid Parallel Attentional Selection Can Be Controlled by Shape and Alphanumerical Category.</gtr:title><gtr:parentPublicationTitle>Journal of cognitive neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/df5dab2d58f5d48407448135c9c4da20"><gtr:id>df5dab2d58f5d48407448135c9c4da20</gtr:id><gtr:otherNames>Jenkins M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0898-929X</gtr:issn><gtr:outcomeId>5829d4a75901b2.41455488</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D7BF8A1A-5A10-45AD-B26B-F49E97EC2A13</gtr:id><gtr:title>Rapid top-down control over template-guided attention shifts to multiple objects.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0270db24e682e72ae46b566ce7a8ad5d"><gtr:id>0270db24e682e72ae46b566ce7a8ad5d</gtr:id><gtr:otherNames>Grubert A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>5829d4a90e3252.23224218</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0C62B4E7-9F84-4EC9-8250-A2AF91275522</gtr:id><gtr:title>The guidance of visual search by shape features and shape configurations.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8bc832b2fae1246009874414de9a6782"><gtr:id>8bc832b2fae1246009874414de9a6782</gtr:id><gtr:otherNames>McCants CW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5a995cba92f520.85400031</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A0665666-54A8-4348-A4D4-63846B5B6977</gtr:id><gtr:title>EPS Mid-Career Award 2014. The control of attention in visual search: Cognitive and neural mechanisms.</gtr:title><gtr:parentPublicationTitle>Quarterly journal of experimental psychology (2006)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cae372acc1c9002d24cd87115d277a93"><gtr:id>cae372acc1c9002d24cd87115d277a93</gtr:id><gtr:otherNames>Eimer M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1747-0218</gtr:issn><gtr:outcomeId>5603caf84e8233.52542138</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>928A8E29-430F-409D-A885-A1CE9EB09112</gtr:id><gtr:title>Category-based guidance of spatial attention during visual search for feature conjunctions.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f08cbc36bc515ad6e21b0c161c28d65"><gtr:id>9f08cbc36bc515ad6e21b0c161c28d65</gtr:id><gtr:otherNames>Nako R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5829d4a7c16216.76498663</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5A8DCBE1-9C64-45F1-A11D-6FD593F0B03C</gtr:id><gtr:title>Lateralized delay period activity marks the focus of spatial attention in working memory: evidence from somatosensory event-related brain potentials.</gtr:title><gtr:parentPublicationTitle>The Journal of neuroscience : the official journal of the Society for Neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1ce947e1da708714fcf7119d6ad1293"><gtr:id>d1ce947e1da708714fcf7119d6ad1293</gtr:id><gtr:otherNames>Katus T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0270-6474</gtr:issn><gtr:outcomeId>5603caf7e3e784.16167518</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/L016400/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E1AC33C6-9927-41AC-B23B-2EED8F593588</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Experimental Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>