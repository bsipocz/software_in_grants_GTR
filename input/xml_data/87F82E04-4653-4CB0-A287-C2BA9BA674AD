<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Sch of Informatics</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/D831A1C6-3B74-45B8-9BA5-E17D177C59BD"><gtr:id>D831A1C6-3B74-45B8-9BA5-E17D177C59BD</gtr:id><gtr:firstName>Barbara</gtr:firstName><gtr:surname>Webb</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/14D92EF7-9F87-4457-8CFB-555B64E93142"><gtr:id>14D92EF7-9F87-4457-8CFB-555B64E93142</gtr:id><gtr:firstName>Michael</gtr:firstName><gtr:surname>Mangan</gtr:surname><gtr:orcidId>0000-0002-0293-8874</gtr:orcidId><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM008479%2F1"><gtr:id>87F82E04-4653-4CB0-A287-C2BA9BA674AD</gtr:id><gtr:title>Exploiting invisible cues for robot navigation in complex natural environments</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M008479/1</gtr:grantReference><gtr:abstractText>Outdoor navigation in natural environments remains a challenge for robotics. Recent breakthroughs in robot navigation have been dependent on specific sensor technologies, such as laser depth sensors and GPS, and advanced image processing. The ability of animals such as ants to navigate effectively without such power- and computation- hungry systems are a proof of principle that alternative cheaper approaches are viable. Ants also have specialised sensing, with a peripheral visual system that has evolved to be sensitive to crucial cues for navigation. Specifically, they make use of non-visible (to humans) light cues in the form of ultraviolet (UV) and polarised light detection. UV detection allows the important signal of the horizon shape against the sky to be easily distinguished. Polarised light detection provides an external compass cue of heading relative to the sun direction, even when only a small portion of the sky is visible. 
We propose to build a sensory system that gathers the full range of light cues available to the ant, in its natural ecological situation, and to analyse the information contained in this signal. We will also analyse how the specific sensor layout (ommatidia array), peripheral receptor characteristics, and the motor behaviour of the ant may contribute to extracting salient information. The data will form a test-bed for comparison of algorithmic and neural models of the processing that underlies the navigation capabilities of the ant. To date, these cues have been considered separately but we believe the navigational success of this system depends on the specific combination. For example, the directional information in the polarised sky may form an important part of visual memories; and UV information may contribute to disambiguation of the polarisation pattern and the robustness of this information under different cloud conditions. 
There has been a substantial increase in the last few years in research into insect neural pathways involved in processing these cues which has yet to be exploited in robot models. In particular there has been breakthrough work on the central brain mechanisms involved in decoding polarised light to obtain heading direction. There is also a rapidly increasing understanding of the circuits involved in learning, a key component of navigation capabilities. 
A key aspect of our method, in comparison to many bio-inspired projects, is that we do not propose to start by designing and manufacturing a sensor as similar as possible to the ant eye. Rather, we consider it essential to first understand what aspects of the eye are actually key to support navigation, and what variation from the exact biological design will conserve the desirable properties while simplifying the manufacture. The final outcome of this research will be a detailed design for a small scale and low power vision system for outdoor navigation. We hypothesis that we can leverage the miniaturisation of CCD cameras (which are normally sensitive to the upper range of UV, but have filters applied to remove it) to compose an omnidirectional compound eye where the filter characteristics of each ommatidia are appropriately tuned for wavelength or polarisation plane, in imitation of the ant eye. Beyond the direct application to robot navigation in natural environments, in tasks such as agriculture, search and rescue, and environmental clean-up, such a design has potential to be developed for other applications, including mobile devices, inspection and surveillance.</gtr:abstractText><gtr:potentialImpactText>UK and European governments have highlighted the robot industry as a key research and growth sector. Currently the market is still dominated by industrial robots confined to factories. Robotic cars have recently shown that mobile autonomy outdoors is possible yet are reliant on energy and computationally expensive sensing (e.g. 3D laser scanners). Development of a novel low-cost sensor for robot navigation is immediately applicable to this emerging market. Mid-term applications for the system include self-guided systems; robots in agriculture; exploration; environmental monitoring and clean-up operations. We also predict there will be potential applications other than robotics for miniaturised, omnidirectional UV and polarisation sensing.

The outcome of this proposal will be a prototype system with a design for manufacture, and patenting possibilities. We will fully investigate the options for commercialisation either through a university spin out or a licencing agreement with industrial partners. Primary market research shall be conducted by attending targeted industrial events at which companies likely to be interested in autonomous navigation (e.g. specialist robot developers, automotive manufacturers, defence contractors) are in attendance. Looking beyond the specific device we propose to develop here, we also hope to demonstrate to industry the real benefit of exploiting natural solutions for specialised sensing problems, to increase direct investment in future research in biomimetics through industrial-academic partnerships. The work undertaken here will provide training and experience in commercialisation to the junior project participants and may ultimately contribute to continued expansion and employment opportunities in this area. 

The key industries where we expect impact are areas of social importance and the approach can contribute in the long term to environmental sustainability, safety and energy reduction goals. In contrast to the conventional public image of robotics as aiming for human-like machines, the research is premised on building small cheap devices that may make a big difference. We look to involve the public and stakeholders in this emerging technology through a series of science communication activities, as detailed in the Pathways to Impact.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-02-28</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>558416</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Presentation at VIIHM &quot;Vision for movement&quot; workshop (Univ of Sussex)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>505224E8-77E0-497A-8184-74A72C0AF725</gtr:id><gtr:impact>A workshop intended to bring researchers with a common interest in the broad area of vision for movement together to describe methods specific to their fields which could be relevant to other areas of research. Outcomes included new collaborations, shared knowledge, and new funding avenues.</gtr:impact><gtr:outcomeId>58c9b581004059.31529495</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Study participants or study members</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Backwards ants press coverage</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F1E21934-BEA4-40C6-854F-3DA531E94733</gtr:id><gtr:impact>A press release was made describing the results of our Current Biology paper &amp;quot;How Ants Use Vision When Homing Backward&amp;quot;. The story was picked up internationally and appeared in the following venues: DAILY MAIL, DAILY MIRROR, THE HERALD, WESTERN DAILY PRESS, WIRED.CO.UK, AFP, EUROPE 1, L'EXPRESS.FR, FRANCE INFO, RTL TVI, LIBERATION, LE POINT (France) ATS, LE JOURNAL DU JURA (Switzerland) APS (Algeria) LA NOUVELLE GAZETTE, LA MEUSE, NORD &amp;Eacute;CLAIR, VRTNIEUWS.NET (Belgium) MONTEVIDEO PORTAL (Uruguay) DIARIO UNO (Argentina) EL FINANCIERO (Mexico) DIARIO MONTANES, LA VANGUARDIA, LA VOZ DIGITAL, ABC.ES, EUROPA PRESS, EL ESPECTADOR (Spain) IRISH DAILY MAIL (Ireland) PEOPLE'S DAILY ONLINE, PUEBLO EN LINEA, CHINA POST, CHINA TIMES, CAN, AFP, WORKER CHINA, KAN KAN NEWS (China) BERLINGSKE (Denmark) CHRISTIAN SCIENCE MONITOR, TERRA DAILY, DISCOVERY CHANNEL SCIENCE NEWS, TECH TIMES (USA) EXPATICA (Netherlands) ENCA, ARGUS, NEWS24, (S Africa) NEW VISION (Uganda) THE JAPAN TIMES (Japan) IRAN DAILY (Iran) PAGINA SIETE (Bolivia) WEBINDIA123.COM, INDO ASIAN NEWS SERVICE, SIASAT DAILY, NETINDIA123.COM, DNA INDIA, BUSINESS STANDARD (India) STRAITS TIMES (Singapore) YAM NEWS, EPOCH TIMES (Taiwan) CANADIAN BROADCASTING CORP (Canada) ZME SCIENCE (Romania).</gtr:impact><gtr:outcomeId>58c67c911e3e90.47810911</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Virtual ant experience</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>5E25392C-1176-4AB2-8FEA-5D03C66C9641</gtr:id><gtr:impact>Using a Oculus Rift headset, we produced a virtual reality game in which the participant experienced the ants point of view in attempting to navigate through the real environment of the ant (a virtual world created from a data set of laser scans in the ant's habitat). This was used at open days and local science festivals.</gtr:impact><gtr:outcomeId>58c67a897cb5f8.29799435</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>70388</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>AHDB PhD Studentship</gtr:description><gtr:end>2020-09-02</gtr:end><gtr:fundingOrg>Agriculture and Horticulture Development Board (AHDB)</gtr:fundingOrg><gtr:id>FBE0E2C7-1788-4770-993E-11FE63D3EBF4</gtr:id><gtr:outcomeId>58c9b1589aeea3.22332126</gtr:outcomeId><gtr:sector>Multiple</gtr:sector><gtr:start>2017-08-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have found it is possible to reconstruct accurate 3D models of the ant's natural environment, and to track ants in natural terrain. Both tools have been developed into general software solutions allowing meshing of natural scenes from laser scan data, and tracking of animals using a handheld camera under natural conditions. We have explored the potential navigational information available in the pattern of polarised light in the sky, and shown how the use of ultraviolet light in conjunction with a spherical harmonic encoding method can improve robot localisation based on sky segmentation under challenging conditions. We have carried out new behavioural experiments showing that ants are capable of navigating backwards and can transfer information between their terrestrial visual memory and celestial compass. We have also developed neural algorithms that can explain ant navigation behaviour and can be used on robots.</gtr:description><gtr:exploitationPathways>This grant is still running. The methods developed should be applicable to navigation technology</gtr:exploitationPathways><gtr:id>CA7744B3-5CD3-4EF5-AFEF-194966F29C36</gtr:id><gtr:outcomeId>56d73830ef80a7.37250292</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Electronics,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Reconstruction of an ant habitat (Myrmecia croslandi) based on 9 clouds captured in Canberra, Australia. The 8800sqm area features several complex Eucalyptus trees. This reconstruction was tuned to demonstrate compressibility features of Habitat3D: While strongly compressing the ground (&amp;gt;99% size reduction; file: ground.ply) all characteristics of the complex trees are preserved (file: vegetation.ply).</gtr:description><gtr:id>8D215D86-6422-4C8E-9BEB-B7FD1FCC7032</gtr:id><gtr:impact>This environment forms a cornerstone of attempts to assess how insects visually navigate their environments. Combined with additional tools research groups will be able to benchmark their models using this data.</gtr:impact><gtr:outcomeId>58c9b35ea0cbb9.06323793</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>3D Mesh of ant habitat in Canberra, Australia</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.insectvision.org/3d-reconstruction-tools/habitat3d</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Reconstruction of a natural foraging environment of desert ants (Cataglyphis velox) based on 56 laser scans and covering an 1018sqm area. This highly cluttered terrain features hundreds of individual plants and the reconstruction was tuned towards overall accuracy: Both, the ground and vegetation reconstruction feature millimetre accuracy while reducing the memory load to 1.5%. The reconstruction is divided into vegetation (inner_vegetation.ply and outer_vegetation.ply) and ground (inner_ground.ply and outer_ground.ply).</gtr:description><gtr:id>31E34958-BA9F-430F-B7DE-A19E432FFAD9</gtr:id><gtr:impact>This environment forms a cornerstone of attempts to assess how insects visually navigate their environments. Combined with additional tools research groups will be able to benchmark their models using this data.</gtr:impact><gtr:outcomeId>58c9b314657228.86744009</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>3D mesh of ant habitat in Seville, Spain</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://www.insectvision.org/3d-reconstruction-tools/habitat3d</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Habitat3D is an open source cross-platform tool to generate photorealistic meshes from point clouds of natural outdoor scenes. All necessary processing steps (filtering, segmentation, feature extraction, meshing, etc.) as well as recipe-based generation of pipelines are incorporated in a GUI-based framework. Habitat3D is written in C++ and requires Qt, PCL, VTK and BOOST.</gtr:description><gtr:id>8699CCFD-03BE-4136-B07A-949AFD3FA9DF</gtr:id><gtr:impact>The tool has been used to recreate ant environments from our own and others field studies</gtr:impact><gtr:outcomeId>58c6788d4d3543.38604454</gtr:outcomeId><gtr:title>Habitat3D</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.insectvision.org/3d-reconstruction-tools/habitat3d</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>1F7BD871-408B-400D-964E-083FCB45155F</gtr:id><gtr:title>Software to convert terrestrial LiDAR scans of natural environments into photorealistic meshes</gtr:title><gtr:parentPublicationTitle>Environmental Modelling &amp; Software</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/46f574b7892529e531a482ae95e76acf"><gtr:id>46f574b7892529e531a482ae95e76acf</gtr:id><gtr:otherNames>Risse B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a748d620af029.91560502</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19AAD1EB-AB6F-4E35-9FC6-65904ECB031C</gtr:id><gtr:title>Using an Insect Mushroom Body Circuit to Encode Route Memory in Complex Natural Environments.</gtr:title><gtr:parentPublicationTitle>PLoS computational biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c31632754e2ef45476c3569e1a84adc6"><gtr:id>c31632754e2ef45476c3569e1a84adc6</gtr:id><gtr:otherNames>Ardin P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1553-734X</gtr:issn><gtr:outcomeId>56d48f76e497d2.03093062</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AA160824-D238-401D-9AB9-14D3C2031807</gtr:id><gtr:title>How Ants Use Vision When Homing Backward.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5de30e6d1092936f748aaef47d5d6436"><gtr:id>5de30e6d1092936f748aaef47d5d6436</gtr:id><gtr:otherNames>Schwarz S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>58c675a42e6640.78207976</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>72AFAD68-DF9D-435C-AB2D-50157088753B</gtr:id><gtr:title>Skyline-based localisation for aggressively manoeuvring robots using UV sensors and spherical harmonics</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d482a8c9459c4f6e0602cb75b20782d"><gtr:id>5d482a8c9459c4f6e0602cb75b20782d</gtr:id><gtr:otherNames>Stone T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c675a40bde71.91879091</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>18998836-6C96-470E-8148-5AF284843962</gtr:id><gtr:title>Neural mechanisms of insect navigation.</gtr:title><gtr:parentPublicationTitle>Current opinion in insect science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aa44d5355dd7da9822557b5a9233ed59"><gtr:id>aa44d5355dd7da9822557b5a9233ed59</gtr:id><gtr:otherNames>Webb B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5881fb833ee711.77524783</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3CA94DC5-6547-465E-9B22-1E8CF84BDC38</gtr:id><gtr:title>Ant Homing Ability Is Not Diminished When Traveling Backwards.</gtr:title><gtr:parentPublicationTitle>Frontiers in behavioral neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c5120af8541bcfcd7b3d6f1549455a5"><gtr:id>9c5120af8541bcfcd7b3d6f1549455a5</gtr:id><gtr:otherNames>Ardin PB</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1662-5153</gtr:issn><gtr:outcomeId>58c675a3ca1974.94079308</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>56B82362-2BF2-445E-92F6-38F8467E5364</gtr:id><gtr:title>An Anatomically Constrained Model for Path Integration in the Bee Brain.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d482a8c9459c4f6e0602cb75b20782d"><gtr:id>5d482a8c9459c4f6e0602cb75b20782d</gtr:id><gtr:otherNames>Stone T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>5aa0214ae14077.81483553</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M008479/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>