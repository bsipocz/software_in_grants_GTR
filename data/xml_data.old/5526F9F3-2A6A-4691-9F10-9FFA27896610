<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/A0A585E0-6B0D-4643-A3A6-47943B4CBFEF"><gtr:id>A0A585E0-6B0D-4643-A3A6-47943B4CBFEF</gtr:id><gtr:name>University of Liverpool</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line2>Abercromby Square</gtr:line2><gtr:line4>Liverpool</gtr:line4><gtr:postCode>L69 3BX</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/514C4B87-4B6D-49C2-A9BC-A8E72C78E198"><gtr:id>514C4B87-4B6D-49C2-A9BC-A8E72C78E198</gtr:id><gtr:name>National Nuclear Laboratory</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/FEFAECEB-34E7-48D1-B83E-B7AD50E4F4D2"><gtr:id>FEFAECEB-34E7-48D1-B83E-B7AD50E4F4D2</gtr:id><gtr:name>Sellafield Ltd</gtr:name><gtr:address><gtr:line1>Sellafield Site</gtr:line1><gtr:line4>Seascale</gtr:line4><gtr:line5>Cumbria</gtr:line5><gtr:postCode>CA20 1PG</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Surrey Space Centre Academic</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/DC2E9FE0-77C5-4D65-8327-E8D9FD0180C4"><gtr:id>DC2E9FE0-77C5-4D65-8327-E8D9FD0180C4</gtr:id><gtr:name>UK Space Agency</gtr:name><gtr:address><gtr:line1>Polaris House</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:line4>Swindon</gtr:line4><gtr:line5>Wiltshire</gtr:line5><gtr:postCode>SN2 1SZ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/897F6C63-96BF-4E36-9C2D-42EE98F18363"><gtr:id>897F6C63-96BF-4E36-9C2D-42EE98F18363</gtr:id><gtr:name>Network Rail Ltd</gtr:name><gtr:address><gtr:line1>Kings Place</gtr:line1><gtr:line2>90 York Way</gtr:line2><gtr:postCode>N1 9AG</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EF81E303-6E50-4348-9599-52B81DED7BC3"><gtr:id>EF81E303-6E50-4348-9599-52B81DED7BC3</gtr:id><gtr:name>BAE Systems</gtr:name><gtr:address><gtr:line1>Chelmsford Office and Technology Park</gtr:line1><gtr:line2>West Hanningfield Road</gtr:line2><gtr:line3>Great Baddow</gtr:line3><gtr:line4>Chelmsford</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CM2 8HN</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A701B57A-A742-4CA4-8D62-AEC0D19CFDF4"><gtr:id>A701B57A-A742-4CA4-8D62-AEC0D19CFDF4</gtr:id><gtr:name>Schlumberger Cambridge Research Ltd</gtr:name><gtr:address><gtr:line1>High Cross</gtr:line1><gtr:line2>Madingley Road</gtr:line2><gtr:postCode>CB3 0EL</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FEFAECEB-34E7-48D1-B83E-B7AD50E4F4D2"><gtr:id>FEFAECEB-34E7-48D1-B83E-B7AD50E4F4D2</gtr:id><gtr:name>Sellafield Ltd</gtr:name><gtr:address><gtr:line1>Sellafield Site</gtr:line1><gtr:line4>Seascale</gtr:line4><gtr:line5>Cumbria</gtr:line5><gtr:postCode>CA20 1PG</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A0A585E0-6B0D-4643-A3A6-47943B4CBFEF"><gtr:id>A0A585E0-6B0D-4643-A3A6-47943B4CBFEF</gtr:id><gtr:name>University of Liverpool</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line2>Abercromby Square</gtr:line2><gtr:line4>Liverpool</gtr:line4><gtr:postCode>L69 3BX</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/514C4B87-4B6D-49C2-A9BC-A8E72C78E198"><gtr:id>514C4B87-4B6D-49C2-A9BC-A8E72C78E198</gtr:id><gtr:name>National Nuclear Laboratory</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/962A2BED-07CA-42E1-9953-A1CC9EF732F8"><gtr:id>962A2BED-07CA-42E1-9953-A1CC9EF732F8</gtr:id><gtr:firstName>Yang</gtr:firstName><gtr:surname>Gao</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ011916%2F1"><gtr:id>5526F9F3-2A6A-4691-9F10-9FFA27896610</gtr:id><gtr:title>Reconfigurable Autonomy</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/J011916/1</gtr:grantReference><gtr:abstractText>As computational and engineering applications become more sophisticated, the need for autonomous systems that can act intelligently without direct human intervention increases. Yet the autonomous control at the heart of many such systems is often ad-hoc and opaque. Since the cost of failure in critical systems is high, a more reliable, understandable and consistent approach is needed. Thus, in this project we aim to provide a rational agent architecture that controls autonomous decision-making, is re-usable and generic, and can be configured for many different autonomous platforms. In partnership with the industrial collaborators we aim to show how such &amp;quot;reconfigurable autonomy&amp;quot; can be achieved in relevant applications.</gtr:abstractText><gtr:fund><gtr:end>2016-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2012-06-29</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>405962</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Sellafield Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Machine vision system for nuclear waste</gtr:description><gtr:id>CBC2CCF4-610D-4D4B-9643-5C2F42DC38E8</gtr:id><gtr:impact>-Visual Classification of Waste Material for Nuclear Decommissioning, journal publication in Robotics and Autonomous Systems.
-A spin-off work has been identified by Sellaffield to use the machine vision technique for bubble counting in nuclear waste pond. An industrial contract is currently under formalization.</gtr:impact><gtr:outcomeId>544f941c6f1669.70644792-1</gtr:outcomeId><gtr:partnerContribution>The National Nuclear Laboratory and Sellafield Ltd provided the nuclear waste simulants, which were extensively used in the experiments as well as gave access to some of their decommissioning facilities in order to gain first-hand knowledge of the requirements and engineering as well as environmental constraints that played a crucial role in the design of the proposed machine vision system.</gtr:partnerContribution><gtr:piContribution>Redundant and non-operational buildings at nuclear sites go through the process of 'decommissioning', involving decontamination of nuclear waste material and demolition of physical infrastructure. One challenging problem currently faced by the nuclear industry is the segregation of redundant waste material into a choice of 'post-processes' based upon the nature and extent of its radioactivity that may pose a serious threat to the environment. Following an initial inspection, waste materials are subjected to treatment, disruption and consigned to various types of export containers. To date, the process of objects (waste) classification is performed manually. In order to automate this process, robotic platforms can be deployed that utilise robust and fast vision systems for visual classification of nuclear waste material. We proposed a novel solution incorporating a machine vision system for autonomous identification of waste material from decommissioned nuclear plants. Using nuclear waste simulants, off-line exhaustive 'proof-of-concept' quantitative assessment of the proposed technique was performed using experimental datasets. The system was also implemented in-situ in real time with plans for future improvement using additional extereoceptive sensors for higher fidelity in terms of detection and recognition.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>National Nuclear Laboratory</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Machine vision system for nuclear waste</gtr:description><gtr:id>FBA480E1-CA04-4E6A-9267-D7D8CDCAE047</gtr:id><gtr:impact>-Visual Classification of Waste Material for Nuclear Decommissioning, journal publication in Robotics and Autonomous Systems.
-A spin-off work has been identified by Sellaffield to use the machine vision technique for bubble counting in nuclear waste pond. An industrial contract is currently under formalization.</gtr:impact><gtr:outcomeId>544f941c6f1669.70644792-2</gtr:outcomeId><gtr:partnerContribution>The National Nuclear Laboratory and Sellafield Ltd provided the nuclear waste simulants, which were extensively used in the experiments as well as gave access to some of their decommissioning facilities in order to gain first-hand knowledge of the requirements and engineering as well as environmental constraints that played a crucial role in the design of the proposed machine vision system.</gtr:partnerContribution><gtr:piContribution>Redundant and non-operational buildings at nuclear sites go through the process of 'decommissioning', involving decontamination of nuclear waste material and demolition of physical infrastructure. One challenging problem currently faced by the nuclear industry is the segregation of redundant waste material into a choice of 'post-processes' based upon the nature and extent of its radioactivity that may pose a serious threat to the environment. Following an initial inspection, waste materials are subjected to treatment, disruption and consigned to various types of export containers. To date, the process of objects (waste) classification is performed manually. In order to automate this process, robotic platforms can be deployed that utilise robust and fast vision systems for visual classification of nuclear waste material. We proposed a novel solution incorporating a machine vision system for autonomous identification of waste material from decommissioned nuclear plants. Using nuclear waste simulants, off-line exhaustive 'proof-of-concept' quantitative assessment of the proposed technique was performed using experimental datasets. The system was also implemented in-situ in real time with plans for future improvement using additional extereoceptive sensors for higher fidelity in terms of detection and recognition.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>National Nuclear Laboratory</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>3D Autonomous nuclear waste management</gtr:description><gtr:id>479D8714-0A96-4E65-AA0B-19EBFEC42BFF</gtr:id><gtr:impact>Full end-to-end lab-based demo at University of Sheffield for industrial partners.
A magazine article &amp;quot;Autonomous Nuclear Waste Management&amp;quot; ready for submission to IEEE Robotics and Automation Magazine.</gtr:impact><gtr:outcomeId>56b36267d0f7d0.25210814-1</gtr:outcomeId><gtr:partnerContribution>The National Nuclear Laboratory and Sellafield Ltd provided invaluable assistance and access to some of their decommissioning facilities in order to gain first-hand knowledge of the requirements and engineering/environmental constraints that played a crucial role in the design of the proposed system. 
University of Sheffield contributed towards the robot control system design
University of Liverpool contributed towards the rational agent system</gtr:partnerContribution><gtr:piContribution>Redundant and nonoperational buildings at nuclear sites are decommissioned over a period of time. The process involves demolition of physical infrastructure which results in large quantity of residual waste material. The resulting waste materials are packed into import containers to be delivered for post-processing either as sealed canisters or an unpackaged assortment of miscellaneous objects. At present this does not happen within the United Kingdom, but Sellafield Ltd. and National Nuclear Laboratory are developing a process for future operation so that upon an initial inspection, imported waste materials undergo two different stages of post-processing before being packed into export containers, namely sort and segregate or sort and disrupt. The sort and segregate facility will remotely sort the waste on a sorting table and place items in an appropriate
container, depending upon waste type and condition for downstream processing. If any of the waste is in sealed containers, it will require disruption, before being placed in the appropriate waste container, which is known as sort and disrupt. The process is to be performed through tele-operation. This work focuses on the design, development, and demonstration of a reconfigurable rational agent-based robotic system that can
potentially automate these processes. The proposed system will be demonstrated using a downsize, lab-based setup incorporating a small-scale robotic arm, a single time of flight camera, and high-level rational agent-based decision making and control framework. 
Surrey specifically designed and developed the TOF camera-based 3D object detection system.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Sellafield Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>3D Autonomous nuclear waste management</gtr:description><gtr:id>DEA4CF56-480B-41CB-9DB4-5D5A8BF7004A</gtr:id><gtr:impact>Full end-to-end lab-based demo at University of Sheffield for industrial partners.
A magazine article &amp;quot;Autonomous Nuclear Waste Management&amp;quot; ready for submission to IEEE Robotics and Automation Magazine.</gtr:impact><gtr:outcomeId>56b36267d0f7d0.25210814-2</gtr:outcomeId><gtr:partnerContribution>The National Nuclear Laboratory and Sellafield Ltd provided invaluable assistance and access to some of their decommissioning facilities in order to gain first-hand knowledge of the requirements and engineering/environmental constraints that played a crucial role in the design of the proposed system. 
University of Sheffield contributed towards the robot control system design
University of Liverpool contributed towards the rational agent system</gtr:partnerContribution><gtr:piContribution>Redundant and nonoperational buildings at nuclear sites are decommissioned over a period of time. The process involves demolition of physical infrastructure which results in large quantity of residual waste material. The resulting waste materials are packed into import containers to be delivered for post-processing either as sealed canisters or an unpackaged assortment of miscellaneous objects. At present this does not happen within the United Kingdom, but Sellafield Ltd. and National Nuclear Laboratory are developing a process for future operation so that upon an initial inspection, imported waste materials undergo two different stages of post-processing before being packed into export containers, namely sort and segregate or sort and disrupt. The sort and segregate facility will remotely sort the waste on a sorting table and place items in an appropriate
container, depending upon waste type and condition for downstream processing. If any of the waste is in sealed containers, it will require disruption, before being placed in the appropriate waste container, which is known as sort and disrupt. The process is to be performed through tele-operation. This work focuses on the design, development, and demonstration of a reconfigurable rational agent-based robotic system that can
potentially automate these processes. The proposed system will be demonstrated using a downsize, lab-based setup incorporating a small-scale robotic arm, a single time of flight camera, and high-level rational agent-based decision making and control framework. 
Surrey specifically designed and developed the TOF camera-based 3D object detection system.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Liverpool</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>3D Autonomous nuclear waste management</gtr:description><gtr:id>B025C475-F052-461F-B03D-6FB723E941B0</gtr:id><gtr:impact>Full end-to-end lab-based demo at University of Sheffield for industrial partners.
A magazine article &amp;quot;Autonomous Nuclear Waste Management&amp;quot; ready for submission to IEEE Robotics and Automation Magazine.</gtr:impact><gtr:outcomeId>56b36267d0f7d0.25210814-4</gtr:outcomeId><gtr:partnerContribution>The National Nuclear Laboratory and Sellafield Ltd provided invaluable assistance and access to some of their decommissioning facilities in order to gain first-hand knowledge of the requirements and engineering/environmental constraints that played a crucial role in the design of the proposed system. 
University of Sheffield contributed towards the robot control system design
University of Liverpool contributed towards the rational agent system</gtr:partnerContribution><gtr:piContribution>Redundant and nonoperational buildings at nuclear sites are decommissioned over a period of time. The process involves demolition of physical infrastructure which results in large quantity of residual waste material. The resulting waste materials are packed into import containers to be delivered for post-processing either as sealed canisters or an unpackaged assortment of miscellaneous objects. At present this does not happen within the United Kingdom, but Sellafield Ltd. and National Nuclear Laboratory are developing a process for future operation so that upon an initial inspection, imported waste materials undergo two different stages of post-processing before being packed into export containers, namely sort and segregate or sort and disrupt. The sort and segregate facility will remotely sort the waste on a sorting table and place items in an appropriate
container, depending upon waste type and condition for downstream processing. If any of the waste is in sealed containers, it will require disruption, before being placed in the appropriate waste container, which is known as sort and disrupt. The process is to be performed through tele-operation. This work focuses on the design, development, and demonstration of a reconfigurable rational agent-based robotic system that can
potentially automate these processes. The proposed system will be demonstrated using a downsize, lab-based setup incorporating a small-scale robotic arm, a single time of flight camera, and high-level rational agent-based decision making and control framework. 
Surrey specifically designed and developed the TOF camera-based 3D object detection system.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Sheffield</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>3D Autonomous nuclear waste management</gtr:description><gtr:id>74833A15-644F-4986-B6FC-F5DF46D1AFC5</gtr:id><gtr:impact>Full end-to-end lab-based demo at University of Sheffield for industrial partners.
A magazine article &amp;quot;Autonomous Nuclear Waste Management&amp;quot; ready for submission to IEEE Robotics and Automation Magazine.</gtr:impact><gtr:outcomeId>56b36267d0f7d0.25210814-3</gtr:outcomeId><gtr:partnerContribution>The National Nuclear Laboratory and Sellafield Ltd provided invaluable assistance and access to some of their decommissioning facilities in order to gain first-hand knowledge of the requirements and engineering/environmental constraints that played a crucial role in the design of the proposed system. 
University of Sheffield contributed towards the robot control system design
University of Liverpool contributed towards the rational agent system</gtr:partnerContribution><gtr:piContribution>Redundant and nonoperational buildings at nuclear sites are decommissioned over a period of time. The process involves demolition of physical infrastructure which results in large quantity of residual waste material. The resulting waste materials are packed into import containers to be delivered for post-processing either as sealed canisters or an unpackaged assortment of miscellaneous objects. At present this does not happen within the United Kingdom, but Sellafield Ltd. and National Nuclear Laboratory are developing a process for future operation so that upon an initial inspection, imported waste materials undergo two different stages of post-processing before being packed into export containers, namely sort and segregate or sort and disrupt. The sort and segregate facility will remotely sort the waste on a sorting table and place items in an appropriate
container, depending upon waste type and condition for downstream processing. If any of the waste is in sealed containers, it will require disruption, before being placed in the appropriate waste container, which is known as sort and disrupt. The process is to be performed through tele-operation. This work focuses on the design, development, and demonstration of a reconfigurable rational agent-based robotic system that can
potentially automate these processes. The proposed system will be demonstrated using a downsize, lab-based setup incorporating a small-scale robotic arm, a single time of flight camera, and high-level rational agent-based decision making and control framework. 
Surrey specifically designed and developed the TOF camera-based 3D object detection system.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>News release on industrial robotics research</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>F2B8D8FE-8012-4090-BA7C-769B4517C522</gtr:id><gtr:impact>News release on industrial robotics research</gtr:impact><gtr:outcomeId>58b98ee5aa8511.00624420</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.surrey.ac.uk/mediacentre/press/2017/leading-robotics-research-aims-address-uk%E2%80%99s-legacy-nuclear-decommissioning</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Short film for University of Surrey's Queen's Anniversary award</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>AE185C67-3B88-4099-800E-D2BDFDC6B9B3</gtr:id><gtr:impact>Our research work has been featured in the short movie commissioned by Royal Anniversary Trust to memorize the Queen's Anniversary Award received by the Surrey Space Centre/University of Surrey in 1996 in space industry teaching and research.</gtr:impact><gtr:outcomeId>56e04d410c88c7.88971201</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://royalanniversarytrust.org.uk/short-films</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Featured article on Vantage Point</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>21E9D956-151D-4BD6-BB78-33B5DBC69C8B</gtr:id><gtr:impact>This was a featured article inVantage Point local magzine which talks about our research and its impact to advance technologies that matter to various industrial sectors.

Requests from several local and regional professional and educational bodies for guest lectures/talks.</gtr:impact><gtr:outcomeId>54579f888f0774.57509417</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.vantagepointmag.co.uk</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>News article on top 10 robots</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A1240970-E2D1-4449-8E00-AC3148F4696B</gtr:id><gtr:impact>Our robot is named one of the top 10 robots at the IET International Robotics Showcase within the UK Robotics Week 2016.</gtr:impact><gtr:outcomeId>58b995a161d2b7.97588284</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://eandt.theiet.org/content/articles/2016/06/uk-?robotics-week/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Daily Mail news article</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FFC487AD-5F02-4EAF-8C98-8D79188EDFCC</gtr:id><gtr:impact>Featured article on &amp;quot;One giant leap for ROBOTS: Machines that walk, swim and climb will replace humans on future space mission&amp;quot;, Daily Mail, UK, July 2016.</gtr:impact><gtr:outcomeId>58b991bb0b1a43.88315691</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.dailymail.co.uk/sciencetech/article-3706808/One-giant-leap-ROBOTS-Machines-walk-swim-climb-replace-humans-future-space-missions.html</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>RAEng publication</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5BCAF446-F958-46E9-8B69-D49D4034A4E7</gtr:id><gtr:impact>Featured article on Engineering for a Successful Nation, Royal Academy of Engineering, March 2015</gtr:impact><gtr:outcomeId>58b9912c96a545.55852199</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.raeng.org.uk/publications/reports/engineering-for-a-successful-nation</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>ITV news report</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>0DFAFE0D-436F-48B7-B207-8843E9ADB9CF</gtr:id><gtr:impact>ITV news report covered our group's field trials at the West Wittering beach, where we tested our prototype systems resulting from the research work carried over the course of this project on autonomous field vehicles.

Following an initial coverage of these field trials, there were further interview requests on our work from other media groups.</gtr:impact><gtr:outcomeId>5457a0c0df06d1.49096613</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.youtube.com/watch?v=F8RB3dsTAXQ</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>News article on UK-Robotics Network</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A468366F-8811-47CB-8228-075AFEEA3735</gtr:id><gtr:impact>A news letter on robotics and autonomous system research following Tim Peake's trip to the International Space Station.</gtr:impact><gtr:outcomeId>56e049de3a6819.70100058</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.uk-ras.org</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TV interview on robotics</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>528F2D2E-5BFA-4FF5-9CB6-D0DB17519FE6</gtr:id><gtr:impact>ITV interview on field trials at West Wittering</gtr:impact><gtr:outcomeId>58b9904cdd4559.24872287</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.itv.com/news/meridian/search/?q=space+rovers</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>IET Public Lecture</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>A2428B4E-1D75-48BF-AA19-9FC8513471E3</gtr:id><gtr:impact>IET public lecture on robotics research</gtr:impact><gtr:outcomeId>58b990b3256a35.50910968</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://iettv.theiet.org/technology/computing/17658.cfm</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>190000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>To consolidate, to collaborate and to challenge</gtr:description><gtr:end>2017-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:id>60A1DCA4-F02D-48D9-9DF0-1008934A6AD1</gtr:id><gtr:outcomeId>58b98ace2c3528.69532128</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>20000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Impact Accerlation Account</gtr:description><gtr:end>2017-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/K503939/1</gtr:fundingRef><gtr:id>03E83E74-D0F6-4746-BE31-801E3B6F1B2F</gtr:id><gtr:outcomeId>58b977f13f3234.06683170</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>150000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>AgriBot: autonomous robot for precision farming</gtr:description><gtr:end>2017-09-02</gtr:end><gtr:fundingOrg>Science and Technologies Facilities Council (STFC)</gtr:fundingOrg><gtr:id>550558D7-B775-4F61-846F-89F8F3241F35</gtr:id><gtr:outcomeId>56e0545855c016.28256732</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>100000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Technology Development for Accurate Planetary Robotic Arm</gtr:description><gtr:end>2016-06-02</gtr:end><gtr:fundingOrg>UKSA</gtr:fundingOrg><gtr:id>F3E7F38B-6179-4D76-9C3D-A85392D10EE4</gtr:id><gtr:outcomeId>56b372d3dab4a4.89195046</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>30000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Automated Waste Management</gtr:description><gtr:end>2017-08-02</gtr:end><gtr:fundingOrg>Sellafield Ltd</gtr:fundingOrg><gtr:id>A0D3475E-311F-4258-8AE6-246A6695CCBD</gtr:id><gtr:outcomeId>58b97cf1a9e240.86963842</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2016-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>90000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>PhD scholarship on robotic data fusion</gtr:description><gtr:end>2019-05-02</gtr:end><gtr:fundingOrg>Nuclear Decomissioning Authority NDA</gtr:fundingOrg><gtr:id>CCB09C15-2F05-4FA6-B38B-B4F332D10940</gtr:id><gtr:outcomeId>56e04705d67f95.06577300</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-07-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>85000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>PhD sponsorship on semi-autonomous operation of nuclear plant</gtr:description><gtr:end>2017-12-02</gtr:end><gtr:fundingOrg>Sellafield Ltd</gtr:fundingOrg><gtr:id>D25CD1B2-885B-497A-BC39-7167F63D6EAB</gtr:id><gtr:outcomeId>54579d532511b2.59013128</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Up to date, the project results have been applied to primarily industrial problems in the nuclear and space sectors. The findings are well published in books, high impact journals and conference proceedings. Software prototypes and demonstrators have been developed for Sellafield to improve productivity of their existing waste management by using robotics and autonomous systems. Further details are reported in the breakdown sections.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>1A01F1AD-7DD6-4427-B1CC-3098AE953F0A</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>544e9579170908.24706357</gtr:outcomeId><gtr:sector>Aerospace, Defence and Marine,Agriculture, Food and Drink,Energy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The research work carried out to date:

1) Used state-of-the art object detection and tracking paradigms that have not received substantial attention in spacecraft navigation applications in the past. More importantly we focused on computer vision techniques that can be as generic as possible (in contrary to engineered models) and can efficiently be integrated into an in-situ reconfigurable architecture. This work specifically focused on saliency based visual modelling techniques that can effectively detect object (e.g., rocks) on a planetary surface. In order to help us develop a sound foundation for visual saliency modelling techniques specifically focused towards autonomous spacecraft navigation (on homogenous planetary surfaces) we started off by carrying out extensive quantitative and qualitative analysis of some of the latest visual saliency models already well known across the computer vision research community. These evaluations were performed on simulated, lab-based and real world recorded datasets that quite closely replicate remote planetary environments. 

This work resulted in the following research findings:

- The use of unsupervised (bottom-up) visual saliency models can indeed prove to be useful in planetary exploration rovers (this may as well be extended to satellite imagery).

- With a remarkable level of distinction among different saliency modelling techniques in terms of their performance measures we are able to differentiate, recognise and understand the crucial sensory excitation parameters that can work better on a homogenous planetary environment.

- This ultimately lead towards a saliency model that is effectively tuned for (the more challenging) homogenous planetary environments

2) Developed a machine vision system for use in nuclear decommissioning process, more importantly for automated classification of nuclear waste material.

- System has real-time processing capabilities
- Has proven to be robust in visual object detection and recognition

3) Developed a lab-based machine vision system potentially for use in nuclear decommissioning process, more specifically for the sort and disrupt problem in Box Encapsulation Plants within the National Nuclear Laboratory:
- The system uses a TOF camera for recording 3D point clouds of the visual scene
- A 3D visual object detection algorithm processes the point clouds to recognise cylinders, estimating pose and geometric properties
- System has been tested in laboratory with a scaled down setup of the BEP at NNL</gtr:description><gtr:exploitationPathways>The current research work is specifically focused towards more generic machine vision and learning algorithms (being adaptive and robust to novel operating environments) that can potentially lead to a wide variety of applications (but not limited to):

- SAR missions

- International maritime services

- Defence: aerial/ground surveillance

- Medical (such as robotic key-hole surgeries)

- Industrial manufacturing and automation

- Automated nuclear waste disposal and management

- Robotic agricultural plants The current work progress will help us to explore novel dimensions in space robotics in future; for instance autonomous systems that are used for planetary exploration missions can be considered as distributed, cognitive systems (having adaptive, anticipatory and goal directed behaviour) rather than isolated engineered system. Such systems would of course require the use of novel (previously unexplored) unconventional methods in space, such as (but are not limited to): computational intelligence, cognitive vision, cognitive machine learning techniques.</gtr:exploitationPathways><gtr:id>8C59B255-0E00-49C0-B570-3CB32C823ECD</gtr:id><gtr:outcomeId>r-469302833.5173641774811e4</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Agriculture, Food and Drink,Digital/Communication/Information Technologies (including Software),Electronics,Energy,Healthcare,Leisure Activities, including Sports, Recreation and Tourism,Manufacturing, including Industrial Biotechology,Culture, Heritage, Museums and Collections,Retail,Security and Diplomacy,Transport</gtr:sector></gtr:sectors><gtr:url>http://www.surrey.ac.uk/ssc/research/star-lab/enabling_technologies/index.htm/index.htm</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset was generated using real-world nuclear waste simulants provided by the National Nuclear Laboratory and Sellafield Ltd. UK. Images recorded using the perspective projections of each simulant were distinctly placed on a sorting table for multiple values of ? and f. In order to maximise the disparity in object orientation within the training/test sample space, a secondary subsampled dataset is generated from the original set of projections by rotating each image over the angular domain of ?. Each rotated image is further scaled by reducing or increasing by {1.1, ..., 1.5} &amp;times; original image, which creates a stack of successively smaller and larger images to simulate size variations. Together, they constitute a superset of 86400 subsampled images of training and test data with ground truth annotations from which, in principle, maximally discriminative mappings of 2-D shape projections to distinct class labels can be derived. This dataset further provides the ability to train and test the system in terms of varying object positions and orientations on the sorting table. Collection of data comprising objects at different positions, orientations and locations on the sorting table helps in training and testing the algorithm with anomalies that may arise due to changes in the perspective camera projections of the objects and the resulting effects in their binary silhouettes.</gtr:description><gtr:id>B8D2AC34-C3EE-427A-9EF3-B9780B4B69E3</gtr:id><gtr:impact>Journal article: Affan Shaukat, Yang Gao, Jeffrey A. Kuo, Bob A. Bowen and Paul E. Mort 
&amp;quot;Visual Classification of Waste Material for Nuclear Decommissioning,&amp;quot; Robotics and Autonomous Systems, Volume 75, Part B, January 2016, Pages 365-378, ISSN 0921-8890, doi: http://dx.doi.org/10.1016/j.robot.2015.09.005.</gtr:impact><gtr:outcomeId>56b3763685e577.30262694</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Binary Images of Real-World Nuclear Waste Simulants for Visual Object Recognition</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://epubs.surrey.ac.uk/808553/</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset was generated by a teleoperated rover, with a monocular web camera, and GPS receiver. Images, teleoperation commands and GPS positions were recorded as the rover moved.</gtr:description><gtr:id>B76B8EC8-30E0-4C08-B9D0-1C9992116DA9</gtr:id><gtr:impact>Conference paper: Affan Shaukat, Abhinav Bajpai, and Yang Gao, &amp;quot;Reconfigurable SLAM Utilising Fuzzy Reasoning&amp;quot;, In 13th Symposium on Advanced Space Technologies in Robotics and Automation, 11-13 May, 2015, ESA/ESTEC, Noordwijk, the Netherlands.

Journal article: Abhinav Bajpai, Guy Burroughes, Affan Shaukat and Yang Gao, &amp;quot;Planetary Monocular SLAM,&amp;quot; Journal of Field Robotics (2015), DOI: 10.1002/rob.21608</gtr:impact><gtr:outcomeId>56b377d3cd6a96.99874381</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>West Wittering Beach Data For Evaluation Of Planetary Monocular Vision-based Localisation, Navigation and Mapping Techniques</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://epubs.surrey.ac.uk/807574/</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The main objective of this visual modelling paradigm is to extract conspicuous features from a natural scene for autonomous environmental modelling as well as detect, track and identify important landmarks, such as rocks on planetary surfaces for scientific and engineering research purposes. The model utilises the natural ability of biological vision systems in scene characterisation and identification.</gtr:description><gtr:id>56B84E74-8DBF-4F5A-B455-FC3BF7FA3757</gtr:id><gtr:impact>The new model can characterise visually salient objects in terms of mathematical descriptors for use in machine visions systems.</gtr:impact><gtr:outcomeId>5457a4423b3271.84872519</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Visual saliency model</gtr:title><gtr:type>Computer model/algorithm</gtr:type></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The dataset was generated using Planet and Asteroid Natural scene Generation Utility (PANGU), (developed at the University of Dundee) and image capture software from the PM-SLAM. PANGU simulates planetary environments using parameters such as the levelness of the terrain and the number, size and distribution of craters and boulders. The PM-SLAM software has the ability to place a virtual camera at a given location, field of view and record images from a PANGU simulation.</gtr:description><gtr:id>82EEA3EF-3551-4F79-847D-7038241BAC46</gtr:id><gtr:impact>Conference paper: Affan Shaukat, Abhinav Bajpai, and Yang Gao, &amp;quot;Reconfigurable SLAM Utilising Fuzzy Reasoning&amp;quot;, In 13th Symposium on Advanced Space Technologies in Robotics and Automation, 11-13 May, 2015, ESA/ESTEC, Noordwijk, the Netherlands.</gtr:impact><gtr:outcomeId>56b377085f3e76.45025425</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Simulated Data For Evaluation Of Planetary Monocular Vision-based Localisation, Navigation and Mapping Techniques</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://epubs.surrey.ac.uk/807574/</gtr:url></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>A collection of image datasets have been created for testing various state-of-the-art visual saliency models. They are have been annotated by experts in this field and well documented.</gtr:description><gtr:id>C231D3FB-8C9E-47FE-8A3C-7EBB224DA0D9</gtr:id><gtr:impact>We haven't shared the databases with other research groups to date, however there are future plans to do so, as we have received some requests from people related to other research groups. We also plan to write and publish a paper describing these databases in order to render them as open source bench mark datasets.</gtr:impact><gtr:outcomeId>5457a26cab8be7.80249737</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Annotated image database</gtr:title><gtr:type>Database/Collection of data</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This is a prototype of autonomous software for robotic systems with ontology-based architecture and reconfigurability.</gtr:description><gtr:id>21E7EE36-75B4-473C-B2B5-60C9A98B7277</gtr:id><gtr:impact>This software architecture is generic, modular and reconfigurable.</gtr:impact><gtr:outcomeId>5457a5622837a5.28706496</gtr:outcomeId><gtr:title>RA software</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The dataset annotation tool provides the end-user with a GUI-based simple to use package for annotating datasets for interesting visual features and landmarks (e.g., planetary surficial rocks). The annotated datasets can be used for evaluating visual object detection/tracking techniques.</gtr:description><gtr:id>D08E3F7A-3D0E-49AF-84D0-E28B73DC6070</gtr:id><gtr:impact>The software has significantly reduced the time required for annotating benchmark datasets required to evaluate constituent (vision-based) software functional blocks of a planetary GNC system. This software can be easily adapted to annotate datasets related to other problem domains.</gtr:impact><gtr:outcomeId>5458d813c3b9a9.38062764</gtr:outcomeId><gtr:title>Dataset annotation tool</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2013</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>35EFC997-E6C8-4CA2-A8A4-EE58C8C5789B</gtr:id><gtr:title>Quasi-Thematic Features Detection And Tracking For Future Rover Long-Distance Autonomous Navigation</gtr:title><gtr:parentPublicationTitle>Symposium on Advanced Space Technologies in Robotics and Automation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e50dd123b6941e91dc1d427ad111762c"><gtr:id>e50dd123b6941e91dc1d427ad111762c</gtr:id><gtr:otherNames>Shaukat, A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>544e9435c45166.85767781</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A1FB409-8CA0-46F3-B16D-E9DFAFE544E9</gtr:id><gtr:title>Computational Intelligence for Space Systems and Operations [Guest Editorial]</gtr:title><gtr:parentPublicationTitle>IEEE Computational Intelligence Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64598b13b7f5157827dbe8dd0963f907"><gtr:id>64598b13b7f5157827dbe8dd0963f907</gtr:id><gtr:otherNames>Gao Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>544e883d4aa7a3.47668218</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9CE85F77-103B-4413-A7B6-03B087A4F73D</gtr:id><gtr:title>Planetary Monocular Simultaneous Localization and Mapping</gtr:title><gtr:parentPublicationTitle>Journal of Field Robotics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/047fd30717cbf30df42152053e3d0b87"><gtr:id>047fd30717cbf30df42152053e3d0b87</gtr:id><gtr:otherNames>Bajpai A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5675f0c697f39</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0006BD75-7F50-4CCF-895B-0AFC7B9D180B</gtr:id><gtr:title>Testing Saliency Based Techniques For Planetary Surface Scene Analysis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52d23e68658bcdb441ea8451e8b1639c"><gtr:id>52d23e68658bcdb441ea8451e8b1639c</gtr:id><gtr:otherNames>Yeomans B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b33a3732d5c0.83776510</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1DBF901C-CB54-4F5F-84C7-BC0701F417E5</gtr:id><gtr:title>Towards Camera-LIDAR Fusion-Based Terrain Modelling for Planetary Surfaces: Review and Analysis.</gtr:title><gtr:parentPublicationTitle>Sensors (Basel, Switzerland)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ee48407c11fd6cf41789c6f8a3599ebe"><gtr:id>ee48407c11fd6cf41789c6f8a3599ebe</gtr:id><gtr:otherNames>Shaukat A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1424-8220</gtr:issn><gtr:outcomeId>58b976e54a8d58.90240679</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E114389-32A7-4A51-80A4-05BA0DB9E6DC</gtr:id><gtr:title>Reconfigurable Autonomy</gtr:title><gtr:parentPublicationTitle>KI - K?nstliche Intelligenz</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4f55a8003eb38385647eda99ecfea1e9"><gtr:id>4f55a8003eb38385647eda99ecfea1e9</gtr:id><gtr:otherNames>Dennis L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>544e9457eb2379.13403942</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1973DB86-E979-4933-9E81-880E351331D8</gtr:id><gtr:title>ExoMars Rover PanCam: Autonomous &amp;amp; Computational Intelligence [Application Notes]</gtr:title><gtr:parentPublicationTitle>IEEE Computational Intelligence Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ea01ac8b581ca7fa74ea6630b6c94cb3"><gtr:id>ea01ac8b581ca7fa74ea6630b6c94cb3</gtr:id><gtr:otherNames>Yuen P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>544e883d857e38.27441222</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DE55DCE5-EB9E-405C-9EC1-2DE9E3EE4055</gtr:id><gtr:title>Reconfigurable SLAM Utilising Fuzzy Reasoning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ee48407c11fd6cf41789c6f8a3599ebe"><gtr:id>ee48407c11fd6cf41789c6f8a3599ebe</gtr:id><gtr:otherNames>Shaukat A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b3397287da57.57059106</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3FD6366E-0A8A-465D-84D0-CC6080BA539E</gtr:id><gtr:title>Next-Generation Rover GNC Architectures</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ee48407c11fd6cf41789c6f8a3599ebe"><gtr:id>ee48407c11fd6cf41789c6f8a3599ebe</gtr:id><gtr:otherNames>Shaukat A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b337da58ec19.27106898</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>36D64297-AB88-419F-8E39-17CCE438ADA1</gtr:id><gtr:title>China's robotics successes abound.</gtr:title><gtr:parentPublicationTitle>Science (New York, N.Y.)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64598b13b7f5157827dbe8dd0963f907"><gtr:id>64598b13b7f5157827dbe8dd0963f907</gtr:id><gtr:otherNames>Gao Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0036-8075</gtr:issn><gtr:outcomeId>544e883d199bc3.51246794</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6F55AE2C-13BC-4CF9-BDDC-EED365E93DF2</gtr:id><gtr:title>Remote Sensing of Martian Terrain Hazards via Visually Salient Feature Detection</gtr:title><gtr:parentPublicationTitle>European Planetary Science Congress 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f9551d1a3fbd3a63b9be6927bf8fea39"><gtr:id>f9551d1a3fbd3a63b9be6927bf8fea39</gtr:id><gtr:otherNames>Al-Milli, S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>544e94fdcd0b91.36368693</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EC508C8-C179-4E05-A692-00BDE3F61F6B</gtr:id><gtr:title>Visual classification of waste material for nuclear decommissioning</gtr:title><gtr:parentPublicationTitle>Robotics and Autonomous Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ee48407c11fd6cf41789c6f8a3599ebe"><gtr:id>ee48407c11fd6cf41789c6f8a3599ebe</gtr:id><gtr:otherNames>Shaukat A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5675f0c5ca894</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8BCB8BCE-4E54-45F2-8D00-C6C02F3F151F</gtr:id><gtr:title>A survey on recent object detection techniques useful for monocular vision-based planetary terrain classification</gtr:title><gtr:parentPublicationTitle>Robotics and Autonomous Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64598b13b7f5157827dbe8dd0963f907"><gtr:id>64598b13b7f5157827dbe8dd0963f907</gtr:id><gtr:otherNames>Gao Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>544e898accc811.23352944</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3FF8AECC-1D00-457E-86AA-BD865D13E55C</gtr:id><gtr:title>Self-Reconfigurable Robotics Architecture Utilising Fuzzy and Deliberative Reasoning</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ee48407c11fd6cf41789c6f8a3599ebe"><gtr:id>ee48407c11fd6cf41789c6f8a3599ebe</gtr:id><gtr:otherNames>Shaukat A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56b33c845a7fb1.01342365</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J011916/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>76783275-A9F8-4B4E-B314-51363124259C</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Fundamentals of Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>