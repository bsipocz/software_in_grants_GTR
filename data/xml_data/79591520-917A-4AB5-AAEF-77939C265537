<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/4C4C60E8-3804-449C-AC1E-C704BEAC2E83"><gtr:id>4C4C60E8-3804-449C-AC1E-C704BEAC2E83</gtr:id><gtr:firstName>M.</gtr:firstName><gtr:otherNames>Jane</gtr:otherNames><gtr:surname>Riddoch</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A536DD0C-05E7-4DCE-899C-25B41BFA1DBB"><gtr:id>A536DD0C-05E7-4DCE-899C-25B41BFA1DBB</gtr:id><gtr:firstName>Glyn</gtr:firstName><gtr:surname>Humphreys</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FE006175%2F1"><gtr:id>79591520-917A-4AB5-AAEF-77939C265537</gtr:id><gtr:title>ACTION-MEDIATED VISUAL ATTENTION IN THE HUMAN BRAIN</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/E006175/1</gtr:grantReference><gtr:abstractText>The ability to attend to stimuli in the environment that are relevant to our behavioural goals is crucially important for survival. To date, most studies into the factors that determine visual attention have concentrated on the basic properties of stimuli to which attention is drawn / for example, their size, orientation, or colour contrast. We have recently shown that, in addition to these basis properties, attention is also determined by the presence of action relations between objects. For example, neurological patients who have problems attending to multiple stimuli find it easier to do so if the objects appear as if in use together (Riddoch et al., Nature Neurosci., 2003). These data may be fundamentally important for understanding how attention operates, and would be relevant not only to evaluating human attention but also for the design of artificial attention systems. In the planned project we aim to assess if action relations between objects affect the allocation of attention in normal observers, to show that prior results are not 'special' to brain injured populations. We will then proceed to explore the neural basis for the action effects. Functional magnetic resonance (fMRI) imaging will be used to assess which brain regions respond to action information, and whether there is modulation of a network of brain regions (including primary sensory regions of the cortex). fMRI will inform us about the spatial localisation of these action effects in the brain, but it provides coarse information about quickly any effects emerge. To examine the time course of action effects, we will record event-related brain potentials, which generate a detailed picture of the time at which effects arise. Finally, transcranial magnetic stimulation will be used to affect ongoing neural processes in targeted brain regions (identified through fMRI), to test whether these regions are necessary for action effects on attention to emerge. The results will enable us to construct a detailed neural and functional account of the effects of action on human attention.</gtr:abstractText><gtr:technicalSummary>Our ability to attend to information in the visual environment is affected by a wide range of factors. Using neuropsychological evidence from patients showing visual extinction, we (Riddoch et al., 2003, Nature Neurosci.) have recently demonstrated evidence for a previously un-explored factor / the action relations between objects. Patients are better able to attend to multiple objects if the objects appear in use together. Thus action information in the environment appears to be critically important for directing visual attention. Here we will assess the functional and neural basis of these action effects. First, we will examine whether attention in normal observers is affected by the action relations between objects. We will then go on to examine the neural substrates of the effects using converging evidence from fMRI, ERPs and TMS. Whole-brain analyses, using fMRI, will be undertaken to evaluate which brain regions respond to action information, and whether there is recruitment of a network of regions down to and including primary visual cortex. ERPs will be employed to provide complementary analyses of the time course of processing, testing how early in time action effects emerge. TMS will be used to evaluate if brain regions identified through fMRI are necessary for action to influence attention. If such areas play a necessary role, then stimulation by TMS will modulate behaviour. The studies will provide the first detailed analysis of action effects on visual attention in the normal brain.</gtr:technicalSummary><gtr:fund><gtr:end>2009-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2007-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>396859</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>DACD21E7-219E-46C4-883A-ECBC02478F20</gtr:id><gtr:title>Effects of action relations on the configural coding between objects.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a10b24849a390a572b7b60c697c06b6b"><gtr:id>a10b24849a390a572b7b60c697c06b6b</gtr:id><gtr:otherNames>Riddoch MJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>pm_53cbfe0bfe0bb33c0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>93B5215C-E29E-4971-8DA6-EAA49DAE7B70</gtr:id><gtr:title>The paired-object affordance effect.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d942b140ff1033f9cc7b18baec117f63"><gtr:id>d942b140ff1033f9cc7b18baec117f63</gtr:id><gtr:otherNames>Yoon EY</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>i38YwN9QKQ7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6432A12D-6E40-444F-87AA-02618C390294</gtr:id><gtr:title>Attention and its coupling to action.</gtr:title><gtr:parentPublicationTitle>British journal of psychology (London, England : 1953)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c6c31f75540e7f6a9e175a517276877d"><gtr:id>c6c31f75540e7f6a9e175a517276877d</gtr:id><gtr:otherNames>Humphreys GW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0007-1269</gtr:issn><gtr:outcomeId>mo5jZDQqNxD</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3C61ACF0-5623-4D36-ADD7-84323FC29040</gtr:id><gtr:title>Neuropsychological evidence for visual- and motor-based affordance: effects of reference frame and object-hand congruence.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Learning, memory, and cognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c6c31f75540e7f6a9e175a517276877d"><gtr:id>c6c31f75540e7f6a9e175a517276877d</gtr:id><gtr:otherNames>Humphreys GW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0278-7393</gtr:issn><gtr:outcomeId>aufnsYK2i8D</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2FEFD2F4-5585-4899-8AA5-AB6F145EFD59</gtr:id><gtr:title>Top-down effects of semantic knowledge in visual search are modulated by cognitive but not perceptual load.</gtr:title><gtr:parentPublicationTitle>Perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fb2aac6cfd73b85e61b58f6b8ec0eb06"><gtr:id>fb2aac6cfd73b85e61b58f6b8ec0eb06</gtr:id><gtr:otherNames>Belke E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0031-5117</gtr:issn><gtr:outcomeId>D1C533120F6</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/E006175/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>