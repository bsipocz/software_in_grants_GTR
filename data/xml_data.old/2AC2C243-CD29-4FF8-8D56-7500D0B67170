<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Dept of Computing</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/01F9DC24-19B8-43F6-AE73-35D843E1FDF7"><gtr:id>01F9DC24-19B8-43F6-AE73-35D843E1FDF7</gtr:id><gtr:name>Realeyes UK</gtr:name><gtr:address><gtr:line1>79 Wardour Street</gtr:line1><gtr:postCode>W1D 6QB</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9F82D3EE-3D56-4472-AA7F-5732813E74B0"><gtr:id>9F82D3EE-3D56-4472-AA7F-5732813E74B0</gtr:id><gtr:name>iProov Limited</gtr:name><gtr:address><gtr:line1>35 Paul Street</gtr:line1><gtr:postCode>EC2A 4UQ</gtr:postCode><gtr:region>London</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/85C74800-12C4-4460-B46C-76652A525C86"><gtr:id>85C74800-12C4-4460-B46C-76652A525C86</gtr:id><gtr:firstName>Stefanos</gtr:firstName><gtr:surname>Zafeiriou</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL026813%2F1"><gtr:id>2AC2C243-CD29-4FF8-8D56-7500D0B67170</gtr:id><gtr:title>Adaptive Facial Deformable Models for Tracking (ADAManT)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/L026813/1</gtr:grantReference><gtr:abstractText>We propose to develop methodologies for automatic construction of person-specific facial deformable models for robust tracking of facial motion in unconstrained videos (recorded 'in-the-wild'). The tools are expected to work well for data recorded by a device as cheap as a web-cam and in almost arbitrary recording conditions. The technology that will be developed in the project is expected to have a huge impact in many different applications including but not limited to, biometrics (face recognition), Human Computer Interaction (HCI) systems, as well as, analysis and indexing of videos using facial information (e.g., YouTube), capturing of facial motion in games and film industry, creating virtual avatars, just to name a few. 

The novelty of the ADAMant technology is multi-faceted. We propose the very first, robust, discriminative deformable facial models that can be customized, in an incremental fashion, so that they can automatically tailor themselves to the person's face using image sequences under uncontrolled recording conditions (both indoors and outdoors). Also, we propose to build and publicly release the first annotated, with regards to facial landmarks, database of facial videos made 'in-the-wild'. Finally, we aim to use the database as the base of the first competition for facial landmark tracking 'in-the-wild', which will run as a satellite workshop of a top vision venue (such as ICCV 2015). 

As a proof of concept, and with a focus on a novel application, the ADAMant technology will be applied for (1) facial landmark tracking for machine analysis of behaviour in response to product adverts watched by people at comfort of their home (indoors) and (2) facial landmark tracking for automatic face verification using videos recorded by mobile devices (outdoors). In an increasingly global economy and ever-ubiquitous digital age, the market can change rapidly. As stipulated by the UK Researcher Councils' Digital Economy Theme, realising substantial transformational impact on how new business models are being created and taking advantage of the digital world is one of the main challenges. As human face is at the heart of many scientific disciplines and business models, the ADAManT project provides technology that can reshape established business models to become more efficient but also create new ones. Within EPSRC's ICT priorities our research is extremely relevant to autonomous systems and robotics, since it enables the development of robots capable of understanding human behaviour in unconstrained environments (i.e., design of robot companions, robots as tourist guide, etc.).</gtr:abstractText><gtr:potentialImpactText>Technologies that can robustly and accurately track facial motion as observed by omnipresent webcams in digital devices would have profound impact on both basic sciences and industrial sector. They would open up important new applications of ICT in basic research, medicine, healthcare, digital economy and business.
(a) basic research - application of computer vision tools for automatic tracking of facial signals in the wild could open up tremendous potential to measure behaviour indicators (in psychology, psychiatry, security sector, etc.) that heretofore resisted measurement because they were too subtle or fleeting to be measured by the human eye.
(b) computer science - affective interfaces, interactive games, implicit-tagging of multimedia content, and various online services would all be enabled or enhanced by this novel technology, effectively leading to the development of the next generation of human-centered computing; furthermore the developed technologies could have a huge impact in movies (special effects) and games industry which still perform facial motion capture with tedious manual procedures or require from the actors to wear special equipment which affect their acting performance; finally the developed tracking methodologies will facilitate the development of robust mobile biometric systems.
(b) robotics: the development of automated tools for tracking of facial motion in unconstrained conditions would enable the development of robots capable of understanding human behavior in both indoor and outdoor environments (i.e., design of robot companions, robots as tourist guide, etc.).
(d) medicine and health care - many disorders in neurology and psychiatry (schizophrenia, suicidal depression, Parkinson's disease) involve aberrations in display and interpretation of facial behaviour. ADAManT technology could provide tracked facial motion with increased reliability, sensitivity, and precision needed to explore the relationship between facial behaviour and mental disorder and lead to new insights and remote diagnostic methods. Also, remote monitoring of conditions like pain and depression, remote assessment of drug effectiveness, etc., would be possible, leading to more advanced personal wellness technologies.
(e) digital economy and commercial applications - Automatic measurement of consumers' liking in response to product ads will have profound impact in automatic market research analysis, mainly because this will offer the possibility of conducting massive market-research studies. This is in striking contrast to standard market research methods for collecting liking ratings that are based on self-reporting techniques, which are known to be notoriously slow, error prone, and tedious. The ADAManT technology will be of great value not only for the companies working in market research analysis (such as RealEyes) but it will extremely valuable to their clients too. This is exactly in the line with challenges listed in the UK Researcher Councils' Digital Economy Theme -- realising substantial transformational impact on how new business models are being created and taking advantage of the digital age. The ADAManT technology will also make possible automatic assessment of the driver's stress level, detection of micro sleeps, spotting driver's puzzlement, and effectively enabling next generation of in-vehicle assistive technology. It will facilitate automatic assessment of student's interest level, puzzlement, and enjoyment in online- and E-education, enabling the development of truly intelligent tutoring systems.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-05-03</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-11-04</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>97996</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Demo of Menpo Platform (open source platform for deformable model learning and fitting) in ICCV 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>3A713717-C118-4E48-B267-06990C0E974A</gtr:id><gtr:impact>We presented our publicly available platform in demo in conjunction with one of the most prestigious computer vision venues, i.e., International Conference on Computer Vision, (ICCV) 2015. The demo attracted a lot of attention and our platform is now used my hundreds of users all over the world (both academic and industrial users).</gtr:impact><gtr:outcomeId>56d328b22fd605.52902789</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.menpo.org/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>A facial landmark tracking software developed through this awarded was perpetually licenced to the company SeeingMachines (http://www.seeingmachines.com/).</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>D5CA270F-F66F-4849-807A-7EF8BE8040B3</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d320c6856b50.22838194</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>A real time facial landmark tracking software. The software tracks in real-time 60+ facial landmarks in arbitrary monocular intensity facial videos. The software has been licenced, perpetually, to SeeingMachines (http://www.seeingmachines.com/).</gtr:description><gtr:grantRef>EP/L026813/1</gtr:grantRef><gtr:id>6C932D69-4B16-4864-9F40-909BBD461152</gtr:id><gtr:impact>To the best of my knowledge the licensed technology is the current facial landmark tracker used by SeeingMachines.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56d32b533a0b84.18666290</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>Facial landmark tracking</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>During this award we developed a semi-automatic methodology for cost and time efficient annotation of facial video frames. The methodology has been used to annotate all the frames of 110+ facial videos (1+min per video) with regards facial landmarks (over 150,000 frames). The annotations have been used to develop the first benchmark for facial landmark tracking, as well as to run the first challenge on the topic (http://ibug.doc.ic.ac.uk/resources/300-VW/).</gtr:description><gtr:exploitationPathways>The benchmark will have huge impact in the field and beyond. Currently, the data are used by the majority of the academic community, as well as the industry working on face analysis of videos. The semi-automatic annotation methodology developed would have impact in analysis of other deformable objects (e.g., human body), since it makes annotation of large amount of images an efficient task. Parts of the methodology is currently open source and publicly available in the menpo project (http://www.menpo.org/).</gtr:exploitationPathways><gtr:id>BAE82841-8000-458C-9052-E9E63C85B53C</gtr:id><gtr:outcomeId>56d32586d746e9.14148597</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://ibug.doc.ic.ac.uk/resources/300-VW/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>300 VW is the first benchmark for facial landmark tracking. It contains more than 150,000 of annotated facial frames (annotated with regards 60+ facial landmarks).</gtr:description><gtr:id>BAF2DDE0-B3DA-4439-9907-94C1A60A0A35</gtr:id><gtr:impact>The database has been used in the first challenge/competition for facial landmark tracking (held in conjunction with on of the top computer vision venues, i.e. ICCV, 2015). Currently, the database/benchmark is used by the state-of-the-art for assessing the performance of facial landmark tracking methodologies.</gtr:impact><gtr:outcomeId>56d32cfb22e870.69311558</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>300 VW Facial Landmark Tracking Bechmark/Database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://ibug.doc.ic.ac.uk/resources/300-VW/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The Menpo Project, hosted at http://www.menpo.io, is a BSD-licensed software platform providing a complete and
comprehensive solution for annotating, building, fitting and evaluating deformable visual models from image data. Menpo
is a powerful and flexible cross-platform framework written in Python that works on Linux, OS X and Windows.
Menpo makes it easy to understand and evaluate the above complex algorithms, providing tools for visualisation, analysis, and
performance assessment. A key challenge in building deformable models is data annotation; Menpo expedites this process by providing a simple web-based annotation tool hosted at http://www.landmarker.io. The Menpo Project is thoroughly documented and provides extensive examples for all of its features. We believe the project is ideal for researchers, practitioners and students alike.</gtr:description><gtr:id>1034D850-CD32-4451-9057-2CCC4F4247F8</gtr:id><gtr:impact>The Menpo project has been used to develop the annotations of the very popular 300VW and 300W (second version) benchmarks. 300VW is the first benchmark for facial landmark tracking methodologies and 300W is the most popular database for facial landmark localisation in arbitrary conditions (used by the majority of the state-of-the-art published in top computer vision venues, such as ICCV, CVPR, ECCV etc.). The menpo project has been demoed in ICCV 2015. Currently the menpo project has hundredths of users both in academia and industry.</gtr:impact><gtr:outcomeId>56d422a1de7ea2.71060709</gtr:outcomeId><gtr:title>Menpo Project: Parametric Image alignment and deformable model construction and fitting.</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.menpo.org/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C9548235-9DB1-4B4A-96CF-4A926CA1A89C</gtr:id><gtr:title>Active Pictorial Structures</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d31621ec5379.16980035</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>99065773-E7F9-4F51-9758-D18B8D0627C5</gtr:id><gtr:title>Online kernel slow feature analysis for temporal video segmentation and tracking.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d185abab3392493297236503003181c4"><gtr:id>d185abab3392493297236503003181c4</gtr:id><gtr:otherNames>Liwicki S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5675e3ab12148</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7519C99F-7D56-4490-871F-E1BBABC76DA9</gtr:id><gtr:title>Unifying holistic and Parts-Based Deformable Model fitting</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75a8af6c7150b2e8892e9f6969aed3d6"><gtr:id>75a8af6c7150b2e8892e9f6969aed3d6</gtr:id><gtr:otherNames>Alabort-i-Medina J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d31480c2e250.60392359</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EEB0CE7-687F-4893-8882-B1110F46464C</gtr:id><gtr:title>Robust Statistical Face Frontalization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d6b8a67d776.48198818</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>506FDFB3-8657-4C83-9185-6304225FD0B5</gtr:id><gtr:title>300 W: Special issue on facial landmark localisation &amp;acirc;??in-the-wild&amp;acirc;??</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d6817c39d46.00773863</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DD5DB711-63CC-48E4-8723-2374C1A039A6</gtr:id><gtr:title>A survey on mouth modeling and analysis for Sign Language recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d315d3906957.69169546</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F724FEC-E50C-4061-A2B9-A9A9E36146D2</gtr:id><gtr:title>A survey on face detection in the wild: Past, present and future</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a9a4fcf7065577f4f52875c6b49e4997"><gtr:id>a9a4fcf7065577f4f52875c6b49e4997</gtr:id><gtr:otherNames>Zafeiriou S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f4dd9f584</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DC3D0241-F3AE-433C-A9D8-673FA7513566</gtr:id><gtr:title>PD
&lt;sup>2&lt;/sup>T: Person-specific Detection, Deformable Tracking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fedd077df45.51348003</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C738987-2144-4ABC-8BD3-4060614B6C30</gtr:id><gtr:title>Offline Deformable Face Tracking in Arbitrary Videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d31530be7fb1.11225808</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ECDC43A6-08E3-474C-AE90-F93A7E94341A</gtr:id><gtr:title>Face Flow</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0cb2fc699d369822b5b95df5b09184"><gtr:id>9a0cb2fc699d369822b5b95df5b09184</gtr:id><gtr:otherNames>Snape P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d316996bc1b3.14076741</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61B922B6-111C-408C-A2BE-12C87FF0E6FB</gtr:id><gtr:title>A Comprehensive Performance Evaluation of Deformable Face Tracking &amp;quot;In-the-Wild&amp;quot;</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d81855cbc62dc7346bc9830ecd6e109"><gtr:id>2d81855cbc62dc7346bc9830ecd6e109</gtr:id><gtr:otherNames>Chrysos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c699197c7ce9.49554486</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DA364D4E-C759-4E58-9FDA-8DA7180E190B</gtr:id><gtr:title>The First Facial Landmark Tracking in-the-Wild Challenge: Benchmark and Results</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/11792d8e57ee799267adfbebcc11e298"><gtr:id>11792d8e57ee799267adfbebcc11e298</gtr:id><gtr:otherNames>Shen J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d314dbdd8af8.52543959</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1064E25A-3FC8-4D9F-B735-691744B25D0E</gtr:id><gtr:title>Statistical non-rigid ICP algorithm and its application to 3D face alignment</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aaf109908e93f5f256369e9311e71523"><gtr:id>aaf109908e93f5f256369e9311e71523</gtr:id><gtr:otherNames>Cheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c69918b2aa02.85553795</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7297BBC5-F261-4CAC-B061-21C2F626A607</gtr:id><gtr:title>300 Faces In-The-Wild Challenge: database and results</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4abaacfb1ae33e37a261e60a574b1de2"><gtr:id>4abaacfb1ae33e37a261e60a574b1de2</gtr:id><gtr:otherNames>Sagonas C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56d312c0f3a099.01714148</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DD85E323-EB63-4548-B694-C9C80C7C2735</gtr:id><gtr:title>Feature-based Lucas-Kanade and active appearance models.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on image processing : a publication of the IEEE Signal Processing Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e2ad6bd09f4c9f959321caa4694d0711"><gtr:id>e2ad6bd09f4c9f959321caa4694d0711</gtr:id><gtr:otherNames>Antonakos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1057-7149</gtr:issn><gtr:outcomeId>5675e44cb67cb</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1999F82A-AD6B-4DB0-A313-A8E3623C59DD</gtr:id><gtr:title>A Unified Framework for Compositional Fitting of Active Appearance Models</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75a8af6c7150b2e8892e9f6969aed3d6"><gtr:id>75a8af6c7150b2e8892e9f6969aed3d6</gtr:id><gtr:otherNames>Alabort-i-Medina J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6983bd9a115.39144595</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1EEEA3D2-6A83-4B43-B0E3-01CB2A861C90</gtr:id><gtr:title>Active nonrigid ICP algorithm</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c231bbf66924a2e2cd3b7fca00a8df93"><gtr:id>c231bbf66924a2e2cd3b7fca00a8df93</gtr:id><gtr:otherNames>Shiyang Cheng</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d315b066fa13.93843350</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>982CC64F-5929-417A-A72A-42C1AE89818D</gtr:id><gtr:title>Automatic construction Of robust spherical harmonic subspaces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9a0cb2fc699d369822b5b95df5b09184"><gtr:id>9a0cb2fc699d369822b5b95df5b09184</gtr:id><gtr:otherNames>Snape P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d3185daabe12.50898827</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>16B33F07-56EF-4CC7-A52D-7D44F147A544</gtr:id><gtr:title>From Pixels to Response Maps: Discriminative Image Filtering for Face Alignment in the Wild.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4654574b83d68a748a38245b2e83cd24"><gtr:id>4654574b83d68a748a38245b2e83cd24</gtr:id><gtr:otherNames>Asthana A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>5675eb5631efa</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E3B4FAC2-E999-4D56-A490-1B95A5720AE7</gtr:id><gtr:title>Probabilistic Slow Features for Behavior Analysis.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on neural networks and learning systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dc5985b05170374836e82dc2ea9e8b7"><gtr:id>1dc5985b05170374836e82dc2ea9e8b7</gtr:id><gtr:otherNames>Zafeiriou L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2162-237X</gtr:issn><gtr:outcomeId>56d311da225253.38473417</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L026813/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>