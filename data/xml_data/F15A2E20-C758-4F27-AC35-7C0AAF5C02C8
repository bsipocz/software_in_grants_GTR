<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/7801F008-7C77-45E7-90E9-4345B47D138E"><gtr:id>7801F008-7C77-45E7-90E9-4345B47D138E</gtr:id><gtr:name>University of Plymouth</gtr:name><gtr:department>Sch of Computing &amp; Mathematics</gtr:department><gtr:address><gtr:line1>Drake Circus</gtr:line1><gtr:line4>Plymouth</gtr:line4><gtr:line5>Devon</gtr:line5><gtr:postCode>PL4 8AA</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7801F008-7C77-45E7-90E9-4345B47D138E"><gtr:id>7801F008-7C77-45E7-90E9-4345B47D138E</gtr:id><gtr:name>University of Plymouth</gtr:name><gtr:address><gtr:line1>Drake Circus</gtr:line1><gtr:line4>Plymouth</gtr:line4><gtr:line5>Devon</gtr:line5><gtr:postCode>PL4 8AA</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name><gtr:address><gtr:line1>Economic and Social Research Council</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:line3>Polaris Way</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:line5>Wiltshire</gtr:line5><gtr:postCode>SN2 1UJ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name><gtr:address><gtr:line1>BBSRC</gtr:line1><gtr:line2>Polaris House</gtr:line2><gtr:line3>North Star Avenue</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:postCode>SN2 1UH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F74A0500-37AF-4643-A8CA-9C4F0D9AAF72"><gtr:id>F74A0500-37AF-4643-A8CA-9C4F0D9AAF72</gtr:id><gtr:firstName>Angelo</gtr:firstName><gtr:surname>Cangelosi</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/31107A72-A0CD-4753-9B64-0FD8A633061D"><gtr:id>31107A72-A0CD-4753-9B64-0FD8A633061D</gtr:id><gtr:firstName>R</gtr:firstName><gtr:surname>Ellis</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF026471%2F1"><gtr:id>F15A2E20-C758-4F27-AC35-7C0AAF5C02C8</gtr:id><gtr:title>VALUE: Vision, Action, and Language Unified by Embodiment</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F026471/1</gtr:grantReference><gtr:abstractText>The primary aim of this project is to develop a simulation of the processes involved in solving the following problem: how to select, based on the agent's knowledge and representations of the world, one object from several, grasp the object and use it in an appropriate manner. This mundane activity in fact requires the simultaneous solution of several deep problems at various levels. The agent's visual system must represent potential target objects, the target must be selected based on task instructions or the agent's knowledge of the functions of the represented objects, and the hand (in this case) must be moved to the target and shaped so as to grip it in a manner appropriate for its use. We propose to develop a robotic simulation model inspired by recent theories of embodied cognition, in which the vision, action and semantic systems are linked together, in a dynamic and mutually interactive manner, within a connectionist architecture. Human experimental work will constrain the temporal and dynamic properties of the system in an effort to develop a psychologically plausible model of embodied selection for action. As much of the cognitive mechanisms leading to the integration between action and vision for actions such as object assembly tasks are not fully known, new empirical studies in this project will also improve our insight of these embodied cognitive dynamics. New experiments and the use of the embodied cognitive model will also be used to further our understanding of language and cognition integration e.g. by providing further predictions and insights on the dynamics of language and action knowledge in object representation.This is an interdisciplinary project which involves expertise and methodologies from cognitive psychology, motor control, and computational/robotics modelling. The interdisciplinary nature of the project and the design and experimentation of cognitive agents make the project highly relevant to the Cognitive Systems Foresight programme</gtr:abstractText><gtr:fund><gtr:end>2012-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>500171</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1202384</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC</gtr:description><gtr:end>2016-10-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/J00457X/1</gtr:fundingRef><gtr:id>56233A52-3930-4921-88A7-22D4458DF086</gtr:id><gtr:outcomeId>5edad5a65edad5c4</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-11-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The aim of this VALUE project, as per original proposal, was to investigate the processes and mechanisms leading to an integration of vision, action and language in natural cognitive systems (namely human participants) and to use this advancement in knowledge for the design of psychologically plausible artificial cognitive agents (simulated iCub robot) able to communicate about the world they perceive and act upon. This scientific and technological aim had three project objectives:

I. To explore the interface between language, action and vision through eye tracking experiments and micro-affordance studies on the action component of object representation (e.g. micro-affordances)

II. To identify the time-course of action, vision and language integration processes in tasks requiring selective attention, object search and object manipulation and assembly under verbal instructions

III. To develop a cognitive robotic model capable of object manipulation and language use based on psychologically plausible embodied cognition principles, as identified through the above objectives. 



The project has successfully achieved all its objectives. The first two objectives where based on numerous psychological experiments using eye-tracking methods (Dundee) and cognitive psychology and ERP Event-Related Potential experiments (Plymouth). The third objective was achieved through modeling experiments with the simulated iCub robot, addressing micro-affordance effects with both single and multiple objects, and short term memory tasks for an assembly-type scenario. The modeling experiments provide a cognitively-inspired methodology for the design of grasping strategy with both precision grip and power grasp strategies, and for the sequential allocation of attention to relevant objects for assembly tasks.</gtr:description><gtr:exploitationPathways>Special Issue

Martin Fischer and Daniele Caligiore were guest editors of the special issue &amp;quot;Vision, Action, and Language Unified by Embodiment&amp;quot; for the Psychological Research journal. This was published in 2012.



International Workshops organized by the project

? International Workshop on Vision, Action and Language Unified by Embodiment (5-6 May 2010, Italian Institute of Technology, Genoa)

? The Mechanics of Embodiment, CogSci-2010 Symposium (13 August 2010, Portland USA) 

? International Workshop on Vision, Action and Language Embodiment (19-20 April 2011, Bishop's Palace, Cefalu' Italy) 

? Potsdam Workshop on Cognitive Robotics (15 September 2011, University of Potsdam, Neues Palais)

Other dissemination activities



An interview in the popular science magazine New Scientist with Martin Fischer was published on 11 November 2011 (http://www.newscientist.com/article/mg21228385.900-virtual-robot-links-body-to-numbers-just-like-humans.html).</gtr:exploitationPathways><gtr:id>C647268D-6C35-4D4C-BD9E-BB1AA668C9E8</gtr:id><gtr:outcomeId>r-7556200872.6933987773841e</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.tech.plym.ac.uk/soc/research/ABC/value/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>99F5ECA6-6F4B-4F6C-A040-B18DD6B2CDAF</gtr:id><gtr:title>The oculomotor resonance effect in spatial-numerical mapping.</gtr:title><gtr:parentPublicationTitle>Acta psychologica</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70570332e65608cf01598bbc299ba3c0"><gtr:id>70570332e65608cf01598bbc299ba3c0</gtr:id><gtr:otherNames>Myachykov A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0001-6918</gtr:issn><gtr:outcomeId>56c19ccfb4d437.71264084</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F8AD54E6-53ED-4E11-8473-0DAC5DF213E7</gtr:id><gtr:title>The mechanics of embodiment: a dialog on embodiment and computational modeling.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d93971f4f21570d991792849625dc7ea"><gtr:id>d93971f4f21570d991792849625dc7ea</gtr:id><gtr:otherNames>Pezzulo G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>doi_53d087087c8f06b8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BC1B2C8F-683F-4ABD-A68A-59E3872EED9A</gtr:id><gtr:title>Ocular drift along the mental number line.</gtr:title><gtr:parentPublicationTitle>Psychological research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70570332e65608cf01598bbc299ba3c0"><gtr:id>70570332e65608cf01598bbc299ba3c0</gtr:id><gtr:otherNames>Myachykov A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0340-0727</gtr:issn><gtr:outcomeId>56c199964ca9b0.08000054</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>101C869B-75F8-45E6-A3ED-CC96DB4FAF68</gtr:id><gtr:title>A cognitive robotic model of grasping</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6dbf6706ef4042a5e9074f47075988ae"><gtr:id>6dbf6706ef4042a5e9074f47075988ae</gtr:id><gtr:otherNames>Angelo Cangelosi (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_374672350513d81fa4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1D8AB53F-EA7B-4866-9489-A8ACD654BA8C</gtr:id><gtr:title>Computational Grounded Cognition: a new alliance between grounded cognition and computational modeling.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d93971f4f21570d991792849625dc7ea"><gtr:id>d93971f4f21570d991792849625dc7ea</gtr:id><gtr:otherNames>Pezzulo G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>doi_53d087087ccdfc3d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>758DE07C-24E1-444A-96E0-C993902E960A</gtr:id><gtr:title>Grounding language in action and perception: from cognitive agents to humanoid robots.</gtr:title><gtr:parentPublicationTitle>Physics of life reviews</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/285d9c01a1539d568a5a4773cffe0c42"><gtr:id>285d9c01a1539d568a5a4773cffe0c42</gtr:id><gtr:otherNames>Cangelosi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1571-0645</gtr:issn><gtr:outcomeId>doi_53d00500584b71df</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA2DA1FE-EF5F-4F79-827D-52F9C23DD0B5</gtr:id><gtr:title>Electrophysiological examination of embodiment in vision and action.</gtr:title><gtr:parentPublicationTitle>Psychological science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a2e5ee5cda0339ee5bc74b90343c2e32"><gtr:id>a2e5ee5cda0339ee5bc74b90343c2e32</gtr:id><gtr:otherNames>Goslin J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0956-7976</gtr:issn><gtr:outcomeId>doi_53d079079df937a8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E22F842-727A-48D4-A679-470776114F68</gtr:id><gtr:title>Attention deployment during memorizing and executing complex instructions.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f886dbadbfc52d5c4f04ac77bfa625f"><gtr:id>8f886dbadbfc52d5c4f04ac77bfa625f</gtr:id><gtr:otherNames>Apel JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>doi_53cfd2fd2819a4cf</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>79481DAC-DFDD-4988-A848-030769304A5A</gtr:id><gtr:title>Visual and linguistic cues to graspable objects.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/70570332e65608cf01598bbc299ba3c0"><gtr:id>70570332e65608cf01598bbc299ba3c0</gtr:id><gtr:otherNames>Myachykov A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>56c19ac2d70674.09315801</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>39542428-94C9-4A00-9543-57B36DFACCD4</gtr:id><gtr:title>How affordances associated with a distractor object affect compatibility effects: a study with the computational model TRoPICALS.</gtr:title><gtr:parentPublicationTitle>Psychological research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cdfd86fbf750a8a235e74005d0180216"><gtr:id>cdfd86fbf750a8a235e74005d0180216</gtr:id><gtr:otherNames>Caligiore D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0340-0727</gtr:issn><gtr:outcomeId>54412ebd228c31.67634958</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F961C624-3E49-4239-9F25-F018504F32B0</gtr:id><gtr:title>Object affordance influences instruction span.</gtr:title><gtr:parentPublicationTitle>Experimental brain research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f886dbadbfc52d5c4f04ac77bfa625f"><gtr:id>8f886dbadbfc52d5c4f04ac77bfa625f</gtr:id><gtr:otherNames>Apel JK</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0014-4819</gtr:issn><gtr:outcomeId>doi_53cfd2fd28464d3a</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F026471/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>