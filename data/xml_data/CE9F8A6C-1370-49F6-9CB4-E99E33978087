<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3DF2F8DD-33A5-44A1-81D7-347A6C2FBDC8"><gtr:id>3DF2F8DD-33A5-44A1-81D7-347A6C2FBDC8</gtr:id><gtr:name>Stanford University</gtr:name><gtr:address><gtr:line1>450 Serra Mall</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/FC4B6823-4D70-494B-BEB4-B5BA76925C4C"><gtr:id>FC4B6823-4D70-494B-BEB4-B5BA76925C4C</gtr:id><gtr:name>59 Productions</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/E1C92180-A246-4DF9-9B1F-ECAD1F776205"><gtr:id>E1C92180-A246-4DF9-9B1F-ECAD1F776205</gtr:id><gtr:name>Fidelio Arts Ltd</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/F8DB4C15-7BB8-4ABC-8871-2FA2DA40FAB2"><gtr:id>F8DB4C15-7BB8-4ABC-8871-2FA2DA40FAB2</gtr:id><gtr:name>Middle East Technical University</gtr:name><gtr:address><gtr:line1>Middle East Technical University</gtr:line1><gtr:postCode>06800</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:department>Engineering</gtr:department><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/318B5D98-4CB4-4B10-A876-08FC93071A56"><gtr:id>318B5D98-4CB4-4B10-A876-08FC93071A56</gtr:id><gtr:name>King's College London</gtr:name><gtr:address><gtr:line1>Capital House</gtr:line1><gtr:line2>2nd Floor, Guys Campus</gtr:line2><gtr:line3>42 Weston Street</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>SE1 3QD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3DF2F8DD-33A5-44A1-81D7-347A6C2FBDC8"><gtr:id>3DF2F8DD-33A5-44A1-81D7-347A6C2FBDC8</gtr:id><gtr:name>Stanford University</gtr:name><gtr:address><gtr:line1>450 Serra Mall</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/FC4B6823-4D70-494B-BEB4-B5BA76925C4C"><gtr:id>FC4B6823-4D70-494B-BEB4-B5BA76925C4C</gtr:id><gtr:name>59 Productions</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E1C92180-A246-4DF9-9B1F-ECAD1F776205"><gtr:id>E1C92180-A246-4DF9-9B1F-ECAD1F776205</gtr:id><gtr:name>Fidelio Arts Ltd</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/F8DB4C15-7BB8-4ABC-8871-2FA2DA40FAB2"><gtr:id>F8DB4C15-7BB8-4ABC-8871-2FA2DA40FAB2</gtr:id><gtr:name>Middle East Technical University</gtr:name><gtr:address><gtr:line1>Middle East Technical University</gtr:line1><gtr:postCode>06800</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/00500CF2-BC27-44BF-B441-5E5A6308F5D6"><gtr:id>00500CF2-BC27-44BF-B441-5E5A6308F5D6</gtr:id><gtr:firstName>Zoran</gtr:firstName><gtr:surname>Cvetkovic</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF001142%2F1"><gtr:id>CE9F8A6C-1370-49F6-9CB4-E99E33978087</gtr:id><gtr:title>Perceptual Sound Field Reconstruction and Coherent Emulation</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F001142/1</gtr:grantReference><gtr:abstractText>The project is concerned with the development of a new 5--10 channel audio technology which would improve over existing ones in terms of (a) realism, (b) accuracy and stability of the auditory perspective, (c) size of the sweet spot, and (d) the envelopment experience. Since the new technology aims to create a 360 degrees auditory perspective, the reproduction will take place over speakers positioned at vertices of a regular polygon. Each speaker will consist of two components, one which will radiate the direct sound field toward a listener, and another which will reproduce diffuse sound field by introducing additional scattering. The goal of the particular tasks, listed below, is to find optimal ways to capture sound field cues and render them using the proposed playback system in a manner which would provide the most convincing illusion of the original or desired sound field.(i) Optimal microphone arrays for the proposed play-back system will be investigated. Arrays considered will consist of microphones placed in the horizontal plane at the vertices of a regular polygon, with the number of microphones equal to the number of speakers. For each array, different diameters, in the range from near coincident up to somewhat beyond the optimal value, and different microphone directivity patterns will be considered. These studies will be repreated for a few diameters of the speaker configuration to investigate if the optimal array diameter depends on the size of the speaker lay-out, and if so to characterize that dependence. Possible dependencies between the optimal microphone directivity patterns and array diameters will be also investigated and characterized. Arrays will be evaluated in critical listening tests according to criteria (a)--(d) stated in the above. Experiments will be guided by simulations which would provide initial objective assessment of ITD and ILD cues generated within the listening area. In parallel, mathematical models of sound fields generated by the proposed technology will be investigated, which could provide some additional insight into the optimal microphone array design. (ii) The impact of play-back with cross-talk cancellation will be be systematically investigated. Existing cross-talk cancellation algorithms will be first used, and if necessary, new algorithms which are numerically efficient and effective in a range of listening environments will be developed. Then optimal microphone arrays for play back with cross-talk cancellation will be investigated, i.e. the work described under (i) will be repeated for reproduction with cross-talk cancellation. Finally, optimal systems with and without cross-talk cancellation will be compared.(iii) Algorithms for direct/diffuse sound field separation will be studied. When the number of instruments does not exceed the number of microphones, multichannel equalization techniques can be used to find dry source signals, which can then be convolved with direct/reverberant parts of room impulse responses to obtain direct/diffuse sound field components, respectively. Multichannel equalization in audio is, however, particularly challenging owing to excessively long impulse responses, and we will develop numerically efficient algorithms for multichannel equalization for audio applications. Then we will study psychoacoustic approximation to direct/diffuse sound field decomposition with no restriction on the number of sources. (iv) Combinations of near-coincident directional microphone arrays, for acquiring direct sound field cues, and widely spaced arrays based on omni-directional or bi-directional microphones, for acquiring diffuse sound field cues, will be systematically investigated in critical listening tests according to criteria (a)--(d). This approach will be evaluated in comparison with the approach described in (i)--(iii) where the same array is used for both sound field components.</gtr:abstractText><gtr:fund><gtr:end>2011-08-17</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-02-18</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>389807</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>An immersive 3D audio-visual installation</gtr:description><gtr:id>F37BDBF7-74A8-4CAD-A96C-5E6762E8E91C</gtr:id><gtr:impact>Demonstration and first public display of the audio technology created on the projects funded by the associated awards.</gtr:impact><gtr:outcomeId>58c442cee7d608.33003887</gtr:outcomeId><gtr:title>Ouroboros</gtr:title><gtr:type>Performance (Music, Dance, Drama, etc)</gtr:type><gtr:url>http://pantar.com/portfolio/ouroboros/</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>59 Productions</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>59 Productions</gtr:description><gtr:id>BBFF9DFB-C85A-4E04-9FAC-77721CCE227D</gtr:id><gtr:impact>A multimedia art installation centred around a performance of pianist Yuja Wang.</gtr:impact><gtr:outcomeId>58c437c1e30635.62521559-1</gtr:outcomeId><gtr:partnerContribution>Design and production of a multimedia art installation centred around a performance of pianist Yuja Wang.</gtr:partnerContribution><gtr:piContribution>Soundscape design for a multimedia art installation centred around a performance of pianist Yuja Wang.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Middle East Technical University</gtr:collaboratingOrganisation><gtr:country>Turkey, Republic of</gtr:country><gtr:department>Institute of Marine Sciences</gtr:department><gtr:description>METU</gtr:description><gtr:id>1F13BFE7-31DC-4342-99E9-48326AD3AE4A</gtr:id><gtr:impact>Joint publications. Development of the audio technology developed on the relevant EPSRC project and its deployment in art projects and installations.</gtr:impact><gtr:outcomeId>58b55715e9f0e2.97601595-1</gtr:outcomeId><gtr:partnerContribution>Expertise, intellectual.</gtr:partnerContribution><gtr:piContribution>Expertise, intellectual.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Surrey</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Institute of Sound Recording, University of Surrey</gtr:description><gtr:id>D2D4DD8E-CBFE-45D2-92E4-9EDF12039902</gtr:id><gtr:impact>Joint publications, grant proposal, and further development and deployment of the audio technology developed with the relevant EPSRC project in art projects and installations.</gtr:impact><gtr:outcomeId>58b555bacc4450.99480605-1</gtr:outcomeId><gtr:partnerContribution>Expertise, intellectual input.</gtr:partnerContribution><gtr:piContribution>Expertise, intellectual input.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Fidelio Arts Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Fidelio Arts</gtr:description><gtr:id>793A5674-7073-4628-8090-70CF10E7E5C0</gtr:id><gtr:impact>A multimedia art installation centred around performance of Yuja Wang, a pianist represented by Fidelio Arts, presently one of leading classical pianists.</gtr:impact><gtr:outcomeId>58c436717118c6.94012466-1</gtr:outcomeId><gtr:partnerContribution>Organisation and management of the project, multimedia art installation centred around a performance of pianist Yuja Wang, including time of the pianist.</gtr:partnerContribution><gtr:piContribution>Soundscape design for a multimedia art installation centred around a performance of pianist Yuja Wang.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Stanford University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Stanford</gtr:description><gtr:id>BD0E86EB-28ED-491E-A841-A91CCAE0118C</gtr:id><gtr:impact>A joint tutorial on multichannel surround systems, to be presented at ICASSP 2015.
A joint paper to be submitted to AT&amp;amp;T Transactions on Audio, Speech, and Language Processing.</gtr:impact><gtr:outcomeId>545cf681164155.18924398-1</gtr:outcomeId><gtr:partnerContribution>Collaborative research.</gtr:partnerContribution><gtr:piContribution>Collaborative research.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>2015 Summer Science Exhibition of the Royal Society.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>E1B56FC8-6F56-43E1-9E98-EFB25095755D</gtr:id><gtr:impact>Our scattering delay network (SDN) technology was showcased during the 2015 Summer Science Exhibition, the flagship event of the Royal Society for science communication to the public. The event, lasting a week, had an attendance of about 15,000 people, in addition to two gala nights with the fellows of the Royal Society. The demonstration was part of the stand &amp;quot;Sound Scape Interaction in a 3D World&amp;quot; organised by a consortium of european universities led by Imperial College London. The demonstration consisted of a rotating platform called &amp;quot;Sound Hunter&amp;quot;.
Visitors wore headphones while standing on the rotating platform and their task was to rotate the platform until a sound source auralised through the headphones was perceived to be in front of them. The SDN was used in cases where users choose to locate the sound source while in a reverberant room.</gtr:impact><gtr:outcomeId>56ddf84b8bb6e8.10131856</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://sse.royalsociety.org/2015</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>25800</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Cultural Institute Award</gtr:description><gtr:end>2017-06-02</gtr:end><gtr:fundingOrg>King's College London</gtr:fundingOrg><gtr:id>64E19DF5-B435-4696-A883-2676861382ED</gtr:id><gtr:outcomeId>58c440d95e4158.28132326</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>21059</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Travel grant</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/K034626/1</gtr:fundingRef><gtr:id>6A301731-0A05-428A-B4B2-E466383B0F45</gtr:id><gtr:outcomeId>58c43f8b70ae00.58381916</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>6000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Impact Acceleration Award</gtr:description><gtr:end>2016-06-02</gtr:end><gtr:fundingOrg>King's College London</gtr:fundingOrg><gtr:id>FED33DA6-C961-4B91-96C5-F0DD20EACAFB</gtr:id><gtr:outcomeId>58c4401acbdbd7.04824267</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-11-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The project was of the fundamental research nature, so it hasn't produced non-academic impact yet, but there is a high potential for impact outside of academe. It was was concerned with multichannel systems for perceptual sound field synthesis and reproduction. The field of spatial sound has so far been mainly geared towards creating special effects and providing a pleasing listening experience, rather than rooted in solid engineering or science. We established a scientific framework for the analysis and design of multichannel systems based on concise modelling of underlying psychoacoustic phenomena. That framework enabled the development of a new multichannel audio technology which improves over state-of-the-art systems in terms of accuracy and stability of the auditory perspective. We also developed a super-real-time software implementation for virtual reality applications, based on further psychoacoustic approximation, as well as a new class of underlying microphones. The initial motivation for this work was the recording of music performances that allow for convincing spatial reproduction and broadcasting. It turns out, however, that a much larger market for our technology lies in virtual reality applications, including gaming, as well as augmented reality as pursued by Google. Contemporary composers, too, are frequently attempting to place their sounds within a specific auditory landscape. Archaeologists, anthropologists, and art historians are trying to recreate the acoustics of important historical venues. We will therefore endeavour to engage in multidisciplinary collaborations involving ICT (i.e. spatial sound) and music, and the humanities, as well as reaching out to the rich cultural and entertainment milieu of London and engage with institutions like the Royal Opera House, Royal Festival Hall, and Tate Modern in joint projects involving sound recording, music, and multimedia performances and installations.

While commercial impact hasn't materialised yet, several companies have shown interest in our intellectual property arising from this project.</gtr:description><gtr:id>1BD6851F-ADC1-41AA-A69F-548538257AE9</gtr:id><gtr:impactTypes/><gtr:outcomeId>545cc625376031.59867444</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>The patent describes a method for emulating acoustic performance in a given venue using dry studio recordings.</gtr:description><gtr:grantRef>EP/F001142/1</gtr:grantRef><gtr:id>5769D9C2-8356-46C5-9D32-2D837E4FA514</gtr:id><gtr:impact>The invention provides a method for synthesis of sound field of desired enclosed spaces.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>r-6453305880.029973d6b1d40</gtr:outcomeId><gtr:patentId>WO2007/060443</gtr:patentId><gtr:protection>Patent granted</gtr:protection><gtr:title>Audio Signal Processing Method and System</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>A method for ultra-real time rendition of multichannel reverberation of acoustic spaces.</gtr:description><gtr:grantRef>EP/F001142/1</gtr:grantRef><gtr:id>A0BCE1AE-2C28-41D3-B3F5-C872A781419C</gtr:id><gtr:impact>No notable impacts yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>m-6935619131.6019937668ebd4</gtr:outcomeId><gtr:patentId>US2013202125</gtr:patentId><gtr:protection>Patent granted</gtr:protection><gtr:title>Electronic Device with Digital Reverberator and Method</gtr:title><gtr:yearProtectionGranted>2014</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput><gtr:intellectualPropertyOutput><gtr:description>The patent describes a class of microphone arrays that are designed so to capture cues needed for convincing perceptual sound field reconstruction.</gtr:description><gtr:grantRef>EP/F001142/1</gtr:grantRef><gtr:id>3D0EB183-12B2-4F4F-832E-DEC35C480C21</gtr:id><gtr:impact>No notable impacts yet.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>r-5177307273.4263793d6b1a66</gtr:outcomeId><gtr:patentId>US12/905,415</gtr:patentId><gtr:protection>Patent granted</gtr:protection><gtr:title>Microphone Array</gtr:title><gtr:yearProtectionGranted>2015</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>There are several key findings of the project:

1) The first scientific and systematic framework for the design of multichannel audio systems based on perceptual criteria.

2) A particular class of multichannel systems for recording and spatially convincing reproduction of acoustic performances.

3) A method for modelling the perception of locatendess of phantom sources created by multichannel systems.

4) A new class of high practical high order differential microphone.

5) A method of super-real-time rendition of perceptually convincing reverberation of acoustic spaces.

6) It has been demonstrated that non-coincident microphone arrays are capable of capturing song field cues needed for its spatially stable reconstruction.</gtr:description><gtr:exploitationPathways>Outcomes of this research are directly applicable to a wide range of multichannel audio technologies, from performance recording for record labels, sound production for film, and broadcasting, through gaming, virtual and augmented reality, to acoustics simulation in architectural design and microphone manufacture. This research lays down scientific foundations for research in the direction of perceptual sound field reconstruction using low-count multichannel systems. It further provides a new class of microphones which opens up possibilities for new developments in sound recording technologies.</gtr:exploitationPathways><gtr:id>1D962484-FE33-4B97-BD47-B0A7F9317495</gtr:id><gtr:outcomeId>r-6051406521.3980227760844a</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The iPhone app aims at delivering the auditory illusion of being in the middle of a virtual rectangular room. This is achieved by means of the scattering delay network (SDN) technology, together with binaural reproduction technique. The app is capable of simulating the acoustics of the room in real time thanks to the extremely low computational complexity of the SDN method, while at the same time delivering important perceptual cues in an accurate manner. The app uses the iPhone gyroscope in order to track the movement of the listener's head and adjusts the simulation accordingly.</gtr:description><gtr:id>497B74AC-1AED-40A9-82D6-9B2263955BBF</gtr:id><gtr:impact>The app was sent to several companies to spur their interest in commercial exploitation of the intellectual property arising from relevant EPSRC projects. Dolby has made several visit to King's College and is presently evaluating our technology.</gtr:impact><gtr:outcomeId>56ddf9b4b10ec0.29290886</gtr:outcomeId><gtr:title>SDN iPhone app</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>EB0357D3-48C8-4677-A077-550902BDA81F</gtr:id><gtr:title>Efficient Synthesis of Room Acoustics via Scattering Delay Networks</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25a83866e70c5df5959c53997f0305da"><gtr:id>25a83866e70c5df5959c53997f0305da</gtr:id><gtr:otherNames>De Sena E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f5943600a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06D1F137-EBAB-49DB-8F31-8804C4EBC09E</gtr:id><gtr:title>Analysis and Design of Multichannel Systems for Perceptual Sound Field Reconstruction</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25a83866e70c5df5959c53997f0305da"><gtr:id>25a83866e70c5df5959c53997f0305da</gtr:id><gtr:otherNames>De Sena E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>542ac0171d7bd4.51109587</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A87ED0AD-D5B6-43CD-A763-8C7F5BE89EC8</gtr:id><gtr:title>Machine Audition - Principles, Algorithms and Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da8f189d4021f3358be8234d63420acb"><gtr:id>da8f189d4021f3358be8234d63420acb</gtr:id><gtr:otherNames>G?nel B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>1615209190</gtr:isbn><gtr:outcomeId>r_871283400463dca73c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>527D2D08-F12D-498D-ADC1-C41903D14846</gtr:id><gtr:title>Allpass variable fractional delay filters by pole loci interpolation</gtr:title><gtr:parentPublicationTitle>Electronics Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9d434d39e9ae05fcf65560d46390041e"><gtr:id>9d434d39e9ae05fcf65560d46390041e</gtr:id><gtr:otherNames>Hacihabibog?lu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>542aba91078203.36959869</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>81DAA6F8-6B6D-4DFF-BAA2-64D37F71D456</gtr:id><gtr:title>Design of a Circular Microphone Array for Panoramic Audio Recording and Reproduction: Array Radius,</gtr:title><gtr:parentPublicationTitle>128th AES Convention</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f478b39532476004233c4c0873bff41"><gtr:id>9f478b39532476004233c4c0873bff41</gtr:id><gtr:otherNames> Enzo  De Sena (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_979274845313df0472</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C8781872-46B5-47A9-9CBC-B0BF713C3C9A</gtr:id><gtr:title>Stereophonic rendering of source distance using DWM-FDN artificial reverberators</gtr:title><gtr:parentPublicationTitle>128th Audio Engineering Society Convention 2010</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/87bc78b48d2917025efc6cf458ea8608"><gtr:id>87bc78b48d2917025efc6cf458ea8608</gtr:id><gtr:otherNames>Mat?-Cid S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>542ae7dc748be9.19745717</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EF577D17-2F09-4643-A7D4-6F07F114ED46</gtr:id><gtr:title>A generalized design method for directivity patterns of spherical microphone arrays</gtr:title><gtr:parentPublicationTitle>IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4619aab1c721d023e39b2435d8ce0e02"><gtr:id>4619aab1c721d023e39b2435d8ce0e02</gtr:id><gtr:otherNames>De Sena Enzo</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d0580582f9395e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>18DF5C55-01DF-4DC3-8293-E1C414EB7484</gtr:id><gtr:title>Simulation of Directional Microphones in Digital Waveguide Mesh-Based Models of Room Acoustics</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38284caf63d9387c109565e1f134718"><gtr:id>b38284caf63d9387c109565e1f134718</gtr:id><gtr:otherNames>Hacihabiboglu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d05c05cecaa60e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BB63EE4-D517-4E62-8588-F309FA806AC4</gtr:id><gtr:title>Perceptual Spatial Audio Recording, Simulation, and Rendering: An overview of spatial-audio techniques based on psychoacoustics</gtr:title><gtr:parentPublicationTitle>IEEE Signal Processing Magazine</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38284caf63d9387c109565e1f134718"><gtr:id>b38284caf63d9387c109565e1f134718</gtr:id><gtr:otherNames>Hacihabiboglu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58b553069d4493.69196963</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F4C6D426-900A-477F-8A6C-C222036A3956</gtr:id><gtr:title>Scattering Delay Network: an interactive reverberator for computer games</gtr:title><gtr:parentPublicationTitle>AES 41st International Conference: Audio for Games</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f478b39532476004233c4c0873bff41"><gtr:id>9f478b39532476004233c4c0873bff41</gtr:id><gtr:otherNames> Enzo  De Sena (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_739173684113df03be</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C49B4139-DA54-46F5-B40B-0BCF53532D1D</gtr:id><gtr:title>Frequency-Domain Scattering Delay Networks for Simulating Room Acoustics in Virtual Environments</gtr:title><gtr:parentPublicationTitle>Signal-Image Technology for Internet-Based Systems, SITIS 2011</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6495f3f6a99092956b3e456cc1c87657"><gtr:id>6495f3f6a99092956b3e456cc1c87657</gtr:id><gtr:otherNames>Hacihabiboglu Huseyin</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d05c05c5f43688</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>798ADAB7-7D78-4EA6-B94D-8223F4ED3074</gtr:id><gtr:title>Design of a circular microphone array for panoramic audio recording and reproduction: Microphone directivity</gtr:title><gtr:parentPublicationTitle>128th Audio Engineering Society Convention 2010</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f81cd19a9b7e0b79256f339da8ffeb35"><gtr:id>f81cd19a9b7e0b79256f339da8ffeb35</gtr:id><gtr:otherNames>Hacihabiboglu H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>542ae7dd2a0f59.02737713</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>323A4817-79B2-4075-9820-63CFEFFFE992</gtr:id><gtr:title>Multichannel Dereverberation Theorems and Robustness Issues</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0558cebba3db4d4b69906d41ca10b87a"><gtr:id>0558cebba3db4d4b69906d41ca10b87a</gtr:id><gtr:otherNames>Hacihabibouglu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d05c05cf17e9d6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B19C85EA-6B46-400C-AB51-89877384BE12</gtr:id><gtr:title>Perceptual evaluation of a circularly symmetric microphone array for panoramic recording of audio,</gtr:title><gtr:parentPublicationTitle>International Symposium on Ambisonics and Spherical Acoustics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f478b39532476004233c4c0873bff41"><gtr:id>9f478b39532476004233c4c0873bff41</gtr:id><gtr:otherNames> Enzo  De Sena (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_678853209213df051c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5BB5B2E5-E6E7-4301-8A84-1CC464581592</gtr:id><gtr:title>Panoramic recording and reproduction of multichannel audio using a circular microphone array</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b38284caf63d9387c109565e1f134718"><gtr:id>b38284caf63d9387c109565e1f134718</gtr:id><gtr:otherNames>Hacihabiboglu H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-3678-1</gtr:isbn><gtr:outcomeId>542ae7dcebeb62.53861312</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E38E01F3-40D2-4FD4-8CFF-2E138181164B</gtr:id><gtr:title>On the Design and Implementation of Higher Order Differential Microphones</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25a83866e70c5df5959c53997f0305da"><gtr:id>25a83866e70c5df5959c53997f0305da</gtr:id><gtr:otherNames>De Sena E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>542ac2cbb2ee54.83095428</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BFCE2EC1-99E5-4355-8315-3552443CB781</gtr:id><gtr:title>A computational model for the estimation of localisation uncertainty</gtr:title><gtr:parentPublicationTitle>IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/15db9b60faf1d4bc8f60510de3702b71"><gtr:id>15db9b60faf1d4bc8f60510de3702b71</gtr:id><gtr:otherNames>De Sena, E.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d058058358206d</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F001142/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>