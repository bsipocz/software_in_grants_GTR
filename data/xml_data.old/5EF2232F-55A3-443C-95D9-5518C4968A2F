<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D5EF406D-A320-44D9-9464-9D6BC3AD1649"><gtr:id>D5EF406D-A320-44D9-9464-9D6BC3AD1649</gtr:id><gtr:name>Nippon Telegraph and Telephone Corporation (NTT)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:department>Engineering</gtr:department><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5EF406D-A320-44D9-9464-9D6BC3AD1649"><gtr:id>D5EF406D-A320-44D9-9464-9D6BC3AD1649</gtr:id><gtr:name>Nippon Telegraph and Telephone Corporation (NTT)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/17C73C17-B084-44B6-B454-7416B4792EF3"><gtr:id>17C73C17-B084-44B6-B454-7416B4792EF3</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Gales</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI006583%2F1"><gtr:id>5EF2232F-55A3-443C-95D9-5518C4968A2F</gtr:id><gtr:title>Generative Kernels and Score Spaces for Classification of Speech</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I006583/1</gtr:grantReference><gtr:abstractText>The aim of this project is to significantly improve the performance of automatic speech recognition systems across a wide-range of environments, speakers and speaking styles. The performance of state-of-the-art speech recognition systems is often acceptable under fairly controlled conditions and where the levels of background noise are low. However for many realistic situations there can be high levels of background noise, for example in-car navigation, or widely ranging channel conditions and speaking styles, such as observed on YouTube-style data. This fragility of speech recognition systems is one of the primary reasons that speech recognition systems are not more widely deployed and used. It limits the possible domains in which speech can be reliably used, and increases the cost of developing applications as systems must be tuned to limit the impact of this fragility. This includes collecting domain specific data and significant tuning of the application itself.The vast majority of research for speech recognition has concentrated on improving the performance of hidden Markov model (HMM) based systems. HMMs are an example of a generative model and are currently used in state-of-the-art speech recognition systems. A wide number of approaches have been developed to improve the performance of these systems under speaker and noise changes. Despite these approaches, systems are not sufficiently robust to allow speech recognition systems to achieve the level of impact that the naturalness of the interface should allow. This project will combine the current generative models developed in the speech community with discriminative classifiers used in both the speech and machine learning communities. An important, novel, aspect of the proposed approach is that the generative models are used to define a score-space that can be used as features by the discriminative classifiers. This approach has a number of advantages. It is possible to use current state-of-the-art adaptation and robustness approaches to compensate the acoustic models for particular speakers and noise conditions. As well as enabling any advances in these approaches to be incorporated into the scheme, it is not necessary to develop approaches that adapt the discriminative classifiers to speakers, style and noise. One of the major problems with speech recognition is that variable length data sequences must be classified. Using generative models also allows the dynamic aspects of speech data to be handled without having to alter the discriminative classifier. The final advantage is the nature of the score-space obtained from the generative model. Generative models such as HMMs have underlying conditional independence assumptions that, whilst enabling them to efficiently represent data sequences, do not accurately represent the dependencies in data sequences such as speech. The score-space associated with a generative model does not have the same conditional independence assumptions as the original generative model. This allows more accurate modelling of the dependencies in the speech data.The combination of generative and discriminative classifiers will be investigated on two very difficult forms of data that current systems perform badly on. The first task is adverse environment recognition of speech. In these situations there are very high levels of background noise which causes severe degradation in system performance. Data of interest for this task will be specified in collaboration with Toshiba Research Europe Ltd. The second task of interest is large vocabulary speech recognition of data from a wide-range of speaking styles and conditions. Google has supplied transcribed data from YouTube to allow evaluation of systems on highly diverse data. The project will yield significant performance gains over current state-of-the-art approaches for both tasks.</gtr:abstractText><gtr:potentialImpactText>The growth of business based on speech-enabled technology has been slower than predicted. A major contributing factor to this slow growth is that speech recognition systems are still not sufficiently robust to changing background noise conditions, speaker-styles, and accents. This results in unacceptable performance for too many users. Furthermore the cost of development of these applications is large as data is typically collected for the specific target domain and the application tuned to reduce the impact of the current fragility of speech recognition systems. Any approach that yields significant improvements in robustness (to noise, speaker and domain changes) would therefore be of enormous direct benefit to the speech industry making many new applications of the technology feasible. A range of companies in the UK would benefit in this case from core speech technology providers, such as Autonomy and Toshiba Research Europe Ltd, to application providers, such as Telephonetics, Acuvoice, through to application designers, such as VoxGen and Edify. The companies, such as major airlines, banks and new media, who would like to make further use of speech recognition to reduce operating costs and enable new applications would also benefit. The outcome of this research will be shared in the first instance with providers of core speech recognition technology. The Speech Group at CUED has close collaborations with a number of UK and international speech companies including Toshiba Research Europe Ltd (TREL), Google and IBM. Data from existing collaborations with TREL and Google will be used to benchmark the technology created within this research project. This will allow the companies to easily identify technical advances over their existing technology. In addition to research publications including conference papers and technical reports, software implementations of the research outcomes will be made available. The software will be released as an extension to the existing HTK toolkit via the HTK website. This will enable broader industry to replicate the results on publicly available databases.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>392127</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Nippon Telegraph and Telephone Corporation (NTT)</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>NTT Research Collaboration</gtr:description><gtr:id>FEC105C8-A2BC-43A4-8433-C1910B220D56</gtr:id><gtr:impact>The outcome has been in the form of papers (conference papers and journal paper).

Currently negotiating longer term agreement with NTT. Initial discussion are for &amp;pound;20000 for collaborative research.</gtr:impact><gtr:outcomeId>5458b35096dea8.54708594-1</gtr:outcomeId><gtr:partnerContribution>Fully funded salary, bench fees and compute equipment.</gtr:partnerContribution><gtr:piContribution>Visitor for NTT for a year to the Speech Group at Cambridge University.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Generative Kernels and Score-Spaces for Classification of Speech: Progress Report II</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>05572750-60E0-4B9B-8E42-AF4F9E6D9F69</gtr:id><gtr:impact>Publication of the second milestone report for the project.

No direct impact. Report referenced in successful application for Google Research Award.</gtr:impact><gtr:outcomeId>5523a405643cc7.93346242</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Efficient Decoding with Generative Score-Spaces Using the Expectation Semiring</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>A1F67AB2-5008-4DAE-B041-7F4640533F15</gtr:id><gtr:impact>Description of efficient feature extraction for use in discriminative models.

No notable direct impact</gtr:impact><gtr:outcomeId>r_61552767330c05982c</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:presentationType>poster presentation</gtr:presentationType><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Structured Discriminative Models for Speech Recognition</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>7DF6E13B-B351-47A8-AA43-649828A9D028</gtr:id><gtr:impact>Keynote speech at International Symposium on Chinese Spoken Language Processing 2012.

Increased interest from colleagues in discriminative models and sequence kernels.</gtr:impact><gtr:outcomeId>r_33604991140c0d2c2c</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:presentationType>keynote/invited speaker</gtr:presentationType><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Structured Discriminative Models for Speech Recognition</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>CD54B9AB-6EC3-4D7C-9E1C-CB441D20118B</gtr:id><gtr:impact>This talk was related to an invitation to visit NTT CS Lab in Kyoto Japan after ICASSP 2012. An overview of discriminative models and the use of score-spaces derived from generative models was presented.

Initiated collaboration with NTT. Visitor to Cambridge, Dr Takuya Yoshioka, in 2013-2014.</gtr:impact><gtr:outcomeId>r-9535047532.5575540c0e3496</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Generative Kernels and Score-Spaces for Classification of Speech: Progress Report III</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>66E89685-23CB-4560-82CD-D3997EF048A5</gtr:id><gtr:impact>Final milestone report for project

No significant external impact. Formed basis of paper submission for ASRU 2015.</gtr:impact><gtr:outcomeId>55c1cffc885ca5.84706085</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html#publications</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Presentation at Google Visit</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>DE378908-0360-4459-A279-FA8991E808C0</gtr:id><gtr:impact>Discussion of collaboration opportunities with Google to continue research in this area.

Plans to visit Google Research in London</gtr:impact><gtr:outcomeId>55239487af9c67.27796164</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Generative Kernels and Score-Spaces for Classification of Speech: Progress Report</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5FE0DC3A-E380-48B4-A01F-58E3868E3954</gtr:id><gtr:impact>On-line publication for milestone report after year 1 of the project.

This is a milestone from the project and made available via the project web-page.</gtr:impact><gtr:outcomeId>5523a0def30b40.24154149</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html</gtr:url><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The exact word error for a lattice - Poster presentation Google</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>55051AD0-531A-4212-9F72-2271BD9FF0C3</gtr:id><gtr:impact>Increased interesting features from lattices uses semi-rings

Google research award obtained</gtr:impact><gtr:outcomeId>5458afbf681499.30719819</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UK Speech - Infinite Structured Support Vector Machines for Speech Recognition</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>232DFEAC-AA09-49A6-81BE-7FB753DAC13A</gtr:id><gtr:impact>Dissemination of information about non-parametric Bayesian classifiers

No notable impact</gtr:impact><gtr:outcomeId>5458b0da61bb44.46409104</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Monoids: efficient segmental features for speech recognition.</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FC1B0889-57D5-4275-B320-8719292EDC58</gtr:id><gtr:impact>Techinical report publication (referenced in subsequent papers). Note there was no tracking of the downloads of this paper.

No notable impact.</gtr:impact><gtr:outcomeId>553f5ba1cb7b93.06540535</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UK Speech - Annotating large lattices with the exact word error</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>52996815-DC25-43E0-988C-DF25ADC74EF0</gtr:id><gtr:impact>Informed UK Speech community of on-going research on the use of finite-state-atomatons in acoustic modelling for speech recognition. Talk prompted a series of questions and informal discussions after the presentation.

No significant impact to date,</gtr:impact><gtr:outcomeId>55c1cf384b2781.11014280</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Participants in your research and patient groups</gtr:primaryAudience><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Dirichlet Process Mixture of Experts Models in Speech Reognition</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>CA8094E8-AE91-45B3-9EEE-BB9A3BE5B52B</gtr:id><gtr:impact>Poster presentation on non-parametric Bayesian approaches for speech recognition.

This was the UK &amp;amp; IE speech meeting

No significant changes</gtr:impact><gtr:outcomeId>r_11589367320c0596e2</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:presentationType>poster presentation</gtr:presentationType><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>10000</gtr:amountPounds><gtr:country>Japan</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>NTT Collaboration Funding</gtr:description><gtr:end>2017-02-02</gtr:end><gtr:fundingOrg>Nippon Telegraph and Telephone Corporation (NTT)</gtr:fundingOrg><gtr:fundingRef>RG78437</gtr:fundingRef><gtr:id>8B1A83E6-16B9-41E8-9832-25D7E1B69DCE</gtr:id><gtr:outcomeId>58c14279234fe4.81835044</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2016-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>77283</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:department>Research at Google</gtr:department><gtr:description>Google Research Awards</gtr:description><gtr:fundingOrg>Google</gtr:fundingOrg><gtr:id>B07D2ED1-5209-4E75-8D65-230853928A21</gtr:id><gtr:outcomeId>5453e63d8bcd88.57786229</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2014-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This research has demonstrated that under challenging speech recognition environments extracting rich features from the audio yields performance gains. Furthermore an efficient algorithm for efficiently extracting these rich features has been proposed, as is currently being evaluated. This work will continue under a Google funded project</gtr:description><gtr:exploitationPathways>The features that can be extracted can be incorporated into a range of classifiers, including those based on deep-learning. This is being investigated under a Google Research Award,</gtr:exploitationPathways><gtr:id>5BBD3574-0435-4ABA-BA5E-71226F6822AA</gtr:id><gtr:outcomeId>545cbad90648e5.59633965</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This software is a Python program that implements a non-parametric method to, given speech and noise distributions and a mismatch function, compute the corrupted speech likelihood. It uses sampling and is exact in the limit. It therefore gives a theoretical bound for model compensation.</gtr:description><gtr:id>DF9419F0-CCAE-496D-B3F2-6CAADEB9CBD6</gtr:id><gtr:impact>This software was used for the results described in:

R. C. van Dalen and M. J. F. Gales (2013). &amp;quot;Importance Sampling to Compute Likelihoods of Noise-Corrupted Speech.&amp;quot; In Computer Speech and Language 27 (1), pp. 322-349.</gtr:impact><gtr:outcomeId>553f427a9292a2.92097733</gtr:outcomeId><gtr:title>Cross-entropy for model compensation - Python program</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html#source</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The Flipsta library deals with finite-state automata. These are concise representations of, say, many word sequences, with probabilities attached to them. Many algorithms in text and speech processing can be expressed in terms of a handful of automaton operations.</gtr:description><gtr:id>8F094D84-82A4-494E-9C9F-A644820E5A6D</gtr:id><gtr:impact>No impact to date (just released)</gtr:impact><gtr:outcomeId>553f6f5a02cd68.76790758</gtr:outcomeId><gtr:title>Flipsta library: manipulate finite-state automata in C++ and Python.</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://mi.eng.cam.ac.uk/~mjfg/Kernel/index.html#source</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>84927B46-40CC-4A92-B86D-2BC0739A308A</gtr:id><gtr:title>System Combination with Log-Linear Models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1c4df4241d0e1193d5aeffe9e19aab3d"><gtr:id>1c4df4241d0e1193d5aeffe9e19aab3d</gtr:id><gtr:otherNames>Yang J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5785123ec8b890.66358512</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EEB14C88-24C3-4EA3-9FF4-32DE532BF88A</gtr:id><gtr:title>STRUCTURED DISCRIMINATIVE MODELS USING DEEP NEURAL-NETWORK FEATURES</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d37c963e627499f2b6c39b3707f0c75d"><gtr:id>d37c963e627499f2b6c39b3707f0c75d</gtr:id><gtr:otherNames>van Dalen R C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56965bcac4c4c7.24278089</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92437851-4870-41B5-A608-F3119D526054</gtr:id><gtr:title>Infinite Support Vector Machines in Speech Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0196bca6f95642e992c5ae51008a5245"><gtr:id>0196bca6f95642e992c5ae51008a5245</gtr:id><gtr:otherNames>Jingzhou Yang (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_654845164513e55520</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2413CE8-7DA6-4A72-B083-EF3DD41C5466</gtr:id><gtr:title>Efficient decoding with continuous rational kernels using the expectation semiring</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb4afb7a0b822b6e40fd901c65174326"><gtr:id>cb4afb7a0b822b6e40fd901c65174326</gtr:id><gtr:otherNames>Mark Gales (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>r_683742111463df7912</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F483825B-D466-416C-AAC0-853063F549D6</gtr:id><gtr:title>Infinite Structured Support Vector Machines in Speech Recognition</gtr:title><gtr:parentPublicationTitle>ICASSP</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1c4df4241d0e1193d5aeffe9e19aab3d"><gtr:id>1c4df4241d0e1193d5aeffe9e19aab3d</gtr:id><gtr:otherNames>Yang J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5458ac52e19157.83479930</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>934EECC7-25BC-4586-9C16-E4208932C93C</gtr:id><gtr:title>Efficient decoding with generative score-spaces using the expectation semiring</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f6d5bcc016e2109ca86a62ee0daa691b"><gtr:id>f6d5bcc016e2109ca86a62ee0daa691b</gtr:id><gtr:otherNames>van Dalen R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53d05805837e49a0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>197DB64B-8C77-481C-8E12-278A7CC33E75</gtr:id><gtr:title>A variational perspective on noise-robust speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f6d5bcc016e2109ca86a62ee0daa691b"><gtr:id>f6d5bcc016e2109ca86a62ee0daa691b</gtr:id><gtr:otherNames>van Dalen R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4673-0365-1</gtr:isbn><gtr:outcomeId>doi_53d0560566fbe263</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>048E19C7-1244-4746-8BA0-AE49A3FA5019</gtr:id><gtr:title>System combination with log-linear models</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1c4df4241d0e1193d5aeffe9e19aab3d"><gtr:id>1c4df4241d0e1193d5aeffe9e19aab3d</gtr:id><gtr:otherNames>Yang J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d566964be87.47766279</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D86DDC63-CCE2-40A6-8422-BEE30CEB6788</gtr:id><gtr:title>Annotating large lattices with the exact word error</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d37c963e627499f2b6c39b3707f0c75d"><gtr:id>d37c963e627499f2b6c39b3707f0c75d</gtr:id><gtr:otherNames>van Dalen R C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>55c1cddcb1a115.06732603</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I006583/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>