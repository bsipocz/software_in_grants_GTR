<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C0805014-A6FD-41C1-81BE-B94AE4139F25"><gtr:id>C0805014-A6FD-41C1-81BE-B94AE4139F25</gtr:id><gtr:name>Max Planck Society</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C0805014-A6FD-41C1-81BE-B94AE4139F25"><gtr:id>C0805014-A6FD-41C1-81BE-B94AE4139F25</gtr:id><gtr:name>Max Planck Society</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/430A04B6-0F63-43BA-B78B-8DD4147CAF43"><gtr:id>430A04B6-0F63-43BA-B78B-8DD4147CAF43</gtr:id><gtr:name>University of Paris South 11</gtr:name><gtr:address><gtr:line1>Batiment 300</gtr:line1><gtr:line4>Orsay cedex</gtr:line4><gtr:line5>F-91405</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>France</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4E8E1501-62B7-4F7D-8E1B-12B9DE37117C"><gtr:id>4E8E1501-62B7-4F7D-8E1B-12B9DE37117C</gtr:id><gtr:name>Wageningen University</gtr:name><gtr:address><gtr:line1>Droevendaalsesteeg 4</gtr:line1><gtr:postCode>NL-6708 PB</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>Netherlands</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B25E8DB2-C5FE-4179-AE1D-7EE91BDCFA92"><gtr:id>B25E8DB2-C5FE-4179-AE1D-7EE91BDCFA92</gtr:id><gtr:firstName>Daniel</gtr:firstName><gtr:otherNames>Frank</gtr:otherNames><gtr:surname>Stowell</gtr:surname><gtr:roles><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FL020505%2F1"><gtr:id>A8259FE5-95C4-4D2D-9008-22FB4D019FAA</gtr:id><gtr:title>Structured machine listening for soundscapes with multiple birds</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/L020505/1</gtr:grantReference><gtr:abstractText>In this Early Career fellowship I will establish a world-leading capability in automatic inference about songbird communications via novel &amp;quot;machine listening&amp;quot; methods, working collaboratively with experts in machine listening but also experts in bird behaviour and communication. Automatic analysis has already shown benefit to researchers in efficiently characterising recorded bird sounds, but there are still many limitations in applicability, such as when many birds sing together. The techniques developed will specifically be designed to handle noisy multi-source audio recordings, and to infer not just the presence of birds but the structure of the signals and the interactions between them. Such methods will be a leap beyond the current state of the art in bioacoustics, allowing researchers to study not just sounds recorded in the lab under controlled conditions, but also field recordings and archive recordings found in public audio archives.

I will develop my techniques through specific application case studies. First through collaboration with David Clayton, an international expert on zebra finch behaviour and genetics, who recently moved his lab to my proposed host institution. The zebra finch is an important &amp;quot;model organism&amp;quot; in biology, because its genome is fully sequenced and it is a useful bird for probing aspects of songbird vocal development. I will collaborate with the Clayton lab to develop methods for automatically inferring the social interactions implicit in audio recordings of zebra finch colonies. Second, I will conduct international research visits to collaborate with other research groups who analyse bird sounds and bird social interactions. Third, I will study the case of automatically detecting bird activity in arbitrary sound archives, such as the soundscape recordings held by the British Library Sound Archive.

Importantly, not only will I apply modern signal processing and machine learning techniques, but I will also develop new techniques inspired by this application area. This fellowship is not about contributing from one field to another, but about building up UK research strength in this cross-discplinary research topic. In order to make the most of this possibility, I will host research workshops and an open data contest to serve as focal points for research attention, and I will also conduct a public engagement initiative to engage the widest possible enthusiasm for this exciting field of possibility.

This fellowship directly aligns with the &amp;quot;Working Together&amp;quot; priority, which is EPSRC's current overriding priority for ICT fellowships.</gtr:abstractText><gtr:potentialImpactText>The prime beneficiaries outside my immediate field will be in research fields benefitting from the structured analysis of animal sounds and interactions. For example the improved techniques in zebra finch analysis will complement ongoing research into songbird genetics and individual differences, or research into conversational interactions in linguistics: the availability of more structured naturalistic data about animal communication could provide stronger empirical foundation to considerations of the evolution of communication systems. (This impact overlaps to some extent with that described in the Academic Beneficiaries section.)

The availability of these sound analysis techniques is also of interest to wildlife monitoring organisations such as the British Trust for Ornithology (BTO). They largely use manual surveying by volunteers and professionals to quantify the distributions of species: however, if high-quality automatic analysis were available their work could be made more efficient. Current academic and commercial software (examples include Raven, XBAT, Seewave, Praat, Sound Analysis Pro) allow users to inspect and detect bird sounds but are unable to analyse communication networks, nor can they use models of communication interactions to ensure high-quality detection. Analysing not just the presence of bird song and calls, but the networks of interaction between them, could be used as an indicator of the population health, reflecting issues such as habitat fragmentation which can impact the viability of a bird population. Downstream, detailed analysis of animal sound can thus form a strong evidence base for ecological policy decisions.

The application to audio archives shows another direct route to impact. Large audio collections such as those in the British Library Sound Archive are highly valuable to society, yet a lot of their value remains locked away because there is very little metadata associated that would facilitate different types of query. This research will directly enable the unlocking of some of this value, helping people to discover the presence of birds in large audio archives which may not be annotated for their bird sounds, indeed may have been collected for entirely different reasons.

The fellowship will also have an impact on the public understanding of bird sounds, bird social interactions, and signal processing and machine learning. These will be explicitly encouraged through the public engagement activities: through engaging talks, articles and exhibits I will aim, not to place the technology between the public and the birds, but to enchant the public with both the wonders of technology and the wonders of bird vocal communication.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-04-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2014-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>506360</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>Collaboration with composer/roboticist Sarah Angliss, to create a musical score to render the sound of birdsong through her robotic bells (carillon).
First shown at Listening in the Wild 2015 workshop.
Second shown at &amp;quot;SoundCamp&amp;quot; in a park in London, on International Dawn Chorus Day.</gtr:description><gtr:id>CBBDA851-88E6-4032-AFDB-C594AA87B9D1</gtr:id><gtr:impact>Public engagement with sound, computation and birds through a novel medium. Reached approx 100 people in public park.</gtr:impact><gtr:outcomeId>58b8318eb5cc39.96907043</gtr:outcomeId><gtr:title>Robotic bells birdsong</gtr:title><gtr:type>Artefact (including digital)</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Max Planck Society</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:department>Max Planck Institute for Ornithology</gtr:department><gtr:description>MPIO Seewiesen</gtr:description><gtr:id>631B201B-400A-4AC9-BE75-DCFBEF4E79EE</gtr:id><gtr:impact>2 journal articles and 1 peer-reviewed conference paper.

Collaboration is multi-disciplinary, across computer science and animal behaviour / ornithology.</gtr:impact><gtr:outcomeId>58a6e0aa3552e6.20545114-1</gtr:outcomeId><gtr:partnerContribution>Data, collaboration time, hosting research visit.</gtr:partnerContribution><gtr:piContribution>Data, collaboration time, hosting research visit, and novel analysis methodology</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Queen Mary University of London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Clayton Lab zebra finch recordings</gtr:description><gtr:id>051DF7FA-BC0C-4ACA-B703-C6306A84DB3C</gtr:id><gtr:impact>One journal publication, introducing and evaluating a new method to animal communication analysis.
One conference paper, on a method for detecting overlapping audio events.
Dataset of audio recordings and annotations.</gtr:impact><gtr:outcomeId>5458b632361378.51372525-1</gtr:outcomeId><gtr:partnerContribution>Provided access to zebra finch facility, advice on study design, and practical support in setting up the recording sessions.</gtr:partnerContribution><gtr:piContribution>Provided recording equipment, my time for running the recording sessions, and paid annotator time to annotate the data.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Media coverage of automatic bird classification results (Summer 2014)</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D97D6EB9-F2BC-4FCF-8E72-7DFC2D4ECBE9</gtr:id><gtr:impact>Issued press release, participated in media interviews (live on-air discussion on Irish national radio RTE1; print interviews for BBC and Science Magazine); plus secondary press resulting from those.



Science
Computer becomes a bird enthusiast
By Kelly Servick
http://news.sciencemag.org/plants-animals/2014/07/computer-becomes-bird-enthusiast
 Tweeted at least 40 times

BBC
Software can decode bird songs
By Claire Marshall BBC environment correspondent 
http://www.bbc.co.uk/news/science-environment-28358123
 Tweeted at least 519 times

The Sun
A little bird told me..

Europa Press
Un 'shazam' para identificar qu&amp;eacute; p&amp;aacute;jaro est&amp;aacute; cantando
http://www.europapress.es/ciencia/laboratorio/noticia-shazam-identificar-pajaro-cantando-20140717173434.html
 Tweeted at least 105 times

El Economista
Un 'shazam' para identificar qu&amp;eacute; p&amp;aacute;jaro est&amp;aacute; cantando
http://ecodiario.eleconomista.es/ciencia/noticias/5949497/07/14/Un-shazam-para-identificar-que-pajaro-esta-cantando.html
 Tweeted at least 22 times

Live Science
'Voice Recognition' System for Birds Can Tell Two Chirps Apart
http://www.livescience.com/46840-bird-songs-decoded.html
 Tweeted at least 26 times

Science 2.0
Birdsongs Decoded
http://www.science20.com/news_articles/birdsongs_decoded-140731
 Tweeted at least 2 times


I have received various email/twitter contacts, both from members of the public and from academics/industry enquiring about the state of the art and possible future deployments, spinouts, etc.</gtr:impact><gtr:outcomeId>5458b469b6deb6.91010155</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/news/science-environment-28358123</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Conversation article</gtr:description><gtr:form>Engagement focused website, blog or social media channel</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FD015A96-183A-4A16-8054-400A798F001A</gtr:id><gtr:impact>Wrote article published in The Conversation long-form news website. Received over 100 Twitter shares, over 300 Facebook shares, and more:
https://theconversation.com/we-made-an-app-to-identify-bird-sounds-and-learned-something-surprising-about-people-65742</gtr:impact><gtr:outcomeId>58a6e1c0909e39.39681518</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>https://theconversation.com/we-made-an-app-to-identify-bird-sounds-and-learned-something-surprising-about-people-65742</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>BBC Radio 4 - Costing The Earth - Acoustic Ecology</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>63C9B675-CE14-409D-A493-95A73B9FD24E</gtr:id><gtr:impact>2016-03 BBC Radio 4 &amp;quot;Costing The Earth&amp;quot; programme, with a feature interview with me about our &amp;quot;Warblr&amp;quot; birdsong app, and sound recognition</gtr:impact><gtr:outcomeId>58a6e2538ebaf0.69217921</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/b071tgby</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Media coverage of Warblr app kickstarter and launch</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>733BF607-ABB2-4CFC-9C1A-BB50D343B5CC</gtr:id><gtr:impact>While launching the Warblr app, which is both a spinout company and a citizen-science data gathering initiative, we were featured in many national and international press outlets, through TV/radio/newspaper interviews as well as second-hand coverage. Warblr has been featured across the BBC (including 5 Live, BBC London News, Radio 4, and the BBC News homepage), on Sky News, in print and online through newspapers such as The Telegraph, The Times, The Sun, The Guardian, The Metro and The Daily Mail, and in publications such as Stylist, Shortlist, Wired, Stuff.TV and Engadget.

The project has gained support from the likes of Stephen Fry, Chris Packham, the Urban Birder and the Royal Society of Arts (RSA), as well as thousands of tweets, posts, mentions and shares across social media, blogs and forums.

The Warblr team have also spoken at conferences including Digital Shoreditch, London National Park, UnLtd Living It Festival, and Stylist Live.

Warblr has won a Queen Mary University of London Public Engagement Award, and was shortlisted for the TechCityinsider's TechCities awards and the IAB (Interactive Advertising Bureau) Creative Showcase. Warblr is one of TechRadar's &amp;quot;Best iPhone apps of 2015&amp;quot; and The Next Web's &amp;quot;Apps of the year&amp;quot;.</gtr:impact><gtr:outcomeId>56a88e4a9afd82.01265132</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://warblr.net/</gtr:url><gtr:year>2015,2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>10000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>QMUL Innovation Fund</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>Queen Mary University of London</gtr:fundingOrg><gtr:id>90A1151F-5C29-4B22-B186-E29E9708B95A</gtr:id><gtr:outcomeId>545266b11076a4.34942707</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-07-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>10000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>QMUL Innovation Fund Supplementary Award</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>Queen Mary University of London</gtr:fundingOrg><gtr:id>57D2340A-2E0C-43DF-9F21-B2F0143A809E</gtr:id><gtr:outcomeId>56a89073bf6ff9.81936995</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-07-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Warblr bird recognition app: Dan, together with an external business partner, secured a &amp;pound;10K grant from QMUL's EPSRC Innovation Fund in summer 2014, which allowed them to work with developers to build a social enterprise involving the British public in a citizen science project to identify and collect bird sound recordings. Following that, the Warblr team launched a Kickstarter campaign, founded a spinout company, and in Summer 2015 launched the smartphone app. It has amassed over 5,000 paying users. To date, Warblr has had more than 45,000 submissions to its database, and an average of 80 submissions per day.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>A278CC4E-7CC5-4D65-92EE-23993E8C7F9A</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545268f5153a20.66447545</gtr:outcomeId><gtr:sector>Environment</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Annotated dataset of audio clips from the Warblr birdsong app. Annotations indicate the presence/absence of birds.</gtr:description><gtr:id>D3355E0A-BCF4-41FF-B773-7D508051D7B0</gtr:id><gtr:impact>Facilitated the &amp;quot;Bird Audio Detection challenge&amp;quot;, which directly stimulated 30 research teams from around the world to develop and test new algorithms for detecting bird sounds.</gtr:impact><gtr:outcomeId>58a6e45d60ff86.67146446</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>warblrb10k</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Annotated dataset of audio clips from the &amp;quot;freefield1010&amp;quot; collection, sourced from Freesound. Annotations indicate the presence/absence of birds.</gtr:description><gtr:id>4DE572AB-703D-4CCB-93EB-62CB1DCE3CD0</gtr:id><gtr:impact>Facilitated the &amp;quot;Bird Audio Detection challenge&amp;quot;, which directly stimulated 30 research teams from around the world to develop and test new algorithms for detecting bird sounds.</gtr:impact><gtr:outcomeId>58a6e4558dd091.73523095</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>freefield1010bird</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>7C6CD16D-1C13-4826-BC85-F1C93EF03974</gtr:id><gtr:title>Detailed temporal structure of communication networks in groups of songbirds</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4193fed805f732438d2edd0ab039c35e"><gtr:id>4193fed805f732438d2edd0ab039c35e</gtr:id><gtr:otherNames>Stowell Dan</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56a886297903f8.15276040</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9B306881-6525-46D0-B106-9C545A102934</gtr:id><gtr:title>On-Bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a8709fb293811.20426934</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EA902EBC-54F0-43A6-AE18-E1D295247031</gtr:id><gtr:title>Acoustic event detection for multiple overlapping similar sources</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f983abfee5f1c8930980aa3371fd098b"><gtr:id>f983abfee5f1c8930980aa3371fd098b</gtr:id><gtr:otherNames>Stowell, D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a8898bebbe87.68095941</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3642F5A8-278C-4D1A-909B-8FC241386BB1</gtr:id><gtr:title>Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning.</gtr:title><gtr:parentPublicationTitle>PeerJ</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545265cabfe8a5.50879208</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B4CB23A8-23D5-4B22-A371-12A43D2822CF</gtr:id><gtr:title>Computational Analysis of Sound Scenes and Events</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d720457c66f1666042ea2bbbcbcf349e"><gtr:id>d720457c66f1666042ea2bbbcbcf349e</gtr:id><gtr:otherNames>Benetos E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a8709fb8951d9.04805016</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C54CE077-6914-4C08-ADF1-ADAE04BDF127</gtr:id><gtr:title>A Generative Model for Natural Sounds Based on Latent Force Modelling</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd7e3084152ad8afcaba04f8acc98a5e"><gtr:id>bd7e3084152ad8afcaba04f8acc98a5e</gtr:id><gtr:otherNames>Wilkinson William J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a870a6c912667.64826862</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6091FADA-142D-49AE-9EF1-F08C85AA0ADA</gtr:id><gtr:title>Computational Analysis of Sound Scenes and Events</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a8709fb57a221.35160343</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96D781B5-5C2F-40FF-90FF-EBB01EAA083C</gtr:id><gtr:title>Efficient Learning of Harmonic Priors for Pitch Detection in Polyphonic Music</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8f7d5d75f11987da4de6405c3d3331a6"><gtr:id>8f7d5d75f11987da4de6405c3d3331a6</gtr:id><gtr:otherNames>Alvarado Pablo A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a870aad5bcbc1.98569970</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>264C87DC-422B-45B8-B95E-05172F4CAE60</gtr:id><gtr:title>Deductive Refinement of Species Labelling in Weakly Labelled Birdsong Recordings</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6e17d5ad4799c6f91a334bdc55347daf"><gtr:id>6e17d5ad4799c6f91a334bdc55347daf</gtr:id><gtr:otherNames>Morfi V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58b82f4e8be8c0.85885397</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C6FAE9F2-6A0B-4DB4-AD8F-41EA59E33C3E</gtr:id><gtr:title>Gaussian processes for music audio modelling and content analysis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/259b55d4cae5217e11f3a72fd922d9af"><gtr:id>259b55d4cae5217e11f3a72fd922d9af</gtr:id><gtr:otherNames>Alvarado P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a6e6cb6ef4c4.94144643</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>22991D4B-57CE-4513-9CF1-999431958CAC</gtr:id><gtr:title>On-bird Sound Recordings: Automatic Acoustic Recognition of Activities and Contexts</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4193fed805f732438d2edd0ab039c35e"><gtr:id>4193fed805f732438d2edd0ab039c35e</gtr:id><gtr:otherNames>Stowell Dan</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a6e762b31c96.10921202</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7AB6D1F6-81BC-4FF1-865C-88602FB89FE4</gtr:id><gtr:title>Detection and Classification of Acoustic Scenes and Events</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a8866e4a51f1.31645128</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>376E789C-9303-4D73-BD64-2665D0134859</gtr:id><gtr:title>Bird detection in audio: A survey and a challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58a6e6cb4bef58.89129732</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FC9ABB2C-B90D-4096-9A97-43A470C6C400</gtr:id><gtr:title>Detailed temporal structure of communication networks in groups of songbirds.</gtr:title><gtr:parentPublicationTitle>Journal of the Royal Society, Interface</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e643022f9b0ced350a17300c3f08bb4"><gtr:id>3e643022f9b0ced350a17300c3f08bb4</gtr:id><gtr:otherNames>Stowell D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1742-5662</gtr:issn><gtr:outcomeId>585d366800e3e2.75893695</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61A9586A-5620-45AA-98F3-C81E8C459F94</gtr:id><gtr:title>Denoising without access to clean data using a partitioned autoencoder</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4193fed805f732438d2edd0ab039c35e"><gtr:id>4193fed805f732438d2edd0ab039c35e</gtr:id><gtr:otherNames>Stowell Dan</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56a88629c59788.58285161</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/L020505/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>70</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>790AD28C-6380-4025-83C2-6881B93C4602</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Animal behaviour</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>