<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/44160F04-5CBF-4E8E-A6C6-C0EF61A5865C"><gtr:id>44160F04-5CBF-4E8E-A6C6-C0EF61A5865C</gtr:id><gtr:name>Lancaster University</gtr:name><gtr:department>Lancaster Inst for the Contemporary Arts</gtr:department><gtr:address><gtr:line1>University House</gtr:line1><gtr:line4>Lancaster</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>LA1 4YW</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/44160F04-5CBF-4E8E-A6C6-C0EF61A5865C"><gtr:id>44160F04-5CBF-4E8E-A6C6-C0EF61A5865C</gtr:id><gtr:name>Lancaster University</gtr:name><gtr:address><gtr:line1>University House</gtr:line1><gtr:line4>Lancaster</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>LA1 4YW</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A4BF5600-8ACF-4B85-ADE0-52862EEF317E"><gtr:id>A4BF5600-8ACF-4B85-ADE0-52862EEF317E</gtr:id><gtr:firstName>Kia</gtr:firstName><gtr:surname>Ng</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B1BCD3E5-0FB9-40B9-9AC8-46361AA7471F"><gtr:id>B1BCD3E5-0FB9-40B9-9AC8-46361AA7471F</gtr:id><gtr:firstName>Alex</gtr:firstName><gtr:surname>McLean</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B88C5D25-0175-4636-A1D4-ECB466369769"><gtr:id>B88C5D25-0175-4636-A1D4-ECB466369769</gtr:id><gtr:firstName>Alan</gtr:firstName><gtr:otherNames>Alexander</gtr:otherNames><gtr:surname>Marsden</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=AH%2FL009870%2F1"><gtr:id>D4A55F5E-807C-469D-BA4E-5075BDE3D050</gtr:id><gtr:title>Optical Music Recognition from Multiple Sources</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/L009870/1</gtr:grantReference><gtr:abstractText>Digital images of scores are now available in very large numbers from services such as IMSLP (International Music Scores Library Project), which has images of about a quarter of a million scores of over 70,000 pieces of music. Most of this information is opaque to computers, because a score image is a picture rather than a source of musical information. To get at the musical information, the score needs to be 'read'. While software to read digital images of words, turning them into encoded text (Optical Character Recognition, OCR), is now common and underlies such services as Google Books, the equivalent task for music (Optical Music Recognition, OMR) has proven to be a very difficult. The accuracy of the results is often low, and some researchers have reported that the time taken to find and correct errors is so great as to make it just as quick to enter the data by hand. In other cases, considerable time is required in training OMR software to get good results from a particular source.

This project aims to turn the problem of large quantities of inaccessible data into an advantage, and make use of the quantity of information available to improve the accuracy of OMR. Multiple images of the score of a piece of music are often available, and to use more than one image increases the information available for the OMR task. This project will take two approaches to improving OMR through the use of multiple sources. The first, post-processing, approach will result in software to take the outputs of multiple runs of OMR software on different sources for the same piece of music, and combine these outputs into a single, more accurate, representation of the piece. The second approach will use multiple information within the OMR software (adapting existing open-source software) to improve the accuracy of recognition. A part of one score which is difficult to read will become easier if information can be used from part of another score which represents the same part of the piece.

The outcome will be new techniques and software modules for OMR (both post-processing of results and new multi-input OMR software), protocols for the use of OMR with multiple sources, and information about the levels of accuracy which can be expected from OMR software.</gtr:abstractText><gtr:potentialImpactText>There is enormous public interest in music, and while much of this is directed at music which, like much popular music, does not rely heavily on scores, there remains substantial interest in music for which the score is central. Making Music, an umbrella group for voluntary music in the UK, has over 3,000 member organisations. A survey in 2008 in the USA found that 3.1% of the adult population was involved in performing or creating classical music and 5.2% in choirs or choruses (2008 Survey of Public Participation in the Arts, National Endowment for the Arts, Washington DC, 2009, p.44). Access to scores and the information in scores is vital for these activities. Reliable and widespread OMR has the potential to transform access to and use of scores in the same way that OCR has transformed access to books. Peachnote.com already gives some flavour of what can be achieved by allowing scores on IMSLP to be searched (to some degree) by a melodic sequence of pitches. Better OMR would both allow this to be more accurate and allow other kinds of searching. A choir director who knows that the sopranos can barely sing above a G but who has some basses capable of singing very low notes could search for music by the ranges of the voices, for example.

There are similar potential benefits for music education. At the moment a teacher who wants to give an example of a particular contrapuntal pattern or procedure needs to know of examples in advance. OMR on a large dataset such as IMSLP would allow examples to be found by search. Students will also benefit from being able to search for music by content.

New ways of work will be facilitated in the music industry. It is common practice now for music for video and film to be selected from existing sources, either 'production music' (pre-recorded short sequences intended to suit particular circumstances, such as 'Scottish open road') or existing compositions. (Newly composed music is typically only used for the biggest productions, for reasons of cost.) This has led to the repeated use of a small number of pieces of music, and the prevalence of low-quality low-impact 'canned' music. The ability to search scores by content will give easier access to a wider range of music, and hopefully lead to better film and video music.

In the creative field, access to scores by content could lead to new ways of putting together pieces, perhaps in a kind of 'score mashup' by analogy with audio and video mashups made from selecting sounds and images by content. (See for example the research on 'audio mosaicing', special issue of Journal of New Music Research, 2006.)

We make a case above that the access to large datasets, which accurate OMR would facilitate, will lead to better musicology. The consequences of this will feed through to impact for the wider community also. We can expect an indirect impact of this research to be better music education and better wider understanding of music.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2014-01-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>77811</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Optical music recognition (OMR) is the process of reading a score by a computer to put the information it contains into a machine-readable form such as MusicXML. By using several editions of a score, plus both parts and score when available, and by combining the outputs of four different pieces of OMR software, we have been able to approximate halve the number of errors in pitch and rhythm typically made by such software. However, a substantial number of errors remain (accuracy is typically 85-95%) and we believe that therefore a new approach to OMR is required.</gtr:description><gtr:exploitationPathways>Our improvements will be useful for musicologists and all those who want to enter music into a computer from a score. Our results suggest that the time necessary for this will be approximately halved. However, the accuracy of the results remains low, and we believe our findings indicate that a new approach to OMR is required.</gtr:exploitationPathways><gtr:id>B6DC9D66-D88E-4D9A-94F9-A01F512E133E</gtr:id><gtr:outcomeId>546a2daf6c10e4.09003917</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This software achieves more accurate results from optical music recognition software by combining the outputs from several recognition engines and several sources. The software includes components to pre-process scans of musical scores, to submit those to multiple optical-music-recognition engines, and to combine the resulting MusicXML files to produce a single, more accurate, result.</gtr:description><gtr:id>8E0F0CD5-CCF1-4305-A353-DC24725D743A</gtr:id><gtr:impact>Impacts are pending.</gtr:impact><gtr:outcomeId>56e004fc8bd624.19331460</gtr:outcomeId><gtr:title>MultiOMR</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/MultiOMR</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>7C7D2D46-CD1F-4CE8-9D48-EB8869D3DBA7</gtr:id><gtr:title>Improving optical music recognition by combining outputs from multiple sources</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bc147f7afcb5f498e24fe20cf2c60dd2"><gtr:id>bc147f7afcb5f498e24fe20cf2c60dd2</gtr:id><gtr:otherNames>Padilla, V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dfff942b33a7.39877564</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">AH/L009870/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>47D79871-3D9E-42F7-83E1-354D081901C7</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Classical Music</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>120E89AC-386D-4E25-9417-A3FE4D6FE83A</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musicology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>