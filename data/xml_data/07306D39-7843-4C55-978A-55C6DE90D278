<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/70F64E36-363A-41BF-A5A5-32B7C555A691"><gtr:id>70F64E36-363A-41BF-A5A5-32B7C555A691</gtr:id><gtr:name>Innsbruck University</gtr:name><gtr:address><gtr:line1>Universitaet Innsbruck</gtr:line1><gtr:line2>Technikerstra?e 13</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Austria</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46B41008-0EB4-4E28-BBFB-E98366999EC5"><gtr:id>46B41008-0EB4-4E28-BBFB-E98366999EC5</gtr:id><gtr:name>Durham University</gtr:name><gtr:department>Music</gtr:department><gtr:address><gtr:line1>Old Shire Hall</gtr:line1><gtr:line2>Old Elvet</gtr:line2><gtr:line4>Durham</gtr:line4><gtr:line5>County Durham</gtr:line5><gtr:postCode>DH1 3HP</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46B41008-0EB4-4E28-BBFB-E98366999EC5"><gtr:id>46B41008-0EB4-4E28-BBFB-E98366999EC5</gtr:id><gtr:name>Durham University</gtr:name><gtr:address><gtr:line1>Old Shire Hall</gtr:line1><gtr:line2>Old Elvet</gtr:line2><gtr:line4>Durham</gtr:line4><gtr:line5>County Durham</gtr:line5><gtr:postCode>DH1 3HP</gtr:postCode><gtr:region>North East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/70F64E36-363A-41BF-A5A5-32B7C555A691"><gtr:id>70F64E36-363A-41BF-A5A5-32B7C555A691</gtr:id><gtr:name>Innsbruck University</gtr:name><gtr:address><gtr:line1>Universitaet Innsbruck</gtr:line1><gtr:line2>Technikerstra?e 13</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Austria</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F105D268-116A-40C3-A6FE-D029E3F1421F"><gtr:id>F105D268-116A-40C3-A6FE-D029E3F1421F</gtr:id><gtr:firstName>Tuomas</gtr:firstName><gtr:surname>Eerola</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FK00753X%2F1"><gtr:id>07306D39-7843-4C55-978A-55C6DE90D278</gtr:id><gtr:title>Tagging online music contents for emotion. A systematic approach based on contemporary emotion research</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/K00753X/1</gtr:grantReference><gtr:abstractText>Current approaches to the tagging of music in online databases predominantly rely on music genre and artist name, with music tags being often ambiguous and inexact. Yet, the possibly most salient feature musical experiences is emotion. The few attempts so far undertaken to tag music for mood or emotion lack a scientific foundation in emotion research. The current project proposes to incorporate recent research on music-evoked emotion into the growing number of online musical databases and catalogues, notably the Geneva Emotional Music Scale (GEMS) - a rating measure for describing emotional effects of music recently developed by our group. Specifically, the aim here is to develop the GEMS into an innovative conceptual and technical tool for tagging of online musical content for emotion. To this end, three studies are proposed. In study 1, we will examine whether the GEMS labels and their grouping holds up against a much wider range of musical genres than those that were originally used for its development. In Study 2, we will use advanced data reduction techniques to select the most recurrent and important labels for describing music-evoked emotion. In a third study we will examine the added benefit of the new GEMS compared to conventional approaches to the tagging of music. 
The anticipated impact of the findings is threefold. First, the research to be described next will advance our understanding of the nature and structure of emotions evoked by music. Developing a valid model of music-evoked emotion is crucial for meaningful research in the social and in the neurosciences. Second, music information organization and retrieval can benefit from a scientifically sound and parsimonious taxonomy for describing the emotional effects of music. Thus, searches for relevant online music databases need not be longer confined to genre or artist, but can also incorporate emotion as a key experiential dimension of music. Third, a valid tagging scheme for emotion can assist both researchers and professionals in the choice of music to induce specific emotions. For example, psychologists, behavioural economists, and neuroscientists often need to induce emotion in their experiments to understand how behaviour or performance is modulated by emotion. Music is an obvious choice for emotion induction in controlled settings because it is a universal language that lends itself to comparisons across cultures and because it is ethically unproblematic.</gtr:abstractText><gtr:potentialImpactText>The current tagging initiative would result in the first musical database to have been tagged for emotion in a systematic and principled manner, inspired by contemporary research in psychology. By incorporating the emotion tags in similarity algorithms, music information retrieval and organization could be substantially enriched. This would benefit four types of users. 

1. Researchers in the social sciences will gain a deeper understanding of the type and frequency of occurrence of emotion induced by emotion. Moreover, they will be able to search for musical excerpts with a particular emotion profile with unprecedented accuracy and efficiency. This would benefit psychologists, neuroscientists, as well as theatre, film and TV experts, who could search the database for stimuli with specific emotion-inducing properties to be used either in experiments or other research studies in which the effects of emotion play a central role.

2. Music educators are also potential beneficiaries of the results of this project. How can students acquire a broad knowledge of the vast repertoire of musical compositions? This is often achieved by moving through genre, composer, artist, or time period. Yet, a potentially more effective, creative, and perhaps also more engaging way to learn about musical styles, songs and compositions is to navigate the musical space with the compass of emotion as is illustrated in the following paragraph. 

3. General public users could benefit from recommendations that are no longer based on surface characteristics such as artist or genre, but on deep emotive responses to music that are shared across the community of users. As mentioned in the proposal this should lead to more intriguing than the prevalent recommendations based on artist or genre. Assume, for example, that Eva Cassidy's &amp;quot;Autumn Leaves&amp;quot; had been most frequently tagged with the GEMS-label &amp;quot;melancholic&amp;quot;. The person listening to this song would then find recommendations for a several pieces also predominantly tagged with &amp;quot;melancholic&amp;quot;, either within the person's preferred genres, or across types of artists, genres, instruments, and time periods. 

4. Online music databases, once suitably tagged for emotion, can be used to infer users' emotional state by online companies. For example, if the search history of an individual listener shows that the listener chose musical items emotionally tagged for interest or wonder, this might predispose the person to seek out novel products, such as technologically advanced goods (mobile phones, tablets, cars). In contrast, if the prevalent state is one of sadness or longing, this might predispose the person to seek out comforting goods (food, restaurants, wellness).</gtr:potentialImpactText><gtr:fund><gtr:end>2015-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2014-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>91294</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Innsbruck</gtr:collaboratingOrganisation><gtr:country>Austria, Republic of</gtr:country><gtr:department>Department of Psychology</gtr:department><gtr:description>Music and emotion research between Universities of Durham and Innsbruck</gtr:description><gtr:id>3E97440C-B561-4C9F-A017-31D792DE1014</gtr:id><gtr:impact>Joints publications under preparation</gtr:impact><gtr:outcomeId>56dd76b479fe55.89336629-1</gtr:outcomeId><gtr:partnerContribution>Knowledge of the pertinent emotion models, sophisticated modelling options, psychological experiments</gtr:partnerContribution><gtr:piContribution>Initiating a joint programme to study activities and situations involved in music and emotions.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>750</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Faculty Research Grant</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>Durham University</gtr:fundingOrg><gtr:id>4DB60117-7DDA-4FF2-B525-53E33464F59D</gtr:id><gtr:outcomeId>56dd7ab132adf1.25629634</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>There is no direct evidence that the findings of the project have been used by the creative economy or digital technologies. We carried out several rounds of negotiations with a non-British (EU) startup company specialising in services involving large-scale music data. They understood the merits of our findings concerning the links between the activities, mood and music, and how they can be harnessed to deliver better recommendation and music retrieval systems. However, we did not agree on the terms of the commercial partnership with the company and thus were not fully able to deliver the economic or digital impact via the company. In a more general level, the findings have put spotlight on the topic in media through more general interest towards music recommendation systems and retrieval, but our main purpose was to carry out rigorous research on the topic, not to deliver societal impact as such.</gtr:description><gtr:id>56BAE5E0-5356-459E-8B23-9BB6F40BBDFF</gtr:id><gtr:impactTypes/><gtr:outcomeId>56852e188a9277.63264387</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We set out to improve the tagging of music based on emotions. Previous research has either relied on utilitarian, music-specific, or ad-hoc emotion categories for music and utilised narrow and small pools of participants from within Western higher education. The utilitarian emotions consist of categories such as happy, sad, angry, or dimensions such as valence and arousal, all of which do not do full justice to the myriad of moods expressed by music or aesthetic objects in general. The music-specific emotion models such as the Geneva Emotion Music Scale (Zentner et al., 2008) have a robust foundation but refer to emotions aroused by music and thus the relevance of such models for tagging moods in music is not known.

The digital age of music consumption boosted by the popularisation of mobile devices enabling streaming of large music catalogues is increasingly bringing music into everyday activities. Previous research has investigated emotions in music while not taking into account the contexts where music is actually prevalent in everyday life.

First, the project established a new structure of moods expressed by music based on unprecedented amount of mood terms (647 in all), participants (5322 across five continents) and music tracks (4780). In three iterative crowdsourcing experiments, the relevance of all mood terms was established, refined and validated based on co-occurrence analyses of the terms across the tracks. In the first experiment, this operation yielded 88 mood terms, which were collapsed in the second experiment into 14 and 3 factors in a hierarchical fashion. These factors were in turn validated by the third experiment.

Second, the project delineated the links between the moods expressed by music and the typical activities associated with music. To this end, mood terms and music tracks were connected with nine core activities (e.g., daily routines, intellectual, entertainment, physical, etc.) synthesised from the literature. Novel interactions between moods and activities were uncovered, and these links could be represented by coherent and functional factors that allow flexible uses of tags in music information retrieval tasks and services.

Third, the project assessed how well self-selected music tracks and mood tags are conveyed to other listeners. This allowed to investigate the communication accuracy of mood tags, and the robustness of mood structures as representations to increase the reliability of this communication. To this end, the above-mentioned large-scale track data set was formed so that participants both submitted music tracks to the set while associating the tracks to a specific mood term, and tagged tracks submitted by others. In addition to novel findings on the communication of moods, this yielded a novel crowd-sourcing paradigm for evaluating the communication accuracy of moods in music using ecologically valid music examples.

Fourth, the project gained novel insights into the relationship between the perception of moods expressed by music, fitting activities, and contextual attributes related to a music listener. This was achieved by probing into the participants' age, region, gender, music preferences and preferred everyday activities.</gtr:description><gtr:exploitationPathways>For a non-academic route to impact, the results have two potential ways to impact music business, particularly the companies specialised in music recommendation, music app design, and streaming services (Spotify, Apple music, Last.fm, Deezer, etc.): (1) To enhance music mood retrieval by utilising the multilevel, hierarchical structure of moods in music that is both comprehensive in terms of vocabulary and consistent with online streaming services, and (2) to increase the relevancy of the music mood search by combining activities and personalised information (e.g., age, music preferences, and gender) in the search.

For academic pathways to impact, the data released in the project has the potential to stimulate and push forward the academic research in music information retrieval and music psychology. This dataset is larger and richer than any of the previous datasets in the field, allowing to test and evaluate mood and music models, and to examine the acoustic correlates of the mood structures with large collection of audio tracks, and to select appropriate and ecologically valid stimuli for further studies involving music and emotions.</gtr:exploitationPathways><gtr:id>269C1E88-17EC-405D-B1CC-71DBD2340994</gtr:id><gtr:outcomeId>56852d8e24b707.99482728</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset contains tagged data concerning moods and activities in music, as well as music preferences and generic demographic information of the taggers as well and brief music metadata. All data is related to a ESRC funded research project called 'Tagging music for emotions' (ES/K00753X/1). The data has been obtained through a series of three tasks (1. mood and activity tagging, 2. track search and tagging, 3. track tagging for moods and activities) designed to map various moods and activities related to music. These tasks have been run in Crowdflower (a crowdsourcing service) using workers of a specific quality. 2508 workers contributed to the dataset with varying number of rounds (which contains all tasks and consistency check) completed. No personal information was recorded in the initial data collection and the demographics reveal gender, age, country and a raw index of musical preferences, language skill, and musical expertise. The mood terms (88) and activities (9) and genres (113) are the results of pilot experiments, and there is a validation stage of the data as well. A full description of the tasks will be available in the scientific reports, which are currently in preparation.</gtr:description><gtr:id>5F269445-D70B-4CC0-883C-39425BFA63D6</gtr:id><gtr:impact>None yet (1/2/2016)</gtr:impact><gtr:outcomeId>56ba1b69d7d687.91971718</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Moods and activities in music (UK ReShare dataset)</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://reshare.ukdataservice.ac.uk/852024/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Novel paradigm to obtain crowdsourced annotations of emotional expression of music with simultaneous mapping of activities, tags and musical examples using only music submitted fully by users, not researchers. This was implemented as a dedicated SQL database with connections to 7digital music examples and the crowdsourcing aspect of the project was obtained by connecting this to crowdflower database. The system had internal validity check at regular intervals and a combinations of tasks to keep the crowdsourced workers motivated.</gtr:description><gtr:id>DBABFD5A-4548-4788-AB21-1E9ADA5EA034</gtr:id><gtr:impact>It allowed simultaneous collections of multiple questions which allow not only to estimate the emotional expression in music using a large array of mood terms, but the communication accuracy of such mapping, since one of the tasks involved in asking participants to assess other people example tracks in terms of moods and activities.</gtr:impact><gtr:outcomeId>56dd80ca12ad83.35123607</gtr:outcomeId><gtr:title>Crowdsourcing emotional expression in music</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>76A5C2BF-8F67-4BC1-8A1F-78E8C56AA63F</gtr:id><gtr:title>Genre-Adaptive Semantic Computing and Audio-Based Modelling for Music Mood Annotation</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7c92816b2a582aaba721e22aa41459e"><gtr:id>d7c92816b2a582aaba721e22aa41459e</gtr:id><gtr:otherNames>Saari P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56dd79b87eef53.36961402</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/K00753X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>4A856E02-8C94-4981-B572-E381EEB60DD2</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Media</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>E88ECDEE-D58E-417E-A379-9FBB35647A90</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Science and Technology Studies</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>972C8509-5001-4523-9E10-7FA67E2F2E69</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Visual arts</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>2D2B04A8-2CC9-41D9-B5EF-78E0AF1A8B24</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Installation &amp; Sound Art HTP</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>3BFAE48F-77C7-453F-8049-A1455B8B0E96</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music &amp; Society</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>2968C870-0FF4-4AE9-A670-9B0161C83B37</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>New Media/Web-Based Studies</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E88ECDEE-D58E-417E-A379-9FBB35647A90</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Science and Technology Studies</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>