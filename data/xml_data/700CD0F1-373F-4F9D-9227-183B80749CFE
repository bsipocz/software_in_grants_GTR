<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:department>Sch of Biology</gtr:department><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6676402F-8287-464E-9141-7E4118B331E7"><gtr:id>6676402F-8287-464E-9141-7E4118B331E7</gtr:id><gtr:name>Natural Environment Research Council</gtr:name><gtr:address><gtr:line1>Polaris House 
North Star Avenue</gtr:line1><gtr:city>Swindon</gtr:city><gtr:postCode>SN2 1EU</gtr:postCode><gtr:region>South West</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/9257587A-5F6F-4480-8BFE-A844FB6138C3"><gtr:id>9257587A-5F6F-4480-8BFE-A844FB6138C3</gtr:id><gtr:firstName>Steven</gtr:firstName><gtr:surname>Sait</gtr:surname><gtr:orcidId>0000-0002-7208-8617</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/4D20A90C-9BE0-4D5B-8244-D9E529C5405F"><gtr:id>4D20A90C-9BE0-4D5B-8244-D9E529C5405F</gtr:id><gtr:firstName>Sarah</gtr:firstName><gtr:surname>Zylinski</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FL008815%2F1"><gtr:id>700CD0F1-373F-4F9D-9227-183B80749CFE</gtr:id><gtr:title>Investigating depth perception in non-human vision using the dynamic camouflage of cuttlefish</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/L008815/1</gtr:grantReference><gtr:abstractText>The complexity of animal colouration patterns suggests we are not alone in our rich perceptual experience of the visual world. For instance, perceptual tricks and optical effects (visual illusions) are common in animal body patterns that convey camouflage. As these have evolved to defeat visual detection or recognition by non-human viewers, their effectiveness against our own visual system suggests common principles of visual processing. The perception of depth is important for interacting with a visually complex environment, a task that is by no means unique to humans. We perceive depth even in the absence of binocular cues, as is simply illustrated by considering a photograph: it is usually trivial for us to interpret depth from this two-dimensional rendition of a three-dimensional scene using cues such as perspective, shading and shadows. Although the ability to perceive visual depth has been clearly documented in many groups (e.g. reptiles, insects and birds) it is more difficult to determine what perceptual processes might be used by non-human animals. There is currently no systematic research on pictorial depth perception in a non-primate system.

This project will study the camouflage behaviour of the cuttlefish Sepia. Sepia offers a unique system for investigating non-human visual perception because each animal can produce thousands of different body patterns for camouflage. The particular pattern that an animals expresses reveals a great deal about its visual perception: how it sees patterns, shapes and objects around it is reflected in the body pattern. These body patterns are determined by direct visual assessment of the local environment and the chromatophores that form them are under neural control. Therefore we are afforded a direct link between visual input and body pattern output via the brain, allowing novel insight into its visual mechanisms. This system is all the more fascinating because cuttlefish are molluscs whose eye and brain have evolved independently from vertebrates, as the common ancestor of vertebrates and molluscs had rudimentary vision at best. We know that the camera-type eyes of cephalopods are highly convergent with the vertebrate eye, and research to date suggests that similar principles underlie the ways in which these two groups process visual information. This proposal aims to understand how depth cues, be they actual or pictorial, influence the camouflage response of Sepia, and how in turn this affects its detectability by potential predators.

In this project I will explore the importance of depth and perspective in the cuttlefish visual system, and what part it plays in making and breaking camouflage. I will achieve this through laboratory and field behavioural experiments and image analysis. The ability to test the response of Sepia to isolated visual information means we can take an approach similar to that used in human psychophysics, testing a series of carefully controlled parameters in turn to build a complete picture of how depth is perceived and used by this animal. However, the biological relevance of such isolated cues in artificial stimuli to the animal is limited, and its visual ecology should not be neglected. Therefore this proposal complements a laboratory based &amp;quot;psychophysics&amp;quot; approach with fieldwork. The body patterns of Sepia are believed to have evolved largely in response to predation pressure from teleost fishes; a further component of this proposal will determine how changes in Sepia's body patterning affect detection and recognition by teleost fishes, a subject largely neglected in studies of Sepia vision and camouflage to date. This work will provide new insights into both non-human visual processing and camouflage making and breaking.</gtr:abstractText><gtr:technicalSummary>Vision has independently evolved in multiple lineages, allowing us to assess how diverse systems process the same real-world signals. The effectiveness of camouflage patterns against the visual systems of multiple animals from different lineages (including humans) suggests shared visual mechanisms. Camouflage patterns often rely on optical effects so can inform us about visual processes in animals, and tackle questions about top-down cognitive versus bottom-up scene-driven processes. The cuttlefish Sepia provides a unique system for studying non-human vision through its rapid adaptive camouflage. It can change its appearance in under a second by the expansion and contraction of dense chromatophores that are visually driven by the local environment and under direct neural control. This has been previously utilized to investigate the mechanisms involved in basic visual tasks such as edge detection, texture perception and object recognition.

This project will investigate the perception of depth information by Sepia via its camouflage patterns. Humans have several means to determine visual depth including binocular stereopsis, depth from motion, and pictorial cues. These sources are processed in separate streams in the visual cortex, but are ultimately integrated to provide a single representation of depth. Little empirical data currently exists on whether non-human animals use multiple cues in depth perception, how these are processed and integrated, and how visual depth contributes to the making and breaking of animal camouflage. Furthermore, it is unknown how important this is in preventing detection/recognition by potential predators. This work will combine laboratory behavioural experiments, fieldwork, and image processing to determine how depth cues are perceived by, and accounted for, in the dynamic camouflage of Sepia. It will then assess how body pattern &amp;quot;fine tuning&amp;quot; in response to depth cues changes the effectiveness of camouflage against teleost viewers.</gtr:technicalSummary><gtr:potentialImpactText>Short-term impact goals summary:
- Research and transferrable skills training for PDRA and RA (academic beneficiaries);
- Present findings at national and international conferences (academic beneficiaries);
- Publish findings in high impact journals (academic beneficiaries);
- Press releases to coincide with publication to disseminate research to public audience (public beneficiaries);
- Production of interactive game for science fairs and schools; talks at public events (public beneficiaries).

Long-term impact goals summary:
- Results will inform defense technology research and development, in determining how a natural camouflage system has fine-tuned its camouflage in response to natural selection (private sector beneficiaries);
- Research will lead to future collaborations with machine vision/ artificial intelligence groups with a view to improving scene rendering (academic beneficiaries);
- Experience gained will enable the PDRA and RA to achieve their next chosen career step (academic beneficiaries));
- Images collected in the field will be made publically available for use by the media, educators, and the research community (public &amp;amp; academic beneficiaries).

Who will benefit from this work, and how:
The proposed research will be of great interest and use to other researchers working on camouflage and/or visual perception at a basic research level, including behavioural ecologists, materials scientists, and visual psychologists. At an applied level I envisage that the results of these studies will benefit machine vision engineers and computer programmers working on scene rendering and/or image segregation, with the potential for bio-inspired solutions to how to create realistic depth information in artificial scenes. We will make contact with researchers in the computer vision field at the host institution to explore possible avenues of collaboration and application. 

The defense technology industry is the most relevant beneficiary of a further understanding of the part visual depth plays in making effective camouflage. Principles drawn from the proposed research could ultimately improve protection of armored personal carriers and other objects of a fixed form that use camouflage markings and are subject to varying illumination regimes. 

The PDRA and RA appointed to this project will benefit from learning research and transferable skills that will enhance their employment prospects should they pursue an academic path or an industry career. These include data management, logistics and planning for time-critical experiments and fieldwork, teamwork, technical writing skills, and presentation skills. 

The charismatic nature of cephalopod research readily catches the attention of the media and the public. Previous research in this area has resulted in features on prime time BBC television and National Public Radio (USA). Further coverage has come from quality media such as the Guardian, BBC, the New York Times and New Scientist. I therefore expect public engagement relating to this work will be well received, and as such will provide an opportunity to inform and educate a broad audience about visual perception, camouflage and optical effects. Beneficiaries will include local schools, with pupils learning about vision, visual perception and camouflage through an interactive computer game to be used at Science Festivals. Adult public will benefit from an increased understanding of natural history and important concept such as convergent evolution through media reporting and presentations at Science Cafes and Dive show platforms. 

Educators, the media, and the research community will benefit from the images collected in the field of Sepia apama using different dynamic camouflage patterns. These will be made publically available following the publication of associated papers.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-10-27</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2014-04-28</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>353592</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>International Society for Behavioral Ecology, University of Exeter</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5EE82F78-FD3B-43A2-9B97-A8FADACF3B6C</gtr:id><gtr:impact>Poster presenting the latest novel results about how cuttlefish perceive depth. This was of interest to those from the fields of general behavioural ecology, but especially those working in the area of the mechanisms underpinning non-human vision.</gtr:impact><gtr:outcomeId>58c682d80afd92.10290713</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.isbe2016.com/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Cephalopod workshop</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>DA754EC1-05F4-44B9-B948-C442FDA6ADF0</gtr:id><gtr:impact>Workshop in Portugal that was intended to improve animal welfare by sharing and disseminating good practice in science and fisheries. Participants included scientists in the field, and other scientists who all engage in cephalopod research as well as those who work in cephalopod fisheries and care.</gtr:impact><gtr:outcomeId>56decc01921218.65822041</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Cephalopods in Action meeting</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>ADAA9E9C-ABAC-4F03-BF37-350A114049EA</gtr:id><gtr:impact>The workshop was on husbandry, care and the law regarding the use of cephalopods in experiments.</gtr:impact><gtr:outcomeId>56dedd918aeff4.62051537</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Engagement with public, SeaLife Centre Brighton</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>7AECCE1A-F1E3-4168-8A66-84E513AC6F85</gtr:id><gtr:impact>Tours around public aquarium at SeaLife Centre, Brighton observed an experiment; science techniques and approaches demonstrated.</gtr:impact><gtr:outcomeId>56dede5f056032.96610420</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Public tour of lab</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>BBA2A154-B37E-4CB0-BA89-71FBA972AF47</gtr:id><gtr:impact>Demonstration of experimental techniques, including fish husbandry and scientific application more broadly. This is was done in order to demonstrate the scientific approach and how science is carried out.</gtr:impact><gtr:outcomeId>56decaf2f1d389.26444057</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Several visual cues are known to be exploited to perceive three-dimensionality and depth in humans. Examples include motion parallax, occlusion, texture gradients, light directionality, shadows, and shading. The visual perception of cuttlefish can be studied by analysing the rapid adaptive camouflage in response to backgrounds that test their visual perception. Principles of 3-D cues based on real 3-dimensional circular moulds of varying lengths, light direction, and shadows were tested on the common cuttlefish Sepia officinalis. Cuttlefish consistently and significantly produced patterns that have only been seen in response to 3-D objects. Several expressed key body components were identified which will make it easier to further study what cues are important in their 3-D visual perception. The size of the 3-D objects made a significant difference on their patterns. That the 3-D moulds were grey or white did not make a difference suggesting that luminance contrast may not play a key role in 3-D perception. Shadows surprisingly also did not appear to increase the perception of depth, however if a shadow was extra long, it had the same effect on the subjects as 2D white spots with black shadow replicates. A possible explanation is that shadows may need higher-level analyses as has previously been suggested in humans. Furthermore, for the first time, cuttlefish responses in the field (Australia) have been investigated and have revealed that camouflage is affected by light direction in terms of matching the background and postures in relation to 3D form.</gtr:description><gtr:exploitationPathways>These results prove that this system can yield exciting and novel results that will help further understanding of their visual perception of depth, and these have led to the design of new sets of experiments to look at texture and polarisation differences. The results may have applications beyond the study system.</gtr:exploitationPathways><gtr:id>D08F46C8-542A-4F72-AAC6-081E45DCC70D</gtr:id><gtr:outcomeId>56dec9aad8b213.08043868</gtr:outcomeId><gtr:sectors><gtr:sector>Environment</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8160E423-8047-4BD7-A8F6-497AAD744098</gtr:id><gtr:title>Cuttlefish see shape from shading, fine-tuning coloration in response to pictorial depth cues and directional illumination.</gtr:title><gtr:parentPublicationTitle>Proceedings. Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/56aef3829c638f9ec7e8da3140d789ab"><gtr:id>56aef3829c638f9ec7e8da3140d789ab</gtr:id><gtr:otherNames>Zylinski S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0962-8452</gtr:issn><gtr:outcomeId>58c67f5492e4c3.24635464</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/L008815/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>1A1A6805-9DC4-4BCE-BC70-9F2AA4FD093B</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Ecol, biodivers. &amp; systematics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>790AD28C-6380-4025-83C2-6881B93C4602</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal behaviour</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>4A2A69ED-37ED-4980-91A7-E54B4F6A9BC6</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal organisms</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>685A8D5E-BD8A-4D8D-BAC5-607439217156</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Behavioural Ecology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F439A20B-A9B0-4A68-B703-7F6AE7570E39</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems neuroscience</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>