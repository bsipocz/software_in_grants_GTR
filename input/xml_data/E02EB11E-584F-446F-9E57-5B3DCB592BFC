<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/4A387D6B-E08F-4191-86CB-E960B8C48229"><gtr:id>4A387D6B-E08F-4191-86CB-E960B8C48229</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:surname>Cox</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF069626%2F1"><gtr:id>E02EB11E-584F-446F-9E57-5B3DCB592BFC</gtr:id><gtr:title>Adaptive cognition for automated sports video annotation (ACASVA)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F069626/1</gtr:grantReference><gtr:abstractText>The development of a machine that can autonomously understand and interpret patterns of real-world events remains a challenging goal in AI. Humans are able to achieve this by developing sophisticated internal representational structures for object and events and the grammars that connect them. ACASVA aims to investigate the interaction between visual and linguistic grammars in learning by developing grammars in a scenario where the number of different events is constrained, by a set of rules, to be small: a sport. We will analyse video footage of a game (e.g. tennis) and use computer vision techniques to progressively understand it as a sequence of (possibly overlapping) events, and build a grammar of events. We will do a similar audio/linguistic analysis on the commentary on the game. Both of these grammars will be used to build a representational structure for understanding the game. Visual representations are additionally constrained by the inference of game rules so that object-classification mechanisms are preferentially tuned to game-relevant entities like 'player' rather than game-irrelevant entities like 'crowd-member'. We will also investigate how the two modes, sight and sound, can influence each other in the learning process; interpretation of the video is affected by the linguistic grammar and vice versa. Furthermore, this coupling of modes will lead to improved recognition of both audio and video events when the grammars from the video modes are used to influence the audio recognition, and vice versa. The psychological component of the ACASVA correspondingly attempts to learn how these capabilities are developed in humans; how visual grammars are organized and employed in the learning problem, how these grammars are modified by prior linguistic knowledge of the domain, how visual grammars map onto linguistic grammars, and how game rule-inferences influence lower-level visual learning (determined via gaze-behaviour). These results will feedback into the machine-learning problem and vice versa, as well as providing a performance benchmark for the system.Potential beneficiaries of ACASVA (in addition to the knowledge beneficiaries within the fields of science and engineering) include the broadcasting and on-line video search industries.</gtr:abstractText><gtr:fund><gtr:end>2013-03-15</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-03-16</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>356801</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Surrey</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Partnership with Surrey University</gtr:description><gtr:id>81177D7F-D80A-4847-A6E8-0A0F19D9297F</gtr:id><gtr:impact>Automatic lip-reading</gtr:impact><gtr:outcomeId>5460e0fdc2b8c1.94819092-1</gtr:outcomeId><gtr:partnerContribution>Surrey: tracking of lips.
UEA: speech recognition</gtr:partnerContribution><gtr:piContribution>Joint work on automatic lip-reading that came about partly as a result of the ACSVA EPSRC project.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The ACASVA project is concerned with teaching a computer how to &amp;quot;understand&amp;quot; events by teaching it how to &amp;quot;see&amp;quot; and &amp;quot;hear&amp;quot; a video and its associated soundtrack. Because we need to start with quite simple events that also have a simple &amp;quot;syntax&amp;quot; (ordering), we have been concentrating on videos of tennis games which have a clear set of rules relating to how events develop. We are particularly interested in how knowledge of the rules affects how the events are &amp;quot;seen&amp;quot; and &amp;quot;heard&amp;quot;; for instance, should the computer pay more attention to entities that are relevant to the rules (e.g players) and less attention to entities that are irrelevant (eg the crowd), and can this be determined automatically? It is clear that humans are able to accomplish this &amp;quot;attentional focus&amp;quot;, however, the mechanisms for this are not well understood - a key aim of the project was there to establish how these processes work in humans, and to feed these insights into the computational domain to establish performance baselines (e.g. are humans best at detecting individual events, or are they best at combining information). Also, can learning of audio and visual information be transferred from one domain to another; what about rule knowledge (e.g. of badminton)?



There have been several strands to the research:



(1) Development and tuning of artificial audio and visual detectors for sport video annotation.



(2) Integrating of information from audio and video sources to improve recognition of events. For instance, the computer learns that a long burst of applause indicates the end of a point, which helps it to segment the events in the game; or it can decide who won a point by a combination of interpreting the video action and listening for the shout of the line-judge at the end of a rally. This combination can be achieved at both the high (verbal ) level and the low (audio) level.



(3) Evaluation of human cognitive abilities in relation sport video understanding by directly measuring human attention using an eye-tracker in a series of audio/visual experiments.



We are particularly interested in how knowledge of the rules affects how the events are &amp;quot;seen&amp;quot; and &amp;quot;heard&amp;quot;; for instance, should the computer pay more attention to entities that are relevant to the rules (e.g players) and less attention to entities that are irrelevant (eg the crowd), and can this be determined automatically? It is clear that humans are able to accomplish this &amp;quot;attentional focus&amp;quot;, however, the mechanisms for this are not well understood - a key aim of the project was there to establish how these processes work in humans, and to feed these insights into the computational domain to establish performance baselines (e.g. are humans best at detecting individual events, or are they best at combining information). Also, can learning of audio and visual information be transferred from one domain to another; what about rule knowledge (e.g. of badminton)?</gtr:description><gtr:id>4FD3720F-DE34-4EB2-91AF-8CEF42F6BDBE</gtr:id><gtr:outcomeId>r-5575337545.5507427775319c</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://cvssp.org/acasva/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>AA44ED0D-BA19-4462-9810-44D36A0EA0FD</gtr:id><gtr:title>Inferring the Structure of a Tennis Game Using Audio Information</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5df6878d7c6b2adb436dfac36c2d2735"><gtr:id>5df6878d7c6b2adb436dfac36c2d2735</gtr:id><gtr:otherNames>Huang Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54589d0a954ec7.90477195</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>89A6FFD2-3168-4478-9E1E-8C2BC88C0F2D</gtr:id><gtr:title>Improved audio event detection by use of contextual noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5df6878d7c6b2adb436dfac36c2d2735"><gtr:id>5df6878d7c6b2adb436dfac36c2d2735</gtr:id><gtr:otherNames>Huang Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5460deac367d02.55359980</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E5ADD8BA-1466-4AEB-9B31-259B53DC6D17</gtr:id><gtr:title>A two layered data association approach for ball tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2161a7b82e55ae2cc0a43abe8c874d5d"><gtr:id>2161a7b82e55ae2cc0a43abe8c874d5d</gtr:id><gtr:otherNames>Zhou X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460de64bc76f1.48336521</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98CCF7B8-60D5-4985-89DF-10B69F20BFC4</gtr:id><gtr:title>Iterative improvement of speaker segmentation in a noisy environment using high-level knowledge</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b94aaf01c4246ff1f06cf41d056bcc79"><gtr:id>b94aaf01c4246ff1f06cf41d056bcc79</gtr:id><gtr:otherNames>Huang Q.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5460dee60c29c1.37860812</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB53CADA-7394-4707-9E13-E2BD32D4E233</gtr:id><gtr:title>Hierarchical language modeling for audio events detection in a sports game</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5df6878d7c6b2adb436dfac36c2d2735"><gtr:id>5df6878d7c6b2adb436dfac36c2d2735</gtr:id><gtr:otherNames>Huang Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-4295-9</gtr:isbn><gtr:outcomeId>5460df65c9a0d4.77628094</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C6820EEA-FA26-44D3-AC1A-6F79CE7BB236</gtr:id><gtr:title>Automatic annotation of tennis games: An integration of audio, vision, and learning</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a8bb95557325dc92e149eaa32cb7a250"><gtr:id>a8bb95557325dc92e149eaa32cb7a250</gtr:id><gtr:otherNames>Yan F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54589d0a40d1d1.25048254</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88671CBB-85EE-4229-B90D-FE978E4D0ECB</gtr:id><gtr:title>Looking to score: the dissociation of goal influence on eye movement and meta-attentional allocation in a complex dynamic natural scene.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/402c8c354bbfdf5adcf5a2909be5a802"><gtr:id>402c8c354bbfdf5adcf5a2909be5a802</gtr:id><gtr:otherNames>Taya S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a351a7287eae8.99833341</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FF1852BA-6111-4EAE-8E00-C20A65A9B419</gtr:id><gtr:title>Trained eyes: experience promotes adaptive gaze control in dynamic and uncertain visual environments.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/402c8c354bbfdf5adcf5a2909be5a802"><gtr:id>402c8c354bbfdf5adcf5a2909be5a802</gtr:id><gtr:otherNames>Taya S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>5a36211f277bf2.48734603</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>218069B7-03FC-4080-8001-6C660FAC2EED</gtr:id><gtr:title>Detection of ball hits in a tennis game using audio and visual information</gtr:title><gtr:parentPublicationTitle>2012 Conference Handbook - Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b94aaf01c4246ff1f06cf41d056bcc79"><gtr:id>b94aaf01c4246ff1f06cf41d056bcc79</gtr:id><gtr:otherNames>Huang Q.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5460de8a6d7f75.87310019</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BD45A202-B333-4C52-8763-34B9C6CE4FBF</gtr:id><gtr:title>Addressing Missing Values in Kernel-Based Multimodal Biometric Fusion Using Neutral Point Substitution</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Information Forensics and Security</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/69f40e7b3b679711a5a7b68fef3f7eae"><gtr:id>69f40e7b3b679711a5a7b68fef3f7eae</gtr:id><gtr:otherNames>Poh N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_55f9529522d224dc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1CBFCE72-342A-429B-A6A2-DC9B1160BF6F</gtr:id><gtr:title>Using high-level information to detect key audio events in a tennis game</gtr:title><gtr:parentPublicationTitle>Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b94aaf01c4246ff1f06cf41d056bcc79"><gtr:id>b94aaf01c4246ff1f06cf41d056bcc79</gtr:id><gtr:otherNames>Huang Q.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>5460df4582bed0.79702087</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AED7D832-DF8A-4F93-A2CA-D1A7A86882EF</gtr:id><gtr:title>A Framework for Hierarchical Perception-Action Learning Utilizing Fuzzy Reasoning.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2c4cbdb9c20ce2c26931edbaa4062b57"><gtr:id>2c4cbdb9c20ce2c26931edbaa4062b57</gtr:id><gtr:otherNames>Windridge D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>doi_55fa9fa9f5830d30</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>60E1B508-8CDF-4FD8-9487-6AB07AC18527</gtr:id><gtr:title>Learning score structure from spoken language for a tennis game</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b94aaf01c4246ff1f06cf41d056bcc79"><gtr:id>b94aaf01c4246ff1f06cf41d056bcc79</gtr:id><gtr:otherNames>Huang Q.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5460defeb94960.92572774</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8DFB9117-8D76-4B67-9536-4F4289CA6E05</gtr:id><gtr:title>Tennis Ball Tracking Using a Two-Layered Data Association Approach</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2161a7b82e55ae2cc0a43abe8c874d5d"><gtr:id>2161a7b82e55ae2cc0a43abe8c874d5d</gtr:id><gtr:otherNames>Zhou X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>585d41a7a98d96.20875804</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C4476174-6FEC-48A4-8D91-6ACA8BEA1307</gtr:id><gtr:title>The mental number line in depth revealed by vection.</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/02ee867da5c24e52827f556c9c58f8d8"><gtr:id>02ee867da5c24e52827f556c9c58f8d8</gtr:id><gtr:otherNames>Seno T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>5a351d1bd3b660.56808158</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F069626/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>