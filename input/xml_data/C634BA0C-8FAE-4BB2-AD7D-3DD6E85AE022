<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/C99FB228-04C1-4C8F-A5C0-66EB3B718CF2"><gtr:id>C99FB228-04C1-4C8F-A5C0-66EB3B718CF2</gtr:id><gtr:name>Bayerische Motoren Werke (BMW)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/4E678C66-D2E0-4752-9BCF-BB239BF46F3C"><gtr:id>4E678C66-D2E0-4752-9BCF-BB239BF46F3C</gtr:id><gtr:name>Oxehealth Ltd</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/6C747631-6855-48CC-BF64-0316AABB5DF8"><gtr:id>6C747631-6855-48CC-BF64-0316AABB5DF8</gtr:id><gtr:name>University of Malta</gtr:name><gtr:address><gtr:line1>Administration Bldg</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B"><gtr:id>CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B</gtr:id><gtr:name>Oxford Brookes University</gtr:name><gtr:address><gtr:line1>Headington Campus</gtr:line1><gtr:line2>Gipsy Lane</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:line5>Oxfordshire</gtr:line5><gtr:postCode>OX3 0BP</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/5C1114A7-CE62-4DEC-BB95-695D6E2666CF"><gtr:id>5C1114A7-CE62-4DEC-BB95-695D6E2666CF</gtr:id><gtr:name>University of Verona</gtr:name><gtr:address><gtr:line1>strada le Grazie 15</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B"><gtr:id>CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B</gtr:id><gtr:name>Oxford Brookes University</gtr:name><gtr:department>Faculty of Tech, Design and Environment</gtr:department><gtr:address><gtr:line1>Headington Campus</gtr:line1><gtr:line2>Gipsy Lane</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:line5>Oxfordshire</gtr:line5><gtr:postCode>OX3 0BP</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B"><gtr:id>CEB496D0-8DC8-4905-8F65-D1ACDAD75C9B</gtr:id><gtr:name>Oxford Brookes University</gtr:name><gtr:address><gtr:line1>Headington Campus</gtr:line1><gtr:line2>Gipsy Lane</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:line5>Oxfordshire</gtr:line5><gtr:postCode>OX3 0BP</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C99FB228-04C1-4C8F-A5C0-66EB3B718CF2"><gtr:id>C99FB228-04C1-4C8F-A5C0-66EB3B718CF2</gtr:id><gtr:name>Bayerische Motoren Werke (BMW)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4E678C66-D2E0-4752-9BCF-BB239BF46F3C"><gtr:id>4E678C66-D2E0-4752-9BCF-BB239BF46F3C</gtr:id><gtr:name>Oxehealth Ltd</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/6C747631-6855-48CC-BF64-0316AABB5DF8"><gtr:id>6C747631-6855-48CC-BF64-0316AABB5DF8</gtr:id><gtr:name>University of Malta</gtr:name><gtr:address><gtr:line1>Administration Bldg</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5C1114A7-CE62-4DEC-BB95-695D6E2666CF"><gtr:id>5C1114A7-CE62-4DEC-BB95-695D6E2666CF</gtr:id><gtr:name>University of Verona</gtr:name><gtr:address><gtr:line1>strada le Grazie 15</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/19A8411E-B2CF-40F2-9D41-C4442D29B29F"><gtr:id>19A8411E-B2CF-40F2-9D41-C4442D29B29F</gtr:id><gtr:firstName>Fabio</gtr:firstName><gtr:surname>Cuzzolin</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI018719%2F1"><gtr:id>C634BA0C-8FAE-4BB2-AD7D-3DD6E85AE022</gtr:id><gtr:title>Tensorial modeling of dynamical systems for gait and activity recognition</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I018719/1</gtr:grantReference><gtr:abstractText>Biometrics such as face, iris, or fingerprint recognition have received growing attention in the last decade, as automatic identification systems for surveillance and security have started to enjoy widespread diffusion. They suffer, however, from two major limitations: they cannot be used at a distance, and require user cooperation, assumptions impractical in real-world scenarios. Interestingly, psychological studies show that people are capable of recognizing their friends just from the way they walk, even when their gait is poorly represented by point light display. Gait has several advantages over other biometrics, as it can be measured at a distance, is difficult to disguise or occlude, can be identified even in low-resolution images, and is non-cooperative in nature. Furthermore, gait and face biometrics can be easily integrated for human identity recognition.Despite its attractive features, though, gait identification is still far from being ready to be deployed in practice. What limits its adoption in real-world scenarios is the influence of a large number of nuisance factors which affect appearance and dynamics of the gait. These include, for instance: walking surface, lighting, camera setup (viewpoint), but also footwear and clothing, objects carried, time of execution, walking speed. Similar issues are shared by other applications of motion classification, such as action and activity recognition. Multilinear or tensorial models, in which a number of (nuisance) factors linearly mix to generate what we observe (in our case the walking gait), have been proven in the recent past to be able to describe the influence of such factors, for instance in the context of face recognition. However, video sequences are more complex objects than single images. We first need to represent video footages in a compact way.Encoding the dynamics of videos by means of some sort of dynamical model has been proven effective in both action recognition and gait identification, in situations in which the dynamics is critically discriminative. Besides, the actions of interest have to be temporally segmented from a video sequence, while actions of sometimes very different lengths might have to be compared. Dynamical representations are very effective in coping with temporal detection and compression, and indeed several researchers have explored the idea of encoding motions via linear, nonlinear, stochastic or chaotic dynamical systems.In this project, therefore, we propose to develop a novel, general framework for the classification of video sequences (with a focus on the walking gait), based on the application of tensorial decomposition techniques to image sequences represented as realizations of suitable dynamical models.The proposed framework will allow us to deal with the issue of the nuisance factors which greatly affect identification from gait and activity recognition in a principled way. The main goal is to push towards a more widespread diffusion of gait ID, as a concrete contribution to enhancing the security levels in the country in the current, uncertain scenarios. With their implications for crime prevention and security, biometrics and surveillance are fast growing business areas, a fact reflected by the increasing number of government-sponsored initiatives in the area in most advanced economies. In addition, the techniques devised in this proposal are extendable to action and identity recognition with immense commercial exploitation potential, ranging from content-based video retrieval from repositories such as YouTube, to HMI, to interactive video games, etcetera.</gtr:abstractText><gtr:potentialImpactText>Within the private sector companies whose core business is in semi-automatic surveillance or biometrics would be the primary beneficiaries of a robust gait identification system. Most of them focus at the moment on cooperative biometrics such as face or iris recognition: investing in behavioral techniques ahead of the rest of the market could provide them with a significant competitive edge. As large-scale tensor modeling can be useful in many applications, however, companies active in vision or medical imaging, physicians specialized in biometric analysis and sport teams labs could also commercially benefit from this research. We expect policy-makers and government agencies to be attracted by the idea of novel surveillance and biometric systems able to improve the general level of security in the country, and that of sensitive areas in particular. Examples are airport management authorities such as BAA, railway companies, underground and public transport authorities (e.g., Transport for London). The wider public will of course be the ultimate beneficiary of any improvement in the security level in public places and transport. A successful outcome of the project will likely boost the competitiveness of the above mentioned private sector actors, to the economic competitive advantage of the United Kingdom. Companies active in biometrics and surveillance will benefit from privileged access to cutting edge technology in behavioral biometrics. The public sector and the government could potentially see the security level of the country dramatically improved by the adoption of some of the outcomes of this research. The infrastructure is basically already there, as millions of active CCTV cameras make the United Kingdom one of the most surveyed western nations. The UK citizen's quality of life will arguably benefit too, in the longer term, by the deployment of behavioral biometric systems as anti-terrorist measures. Among the indirect benefits to the wider public it is worth to mention the impact on people's health of providing physicians with automatic support in diagnosis based on sophisticated data (e.g., 3D MRI scans) as an application of tensor models. Realistic timescales for such benefits vary with the specific target. To start seeing commercial applications in an initially limited context (e.g. biometric access), an additional three or four years of R&amp;amp;D after the end of the proposed research would be needed, possibly in partnership with a company. It is safer to assume a longer time scale when targeting a wider application to security in public areas. Government or agency support will in this case be crucial. The research assistant working full time on the project will develop a variety of skills he/she could exploit in many possible future employment scenarios. These include: project and website management, Matlab coding and use of sophisticated statistical toolboxes, report and paper writing, professional networking. The proposer belongs to the Oxford Brookes Computer Vision Group, which has very well established channels for technology transfer from research to product and a track record of achieving this. Intellectual Property Rights management and exploitation will be managed by the Research and Business Development Office (RBDO) at Oxford Brookes University. To disseminate our results we plan to arrange seminar days with relevant companies active in the Oxford/London area and beyond. Towards the end of the project, public agencies such as the Transport for London or BAA could be contacted with concrete results to show in order for a proof of concept arrangement to be set up. The Vision Group already enjoys links with HMGCC, the government centre of excellence. More details are given in the impact plan.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-01-11</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-06-30</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98363</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Oxford University on neural video captioning</gtr:description><gtr:id>5ACDAF94-42F3-4BB5-86EF-81FD1859DABF</gtr:id><gtr:impact>This collaboration has led to an outline Project Grant submitted to the Leverhulme Trust. The outline proposal has been accepted on March 3 2017, and the Trust has invited us to submit a full proposal within 9 months. We plan to submit the full proposal by March 21 2017.</gtr:impact><gtr:outcomeId>545cd714f21763.74609780-1</gtr:outcomeId><gtr:partnerContribution>Oxford University's group led by Professor Lukasiewicz will work on the suitable methodologies to teach the network that different sentences can be semantically equivalent, so that the network is able to describe a new video using its own words.</gtr:partnerContribution><gtr:piContribution>This is a collaboration with Professor Thomas Lukasiewicz of Oxford University on novel framework for neural video captioning based on analysing the semantic content of videos. The contribution of our research group is on the extracting of semantic information in the form of a plot or storyline using novel discriminative deformable part based models published in outcomes listed under this project. This can provide an attention model for the neural network to focus on the 'important' parts of a video.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Bayerische Motoren Werke (BMW)</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>Collaboration with BMW Group</gtr:description><gtr:id>E8F813E7-D1AB-4BE0-A7D2-B93A55BF6227</gtr:id><gtr:impact>Negotiations are still in the making.</gtr:impact><gtr:outcomeId>545cdabdb1abe6.55449424-2</gtr:outcomeId><gtr:partnerContribution>BMW is discussion what form their contribution is going to take. Several project topics are under discussion, and consultancy seems to be the most attractive option.</gtr:partnerContribution><gtr:piContribution>This collaboration with BMW Group aims at providing visual recognition technologies which can enable to group to improve their processes, affecting a number of departmens from production, to logistics, paint, body in white. Marko Kosenina of their IT Research department is coordinating the effort from BMW's side.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with BMW Group</gtr:description><gtr:id>C0A37469-A532-4547-A1F5-65FD9A62B85E</gtr:id><gtr:impact>Negotiations are still in the making.</gtr:impact><gtr:outcomeId>545cdabdb1abe6.55449424-1</gtr:outcomeId><gtr:partnerContribution>BMW is discussion what form their contribution is going to take. Several project topics are under discussion, and consultancy seems to be the most attractive option.</gtr:partnerContribution><gtr:piContribution>This collaboration with BMW Group aims at providing visual recognition technologies which can enable to group to improve their processes, affecting a number of departmens from production, to logistics, paint, body in white. Marko Kosenina of their IT Research department is coordinating the effort from BMW's side.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Oxehealth Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with the Movement Science Group on the diagnosis of dementia using machine learning techniques</gtr:description><gtr:id>53D6D391-F4B0-452D-A2BF-F76F327F1FF0</gtr:id><gtr:impact>We did submit a proposal to the SIDD call in April 2014. The proposal has received very high scores (6,6,5 and 3) but, disappointingly, was not selected for funding. 
As mentioned above, we will submit a bid to the 2017 Healthcare Technologies call, and a separate KTP application with Oxehealth as industrial partner.</gtr:impact><gtr:outcomeId>545cd9d81f50b1.73835398-2</gtr:outcomeId><gtr:partnerContribution>The Movement Science group contributes with their 12 year expertise on the topic, their link with a company producing smartphone apps and user groups, and with a huge amount of data already in their possession. Oxehealth will contribute with their infrastructure, data and expertise in the monitoring of health conditions in home environments.</gtr:partnerContribution><gtr:piContribution>This is a collaboration with Professor Helen Dawes of the Movement Science group on the formulation of a machine learning framework for the early diagnosis of dementia using machine learning techniques, applied to data capture by smartphone devices. Our group contributes with our expertise on advanced machine learning techniques, such as those published in the publication outcomes related to this project.
In 2017 the collaboration has been extended to Oxehealth, the award-winning Oxford University spinoff active in healthcare monitoring via video cameras. This is developing into two separate proposal submissions: a KTP via InnovateUK, with Oxehealth, and a new EPSRC Healthcare Technologies grant proposal to be submitted by April 2017.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Oxford Brookes University</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with the Movement Science Group on the diagnosis of dementia using machine learning techniques</gtr:description><gtr:id>ED0DD5C2-7C01-4FB7-9EA4-574FA58FBB96</gtr:id><gtr:impact>We did submit a proposal to the SIDD call in April 2014. The proposal has received very high scores (6,6,5 and 3) but, disappointingly, was not selected for funding. 
As mentioned above, we will submit a bid to the 2017 Healthcare Technologies call, and a separate KTP application with Oxehealth as industrial partner.</gtr:impact><gtr:outcomeId>545cd9d81f50b1.73835398-1</gtr:outcomeId><gtr:partnerContribution>The Movement Science group contributes with their 12 year expertise on the topic, their link with a company producing smartphone apps and user groups, and with a huge amount of data already in their possession. Oxehealth will contribute with their infrastructure, data and expertise in the monitoring of health conditions in home environments.</gtr:partnerContribution><gtr:piContribution>This is a collaboration with Professor Helen Dawes of the Movement Science group on the formulation of a machine learning framework for the early diagnosis of dementia using machine learning techniques, applied to data capture by smartphone devices. Our group contributes with our expertise on advanced machine learning techniques, such as those published in the publication outcomes related to this project.
In 2017 the collaboration has been extended to Oxehealth, the award-winning Oxford University spinoff active in healthcare monitoring via video cameras. This is developing into two separate proposal submissions: a KTP via InnovateUK, with Oxehealth, and a new EPSRC Healthcare Technologies grant proposal to be submitted by April 2017.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Oxford University on Action Recognition in the wild</gtr:description><gtr:id>4EEBE78C-B328-41E7-98AD-E12F3CEC77A0</gtr:id><gtr:impact>An EPSRC proposal on the topic is in preparation and will be submitted in early 2016.</gtr:impact><gtr:outcomeId>545cd8814a53b0.85788510-1</gtr:outcomeId><gtr:partnerContribution>Prof Torr's group contributes with their expertise on scene understanding in order to integrate scene context with motion analysis for a comprehensive understanding of the environment and the events taking place there.</gtr:partnerContribution><gtr:piContribution>This is a collaboration with Professor Philip Torr concerning the preparation of a new EPSRC proposal on the use of deep learning techniques for online action recognition and future action prediction The contribution of our research team is focused on the deep learning part, and on the detection and tracking of appropriate &amp;quot;action tubes&amp;quot; from the input video sequences. We also provide an HPC on which to run the necessary tests.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Verona</gtr:collaboratingOrganisation><gtr:country>Italy, Italian Republic</gtr:country><gtr:department>Department of Computer Science</gtr:department><gtr:description>Horizon2020 MAGIS consortium partecipation (Human-Multirobot Cooperation and Coordination for Semi-autonomous Minimally Invasive Surgery)</gtr:description><gtr:id>45ABD107-D445-470E-9439-5D1A9B87FB63</gtr:id><gtr:impact>The main outcome of this collaboration is an ongoing series of applications to Horizon 2020 funding. The collaboration is inherently multidisciplinary as it involves industry partners, groups with expertise in anatomical phantoms for surgery purposes, and hospitals in Italy and elsewhere.</gtr:impact><gtr:outcomeId>58b982b6410821.76178275-1</gtr:outcomeId><gtr:partnerContribution>University of Verona are coordinating the consortium. The latter including a number of other partners, namely: 
UNIVERSITA DEGLI STUDI DI MODENA E REGGIO EMILIA Italy
UNIVERSITA DEGLI STUDI DI FERRARA Italy
OSPEDALE SAN RAFFAELE SRL Italy
UNIVERSITAT POLITECNICA DE CATALUNYA Spain
UNIVERSITY OF DUNDEE United Kingdom
OXFORD BROOKES UNIVERSITY United Kingdom
Medineering GmbH Germany
ACMIT GMBH</gtr:partnerContribution><gtr:piContribution>The AI and Vision group led by Fabio Cuzzolin are Workpackage Leader for a Horizon2020 consortium on robotic surgery. The role of the team is to take care of the computer vision and cognitive functionalities of the system. The proposal was first submitted in April 2016 and received a score of 11.5/15. It will be resubmitted by April 25 2017.
The total budget is 4,315,640 euros. Oxford Brookes' share is 596,073 euros.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Malta</gtr:collaboratingOrganisation><gtr:country>Malta, Republic of</gtr:country><gtr:description>Collaboration with University of Malta on the multilinear classification of EEG signals for BCI applications</gtr:description><gtr:id>639DF25E-339A-41B0-9B04-BF4847ACC8AB</gtr:id><gtr:impact>We have recently submitted an Outline Research Grant application to the Leverhulme Trust. The proposal was invited for full submission in January 2016. We will submit it by the March 21 2016 deadline.</gtr:impact><gtr:outcomeId>b9d8ec12b9d8ec26-1</gtr:outcomeId><gtr:partnerContribution>The partners have extensive expertise in EEG classification. They have captured a significant amount of data in the form suitable to be processed by a multilinear classifier.</gtr:partnerContribution><gtr:piContribution>The project has led to a collaboration in progress with Professor Kenneth Camilleri of the University of Malta on the possible use of the tensorial models developed here to EEG classification for Brain Computer Interfaces:

https://www.um.edu.mt/eng/sce/research/biomedical/braincomputerinterfacing

The group in Malta is collecting EEG data on which to apply our tensorial classification methodology, in the perspective of a future EU partnership.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>BMW Knowledge Day</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>30B33A50-ED27-4FB0-BC32-56328A1C2DEC</gtr:id><gtr:impact>BMW Group, with their Cowley plant invited Prof Cuzzolin and Dr Fridolin Wild, a colleague from the Department of Computing, to talk to the group's annual Knowledge Day.
The presentation was attended by groups of attendees over several BMW plants in the UK, Germany and elsewhere.
The title of the presentation was &amp;quot;Disruptive visual AI for smart factories and cars&amp;quot;, and was directly inspired by the group's work in action detection and recognition which was kickstarted by this EPSRC grant.</gtr:impact><gtr:outcomeId>58b9809e83c228.21142992</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Risk Group LLC: invited podcast on &quot;Advances in Artificial Intelligence: Gesture and Action Recognition&quot;</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EA62669E-C93A-4934-852D-9D2C08B55790</gtr:id><gtr:impact>Prof. Fabio Cuzzolin, Head of Artificial Intelligence and Vision at Oxford Brookes University, Oxford, United Kingdom participated in Risk Roundup to discuss ''Advances in Artificial Intelligence: Human and Non-Human Gesture and Action Recognition''. 

How would we define and describe man-machine or a machine-machine interface and why is it relevant to understanding Artificial Intelligence? Mediator between human (and non-human users) and machines, a man-machine or machine-machine interface, is basically a system that takes care of the entire human-non-human communication process. It is responsible for the delivery of the machine or computer knowledge, functionality and available information, in a way that is compatible with the end-user's communication channels, be it human or non-human. It then translates the user's (human or non-human) actions (user input) into a form (instructions/commands) that is understandable by a machine. 

When increasingly complex Artificial Intelligence based systems, products and services are rapidly emerging across nations, the necessity for more user friendly man-machine or machine-machine interface is becoming increasingly necessary for their effective utilization, and consequently for the success that they were designed for. 

Published on Risk Group: https://www.riskgroupllc.com/advances-in-artificial-intelligence-human-and-non-human-gesture-and-action-recognition/</gtr:impact><gtr:outcomeId>58b97fb40abb65.56875339</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://www.riskgroupllc.com/news/press-releases/advances-in-artificial-intelligence-human-and-non-human-gesture-and-action-recognition/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Publication of an article on International Innovation magazine</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D051FACF-D8C1-45EC-9E2A-662B018F5A4F</gtr:id><gtr:impact>Article sparked several requests from editor for our interest in further publication of our results.</gtr:impact><gtr:outcomeId>545cde687f7414.02147612</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other academic audiences (collaborators, peers etc.)</gtr:primaryAudience><gtr:url>http://www.research-europe.com/index.php/international-innovation/</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our findings have been the basis for follow up research in a number of application scenarios: action recognition in the wild, EEG emotion classification for the design of robotic avatars, pedestrian behavior analysis and driver's monitoring in the automotive industry, the early diagnosis of dementia via machine learning in the healthcare context.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>04267DD6-85CD-4801-8C2A-E23EFCBF3E4D</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545cdd54269857.24462955</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project is exploring different routes to the computer analysis of human motion captured by traditional cameras. In particular, we are interested in testing the possibility of recognizing people's identities at a distance, from the way the walk, or from their gesturing style. The problem is made extremely difficult by the presence of numerous factors which affect recognition, such as different camera viewpoints, illumination conditions, clothing. We have also explored the use of new modeling techniques to automatically learn, represent and recognize complex human activities as those captured by videos stored on YouTube and elsewhere. In the first step crucial information needs to be extracted from these videos. Based on those measurements, the system automatically learn which parts of the video are the most &amp;quot;discriminative&amp;quot; (relevant for recognition purposes) and assembles them in a coherent hierarchy able to describe complex activities, or the presence of multiple actors.

We have obtain significant preliminary results which show that, via these new modelling techniques, recognition rates considerable improve over the current state of the art. Most significantly, we can also localize the presence of an action in space and time within a given video sequence.</gtr:description><gtr:exploitationPathways>The societal impact and market potential of reliable automatic action recognition is enormous. Human-machine interfaces allowing humans to gesturally interact with their laptops, smartphones and even cars are being envisaged right now. ABI Research forecast that 600 million smartphones with gesture recognition features will be shipped in 2017 (http://blog.geoactivegroup.com/2012/07/new-applications-for-gesture.html). EyeSight (http://www.eyesight-tech.com/) already produces software solutions that &amp;quot;allow users to control mobile and portable devices with simple hand gestures&amp;quot;. Smart rooms are being imagined, in which people are assisted in their everyday activities by distributed intelligence in their own homes (switching lights when they move through the rooms, interpreting their gestures to replace remote controls and switches, etcetera). At the Consumer Electronics Show in Las Vegas in January, Mercedes-Benz showed an experimental system (DICE) which lets drivers perform basic functions with a hand gesture. Given our rapidly ageing population, semiautomatic assistance to non-autonomous elderly people and remote clinical monitoring are rapidly gaining interest. A hand-gesture recognition system that enables doctors to manipulate digital images during medical procedures has recently been tested at the Washington Hospital (www.whcenter.org). Security personnel can be assisted by algorithms able to signal anomalous events to their attention for surveillance purposes, improving the general level of security of the European Union (and of senstive areas such as airports or train stations in particular) in uncertain times such as ours. In the US, DARPA's Video and Image Retrieval and Analysis Tool (VIRAT) and Persistent Stare Exploitation and Analysis System (PerSEAS) programs may soon enable better warfighter analysis of huge amounts of data generated from multiple types of sensors. Companies are investing in &amp;quot;behavioral&amp;quot; biometrics, based on people's distinctive gait pattern, to

achieve a significant competitive edge. Finally, techniques able to efficiently datamine the thousands of videos people post, say, on Facebook or YouTube are in dire need: the potential of a &amp;quot;drag and drop&amp;quot; application, similar to that set up by Google for images, able to retrieve videos with a same &amp;quot;semantic&amp;quot; content is easy to imagine. All these companies are investing huge money on internet video retrieval as the

next level in the browsing experience. Truly robust action recognition is likely to contribute enormously (via significant gains in productivity) to boost all these economic sectors in the near future. A number of routes are open for the exploitation of the results of this project.

New consoles (e.g. Microsoft's Kinect) have opened up novel directions in the gaming industry: yet, these only track the user's movements, without any real interpretation of their actions which could &amp;quot;spice up&amp;quot; the gaming experience. Intelligent action recognition can render games which merely track the user's body posture out of fashion: however, gesture recognition with kinect is still in its infancy (http://www.youtube.com/watch?v=H1wIQ2o4INo). We are exploring the possibility of a collaboration with Sony Entertainment (which has a successful history of KTP project with our group) along these lines. 

We have had a series of promising meetings with Magna International (http://www.magna.com/), the car component company, for a collaboration on pedestrian behavior interpretation in low speed (e.g. parking lot) scenarios, and on driver's gesture recognition inside the car.

Together with Professor Helen Dawes we are also exploring the possibility of using gesture recognition techniques to monitor patients' disease progression from their homes via the analysis of the quality of their exercises, labelled according to standard clinical scales such as SARA (http://cms.brookes.ac.uk/staff/FabioCuzzolin/grants/i4i.pdf).

The results of this project have led to spin off grant applications to the Leverhulme Trust (http://cms.brookes.ac.uk/staff/FabioCuzzolin/grants/leverhulme12video-outline.pdf), NIHR (see above) and the European Union (via an ERC Consolidator grant).

Finally (but the list is not exhaustive), we are considering a new EPSRC application on brain wave remote control based on the tensorial/multilinear analysis developed in this project for identity recognition from gait, as they can be very effective for the classification of EEG signals as well.</gtr:exploitationPathways><gtr:id>F1E99B32-BC1D-4615-8A62-9482337BB93B</gtr:id><gtr:outcomeId>r-5870105368.8955547774bb86</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare,Manufacturing, including Industrial Biotechology,Security and Diplomacy</gtr:sector></gtr:sectors><gtr:url>http://cms.brookes.ac.uk/staff/FabioCuzzolin/grants.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>7C7346F6-31E5-4910-9CD1-E54B5634BB45</gtr:id><gtr:title>Learning Pullback HMM Distances.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eea240e3dd066c41853fa6d5ba37d48c"><gtr:id>eea240e3dd066c41853fa6d5ba37d48c</gtr:id><gtr:otherNames>Cuzzolin F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_53d05e05efeae522</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55B14259-364E-4EE4-84A7-74DB9D72E0E7</gtr:id><gtr:title>Metric learning for Parkinsonian identification from IMU gait measurements.</gtr:title><gtr:parentPublicationTitle>Gait &amp; posture</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/eea240e3dd066c41853fa6d5ba37d48c"><gtr:id>eea240e3dd066c41853fa6d5ba37d48c</gtr:id><gtr:otherNames>Cuzzolin F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0966-6362</gtr:issn><gtr:outcomeId>58b9790dbe0cc6.31635502</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E2975021-0228-4200-AAC6-47AFB2A4E634</gtr:id><gtr:title>Belief modeling regression for pose estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f2bfe8c5b37a52188eaf17a094b9045"><gtr:id>3f2bfe8c5b37a52188eaf17a094b9045</gtr:id><gtr:otherNames>Fabio Cuzzolin (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_910656521213df7a9c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>41510EF4-4678-46FC-84D7-D1373BE91B0D</gtr:id><gtr:title>Fisher Tensor Decomposition for Unconstrained Gait Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b0e385df1cae982c1509bb4c1b4993fe"><gtr:id>b0e385df1cae982c1509bb4c1b4993fe</gtr:id><gtr:otherNames>Wenjuan Gong (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>m_7002658117140a2580</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4AE5A913-3BB9-4EAA-AB63-CD9526CA0D8A</gtr:id><gtr:title>Robust classification of multivariate time series by imprecise hidden Markov models</gtr:title><gtr:parentPublicationTitle>International Journal of Approximate Reasoning</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dad8ed89be8b2b98e9cdca430bffcfb7"><gtr:id>dad8ed89be8b2b98e9cdca430bffcfb7</gtr:id><gtr:otherNames>Antonucci A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>545cd46cf06920.39360753</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>424CA482-4DBE-409D-A31B-CEE68672A21B</gtr:id><gtr:title>Learning Discriminative Space-Time Action Parts from Weakly Labelled Videos</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d51c9d60cb7ff175c0874f811f2391fd"><gtr:id>d51c9d60cb7ff175c0874f811f2391fd</gtr:id><gtr:otherNames>Sapienza M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53cfdefdef9c5ae4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>115273BD-9E72-4FF0-B781-41F668051641</gtr:id><gtr:title>A belief-theoretical approach to example-based pose estimation</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Fuzzy Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/de4d6ebdbd9a57aa47162d83ad5fc039"><gtr:id>de4d6ebdbd9a57aa47162d83ad5fc039</gtr:id><gtr:otherNames>Gong W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2fe7026b7f36.23471476</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>585291FD-5064-4DCA-B0D2-738FE206CDED</gtr:id><gtr:title>Learning discriminative space-time actions from weakly labelled videos</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3f2bfe8c5b37a52188eaf17a094b9045"><gtr:id>3f2bfe8c5b37a52188eaf17a094b9045</gtr:id><gtr:otherNames>Fabio Cuzzolin (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m-5575284916.51200213df7b46</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD592A47-23DE-4928-B0E1-8CA1A61ACAF9</gtr:id><gtr:title>Online action recognition via nonparametric incremental learning</gtr:title><gtr:parentPublicationTitle>Proceedings of BMVC 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4146f39dd2b2978a013b6c02aa0cbbc0"><gtr:id>4146f39dd2b2978a013b6c02aa0cbbc0</gtr:id><gtr:otherNames>De Rosa R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>545cd3652f5f23.17283607</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I018719/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>