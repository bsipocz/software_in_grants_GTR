<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:department>Philosophy</gtr:department><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2646F1BE-DBD2-411B-9D89-C143536D1D43"><gtr:id>2646F1BE-DBD2-411B-9D89-C143536D1D43</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Glennerster</gtr:surname><gtr:orcidId>0000-0002-8674-2763</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/49C0580B-CC7B-4148-BF55-0EB631D51475"><gtr:id>49C0580B-CC7B-4148-BF55-0EB631D51475</gtr:id><gtr:firstName>James</gtr:firstName><gtr:surname>Stazicker</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=AH%2FN006011%2F1"><gtr:id>7EFAFEDB-0F0C-4A73-AE9F-54DE3AA50215</gtr:id><gtr:title>The action-based brain: a provocation to philosophy, robotics and the cognitive sciences</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/N006011/1</gtr:grantReference><gtr:abstractText>We often assume that our eyes deliver a detailed image of the world around us, but this is a mistake. Instead, as soon as we want to know the detail in a part of the scene, our eyes flick towards it and the detail is revealed. In this example, perception depends on the world, the motor system and an internal representation that enables the world to act as a 'store' of information that is always retrievable when required. Philosophical theories of 'embodied cognition' emphasise this link between the mind and the motor system. What this analysis lacks, however, is a firm neural and computational grounding specifying the proposed implementation. By looking in detail at a specific version of embodiment that has arisen within the neuroscience community and by exposing it to the critical tools of philosophers, we aim to assess the scope and limits of one particular proposal, and also to draw broader lessons for how neuroscience, robotics and philosophy ought to interact, with respect to these general questions about the nature of cognition.
 
The network will be inherently multidisciplinary, based on the involvement of mathematicians, engineers, roboticists, psychologists, neuroscientists and philosophers, each with different, complementary inputs. The project is based in Philosophy in Reading, led by a philosopher and already includes key international experts in philosophy of mind and embodied cognition. International experts in neuroscience and robotics have also agreed to take part in the discussion and we will extend the group progressively.

The interactions between members of the network will take place through (i) discussion on a wiki page, with new sets of pages being introduced as the debate develops; (ii) joint video conference meetings focused on specific, contentious areas and (iii) a two day conference towards the end of the 12 month period involving international experts in neuroscience, philosophy of mind and cognitive robotics.

We will disseminate the results of the network primarily through a Brain and Behavioral Sciences article. This is a forum with high impact in both the philosophical and neuroscience communities. We will also publish more specialist articles in journals for neuroscience, engineering and philosophy. 
By the end of the year, we will develop a more public-facing portal to the wiki and encourage engagement with a broader audience outside academia. At this point we will use facilities such as ResearchGate to reach a wider group of researchers and social media such as Twitter to advertise the discussion within and beyond the academic community.</gtr:abstractText><gtr:potentialImpactText>The most immediate planned impacts of the work will be in applied fields such as robotics and the guidance of autonomous vehicles. Currently, the predominant type of representation is based on a 3D, world-centred frame. There are good reasons to believe that this explicit reconstruction is rather different from the way that humans or other animals represent their environment and the robotics community has a strong interest in developing workable alternatives. For example, we collaborate with Paul Newman who heads the Mobile Robotics Group in Oxford and there is a keen interest within the MRG for developing novel, view-based approaches to navigation and mapping, of the kind our network will investigate. The pathways to impact document expands on the pathways by which the network can influence the development of this type of representation.

In computer vision, there may also be impact in the medium term. Here, we collaborate with researchers at Microsoft Research who are interested in object representation and scene representation using image-based techniques that avoid 3D reconstruction. The development of this type of approach is likely to be informed by biological models and human experiments of the sort that are central to the topic discussed in the network. The proposal about brain-function which our network will discuss is partly inspired by problems and developments in computer vision, and this is a topic where theoretical and practical developments provide crucial feedback for one another. Again, the pathways to impact document provides more detail on the way that these links with industry will be developed.

Of course, the long term consequences of discovering a coherent description of brain function including perception, action and thought will be far reaching, with a profound impact on many areas of health, education and industry. The research network will also contribute to this wider long term goal.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2016-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>30981</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>So far the impact of the project is at an early stage, developing and pursuing pathways to impact. (The grant period was one year, ending February 2017.) Through meetings of the Research Network and other invited visits to collaborators, we have developed connections with:
U.S. Navy;
Andrew Fitzgibbon at Microsoft;
Lindsay Pickup at Optellum Ltd;
Abhinav Gupta's robotics and computer vision group at Carnegie Mellon University;
Tony Prescott's robotics group at the University of Sheffield;
Paul Newman of Oxford Robotics and the commercial spin-off working on driverless cars, Oxbotica;
The Arts and Humanities Research Council 'Science in Culture' theme, including collaboration with the UK Space Agency.

These collaborations constitute developing pathways to impact in the commercial as well as the academic sphere. In particular, we have identified close connections between human perception and the deep learning systems exploited for artificial vision in commercial navigation systems. This provides a platform for commercial developers and academics to learn from one another's work about artificial cognition and action-based cognition in the human brain. In addition to our publicly available wiki and our planned target article with peer commentary by roboticists, the most concrete pathway to impact so far is Glennerster's collaboration with the U.S. Navy (which has attracted substantial funding from the U.S. Navy, via the Multidisciplinary University Research Initiatives Program).</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>F58995BC-3D5C-462A-BACB-4B83DAC4DB18</gtr:id><gtr:impactTypes/><gtr:outcomeId>58c28e5c0af5e8.82200197</gtr:outcomeId><gtr:sector>Aerospace, Defence and Marine</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>By growing and collaborating with our Research Network of philosophers, neuroscientists and experts in robotics and computer vision, we have exposed action-based theories of perception and cognition to multidisciplinary analysis. Our key findings focus on the hypothesis that an action-based process, rather than a 3D model or reconstruction of a scene, can explain an observer's visual perception of the 3D layout of the scene and her consequent actions. Key findings include:

1. (Philosophy) There is a definitional problem about distinguishing between action-based and model-based perception, given that both are ways of perceiving in three dimensions. We can solve this problem, distinguishing between the two forms of perception, if we (i) focus on perception of 'vista space' (the region of space that's visible as you move your head and eyes) and (ii) understand action-based perception as a process of comparing current contexts with stored contexts (a process of the kind traditionally invoked to explain motor control). At the level of vista-space perception, such an action-based theory makes substantively different predictions from a model-based theory. For example, the action-based theory predicts that observers' spatial perception may admit of no consistent mapping onto a coordinate frame defined by three axes (a prediction that's confirmed by previous work in Glennerster's lab). Importantly, this kind of action-based theory avoids charges levelled at previous action-based theories in the philosophical literature, such as the charge of 'behaviourism'.

2. (Neuroscience) This action-based theory has a key advantage over model-based theories: we know how the former, but not the latter, can be implemented by neurons, as reflected in previous work on motor control in the cerebellum. However, our research suggests that it is an open question whether the cerebellum itself has the storage capacity required for action-based perception. Further brain areas are probably crucial.

3. (Robotics and Computer Vision) One challenge to our action-based theory turns on the question how stored contexts are learned. Recent work on artificial visual processing by deep learning systems provides a way of understanding how this could be achieved. Some of the latest systems (developed at Carnegie Mellon University) have much in common with our proposed mechanism. For example, they learn stored contexts which combine sensory and motivational information and they do not represent a scene in terms of a 3D coordinate model. An action-based theory of the brain and this work in computer vision have much to learn from one another.

Although these findings are listed here according to discipline, each of them has clear multidisciplinary implications and motivations. This meets our objectives of testing the action-based theory through multidisciplinary analysis, and of setting an agenda for future work across disciplines.</gtr:description><gtr:exploitationPathways>Academic philosophers will benefit from our detailed description of how action-based perception might be realized in the brain. Philosophical discussion has tended to focus on action-based theories which replace perceptual representation with action, criticizing action-based theories for being unacceptably behaviourist. We show how a physiologically realistic action-based theory need not replace perceptual representation with action, but can instead involve the kind of representation traditionally accepted as a means of motor control. This marks an important possibility which should inform subsequent arguments about the nature of perception and action.

The close connections we have discovered between our hypothesis and artificial deep learning systems mean that work in neuroscience and computer vision / robotics can learn a great deal from one another. In particular, we have identified a specific niche in which neuroscientific theories can learn from deep learning systems: explaining how the brain learns stored contexts for vision and action without relying on 3D coordinate-frame reconstructions. Similarly, as this work progresses, computer vision and robotics will be in a position to draw on neuroscientists' models of the brain.

Our target article, wiki and future collaborations provide a substantial platform for our work to have these impacts.</gtr:exploitationPathways><gtr:id>EA8D3D4D-D37C-4D9E-A45A-F7E90BB7AE5B</gtr:id><gtr:outcomeId>58c14dce084db8.29562241</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Electronics,Manufacturing, including Industrial Biotechology,Transport</gtr:sector></gtr:sectors><gtr:url>https://jamesstazicker.com/research/the-action-based-brain/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9EDF1027-2396-4B6A-AAB1-65DD7E5CFD0E</gtr:id><gtr:title>A moving observer in a three-dimensional world.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cdd60f011d094818d0c8d752c2bf5814"><gtr:id>cdd60f011d094818d0c8d752c2bf5814</gtr:id><gtr:otherNames>Glennerster A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>585d5700de1a04.55361840</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D8F12F4-4587-48E4-AA2F-3254B6E7BD64</gtr:id><gtr:title>Pointing Errors in Non-Metric Virtual Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da99919b0cfba3a6fcce657634c3700a"><gtr:id>da99919b0cfba3a6fcce657634c3700a</gtr:id><gtr:otherNames>Muryy A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a9efc801dc816.63932360</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CC26DAB0-7502-4021-9AFE-353189B10939</gtr:id><gtr:title>Comparison of view-based and reconstruction-based models of human navigational strategy.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/802b69ca74544d30568775d34709725a"><gtr:id>802b69ca74544d30568775d34709725a</gtr:id><gtr:otherNames>Gootjes-Dreesbach L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>5a9416bd0d7a35.07024274</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F6B6E788-BD87-4568-9792-59BFD278050B</gtr:id><gtr:title>Navigation and pointing errors in non-metric environments.</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da99919b0cfba3a6fcce657634c3700a"><gtr:id>da99919b0cfba3a6fcce657634c3700a</gtr:id><gtr:otherNames>Muryy A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a941adf4fcb65.18821822</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">AH/N006011/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>946C0541-464B-43A5-9EE3-65C23360E959</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Philosophy</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>53F27348-198B-4AEF-A34B-8307067F507C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>E457FFDE-A4C1-4907-AE12-A394D95A3AE5</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Cognitive Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>C3FAF999-8402-446C-8A80-016962744E22</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Epistemology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A72E6EED-2213-4658-9EB5-5A5DD2F65F2D</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Intelligent &amp; Expert Systems</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>143EF6F8-CF59-4D04-9EA7-40D70FC41177</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Philosophy Of Mind</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0C6428B5-4A75-42D5-9D5B-5DF34F513B7E</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Science &amp; Maths, &amp; Math Logic</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>