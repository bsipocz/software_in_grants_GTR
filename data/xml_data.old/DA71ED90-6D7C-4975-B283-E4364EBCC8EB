<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/EB4118EE-7395-4BE1-9EFE-80B2962FB869"><gtr:id>EB4118EE-7395-4BE1-9EFE-80B2962FB869</gtr:id><gtr:firstName>Danail</gtr:firstName><gtr:surname>Stoyanov</gtr:surname><gtr:orcidId>0000-0002-0980-3227</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN013220%2F1"><gtr:id>DA71ED90-6D7C-4975-B283-E4364EBCC8EB</gtr:id><gtr:title>Image guided surgery through spatio-temporal signal amplification</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N013220/1</gtr:grantReference><gtr:abstractText>Through advances in instrumentation and high resolution digital video, surgical techniques are increasingly becoming more minimally invasive. Reducing the access trauma of surgery has many advantages for the patient such as reduced hospitalisation, scarring, co-morbidity and post-operative pain. However, limiting the surgeon's access to the surgical site inevitably increases the complexity of operations. Clinically, it is crucial to enhance visualisation during minimally invasive surgery and in particular to enable the surgeon to see structures underneath the exposed organ surface and to observe the functional characteristics of tissues. The availability of this information in real-time during surgery can assist the surgeon to prevent damage to critical anatomical structures and to preserve the viability of healthy tissues. 
 
Information about the location of blood vessels is actually inherently embedded in the endoscopic video signal from minimally invasive surgery in the form of motion. This can be observed easily when the vessel is large and near the tissue's surface. However, when the vessel is small and embedded within the tissue, the motion may be very subtle and not naturally visible by the naked eye because the human visual system is tuned to specific frequencies and motion amplitudes. Similar variations are present in the radiometric channels of endoscopic video, where colour fluctuations are surrogate measures of changes in tissue perfusion linked to the cardiac cycle. These subtle spatio-temporal video variations can be computationally detected and measured which constitutes the focus of the proposed project.
 
The difficulty in exposing subtle variations in endoscopic video is that the surgical site is highly deformable and dynamic, which typically obscures the location of sub-surface vessels. To compensate for large scene dynamics, we will use a combination of registration and tracking techniques that temporally align specific regions of tissue. Once small variations have been identified, observations from different angles acquired either by a stereo endoscope or by a moving device will facilitate the localisation of vessels under the tissue. Mathematically, the theory of sparsity regularization and non-smooth regularisation will be exploited to solve for the vessel position leveraging existing research in tomography imaging e.g. using non-convex penalties and adaptive algorithms.

The computational techniques to be developed, in the form of inverse problems for recovering and localising the source of subtle motion or colour variations, have wide applicability to many image and vision computing problems. In particular, applications where large dynamic effects need to be considered and removed apriori and where the problem is under determined due to the complexity of the structures under investigation.</gtr:abstractText><gtr:potentialImpactText>Integration of the proposed research methods into clinical practice can lead to significant patient benefits by providing more effective and less traumatic minimally invasive surgical procedures with reduced complication rates. The overall effect of increasing the uptake and efficacy of minimally invasive surgery can have dramatic advantages for healthcare delivery and the related social/economic factors. Enabling patients to return to normal living faster due to minimised trauma reduces the costs of medical treatment and the overall long term effect of interventional healthcare.

For example, in laparoscopic liver resection procedures where the target organ is highly vascular but the surgeon has a very limited view of the embedded blood vessels, highlighting the location of vessels can shorten procedural length and reduce the risk of vessel rupture. Amplifying subtle colour variations in the video signal can further provide additional information about organ perfusion and the viability of anastomosis or organ function post-revascularisation. This clinical application will be evaluated during the proposed project in collaboration with Professor Brian Davidson at the Royal Free Hospital and his newly established perfusion and imaging laboratory.

The proposed research could also facilitate the advancement of new surgical techniques. For example in new surgical specialisations such as fetal therapy for twin-twin-transfusion syndrome, the location of blood vessels beneath the surface of the placenta is critical information to ensure the viability of laser ablation and the overall success of an intervention. Additional important clinical applications will emerge when the developed techniques are transferred to different intra-operative imaging modalities such as, e.g. fluoroscopy and ultrasound. 

The project is well aligned with the EPSRC grand challenges in Healthcare Technologies. In particular for developing new ways to enhance efficacy, minimise costs and reduce risk to patients during surgery and push the frontiers of physical interventions to achieve repeatable high precision with minimal invasiveness. The cross-cutting capabilities developed in the project to target these grand challenges include disruptive technologies for sensing and analysis; novel computational and mathematical sciences; and novel imaging technologies. Specifically the lattes theme where the project will develop higher performance, lower cost imaging with techniques for image reconstruction and high throughput, real-time imaging at the point of care.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98250</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1239250</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Early Career Fellowship</gtr:description><gtr:end>2022-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/P012841/1</gtr:fundingRef><gtr:id>153704E8-66C9-43C6-8870-3ABE507CEA29</gtr:id><gtr:outcomeId>58ca425d657766.18425260</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>96000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Faculty of Engineering</gtr:department><gtr:description>DTG Scholarship</gtr:description><gtr:end>2020-05-02</gtr:end><gtr:fundingOrg>University of Leeds</gtr:fundingOrg><gtr:id>C7B17993-67FA-45FE-A2EA-C26CDBD1F9D2</gtr:id><gtr:outcomeId>58ca3e68352df7.16806274</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-05-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B516BB36-51B5-4E9C-8809-F2D944E74273</gtr:id><gtr:title>A Combined EM and Visual Tracking Probabilistic Model for Robust Mosaicking: Application to Fetoscopy</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da881b8e0743bb2f36f4fd4ef8f97ca6"><gtr:id>da881b8e0743bb2f36f4fd4ef8f97ca6</gtr:id><gtr:otherNames>Tella M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a915d7df14164.58582401</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6D5DB31F-3428-40A0-8D63-B5B300200164</gtr:id><gtr:title>Robust Catheter and Guidewire Tracking Using B-Spline Tube Model and Pixel-Wise Posteriors</gtr:title><gtr:parentPublicationTitle>IEEE Robotics and Automation Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d2f2a3c329a2fa17c2343c3bd114ff5"><gtr:id>8d2f2a3c329a2fa17c2343c3bd114ff5</gtr:id><gtr:otherNames>Chang P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a915d800e9e15.94517632</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>31A1C992-AF7C-446B-B957-E507369378D6</gtr:id><gtr:title>Vision-based and marker-less surgical tool detection and tracking: a review of the literature.</gtr:title><gtr:parentPublicationTitle>Medical image analysis</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4c0e72d009827438edf6f974de62f11e"><gtr:id>4c0e72d009827438edf6f974de62f11e</gtr:id><gtr:otherNames>Bouget D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1361-8415</gtr:issn><gtr:outcomeId>58bd433f686679.12845895</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>94A91AB7-CBD8-4EB3-832D-768B07870ACA</gtr:id><gtr:title>Medical-grade Sterilizable Target for Fluid-immersed Fetoscope Optical Distortion Calibration.</gtr:title><gtr:parentPublicationTitle>Journal of visualized experiments : JoVE</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8cf0ef52097e9d1cb8c89754ff2e5dd1"><gtr:id>8cf0ef52097e9d1cb8c89754ff2e5dd1</gtr:id><gtr:otherNames>Nikitichev DI</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1940-087X</gtr:issn><gtr:outcomeId>5a35e3bb454709.37764781</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EB42C857-F0A5-4D64-ACF4-0EC9DD1F8FBF</gtr:id><gtr:title>Surgical robotics beyond enhanced dexterity instrumentation: a survey of machine learning techniques and their role in intelligent and autonomous surgical actions.</gtr:title><gtr:parentPublicationTitle>International journal of computer assisted radiology and surgery</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9f4ce2f1f220f9ac385655d37177157"><gtr:id>c9f4ce2f1f220f9ac385655d37177157</gtr:id><gtr:otherNames>Kassahun Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1861-6410</gtr:issn><gtr:outcomeId>57067771788155.47055920</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>14F96AC5-94D0-4DBE-B591-A0B03A45ABEF</gtr:id><gtr:title>Bayesian Estimation of Intrinsic Tissue Oxygenation and Perfusion From RGB Images.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on medical imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd82d2282685ad5be4a70b6d49034f67"><gtr:id>bd82d2282685ad5be4a70b6d49034f67</gtr:id><gtr:otherNames>Jones G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0278-0062</gtr:issn><gtr:outcomeId>5a2fea65043e02.06460584</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>64C66540-DA46-43AD-938C-AB55EC52B086</gtr:id><gtr:title>Erratum to: Surgical robotics beyond enhanced dexterity instrumentation: a survey of machine learning techniques and their role in intelligent and autonomous surgical actions.</gtr:title><gtr:parentPublicationTitle>International journal of computer assisted radiology and surgery</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9f4ce2f1f220f9ac385655d37177157"><gtr:id>c9f4ce2f1f220f9ac385655d37177157</gtr:id><gtr:otherNames>Kassahun Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1861-6410</gtr:issn><gtr:outcomeId>570677fa591893.16943110</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D3416F7-D656-4AB4-BBE7-E1FF181B626D</gtr:id><gtr:title>Hand-eye calibration for robotic assisted minimally invasive surgery without a calibration object</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/214b46cf11ba3e4f4d8ac262190f6b33"><gtr:id>214b46cf11ba3e4f4d8ac262190f6b33</gtr:id><gtr:otherNames>Pachtrachai K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5a915d7eb9bfd2.67203953</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N013220/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>