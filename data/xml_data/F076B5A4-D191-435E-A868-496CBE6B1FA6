<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:department>Sch of Systems Engineering</gtr:department><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/0F1AF0FF-4364-4AFE-9B6A-6EE2F56D80B9"><gtr:id>0F1AF0FF-4364-4AFE-9B6A-6EE2F56D80B9</gtr:id><gtr:firstName>Atta</gtr:firstName><gtr:surname>Badii</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=DT%2FE006140%2F1"><gtr:id>F076B5A4-D191-435E-A868-496CBE6B1FA6</gtr:id><gtr:title>DREAM: Dynamic RetriEval, Analysis and semantic metadata Management (DREAM)</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>DT/E006140/1</gtr:grantReference><gtr:abstractText>DREAM will develop efficient storage, management, indexing, search, use and re-use of digital media assets for the postproduction industry. The R&amp;amp;D focuses on the creation and use of metadata generated from low-level content analysis and high-level semantic awareness, and the incorporation of semantic metadata into the workflow process. A novel Metadatabase and semantic intervention will support data analysis, indexing and retrieval. DREAM will create media industry products in the mid-term, and enable the subsequent development of applications in other sectors including security, medical, corporate, and consumer. The project is led by award-winning UK postproduction technology developer FilmLight, working with a company specialising in media software plug-ins (The Foundry), a major user partner (Double Negative) and a university partner (University of Reading IMSS) specialising in semantic technologies. There are three technical strands. The first will define the semantic infrastructure, involving metadata design and domain ontologies and develop tools for semantic video indexing, query formulation and retrieval. In this we will address the 'semantic gap' by combining metadata extracted by image processing with semantic descriptions derived from a network of ontologies, behavioural models and interactive techniques that recognise user behaviour and context. Where automatic detection from low-level processes gives insufficient clues, iterative cycles of image processing may be interleaved with feature extraction and semantic labelling until there is resolution. The second strand will develop new algorithms and tools to extract very rich, persistent metadata based on image processing and motion analysis. The image analysis will be linked to semantic knowledge to improve the accuracy of metadata extraction, and the resulting metadata will in turn improve the object recognition for information retrieval (by for example producing much more accurate human maps than previously possible). The third strand addresses the database level and data management, associating the metadata and data as they move through the system. In an image-based data world, such as digital cinema, the objects are too large and numerous to hold in a database. DREAM will create a Metadatabase to track the metadata associated with files held in any kind of distributed file system. The Metadatabase may be both physical and virtual, since some of the metadata types (motion vector analysis and derivatives) are large files that may be stored in the file system itself. The resulting system may be based on a single site or on multiple sites connected by a secure VPN. The project will result in new scientific and technological knowledge that forms the basis for a range of media industry products in the mid-term, and for further R&amp;amp;D leading to their long-term application to other sectors. The principal deliverables will be: - Ontologies and semantic models of postproduction processes, domain specific models relating to genres (such as action, drama) and objects appearing in them - A semantic resolution engine for labelling and query support - Smart proxy enabled tools for detection, labelling and transcoding data - Templates and role-specific interfaces for semi-automatic semantic labelling, query and retrieval - Algorithms for metadata extraction from low-level features of moving images, with semantic links to higher-level features - Metadata format definitions and interface for the OFX plug-in standard - New tools and plug-ins for motion estimation, object and character matting, feature extraction and modification based on the use of persistent metadata and semantic analysis - A metadatabase linked to a semantically enabled data management application for very large distributed file systems - An integrated prototype solution combining semantic retrieval tools, image analysis and metadata extraction, metadatabase and data management system</gtr:abstractText><gtr:fund><gtr:end>2009-04-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-10-02</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>441421</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>928C8F80-5C06-40DD-A25B-F7DC25557217</gtr:id><gtr:title>Scaling Topic Maps</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1d3dc9907b0846d1110c8d11adcd0306"><gtr:id>1d3dc9907b0846d1110c8d11adcd0306</gtr:id><gtr:otherNames>Badii A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-3-540-70873-5</gtr:isbn><gtr:outcomeId>doi_53cfccfcc89770bc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9C6A690-9276-4BFA-A01F-3E26C274F7C2</gtr:id><gtr:title>Semantic-associative visual content labelling and retrieval: A multimodal approach</gtr:title><gtr:parentPublicationTitle>Signal Processing: Image Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8b7bffb57887ef822c8d5b28af54dfc5"><gtr:id>8b7bffb57887ef822c8d5b28af54dfc5</gtr:id><gtr:otherNames>Zhu M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53cff7ff7597a0c8</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">DT/E006140/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>