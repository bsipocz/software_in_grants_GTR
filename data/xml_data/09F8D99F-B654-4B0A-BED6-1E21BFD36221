<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D5EF406D-A320-44D9-9464-9D6BC3AD1649"><gtr:id>D5EF406D-A320-44D9-9464-9D6BC3AD1649</gtr:id><gtr:name>Nippon Telegraph and Telephone Corporation (NTT)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/BAB368AC-F53E-4B01-A4D9-23CBC4A1996D"><gtr:id>BAB368AC-F53E-4B01-A4D9-23CBC4A1996D</gtr:id><gtr:name>Google</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Ear Institute</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5EF406D-A320-44D9-9464-9D6BC3AD1649"><gtr:id>D5EF406D-A320-44D9-9464-9D6BC3AD1649</gtr:id><gtr:name>Nippon Telegraph and Telephone Corporation (NTT)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BAB368AC-F53E-4B01-A4D9-23CBC4A1996D"><gtr:id>BAB368AC-F53E-4B01-A4D9-23CBC4A1996D</gtr:id><gtr:name>Google</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5419869-1D80-43BA-B2E7-150F32A6B3EB"><gtr:id>D5419869-1D80-43BA-B2E7-150F32A6B3EB</gtr:id><gtr:name>NNT Group (Nippon Teleg Teleph Corp)</gtr:name><gtr:address><gtr:line1>3-1 Morinosato-Wakamiya</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6F16E2CF-56F5-4EF2-8125-A28EC7D26D91"><gtr:id>6F16E2CF-56F5-4EF2-8125-A28EC7D26D91</gtr:id><gtr:firstName>Maria</gtr:firstName><gtr:surname>Chait</gtr:surname><gtr:orcidId>0000-0002-7808-3593</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FL026864%2F1"><gtr:id>09F8D99F-B654-4B0A-BED6-1E21BFD36221</gtr:id><gtr:title>Measuring and understanding auditory salience and distraction</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/L026864/1</gtr:grantReference><gtr:abstractText>Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.</gtr:abstractText><gtr:fund><gtr:end>2016-12-15</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2014-10-03</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>32794</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Google</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Google</gtr:description><gtr:id>2B8BD5CB-9C9F-417C-8673-31A3ABC88A66</gtr:id><gtr:impact>we have recently submitted a BBSRC project grant based on this work (UCL, Google, NTT collaboration)</gtr:impact><gtr:outcomeId>58ca798aa9cf12.68373777-1</gtr:outcomeId><gtr:partnerContribution>Google is contributing algorithms and the experimental platform (mechanical turk)</gtr:partnerContribution><gtr:piContribution>the initial work on auditory salience published this year (Petsas et al, 2016) has attracted the interest of the machine hearing team in Google. we have not embarked in a formal collaboration to use this paradigm to understand which sounds are 'more salient'</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Nippon Telegraph and Telephone Corporation (NTT)</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>collaboration with NTT (Japan Telecom) on Change detection and Salience</gtr:description><gtr:id>8247913B-194D-4ACC-944E-08BFFE83860E</gtr:id><gtr:impact>See more information about the work under the BBSRC partenring award (BB/L026864/1) form.</gtr:impact><gtr:outcomeId>58ca7557ed4132.17474115-1</gtr:outcomeId><gtr:partnerContribution>See more information about the work under the BBSRC partenring award (BB/L026864/1) form.</gtr:partnerContribution><gtr:piContribution>prompted by mutual interests in change detection, the collaboration resulted in a BBSRC partenring award, one published co-authored paper (Petsas et al) and 2 more papers being prepared for publication. See more information about the work under the BBSRC partenring award (BB/L026864/1) form.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>School visit (Hackney) part of iDiscover program</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>E23F6532-A1F5-472D-A3AB-37FAA9B951FB</gtr:id><gtr:impact>Visit to a primary school in Hackney as part of the iDiscover program http://www.inspire-ebp.org.uk/article/items/iDiscover-new-year-new-themes-new-funding.html</gtr:impact><gtr:outcomeId>56e0ab61bca0b1.51876155</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UCL-PSL workshop on sensory systems in complex environments</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>D99D05E8-DB04-4120-B843-6939F7A16124</gtr:id><gtr:impact>I organized a two day workshop on &amp;quot;Sensory Systems in Complex Environments&amp;quot; which brought together researchers (PIs and students) from UCL and Paris.

More details are here: http://audition.ens.fr/workshop/ws_ucl_psl_2015/

The workshop was funded by a generous contribution from the UCL and PSL international offices.</gtr:impact><gtr:outcomeId>56e0aef472b919.53884627</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://audition.ens.fr/workshop/ws_ucl_psl_2015/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>secrets of the brain</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9D3F297C-E3C2-4FC3-9DCD-B09703401D2B</gtr:id><gtr:impact>filming took place in my lab on Jan 2017 for a documentary &amp;quot;Secrets of the Brain&amp;quot; (Lambent productions), which will air at the end of 2017.
Our work will be featured in an episode devoted to auditory processing and music.</gtr:impact><gtr:outcomeId>58ca77a4dd8d98.23121376</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Science museum</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F454EADA-B7F6-4D5E-B40A-43FD0D46E374</gtr:id><gtr:impact>The Science museum filmed work in our lab, to be featured in the permanent exhibition within the new mathematics gallery (due to open in December 2016).</gtr:impact><gtr:outcomeId>58124942ab1f97.07255505</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Guardian project</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>708478BB-1C8E-4692-BFC7-6BE90FA7455A</gtr:id><gtr:impact>Guardian project on brain and music. Short films featuring musicians listening to their favourite songs and the associated brain activity. Filming took place in my lab over the summer. Published on the guardian website in October 2016.</gtr:impact><gtr:outcomeId>58ca77106e1122.76404247</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>https://www.ucl.ac.uk/ear/research/chaitlab/Guardian</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The partnering award is now entering it's second year (project end date is October 2016). 

So far, we have met objective #1 (design of behavioural paradigm) and have completed several experiments (manuscript submitted) towards objective #3 (understanding auditory distraction). The aim for the upcoming period is to design and run a combined EEG/eye tracking experiment towards aim 2. 
 Our behavioural work to date is summarized in a recently submitted manuscript - Petsas, Harrison, Furukawa, Kashino &amp;amp; Chait (in review; Hearing research) The effect of distraction on change detection in crowded acoustic scenes. In this work, we examined the effect of various auditory distractors on change detection. Our results demonstrate that the maintenance of scene information in short term memory is affected by the (un) availability of attentional resources during the gap, and that this dependence is modality specific. We also show that these processes are susceptible to bottom up driven distraction even in situations when the distractors are not novel, but occur regularly on each trial. The extent of the disruption is systematically linked with the, independently determined, perceptual salience of the distractor.</gtr:description><gtr:exploitationPathways>This award is tightly linked to my BBSRC award and enabled us to extend our experiments (and expertise) to cutting edge eye tracking (micro-saccade based) techniques. This has already resulted in a publication in the context of the ongoing BBSRC grant, and has also inspired further ongoing work in my lab. 
 The issue of distraction is currently gaining interest worldwide, notably in the context of human/machine interface design the complementary expertise of the two groups are key to take the work forward. We are excited about the upcoming experiments in the second year.</gtr:exploitationPathways><gtr:id>AAA0A7EF-EC3C-421B-B593-3E952643EB3A</gtr:id><gtr:outcomeId>56e0b42c9d1d34.77221122</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Other</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F8FDA518-FEC0-43C3-AF88-02F36BF72812</gtr:id><gtr:title>How the Eyes Detect Acoustic Transitions: A Study of Pupillary Responses to Transitions Between Regular and Random Frequency Patterns</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f921a43055382499df81106764fdf591"><gtr:id>f921a43055382499df81106764fdf591</gtr:id><gtr:otherNames>Hsin I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c9d876bc4c22.75731864</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>33681A22-489D-47DB-8BB6-2CE96C24A7B2</gtr:id><gtr:title>The effect of distraction on change detection in crowded acoustic scenes.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3895368b0b3b6fef88c998698bf5337c"><gtr:id>3895368b0b3b6fef88c998698bf5337c</gtr:id><gtr:otherNames>Petsas T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>56e0b69776ae71.96442097</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>90A779C3-763E-4E2C-AD62-96C93BAA24F6</gtr:id><gtr:title>Eye metrics - a measure of auditory distraction?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/22f6a291764e3852d3e3c37adb1f1a9d"><gtr:id>22f6a291764e3852d3e3c37adb1f1a9d</gtr:id><gtr:otherNames>Yoneya M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c9d8c53d36e6.72438196</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/L026864/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>