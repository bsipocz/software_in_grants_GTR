<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/392BB49E-1FD6-4E64-864F-2D30C29EEF98"><gtr:id>392BB49E-1FD6-4E64-864F-2D30C29EEF98</gtr:id><gtr:firstName>Nicholas Edward</gtr:firstName><gtr:surname>Scott-Samuel</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0CBE9996-1294-4035-8D27-F56940A8C2AE"><gtr:id>0CBE9996-1294-4035-8D27-F56940A8C2AE</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Gibson</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3F2F0DDC-1DE5-4F0C-A129-F6513FEE44FA"><gtr:id>3F2F0DDC-1DE5-4F0C-A129-F6513FEE44FA</gtr:id><gtr:firstName>Christopher</gtr:firstName><gtr:surname>Benton</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D7400F7E-A0B3-474A-BDCD-A01D69625AE1"><gtr:id>D7400F7E-A0B3-474A-BDCD-A01D69625AE1</gtr:id><gtr:firstName>Neill</gtr:firstName><gtr:surname>Campbell</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD506549%2F1"><gtr:id>5ED1F2A0-B468-4876-A71E-1C851B83C510</gtr:id><gtr:title>Understanding Biological Motion using Moving Light Displays</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D506549/1</gtr:grantReference><gtr:abstractText>The major inspiration for this work is that the automatic extraction and understanding of biological motions is largely an unsolved problem. Because of the complexity and variability of natural motions (and hence the image sequences that encode them), automatic computer vision based analysis of such data is under-developed. Current techniques rely on highly constrained data collection methods or intrusive physical interaction. Traditional motion capture relies on the placement of trackable locators (often affixed to lycra suits) which is often impractical and sometimes impossible. Apart from the expense and the need for a constrained filming environment, locators cannot easily be placed on wild or small creatures. To be able to automatically extract characteristic motion from existing film footage is desirable since many applications could benefit from such information. Such applications include media database searching and understanding, accurate automated surveillance systems, medical based gait analysis, computer animation and robotics.We propose that a computer vision based approach to the understanding of the Moving Light Display phenomenon will improve, in a general sense, the interpretation of motions shown in image sequences of animated animals and humans. The MLD (also known as the Point Light Display, PLD), consists of a small set of point lights that describe an action or behaviour (walking or dancing for example), of a subject. A great deal of work has been carried out on the analysis of MLDs and the design of stimuli for psychophysical experiments. Psychologists have shown that high-level information can be extracted from the temporally varying point light set moving such that actions or behaviour such as gait, personal identity, gender and even mood can be determined. It has also been shown that an average observer can deal with a significant amount of MLD perturbation such as noise, occlusion, translation and rotation (in both 2D and 3D). This observation implies that a large amount of information can be extracted from a small set of moving point lights.It is interesting to note that all the major advances in human tracking utilising computer vision over the last ten years, have been based around the appearance (colour, texture etc.) of the subject. This is not surprising since, using a constrained or known background and making assumptions about coherent clothing, strong edges or skin colour, such approaches have led to some impressive algorithms and systems. One of the main claims here, however, is that by doing so, the field at large has missed an important step. A small number of point lights representing an action, motion or emotion is easily perceived by humans, but we still have almost no underlying theorems as to why this should be so. Crucially, we have no 'manual' as how to transfer this skill of humans so as to provide an engineering solution to a range of important visual inspection tasks. This proposal aims to address this gap in our knowledge.</gtr:abstractText><gtr:fund><gtr:end>2009-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>354777</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>35734BB6-27C9-4942-9572-A67503BDF7DF</gtr:id><gtr:title>Form overshadows 'opponent motion' information in processing of biological motion from point light walker stimuli.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dffd2f67e976b025e3ffbb60528e3079"><gtr:id>dffd2f67e976b025e3ffbb60528e3079</gtr:id><gtr:otherNames>Thirkettle M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>56758cad47ea1</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AD83B96F-A7FF-4CC9-837A-B100958961E7</gtr:id><gtr:title>Monocular 3D human pose estimation using sparse motion features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845835ce2968e1266c7361749b987a55"><gtr:id>845835ce2968e1266c7361749b987a55</gtr:id><gtr:otherNames>Daubney B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4442-7</gtr:isbn><gtr:outcomeId>doi_53d0580586e5e5bc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>936E2C03-97EB-453A-971A-1F4563149CE2</gtr:id><gtr:title>Contributions of form, motion and task to biological motion perception.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dffd2f67e976b025e3ffbb60528e3079"><gtr:id>dffd2f67e976b025e3ffbb60528e3079</gtr:id><gtr:otherNames>Thirkettle M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770776101cf8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9E78B16F-515D-4D18-ACB0-323FB31E7635</gtr:id><gtr:title>Estimating Gait Phase using Low-Level Motion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/845835ce2968e1266c7361749b987a55"><gtr:id>845835ce2968e1266c7361749b987a55</gtr:id><gtr:otherNames>Daubney B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-1-4244-2000-1</gtr:isbn><gtr:outcomeId>doi_53d060060bbe04bb</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D506549/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>