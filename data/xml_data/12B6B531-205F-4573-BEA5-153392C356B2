<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/EE4840D3-84C0-47A3-9109-30F67D0D550F"><gtr:id>EE4840D3-84C0-47A3-9109-30F67D0D550F</gtr:id><gtr:name>Aalto University</gtr:name><gtr:address><gtr:line1>PO Box 11100</gtr:line1><gtr:line2>00076 Aalto University</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Finland</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/103A8DC6-57CA-41B0-AF1F-D4E73199B113"><gtr:id>103A8DC6-57CA-41B0-AF1F-D4E73199B113</gtr:id><gtr:name>nVIDIA</gtr:name><gtr:address><gtr:line1>2701 San Tomas Expressway</gtr:line1><gtr:postCode>95050</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA"><gtr:id>1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>Claverton Down</gtr:line1><gtr:city>Bath</gtr:city><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/9DE31489-7E3B-4E34-BA75-DD367FF833FB"><gtr:id>9DE31489-7E3B-4E34-BA75-DD367FF833FB</gtr:id><gtr:name>The Foundry Visionmongers Ltd</gtr:name><gtr:address><gtr:line1>The Foundry</gtr:line1><gtr:line2>5 Golden Square</gtr:line2><gtr:postCode>W1F 9HT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/1EEA6D11-7941-4601-B6D5-9385B867218F"><gtr:id>1EEA6D11-7941-4601-B6D5-9385B867218F</gtr:id><gtr:name>University of Siegen</gtr:name><gtr:address><gtr:line1>Gesamthochschule Siegen</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/04DA7C22-39FD-419C-858C-6C49364C0D54"><gtr:id>04DA7C22-39FD-419C-858C-6C49364C0D54</gtr:id><gtr:name>Gobo Games</gtr:name></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/456959E8-6263-4AD9-913A-4D8FD0F9985E"><gtr:id>456959E8-6263-4AD9-913A-4D8FD0F9985E</gtr:id><gtr:name>Double Negative</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EE4840D3-84C0-47A3-9109-30F67D0D550F"><gtr:id>EE4840D3-84C0-47A3-9109-30F67D0D550F</gtr:id><gtr:name>Aalto University</gtr:name><gtr:address><gtr:line1>PO Box 11100</gtr:line1><gtr:line2>00076 Aalto University</gtr:line2><gtr:region>Outside UK</gtr:region><gtr:country>Finland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/96A4DF35-42C5-4C25-8810-856526BFC86B"><gtr:id>96A4DF35-42C5-4C25-8810-856526BFC86B</gtr:id><gtr:name>British Broadcasting Corporation (BBC)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/103A8DC6-57CA-41B0-AF1F-D4E73199B113"><gtr:id>103A8DC6-57CA-41B0-AF1F-D4E73199B113</gtr:id><gtr:name>nVIDIA</gtr:name><gtr:address><gtr:line1>2701 San Tomas Expressway</gtr:line1><gtr:postCode>95050</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA"><gtr:id>1D3C9ADE-5860-4017-88E9-BCDCD95A7CBA</gtr:id><gtr:name>University of Bath</gtr:name><gtr:address><gtr:line1>Claverton Down</gtr:line1><gtr:city>Bath</gtr:city><gtr:postCode>BA2 7AY</gtr:postCode><gtr:region>South West</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/9DE31489-7E3B-4E34-BA75-DD367FF833FB"><gtr:id>9DE31489-7E3B-4E34-BA75-DD367FF833FB</gtr:id><gtr:name>The Foundry Visionmongers Ltd</gtr:name><gtr:address><gtr:line1>The Foundry</gtr:line1><gtr:line2>5 Golden Square</gtr:line2><gtr:postCode>W1F 9HT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1EEA6D11-7941-4601-B6D5-9385B867218F"><gtr:id>1EEA6D11-7941-4601-B6D5-9385B867218F</gtr:id><gtr:name>University of Siegen</gtr:name><gtr:address><gtr:line1>Gesamthochschule Siegen</gtr:line1><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/04DA7C22-39FD-419C-858C-6C49364C0D54"><gtr:id>04DA7C22-39FD-419C-858C-6C49364C0D54</gtr:id><gtr:name>Gobo Games</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/456959E8-6263-4AD9-913A-4D8FD0F9985E"><gtr:id>456959E8-6263-4AD9-913A-4D8FD0F9985E</gtr:id><gtr:name>Double Negative</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/F317CBE0-6779-4F47-9BEF-6CC179014F93"><gtr:id>F317CBE0-6779-4F47-9BEF-6CC179014F93</gtr:id><gtr:firstName>Tim</gtr:firstName><gtr:otherNames>Alexander</gtr:otherNames><gtr:surname>Weyrich</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/77201EFF-0EDB-4AF0-8DC6-8908902F301D"><gtr:id>77201EFF-0EDB-4AF0-8DC6-8908902F301D</gtr:id><gtr:firstName>Gabriel</gtr:firstName><gtr:otherNames>Julian</gtr:otherNames><gtr:surname>Brostow</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FK023578%2F1"><gtr:id>12B6B531-205F-4573-BEA5-153392C356B2</gtr:id><gtr:title>Acquiring Complete and Editable Outdoor Models from Video and Images</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K023578/1</gtr:grantReference><gtr:abstractText>Imagine being able to take a camera out of doors and use it to capture 3D models of the world around you. The landscape at large, including valleys and hills replete with trees, rivers, waterfalls, fields of grass, clouds; seasides with waves rolling onto shore here and crashing onto rocks over there; urban environments complete with incidentals such as lamposts, balconies, and the detritus of modern life. Imagine models that look and move like the real thing. Models that you can use with to make up new scenes of your own, which you can control as you please, and render in how you like. You can zoom into to see details, and out to get a wide impression.

This is an impressive vision, and one that is well beyond current know-how. Our plan is to take a major step towards meeting this vision. We will enable users to use video and images to capture large scale scenes of selected types and populate them with models trees, fountains, street furniture and such like, again carefully selecting the types of objects. We will provide software that recognises the sort of environment the camera is in, and objects in that environment, so that 3D moving models can be automatically created.

This will prove very useful to our intended user group, which is the creative industries in the UK: films, games, broadcast. Modelling outdoor scenes is expensive and time consuming, and the industry recognises that video and images are excellent sources for making models they can use. To help them further we will develop software that makes use of their current practice of acquiring survey shots of scenes, so that all data is used at many levels of detail. Finally we will wrap all of our developments into a single system that shows the acquisition, editing and control of complete outdoor environments is one step closer.</gtr:abstractText><gtr:fund><gtr:end>2017-02-28</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-10-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>630625</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Gobo Games</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>OAK Industrial Advisory Panel (BBC, Gobo, DNeg, Foundry)</gtr:description><gtr:id>9573ACA2-245B-46AC-94C6-48B8278CE1C7</gtr:id><gtr:impact>We are actively engaged with partners to transfer our research IP into their hands. Both UCL's PI and Co-I have practical experience with such transfer, through UCL Business and UCL Media Institute.</gtr:impact><gtr:outcomeId>56e168f33e36a3.22750060-1</gtr:outcomeId><gtr:partnerContribution>Partners provide problem directions, and avenue for industrial impact.</gtr:partnerContribution><gtr:piContribution>We provide core research capability and academic impact.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University College London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Institute of Healthy Ageing</gtr:department><gtr:description>Collaboration with UCL Institute of Healthy Ageing</gtr:description><gtr:id>DEDA7A3F-F228-4911-827B-DE85FEA1159F</gtr:id><gtr:impact>One paper has been published in PlosOne, and the other is in preparation for submission to Nature Methods.</gtr:impact><gtr:outcomeId>56e177533ee911.21922476-1</gtr:outcomeId><gtr:partnerContribution>The biology-focused partners handled all the training of the student to handle and breed flies, all lab-specific skills, and provided the users for user-testing of algorithms developed in the project.</gtr:partnerContribution><gtr:piContribution>The collaboration with Dr Matt Piper led to a proposal that was granted by the Crucible Centre, which funded a 4-year PhD studentship. The student works in both our labs (UCL Computer Science and UCL Institute of Healthy Ageing), learning and innovating on Machine Vision and lifespan-analysis of fruit flies.

This collaboration has let to several important outcomes, published and (for now) unpublished, where computer vision is applied to real and massive visual datasets. For example, we have analyzed the longest-known video, recorded by our student, which filmed the whole life a fruit-fly of 3 months.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2012-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>The Foundry Visionmongers Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>OAK Industrial Advisory Panel (BBC, Gobo, DNeg, Foundry)</gtr:description><gtr:id>E0CBB09E-A8CB-45A2-8F87-1EFF9CA4356D</gtr:id><gtr:impact>We are actively engaged with partners to transfer our research IP into their hands. Both UCL's PI and Co-I have practical experience with such transfer, through UCL Business and UCL Media Institute.</gtr:impact><gtr:outcomeId>56e168f33e36a3.22750060-4</gtr:outcomeId><gtr:partnerContribution>Partners provide problem directions, and avenue for industrial impact.</gtr:partnerContribution><gtr:piContribution>We provide core research capability and academic impact.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>British Broadcasting Corporation (BBC)</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>BBC Research &amp; Development</gtr:department><gtr:description>OAK Industrial Advisory Panel (BBC, Gobo, DNeg, Foundry)</gtr:description><gtr:id>5346324A-6595-4BCE-85E8-50CB5AAE1DC6</gtr:id><gtr:impact>We are actively engaged with partners to transfer our research IP into their hands. Both UCL's PI and Co-I have practical experience with such transfer, through UCL Business and UCL Media Institute.</gtr:impact><gtr:outcomeId>56e168f33e36a3.22750060-2</gtr:outcomeId><gtr:partnerContribution>Partners provide problem directions, and avenue for industrial impact.</gtr:partnerContribution><gtr:piContribution>We provide core research capability and academic impact.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Bath</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Department of Computer Science</gtr:department><gtr:description>OAK Academic Partners at Bath</gtr:description><gtr:id>5F511AD6-6CFC-401B-B3B7-FF7ED353CC09</gtr:id><gtr:impact>We have held several meetings, including with the IAP. We have identified further leads, such as a collaboration with the British Library.</gtr:impact><gtr:outcomeId>56e169b87eda24.40128480-1</gtr:outcomeId><gtr:partnerContribution>We collaborate on research, each way. This includes Bath staff visiting UCL.</gtr:partnerContribution><gtr:piContribution>We collaborate on research, each way. This includes UCL staff visiting Bath.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>NVIDIA</gtr:collaboratingOrganisation><gtr:country>Global</gtr:country><gtr:description>Appearance Capture with Aalto and NVIDIA</gtr:description><gtr:id>E636141D-1C39-4DC5-9599-17BE98565E5B</gtr:id><gtr:impact>The primary outputs were two publications at ACM SIGGRAPH, which already are considered milestone contributions by many:

Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. Practical SVBRDF capture in the frequency domain. ACM Trans. on Graphics (Proc. SIGGRAPH), 32(4):110:1-110:12, 2013.

Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. Two-shot SVBRDF capture for stationary materials. ACM Trans. Graph. (Proc. SIGGRAPH), 34(4):110:1-110:13, July 2015.

In addition, our team is currently preparing a patent application (preliminary application already filed) on the two-shot SVBRDF capture approach. We anticipate economic impact soon.</gtr:impact><gtr:outcomeId>56dd9e9815d3b5.82116013-2</gtr:outcomeId><gtr:partnerContribution>Prof. Lehtinen, who has ample experience in industrial computer graphics research, specialising in rendering and natural light and reflectance phenomena, contributed both through his significant technical and scientific background, but also through the direct supervision of his graduate student Miika Aittala, who himself worked on material appearance capture for computer graphics applications.</gtr:partnerContribution><gtr:piContribution>In joint research on practical appearance capture, I contributed with my general background in the field: within our team, I was the senior domain expert on appearance capture. Through weekly project meetings with Prof. Lehtinen (Aalto University, NVIDIA Research) and his graduate student Miika Aittala, we jointly pushed forward Mr. Aittala's research agenda, leading to publications at the top venue in our field.

Later in the collaboration, there were additional synergies with my EPSRC-funded research on landscape appearance acquisition.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Siegen</gtr:collaboratingOrganisation><gtr:country>Germany, Federal Republic of</gtr:country><gtr:description>Depth-Camera-Based Scene Reconstruction with University of Siegen and MS Research</gtr:description><gtr:id>CB0C45DB-26B6-44BF-9FF3-2589C14137A2</gtr:id><gtr:impact>So far, one successful publication, and one journal submission currently undergoing minor revisions:

[accepted]
Damien Lefloch, Tim Weyrich, and Andreas Kolb. Anisotropic point-based fusion. In Proceedings of International Conference on Information Fusion (FUSION), pages 1-9. ISIF, July 2015.

[undergoing minor revisions]
Damien Lefloch, Hamed Sarbolandi, Tim Weyrich, Andreas Kolb. Comprehensive Use of Curvature For Robust And Accurate Online Surface Reconstruction. Conditionally accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI).</gtr:impact><gtr:outcomeId>56dda24d12d384.15046069-1</gtr:outcomeId><gtr:partnerContribution>University of Siegen contributed through the work by the graduate student Damien Lefloch, under the supervision of Prof. Andreas Kolb, and more recently through additional support by their graduate students Hamed Sarbolandi and Makus Kluge.</gtr:partnerContribution><gtr:piContribution>My key contribution was as domain expert in the area of point-based computer graphics, with secondary contributions through my general experience in computational photography and 3D reconstruction. No other team member from my group at UCL contributed.

The (still ongoing) collaboration has great synergies with my EPSRC-funded work on 3D reconstruction of landscapes.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Double Negative</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>OAK Industrial Advisory Panel (BBC, Gobo, DNeg, Foundry)</gtr:description><gtr:id>604FC469-597F-4117-895C-B644EE8BC949</gtr:id><gtr:impact>We are actively engaged with partners to transfer our research IP into their hands. Both UCL's PI and Co-I have practical experience with such transfer, through UCL Business and UCL Media Institute.</gtr:impact><gtr:outcomeId>56e168f33e36a3.22750060-3</gtr:outcomeId><gtr:partnerContribution>Partners provide problem directions, and avenue for industrial impact.</gtr:partnerContribution><gtr:piContribution>We provide core research capability and academic impact.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Aalto University</gtr:collaboratingOrganisation><gtr:country>Finland, Republic of</gtr:country><gtr:department>Department of Computer Science</gtr:department><gtr:description>Appearance Capture with Aalto and NVIDIA</gtr:description><gtr:id>668A7103-28DA-4AA3-A695-B80CCF56A05B</gtr:id><gtr:impact>The primary outputs were two publications at ACM SIGGRAPH, which already are considered milestone contributions by many:

Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. Practical SVBRDF capture in the frequency domain. ACM Trans. on Graphics (Proc. SIGGRAPH), 32(4):110:1-110:12, 2013.

Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. Two-shot SVBRDF capture for stationary materials. ACM Trans. Graph. (Proc. SIGGRAPH), 34(4):110:1-110:13, July 2015.

In addition, our team is currently preparing a patent application (preliminary application already filed) on the two-shot SVBRDF capture approach. We anticipate economic impact soon.</gtr:impact><gtr:outcomeId>56dd9e9815d3b5.82116013-1</gtr:outcomeId><gtr:partnerContribution>Prof. Lehtinen, who has ample experience in industrial computer graphics research, specialising in rendering and natural light and reflectance phenomena, contributed both through his significant technical and scientific background, but also through the direct supervision of his graduate student Miika Aittala, who himself worked on material appearance capture for computer graphics applications.</gtr:partnerContribution><gtr:piContribution>In joint research on practical appearance capture, I contributed with my general background in the field: within our team, I was the senior domain expert on appearance capture. Through weekly project meetings with Prof. Lehtinen (Aalto University, NVIDIA Research) and his graduate student Miika Aittala, we jointly pushed forward Mr. Aittala's research agenda, leading to publications at the top venue in our field.

Later in the collaboration, there were additional synergies with my EPSRC-funded research on landscape appearance acquisition.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>BBC Interview for Handwriting Synthesis project</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>DDC33038-2E7A-4BF2-9EDF-447A3331663C</gtr:id><gtr:impact>Rory Cellan-Jones, BBC Technology correspondent came and interviewed me and postdoc Dr Tom Haines, to showcase our research. The project allows us to scan someone's handwriting sample, and then to create new text in that person's handwriting. The report and BBC video from this interview were featured as the main Technology story on the BBC website. Our youtube video has been viewed over 52,000 times.</gtr:impact><gtr:outcomeId>58c7dd18a108d7.83985410</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/news/technology-37046477</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>3Dami</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>EA3F0E47-C67B-4CAB-BE19-187DCE1497A0</gtr:id><gtr:impact>45 students formed teams of 9 to make short 3D animated films in two seven day workshops, operating at UCL and University of Bath; over 200 students applied to attend and the premi&amp;egrave;res had large audiences. Questionnaires indicated that they learnt more than a similar period in a classroom, got to do an activity far outside those normally made available by schools (teamwork to create a large, technically complex, project) and were interested in doing more. After the event three separate teams have formed and made further films over the internet, of their own volition, which they never would have done before.</gtr:impact><gtr:outcomeId>56df26ae703153.20892333</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://3dami.org</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>SketchX Workshop (BCS,London)</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>5DB2433E-005A-4AB8-AAE8-ABDE0E6989A7</gtr:id><gtr:impact>Attendance of technical meeting with approximately 50 attendees. Poster presented acknowledging the OAK grant for attendance on prior work, aiming to motivate conversation and collaboration for sketch interfaces of landscape generation project.</gtr:impact><gtr:outcomeId>56df0086c84b87.38314945</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://www.eventbrite.co.uk/e/bmva-technical-meeting-sketchx-human-sketch-analysis-and-its-applications-registration-19754346783#</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>BACKGROUND
A virtual world is a simulated environment in which users may interact with virtual objects and locations of the virtual world. Each user may control a respective avatar through which the user may interact with other users' avatars in the virtual world. An avatar generally provides a graphical representation of an individual within the virtual world environment. Avatars are usually presented to other users as two or three-dimensional graphical representations that resembles a human individual. Frequently, virtual worlds allow multiple users to enter the virtual environment and interact with one another. Virtual worlds are said to provide an immersive environment, as they typically appear similar to the real world and objects tend to follow rules related to gravity, topography, locomotion, physics and kinematics. Of course, virtual worlds can suspend or alter these rules as well as provide other imaginative or fanciful environments. Users typically communicate with one another through their avatars using text messages sent between avatars, real-time voice communication, gestures displayed by avatars, symbols visible in the virtual world, and the like.

Some virtual worlds are described as being persistent. A persistent world provides an immersive environment (e.g., a fantasy setting used as a setting for a role-playing game, or a virtual world complete with land, buildings, towns, and economies) that is generally always available and where events continue to occur regardless of the presence of a given avatar. Thus, unlike more conventional online games or multi-user environments, the virtual world continues to exist and plots and events continue to unfold as users enter (and exit) the virtual world. Virtual environments are presented as images on a display screen and some virtual environment may allow users to record events that occur within the virtual environment.

SUMMARY
Embodiments presented in this disclosure provide a computer-implemented method of scene generation. The method includes receiving an image depicting a scene and annotated by a sparse set of labels. The method also includes generating, based on the sparse set of labels, a dense set of labels annotating the image and a density map associated with the image. The method also includes generating a virtual scene based on the dense set of labels and the density map, where the virtual scene is output.

Other embodiments presented in this disclosure provide a computer-readable medium for scene generation and containing a program which, when executed, performs an operation that includes receiving an image depicting a scene and annotated by a sparse set of labels. The operation also includes generating, based on the sparse set of labels, a dense set of labels annotating the image and a density map associated with the image. The operation also includes generating a virtual scene based on the dense set of labels and the density map, where the virtual scene is output.

Still other embodiments presented in this disclosure provide a system for scene generation. The system includes one or more computer processors and a memory containing a program which, when executed by the one or more computer processors, is configured to perform an operation that includes receiving an image depicting a scene and annotated by a sparse set of labels. The operation also includes generating, based on the sparse set of labels, a dense set of labels annotating the image and a density map associated with the image. The operation also includes generating a virtual scene based on the dense set of labels and the density map, where the virtual scene is output.</gtr:description><gtr:grantRef>EP/K023578/1</gtr:grantRef><gtr:id>234B1593-FB1D-4873-A6F4-05944BAF15B5</gtr:id><gtr:impact>None so far.</gtr:impact><gtr:licensed>No</gtr:licensed><gtr:outcomeId>58c7c28f30b5a4.48997766</gtr:outcomeId><gtr:patentId>US9262853</gtr:patentId><gtr:protection>Patent granted</gtr:protection><gtr:title>VIRTUAL SCENE GENERATION BASED ON IMAGERY</gtr:title><gtr:yearProtectionGranted>2016</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>1. We discovered general principles to create naturally looking output that mimics a given style. We demonstrate this in one publication on generation of landscapes with realistically looking vegetation (which is very close to the original proposal), but also in more general terms, as evidenced by a recently accepted publication on synthesis of different handwriting styles.

2. We discovered a novel approach to capture complex reflectance properties of a natural surface material in a practical setting -- with nothing more than a hand-held camera phone. This is a critical ingredient of OAK's overarching goal of practical capture of natural phenomena.

3. Similarly, we contributed improved methods for practical, hand-held geometry capture using inexpensive, off-the-sheld depth cameras.

4. We further developed a fundamentally new geometry capture approach that works for highly specular surfaces, a regimen where traditional scanning approaches fail, by extracting shape directly from observed surface reflections.

5. We improved our methodology for combining image analysis and machine learning, and evaluated them in an applied context, by training a system to accurately count drosophila eggs in microscope images.</gtr:description><gtr:exploitationPathways>Many of the base technologies we developed and published could be used by others (subject to licensing requirements - with one method we are currently pursuing a patent application.)

We hope that, in the upcoming year, these technologies will further combine to create overarching content creation solutions that are of more immediate value to our industrial partners from the special-effects and games industry.

In addition, we started putting these technologies to a use in cultural-heritage digitisation applications.</gtr:exploitationPathways><gtr:id>A9A1FF91-A1AC-40E5-8802-6EAD692407E8</gtr:id><gtr:outcomeId>56e16e7a217ad2.69398787</gtr:outcomeId><gtr:sectors><gtr:sector>Construction,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset has our code and annotated samples of people's handwriting, allowing new users to process these inputs and generate newly authored text in these writers' handwriting.</gtr:description><gtr:id>EA588189-3573-4687-90DB-05635059E589</gtr:id><gtr:impact>Just posted online last week.</gtr:impact><gtr:outcomeId>56da0184aefb82.22808229</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>My Text in Your Handwriting</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/handwriting/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset contains photos and calibration for many camera views of each specular object. It also contains mask images, ground-truth 3D laser-scans, and environment photographs.</gtr:description><gtr:id>C63982BA-F883-4558-955B-948AA0829865</gtr:id><gtr:impact>N/A</gtr:impact><gtr:outcomeId>56da00c0e385a6.12035804</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Specular Surfaces 3DV 2015 datasets</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/shapefromreflections/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This software allows a user to 1st train a statistical model of appearance, and 2nd to count the number of instances of that object. It was designed and most tested for counting of hundreds of fly-eggs in microscope images.</gtr:description><gtr:id>A85E085F-0CC4-41B4-A71A-696AD4CE5AD9</gtr:id><gtr:impact>The UCL Genetics lab on Healthy Aging is using this software on a daily basis to measure the number of fly eggs in their vials. The number of eggs is an indicator of fly health.</gtr:impact><gtr:outcomeId>56da02bf6f0c64.98788720</gtr:outcomeId><gtr:title>Quantifly software</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127659</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The software has two parts:
1) Interactive software that helps a user annotate a scanned image of handwritten text, extracting and recognizing the characters (or glyphs) to make them re-usable by the synthesis algorithm.

2) Synthesis software that takes a learned model of a specific user's handwriting, and also expects some typed text that the user wishes to render in the handwritten style. The system produces images that can be printed on paper and look convincingly like handwriting.</gtr:description><gtr:id>63D99E32-BA1A-4EB7-81EF-0AF62D9E8470</gtr:id><gtr:impact>N/A: This software has just been released online, and the public announcement is pending.</gtr:impact><gtr:outcomeId>56da05ba2bb1a9.60070210</gtr:outcomeId><gtr:title>Handwriting Annotation and Synthesis Software</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://visual.cs.ucl.ac.uk/pubs/handwriting/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Dr Tom SF Haines developed this Mean Shift library.
Mean shift is usually associated, in computer vision at least, with the segmentation of an image. Whilst this library supports that scenario, it is far more general. Mean shift is a gradient ascent method for finding the modes of a kernel density estimate, so this library is as much a kernel density estimation library as it is a mode finder. It includes the usual kernel bandwidth estimation methods and also supports subspace constrained mean shift, which finds edges/manifolds in noisy data. Support goes far beyond the typical Gaussian and Uniform kernels: It has ten kernel types, as well as the ability to combine them, with different kernels on different parts of a feature vector. The kernels include directional distributions, so it supports density estimation over the position and orientation of an object, for instance. It also supports the multiplication of density estimates, which allows you to perform non-parametric belief propagation using mean shift objects as the messages between random variables. Also has methods to approximate values such as the entropy of a density estimate, and the KL divergence between two estimates.</gtr:description><gtr:id>30BCD891-D21F-42E2-B87F-0B29EDFEB1DF</gtr:id><gtr:impact>At this point, we have no reliable information on the user basis of this library.</gtr:impact><gtr:outcomeId>58ca2604994347.35691665</gtr:outcomeId><gtr:title>Mean Shift Library; 02/2016</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://reality.cs.ucl.ac.uk/projects/haines/haines16meanshift.html</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F6394AB6-7283-4EA8-BD63-FE2299EA0044</gtr:id><gtr:title>Blur robust optical flow using motion channel</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce73e661495882f5061c864c6e2c1985"><gtr:id>ce73e661495882f5061c864c6e2c1985</gtr:id><gtr:otherNames>Li W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>585d6c7024c651.34388757</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7AA9358D-B758-43CF-B179-82858048F4EA</gtr:id><gtr:title>Roto++</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce73e661495882f5061c864c6e2c1985"><gtr:id>ce73e661495882f5061c864c6e2c1985</gtr:id><gtr:otherNames>Li W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0730-0301</gtr:issn><gtr:outcomeId>58c7c63fd087c2.77077523</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7D8A0818-5258-4D36-BC18-A6F0E589284D</gtr:id><gtr:title>Sequential Monte Carlo Instant Radiosity.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f08b99c04b6a7da6f02ff2f1e2a9fb80"><gtr:id>f08b99c04b6a7da6f02ff2f1e2a9fb80</gtr:id><gtr:otherNames>Hedman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>5a2fe6c6b4fc32.30432196</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0C627E64-28A5-4F86-9A27-4F35EDFEEE07</gtr:id><gtr:title>Guided Ecological Simulation for Artistic Editing of Plant Distributions in Natural Scenes</gtr:title><gtr:parentPublicationTitle>Journal of Computer Graphics Techniques (JCGT)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f744ab4d5be5a6ef0f48b9e1c20a15ff"><gtr:id>f744ab4d5be5a6ef0f48b9e1c20a15ff</gtr:id><gtr:otherNames>Bradbury G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd8c41ee0750.06280892</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E84CCCDB-9C99-4509-897D-13553EF2A1CD</gtr:id><gtr:title>Scalable inside-out image-based rendering</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f08b99c04b6a7da6f02ff2f1e2a9fb80"><gtr:id>f08b99c04b6a7da6f02ff2f1e2a9fb80</gtr:id><gtr:otherNames>Hedman P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d46f48ad119.63218254</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B5B041FC-8028-47FA-9A01-6205223306FD</gtr:id><gtr:title>My Text in Your Handwriting</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c492bb39ff2308f7f364eb19212b22b"><gtr:id>9c492bb39ff2308f7f364eb19212b22b</gtr:id><gtr:otherNames>Haines T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d59761c89a8.76615981</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B5082C1-8EE3-40D2-914E-01628DA63C67</gtr:id><gtr:title>Multi-view Reconstruction of Highly Specular Surfaces in Uncontrolled Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/308114aafaed1f4ad17b2505b8f7b532"><gtr:id>308114aafaed1f4ad17b2505b8f7b532</gtr:id><gtr:otherNames>Godard C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9c828266012.76812030</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B927F4F7-7547-4FE2-B21F-2E6674DAF405</gtr:id><gtr:title>Unsupervised Monocular Depth Estimation with Left-Right Consistency</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/308114aafaed1f4ad17b2505b8f7b532"><gtr:id>308114aafaed1f4ad17b2505b8f7b532</gtr:id><gtr:otherNames>Godard C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c7bc66056d69.34275793</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38C799FF-3E98-4502-94ED-2D0DC5814116</gtr:id><gtr:title>Learn to model blurry motion via directional similarity and filtering</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce73e661495882f5061c864c6e2c1985"><gtr:id>ce73e661495882f5061c864c6e2c1985</gtr:id><gtr:otherNames>Li W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a2feffbcbcd75.07896571</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CF24E315-2FB3-487A-AF43-8BE7D19BDF77</gtr:id><gtr:title>QuantiFly: Robust Trainable Software for Automated Drosophila Egg Counting.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5757ac37df6126bc5f5c2b6f086b6e17"><gtr:id>5757ac37df6126bc5f5c2b6f086b6e17</gtr:id><gtr:otherNames>Waithe D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>56d9c828911397.93545747</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7BF54015-FA3F-4124-B5BF-E8B98C0927EC</gtr:id><gtr:title>Two-shot SVBRDF capture for stationary materials</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1c490f24eb6b47839d4dbaa01ce55825"><gtr:id>1c490f24eb6b47839d4dbaa01ce55825</gtr:id><gtr:otherNames>Aittala M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd88bb0ae3a4.78839976</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FE77D0FB-7FF1-4AA8-8784-C7C7ADBC3679</gtr:id><gtr:title>Texture Stationarization: Turning Photos into Tileable Textures</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c83051af7c118daea7c88992f61c6f05"><gtr:id>c83051af7c118daea7c88992f61c6f05</gtr:id><gtr:otherNames>Moritz J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0167-7055</gtr:issn><gtr:outcomeId>5a36224c01ce90.74332276</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3E891646-FBA2-4FBE-9EBF-5388DAB9E14A</gtr:id><gtr:title>Scattering-aware texture reproduction for 3D printing</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/81ae87c21b77c544eec70365f9b4873c"><gtr:id>81ae87c21b77c544eec70365f9b4873c</gtr:id><gtr:otherNames>Elek O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a2ff0041f42a3.65836064</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92C5715C-977E-49AA-B8A3-03A1302F0267</gtr:id><gtr:title>Harmonic Networks: Deep Translation and Rotation Equivariance</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1dedc8dfcfa05fba6a5b96bd79a1d91f"><gtr:id>1dedc8dfcfa05fba6a5b96bd79a1d91f</gtr:id><gtr:otherNames>Worrall D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c7ba6a4cd729.09059591</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D9496C39-4612-4F39-9F8D-34A45931E50D</gtr:id><gtr:title>Comprehensive Use of Curvature for Robust and Accurate Online Surface Reconstruction.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/13d2cad01e814b9ebf63dc102c627d06"><gtr:id>13d2cad01e814b9ebf63dc102c627d06</gtr:id><gtr:otherNames>Lefloch D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>58c7bee22a79f6.55379863</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>81D3D702-D810-46E3-96AB-B4CB873FCD46</gtr:id><gtr:title>My Text in Your Handwriting</gtr:title><gtr:parentPublicationTitle>Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a1150899f8d1ec2eb03c3835ce16391"><gtr:id>2a1150899f8d1ec2eb03c3835ce16391</gtr:id><gtr:otherNames>Haines TSF</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56df2652a39317.86591067</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB72BE1C-68B2-4CB5-8EFC-911C31E1692F</gtr:id><gtr:title>Video interpolation using optical flow and Laplacian smoothness</gtr:title><gtr:parentPublicationTitle>Neurocomputing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ce73e661495882f5061c864c6e2c1985"><gtr:id>ce73e661495882f5061c864c6e2c1985</gtr:id><gtr:otherNames>Li W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>585d6c75db53c6.35607442</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A89D52B1-1E15-4CD6-A70E-5B3E9ABFEB30</gtr:id><gtr:title>Anisotropic Point-Based Fusion</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/13d2cad01e814b9ebf63dc102c627d06"><gtr:id>13d2cad01e814b9ebf63dc102c627d06</gtr:id><gtr:otherNames>Lefloch D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd887f7fd7d6.66846877</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K023578/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>