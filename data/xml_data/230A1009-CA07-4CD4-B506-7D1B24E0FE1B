<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/E4F53ABD-8ECF-4261-A666-0358CD9BCCBD"><gtr:id>E4F53ABD-8ECF-4261-A666-0358CD9BCCBD</gtr:id><gtr:name>Qinetiq Ltd</gtr:name><gtr:address><gtr:line1>St Andrews Road</gtr:line1><gtr:line4>Malvern</gtr:line4><gtr:line5>Worcestershire</gtr:line5><gtr:postCode>WR14 3PS</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E4757A6E-7326-472B-9979-B47D77A65446"><gtr:id>E4757A6E-7326-472B-9979-B47D77A65446</gtr:id><gtr:name>Aberystwyth University</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>King Street</gtr:line2><gtr:line3>Ceredigion</gtr:line3><gtr:line4>Aberystwyth</gtr:line4><gtr:line5>Dyfed</gtr:line5><gtr:postCode>SY23 2AX</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E4757A6E-7326-472B-9979-B47D77A65446"><gtr:id>E4757A6E-7326-472B-9979-B47D77A65446</gtr:id><gtr:name>Aberystwyth University</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>King Street</gtr:line2><gtr:line3>Ceredigion</gtr:line3><gtr:line4>Aberystwyth</gtr:line4><gtr:line5>Dyfed</gtr:line5><gtr:postCode>SY23 2AX</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E4F53ABD-8ECF-4261-A666-0358CD9BCCBD"><gtr:id>E4F53ABD-8ECF-4261-A666-0358CD9BCCBD</gtr:id><gtr:name>Qinetiq Ltd</gtr:name><gtr:address><gtr:line1>St Andrews Road</gtr:line1><gtr:line4>Malvern</gtr:line4><gtr:line5>Worcestershire</gtr:line5><gtr:postCode>WR14 3PS</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/34D02A5D-F2F4-45CF-B49C-703368711B1C"><gtr:id>34D02A5D-F2F4-45CF-B49C-703368711B1C</gtr:id><gtr:firstName>Qiang</gtr:firstName><gtr:surname>Shen</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8DFF01B9-B536-4783-9F47-36BF63519062"><gtr:id>8DFF01B9-B536-4783-9F47-36BF63519062</gtr:id><gtr:firstName>Mark</gtr:firstName><gtr:surname>Lee</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/EF76DAE9-6D56-4AE4-886F-357AEC305A19"><gtr:id>EF76DAE9-6D56-4AE4-886F-357AEC305A19</gtr:id><gtr:firstName>Patricia</gtr:firstName><gtr:otherNames>Hazel</gtr:otherNames><gtr:surname>Shaw</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/73A0DD2F-992B-4767-BB73-CEAAED695231"><gtr:id>73A0DD2F-992B-4767-BB73-CEAAED695231</gtr:id><gtr:firstName>James</gtr:firstName><gtr:otherNames>Alexander</gtr:otherNames><gtr:surname>Law</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FM013510%2F1"><gtr:id>230A1009-CA07-4CD4-B506-7D1B24E0FE1B</gtr:id><gtr:title>Developmental algorithms for robotics: Understanding the world of objects, interactions and tools.</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M013510/1</gtr:grantReference><gtr:abstractText>This research project is a psychologically-inspired investigation of an analogy of infant play as the central mechanism for autonomous, self-motivated robots that learn the local physics of their world.

We note that infants and children at play exhibit exactly the kind of autonomous learning that would be very desirable in robotics. Infant play has a major role in the acquisition of new skills and cognitive growth. Noticing that early infants spend hours in play, we have designed a computer analogy of infant play and this project is an in-depth investigation into the use of play as a means of building subjective understanding of the physics of the local world.

The project will implement a play generator algorithm on an iCub humanoid robot and perform experiments with a wide range of scenarios involving varieties of objects. This includes playing solitarily with objects to learn their properties, and interactive play with a human participant. We also include experiments with tool use (using one object as a tool for acting on another) to investigate how objects may become extensions of self.

A panel of selected scientific experts on infants and play will provide their psychological expertise throughout the project and will also assist with the design of a series of matching experiments that will compare results from the robot model with those from selected psychological experiments on infants.

The data from the experiments will be analysed and interpreted to shed light on a set of scientific issues. When we report on the results we will also extract some general principles for robot learning through play. We will examine the applicability of these principles in new robotic and intelligent systems developments. For example, we anticipate particular applications in areas such as assistive technology and home care where the re-programming of mass-produced systems is not feasible. We believe technology with a developmental approach will have wide implications and provide an alternative to &amp;quot;building robots&amp;quot; by establishing the idea of &amp;quot;developing robots&amp;quot; for applications.</gtr:abstractText><gtr:potentialImpactText>The scientific output of this project will produce new algorithms and methods for a form of robot learning based on development. The knowledge and understanding gained will be interpreted in terms of principles for autonomous operation and the safe handling of novel experiences. Apart from the academic beneficiaries, we see scope for considerable societal and economic impacts.

There are three immediate fields that will benefit: (1) the robotics and associated manufacturing industries; (2) the healthcare, medical, and domestic care sectors; and (3) science education and public awareness.

(1) Industrial robotics is now a mature field but most robot devices in real-world environments are still controlled by either fixed programs or human operators. Examples include robot assistants, search and rescue robots, and surveillance and military robots. Unlike factory robots, it is often the case that real-world robots cannot be programmed to cover every task contingency. Each new task scenario demands knowledge appropriate to that specific case and it would be desirable for the robot to accumulate this knowledge rather than rely on the operator every time. Our results will address how this can be achieved and how operators may shape or train robots to improve their performance, thus achieving some operational autonomy, at least for part of the task. Such autonomy is very desirable for reducing operator stress and costs, increasing functionality, and extending application areas. This could provide a technological advance that has major benefits for robot product innovators, commercial robotics companies, and application developers.

(2) Assistive technology is a current world priority and a major challenge, driven by the demographic shift towards care for the aging population. This crucial need for flexible, adaptive technology involves all kinds of assistive, service, healthcare and domestic applications. As for (1) these are all real-world robotics scenarios that require flexibility in coping with the physical world, and the capability to adapt to the many and varied domestic needs and personal environments. Providing some autonomy in such domestic support devices should have a considerable impact on the quality of people's lives and bring real benefits to a range of organisations both public (e.g. NHS) and private sector (service and technology providers). The benefits will include; releasing demand for care staff, increasing the independence of users, generating new products and service industry possibilities, and increasing UK competitiveness in the service and healthcare domains.

Our project partners, QinetiQ, will be actively engaged in identifying new application opportunities in both the above fields. They are especially interested in new ways of task configuring, calibrating and controlling robotic systems that are cheaper to implement and more robust.

(3) Our use of the local Technocamps organisation (http://www.technocamps.com/) will require new materials to be prepared for school pupils aged 11-19. The Technocamps staff will prepare these and this will result in the availability of several workpackages of material. These will have wide applicability for public awareness of science promotions and will be made openly available.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-06-26</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>559077</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Qinetiq</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>QinetiQ</gtr:description><gtr:id>D267C6D7-461F-4FBC-9730-27DCAE3D3998</gtr:id><gtr:impact>N/A.</gtr:impact><gtr:outcomeId>58c7c3c6f0f341.93902227-1</gtr:outcomeId><gtr:partnerContribution>Scientific advisory committee member.</gtr:partnerContribution><gtr:piContribution>Robotic systems.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>EPSRC Pioneers, Advances Wales, BBC</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>83D44EEE-5BD9-49EC-B0F1-D06FC848B02C</gtr:id><gtr:impact>Research featured in EPSRC Pioneers and Advances Wales magazines, as well as on BBC.</gtr:impact><gtr:outcomeId>58c6c1d559ca32.27289421</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.bbc.co.uk/programmes/b08fwtlt;https://www.epsrc.ac.uk/newsevents/pubs/pioneer17/;https://businesswales.gov.wales/sites/business-wales/files/documents/Advances%2079%20final.pdf</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPSRC photo competition 2014</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>61D2E0E1-95A8-4350-95CD-21134AD43663</gtr:id><gtr:impact>EPSRC photo competition 2014 - 1st place in People category, promoting outstanding UK engineering and physical sciences.</gtr:impact><gtr:outcomeId>58c6bea80fa543.60195763</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.flickr.com/photos/epsrc/15752505647/in/album-72157651335202305/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>EPSRC photo competition 2015</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>D8D7DB5D-C516-4476-A36C-987146950532</gtr:id><gtr:impact>EPSRC photo competition 2015 - 1st place in People category, second year in a row, promoting outstanding UK engineering and physical sciences.</gtr:impact><gtr:outcomeId>58c6bf3d24a7b5.23908756</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.flickr.com/photos/epsrc/24309680141/in/album-72157666782307310/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>45000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Aberystwyth University PhD Scholarship</gtr:description><gtr:end>2018-02-02</gtr:end><gtr:fundingOrg>Aberystwyth University</gtr:fundingOrg><gtr:id>25AF492C-CFF7-4563-8E5D-53436F386BA6</gtr:id><gtr:outcomeId>58c6c429a611f9.50079326</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Developed a model of how infants visual perception changes over the first year of life. Using this, we can measure the impact this has on an infant's ability to recognise and distinguish different objects at different stages in their development. Early on the limited vision means all objects look pretty similar, with just the bright or moving features being distinguishable. As vision gradually develops it is possible to distinguish an increasing number of different colours and eventually the edges of objects. Combining in information from the feel and hand shape for holding objects gives a rich description for distinguishing multiple objects.

A higher level play generating mechanism allows for the discovery of object behaviours and goal-directed behaviour. All of this is linked into psychological studies of infant behaviour with models that may help to understand the internal mechanisms and developmental changes resulting in behaviour changes in infants.</gtr:description><gtr:exploitationPathways>Currently discussing possibilities with a leading AI company from America who are keen to apply our models for learning skills and coordination as part of the robotic system they are developing.</gtr:exploitationPathways><gtr:id>5DD86ADE-4BD7-4CFE-A5AA-AD9DB9D86547</gtr:id><gtr:outcomeId>56dc2b4dcaaf71.63674742</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>https://www.aber.ac.uk/en/cs/research/ir/robots/icub/dev-icub/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>No legal or ethical constraints affecting the models or data.

Models are described in the external publications.

Datasets from models are available at: http://icub.dcs.aber.ac.uk/index.php/publications</gtr:description><gtr:id>9719279D-461E-4D43-AB96-2EFC64F814A1</gtr:id><gtr:impact>N/A other than observed downloads.</gtr:impact><gtr:outcomeId>58c7e1d9ad2881.41702926</gtr:outcomeId><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>Data and models</gtr:title><gtr:type>Computer model/algorithm</gtr:type></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>90ABB80F-7BF4-460B-83F3-73B86DEF026D</gtr:id><gtr:title>Perception of Localized Features During Robotic Sensorimotor Development</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Cognitive and Developmental Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d6cac4d4a2b4cd59f7d488ec1d303267"><gtr:id>d6cac4d4a2b4cd59f7d488ec1d303267</gtr:id><gtr:otherNames>Giagkos A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c6af2f8a7427.81824069</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88C83DCC-0E94-4E58-983F-DD9E423AE802</gtr:id><gtr:title>Developing object understanding through schema generalisation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/22e459c4c42360108e16cea2f07aec37"><gtr:id>22e459c4c42360108e16cea2f07aec37</gtr:id><gtr:otherNames>Kumar S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6b178cf85f6.32011176</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>12F1631B-C547-4BC4-9445-1EAB3A4E8C64</gtr:id><gtr:title>Babybot Challenge: Motor Skills</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aae485059d5b9fe0e267864c2f12018f"><gtr:id>aae485059d5b9fe0e267864c2f12018f</gtr:id><gtr:otherNames>Shaw, P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dc16f0c6f842.14473628</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>692A2DA1-4BC8-4850-B020-F6AD457C2C67</gtr:id><gtr:title>Towards learning strategies and exploration patterns for feature perception</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f8d7ef14eb77958557f57312798a0e0b"><gtr:id>f8d7ef14eb77958557f57312798a0e0b</gtr:id><gtr:otherNames>Lewkowicz D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>58c6b2955a1270.09512039</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>38743958-414D-4233-8226-80EA64182DDB</gtr:id><gtr:title>Representations of body schemas for infant robot development</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aae485059d5b9fe0e267864c2f12018f"><gtr:id>aae485059d5b9fe0e267864c2f12018f</gtr:id><gtr:otherNames>Shaw, P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dc187471b8a4.98736235</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M013510/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>9A6079E0-357B-44DA-80F9-C5953586BD0C</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Developmental psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>15</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>5</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>