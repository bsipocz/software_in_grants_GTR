<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/956E999D-84FF-4CBE-8DDF-49169EA9E142"><gtr:id>956E999D-84FF-4CBE-8DDF-49169EA9E142</gtr:id><gtr:name>Carnegie Mellon University</gtr:name><gtr:address><gtr:line1>5000 Forbes Avenue</gtr:line1><gtr:line4>Pittsburgh</gtr:line4><gtr:line5>PA 15213</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:department>Sch of Psychology and Clinical Lang Sci</gtr:department><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E89C3602-0FB4-4044-A918-58966B8A10B2"><gtr:id>E89C3602-0FB4-4044-A918-58966B8A10B2</gtr:id><gtr:name>University of Reading</gtr:name><gtr:address><gtr:line1>Whiteknights House</gtr:line1><gtr:line2>PO Box 217</gtr:line2><gtr:line4>Reading</gtr:line4><gtr:line5>Berkshire</gtr:line5><gtr:postCode>RG6 6AH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/956E999D-84FF-4CBE-8DDF-49169EA9E142"><gtr:id>956E999D-84FF-4CBE-8DDF-49169EA9E142</gtr:id><gtr:name>Carnegie Mellon University</gtr:name><gtr:address><gtr:line1>5000 Forbes Avenue</gtr:line1><gtr:line4>Pittsburgh</gtr:line4><gtr:line5>PA 15213</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/2646F1BE-DBD2-411B-9D89-C143536D1D43"><gtr:id>2646F1BE-DBD2-411B-9D89-C143536D1D43</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Glennerster</gtr:surname><gtr:orcidId>0000-0002-8674-2763</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN019423%2F1"><gtr:id>29D984FF-A8CB-4EC2-9389-AD464BFBD742</gtr:id><gtr:title>Understanding Scenes and Events through Joint Parsing, Cognitive Reasoning and Lifelong Learning</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N019423/1</gtr:grantReference><gtr:abstractText>The goal of this MURI team is to develop machines that have the following capabilities:
i) Represent visual knowledge in probabilistic compositional models in spatial, temporal, and causal hierarchies augmented with rich attributes and relations, use task-oriented representations for efficient task-dependent inference from an agent's perspective, and preserve uncertainties;
ii) Acquire massive visual commonsense via web scale continuous lifelong learning from large and small data in weakly supervised HCI, and maintain consistence via dialogue with humans;
iii) Achieve deep understanding of scenes and events through joint parsing and cognitive reasoning about appearance, geometry, functions, physics, causality, intents and belief of agents, and use joint and long-range reasoning to fill the performance gap with human vision;
iv) Understand human needs and values, interact with humans effectively, and answer human queries about what, who, where, when, why and how in storylines through Turing tests.

Collaboration with US:
Principal Investigator: Dr. Song-Chun Zhu
Tel. 310-206-8693, Fax. 310-206-5658, email: sczhu@stat.ucla.edu
Institution: University of California, Los Angeles
Statistics and Computer Science
8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095
Institution proposal no. 20153924

Other universities in the US
CMU: Martial Hebert Computer Vision, Robotics &amp;amp; AI
Abhinav Gupta Computer Vision, Lifelong Learning
MIT: Joshua Tenenbaum Cognitive Modeling and Learning
Nancy Kanwisher Cognitive Neuroscience
Stanford: Fei-Fei Li Computer Vision, Psychology &amp;amp; AI
UIUC Derek Hoiem Computer Vision, Machine Learning
Yale Brian Scholl Psychology, Cognitive Science</gtr:abstractText><gtr:fund><gtr:end>2019-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>443434</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Carnegie Mellon University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:department>Robotics Institute</gtr:department><gtr:description>Collaboration with Professor Abhinav Gupta's group, Carnegie Mellon University</gtr:description><gtr:id>B40BF406-0E8C-4910-AE55-F165FBEAC7A0</gtr:id><gtr:impact>No outputs so far. The collaboration is mulit-disciplinary, between neuroscience and computer vision.</gtr:impact><gtr:outcomeId>58a9e7002a5575.01115865-1</gtr:outcomeId><gtr:partnerContribution>Gupta's group will design reinforcement learning routines that will use the same Unity environments and task as human participants so see whether the networks learn in the same way. We will also explore whether apprentice learning can improve the reinforcement learning.</gtr:partnerContribution><gtr:piContribution>I visited Gupta's group with Alex and Luise (PDRAs on this grant) to design psychophysical experiments that we will carry out in Reading as part of a joint project on 3D representation without 3D coordinates.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Oxford</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>Department of Engineering Science</gtr:department><gtr:description>Collaboration with Phil Torr's group in Robotics, University of Oxford</gtr:description><gtr:id>2518A5F5-ADD4-4752-A624-81ACB9CE0CD5</gtr:id><gtr:impact>Multidisciplinary: neuroscience and computer vision/machine learning.</gtr:impact><gtr:outcomeId>56cd8f4fbd7db5.16568225-1</gtr:outcomeId><gtr:partnerContribution>The Torr group will carry out the modelling described above.</gtr:partnerContribution><gtr:piContribution>We have begun a collaboration that will be extended as part of EPSRC grant EP/N019423/1. We will provide access to the Virtual Reality lab in Reading and psychophysical expertise. The aim is to compare human performance on navigation tasks with that of reinforcement learning techniques trained on games that require navigation to obtain rewards.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The review article published on this grant presents a novel hypothesis about the way that a moving observer represents a 3D scene. Most important, we have developed links with Prof Gupta's group at Carnegie Mellon and Prof Torr's group at the University of Oxford that looks likely to lead to new hypotheses about the type of representation people may use when navigating and carrying out other tasks in a 3D environment. We can then design experimental tests of these hypotheses in our VR lab.</gtr:description><gtr:exploitationPathways>We hope to start a widespread debate about the extent to which the brain reconstructs models of the outside world and the way in which new findings from reinforcement learning in machine vision provide new hypotheses for representation in the brain (see above).</gtr:exploitationPathways><gtr:id>B24AABAB-DFB3-466E-ACE7-DAA1B7CC9AC7</gtr:id><gtr:outcomeId>58a9e9829e3920.95190628</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://wiki.glennersterlab.com/index.php</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>DB4FE493-2512-4729-94BF-24D616374399</gtr:id><gtr:title>Navigation and pointing errors in non-metric environments.</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da99919b0cfba3a6fcce657634c3700a"><gtr:id>da99919b0cfba3a6fcce657634c3700a</gtr:id><gtr:otherNames>Muryy A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a941adf4fcb65.18821822</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BEFFB60D-5472-4679-9052-50718728F6CE</gtr:id><gtr:title>Comparison of view-based and reconstruction-based models of human navigational strategy.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/802b69ca74544d30568775d34709725a"><gtr:id>802b69ca74544d30568775d34709725a</gtr:id><gtr:otherNames>Gootjes-Dreesbach L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>5a9416bd0d7a35.07024274</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>58A7B3C1-2F7B-4F00-ADD7-8223824C9B12</gtr:id><gtr:title>A moving observer in a three-dimensional world.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cdd60f011d094818d0c8d752c2bf5814"><gtr:id>cdd60f011d094818d0c8d752c2bf5814</gtr:id><gtr:otherNames>Glennerster A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>585d5700de1a04.55361840</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2EBD944B-0A01-4A19-9D63-BAA959576221</gtr:id><gtr:title>Pointing Errors in Non-Metric Virtual Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/da99919b0cfba3a6fcce657634c3700a"><gtr:id>da99919b0cfba3a6fcce657634c3700a</gtr:id><gtr:otherNames>Muryy A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2018-01-01</gtr:date><gtr:outcomeId>5a9efc801dc816.63932360</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N019423/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E457FFDE-A4C1-4907-AE12-A394D95A3AE5</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Cognitive Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>