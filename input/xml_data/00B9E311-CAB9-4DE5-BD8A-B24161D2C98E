<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/D5EF406D-A320-44D9-9464-9D6BC3AD1649"><gtr:id>D5EF406D-A320-44D9-9464-9D6BC3AD1649</gtr:id><gtr:name>Nippon Telegraph and Telephone Corporation (NTT)</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Ear Institute</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5EF406D-A320-44D9-9464-9D6BC3AD1649"><gtr:id>D5EF406D-A320-44D9-9464-9D6BC3AD1649</gtr:id><gtr:name>Nippon Telegraph and Telephone Corporation (NTT)</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/6F16E2CF-56F5-4EF2-8125-A28EC7D26D91"><gtr:id>6F16E2CF-56F5-4EF2-8125-A28EC7D26D91</gtr:id><gtr:firstName>Maria</gtr:firstName><gtr:surname>Chait</gtr:surname><gtr:orcidId>0000-0002-7808-3593</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FK003399%2F1"><gtr:id>00B9E311-CAB9-4DE5-BD8A-B24161D2C98E</gtr:id><gtr:title>Change detection in complex acoustic scenes</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/K003399/1</gtr:grantReference><gtr:abstractText>The ability to detect changes around us, such as the appearance or disappearance of an object, has far-reaching implications for survival. These processes have received considerable attention in Vision research. In contrast, the processes by which listeners detect the appearance or disappearance of objects in busy acoustic scenes, comprised of many concurrent sources, remain poorly understood. This is surprising because often it is sound that alerts us to important changes in the scene: Hearing is sensitive to a much larger space than the other senses and in many cases we hear change before we see it (for example, somebody entering the room while your back is to the door; Sudden quiet from the kids' playroom indicating they are up to mischief..). Indeed, the auditory system is commonly assumed to play a key role in the brain's change-detection network by serving as an 'early warning device', rapidly directing attention to new events in the scene.
 The present project is classified as 'basic' research with the goal of understanding how listeners with normal hearing detect and process change-events (appearance or disappearance of sources) in auditory scenes. The behavioural and functional brain imaging experiments detailed here are designed to systematically explore listeners' change detection behavior, understand the relevant processes and identify their neural underpinnings: How are object appearance and disappearance events detected? What brain mechanisms are involved? Are change events detected automatically by the brain, even when listeners' attentional focus is elsewhere? What makes certain change events fundamentally more salient than others? Under what conditions do listeners perform well, and which situations result in reduced performance? In busy scenes, such as those we often face in the environment, behaviourally relevant scene changes often coincide in time with other events. How resilient are the auditory change detection mechanisms to irrelevant events occurring at the same time as the auditory change? Do auditory and visual perturbing events have the same detrimental effect on performance?
 These issues are important from the point of view of understanding perception and how the brain analyses and represents the dynamics of our surrounding environment. Furthermore, understanding what makes certain events pop out and grab attention, while other events go unnoticed is important for designing human-computer interfaces, and other devices intended to help professionals (operating room personnel, air traffic controllers, pilots, etc.) operate effectively in environments where the detection of change is critical. Additionally, since change detection is a major contributor to efficient interaction with the environment, understanding the profile of change detection in normal listeners can provide a measure against which to evaluate hearing impairment as well as the benefit obtained from hearing aids.</gtr:abstractText><gtr:technicalSummary>The ability to detect changes around us, such as the appearance or disappearance of objects, has far-reaching implications for survival. The auditory system is commonly assumed to play a key role in the brain's change-detection network by serving as an 'early warning device', rapidly directing attention to new events in the scene, however these processes remain poorly understood. The present project is comprised of psychophysics, as well as functional brain imaging (MEG and fMRI) experiments, designed to understand how (normal hearing) listeners detect the appearance or disappearance of sources in busy acoustic scenes. The experiments employ a new stimulus paradigm, based on tightly controlled acoustic scenes designed to model the dynamics of natural 'soundscapes', but devoid of semantic attributes so as to tap only low level (pre-semantic) processes involved in change detection. Using this paradigm I aim to systematically explore listeners' scene-change detection behavior, in order to identify the underlying brain mechanisms and understand what combination of conditions (physical aspects of sound; presence of other concurrent events in the environment; listeners' perceptual state) make such changes easy to detect, and, conversely, in what circumstances listeners fail to detect prominent changes in acoustic scenes. These issues are important from the point of view of understanding perception and how the brain analyses and represents scene dynamics but also for the design of human-computer interfaces and other devices aimed at aiding professionals operate in 'busy' acoustics scenes where the detection of appearing or disappearing sources is critical. Additionally, since change detection is a major contributor to efficient interaction with the environment, understanding the profile of change detection in normal listeners would be useful for evaluating hearing impairment and measuring the benefit obtained from hearing aids.</gtr:technicalSummary><gtr:potentialImpactText>The present project is classified as 'basic' research with the goal of understanding how normal hearing listeners detect and process change-events (appearance or disappearance of objects) in auditory scenes. In particular, we aim to understand what combination of conditions (physical structure of sound; presence of other concurrent events in the environment; listeners' perceptual state) make such changes easy to detect, and, conversely, in what circumstances listeners fail to detect prominent changes in acoustic scenes. Understanding the normal 'profile' of change detection could have wide ranging implications for health and wealth.
(1) The ability to detect changes around us is fundamental to successful interaction with the environment. The auditory system plays a major role in this process -we often hear important change-events before we see them (for example, somebody entering the room while your back is to the door; Sudden quiet from the kids' playroom indicating they are up to mischief..). Indeed individuals with hearing impairment tend to rank inability to efficiently respond to such changes among the most debilitating aspects of deafness. However, despite its important implications to quality of life, change detection ability is not currently captured by existing hearing-impairment evaluation techniques which are mainly based on simple audiometry and on measuring speech understanding. Mapping out the characteristics of normal listeners' sensitivity to change can provide a measure against which to evaluate hearing impairment. The testing procedures developed during this project could be used in the clinic or in the context of scientific research, to estimate the degree to which change detection performance differs from 'normal'. This might be useful for evaluating hearing impairment and provide a clearer assessment of benefit to quality of life obtained from a hearing aid. This will benefit individuals, health professionals as well as NHS policy makers.

(2) Furthermore, understanding the properties of listeners' sensitivity to change, what makes certain events pop out and grab attention, while other events go unnoticed is important for designing human-computer interfaces, and other devices intended to help professionals (operating room personnel, air traffic controllers, pilots, etc.) operate effectively in environments where the detection of change is critical. In an age when the environments that such professionals must deal with become increasingly complex, and where many applications actively immerse users in artificial acoustic scenes, understanding the brain processes underlying change detection could inform the design of these systems (for example by programming them so that they produce change events that listeners are particularly sensitive to) or lead to new technological developments that improve auditory change detection through advanced audio signal processing, brain-machine interfaces, etc. Such technological developments could benefit individuals, health professionals, government agencies, and the commercial private sector. 

(3) Similarly, Art (design of 'sound-scapes') and the computer gaming industry could benefit from the technological advancements described above, as these fields are currently moving to using artificial environments and brain-machine interfaces for the enhancement of user experience.

(4) Analysis tools developed in the course of this project (especially those related to proposed experiment MEG1) with the aim of relating on-going brain activity to the listener's perceptual experience of change, might also be of use to brain-machine applications thus benefiting government agencies, and the commercial private sector.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-10-14</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2013-04-15</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>481122</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Nippon Telegraph and Telephone Corporation (NTT)</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>collaboration with NTT (Japan Telecom) on Change detection and Salience</gtr:description><gtr:id>18FB7746-66CF-4374-92CD-306D11F1FFCF</gtr:id><gtr:impact>See more information about the work under the BBSRC partenring award (BB/L026864/1) form.</gtr:impact><gtr:outcomeId>58ca7557ed4132.17474115-1</gtr:outcomeId><gtr:partnerContribution>See more information about the work under the BBSRC partenring award (BB/L026864/1) form.</gtr:partnerContribution><gtr:piContribution>prompted by mutual interests in change detection, the collaboration resulted in a BBSRC partenring award, one published co-authored paper (Petsas et al) and 2 more papers being prepared for publication. See more information about the work under the BBSRC partenring award (BB/L026864/1) form.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2010-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>UCL-PSL workshop on sensory systems in complex environments</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>3309BC48-6DF0-4132-86A5-3C9E44849AA4</gtr:id><gtr:impact>I organized a two day workshop on &amp;quot;Sensory Systems in Complex Environments&amp;quot; which brought together researchers (PIs and students) from UCL and Paris.

More details are here: http://audition.ens.fr/workshop/ws_ucl_psl_2015/

The workshop was funded by a generous contribution from the UCL and PSL international offices.</gtr:impact><gtr:outcomeId>56e0aef472b919.53884627</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://audition.ens.fr/workshop/ws_ucl_psl_2015/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>secrets of the brain</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>75080C78-0E92-4424-B9F4-75BA660BCB8F</gtr:id><gtr:impact>filming took place in my lab on Jan 2017 for a documentary &amp;quot;Secrets of the Brain&amp;quot; (Lambent productions), which will air at the end of 2017.
Our work will be featured in an episode devoted to auditory processing and music.</gtr:impact><gtr:outcomeId>58ca77a4dd8d98.23121376</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Guardian project</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>C1EBFF07-AA9C-42E6-AA0E-4127D3809AB8</gtr:id><gtr:impact>Guardian project on brain and music. Short films featuring musicians listening to their favourite songs and the associated brain activity. Filming took place in my lab over the summer. Published on the guardian website in October 2016.</gtr:impact><gtr:outcomeId>58ca77106e1122.76404247</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>https://www.ucl.ac.uk/ear/research/chaitlab/Guardian</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>School visit.</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>8BC40641-E555-4DEA-B704-CE78A73316C0</gtr:id><gtr:impact>iDiscover is an organization that runs primary school programs aiming to engage young people from inner London primary schools in science and discovery. Next scheduled activity in November 2016 and January 2017.</gtr:impact><gtr:outcomeId>5812488025e484.15229384</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016,2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>participation in 'The lab project' - an art project ran in kingsgate workshops</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>5D669345-B85A-4E4B-8A8C-921D230EAED0</gtr:id><gtr:impact>The Lab Project is an experimental month long exhibition and events program that explores the interactions between art and science. Our work is featured in the 'step 1' symposium which brings together scientists and creative practitioners to discuss possible 'entanglements' between science and art: &amp;quot; How can we use sound to affect one's experience of their surroundings?&amp;quot;</gtr:impact><gtr:outcomeId>56e0ac6b41c3b5.05869282</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://thelabproject.tumblr.com/about</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>workshop on prediction in heating Leipzig university</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FD6EAC73-5C87-4D5E-8DA9-D3E5F55ACDF2</gtr:id><gtr:impact>an information workshop in Leipzig to discuss auditory scene analysis. I (Maria Chait) and Ed Sohoglu gave presentations on change detection in complex acoustic scenes</gtr:impact><gtr:outcomeId>56e0b1f93fe1f1.08702949</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>The Science museum</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>EFB04A43-77F4-43C9-BF18-9B9D66EF25A0</gtr:id><gtr:impact>The Science museum filmed work in our lab, to be featured in the permanent exhibition within the new mathematics gallery (due to open in December 2016).</gtr:impact><gtr:outcomeId>58124942ab1f97.07255505</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>UCL Neuroscience Symposium</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>0C5F5BF0-B192-4433-A2A4-26126C473FEE</gtr:id><gtr:impact>We presented a lab poster, detailing work in our lab including the major projects supported by this BBSRC grant. The poster was very popular and won first prize in the 'lab poster' competition.</gtr:impact><gtr:outcomeId>56e0ae9c817207.38796673</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>School visit (Hackney) part of iDiscover program</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>B5F58E65-BC3C-49AB-9EEA-F211B8844D59</gtr:id><gtr:impact>Visit to a primary school in Hackney as part of the iDiscover program http://www.inspire-ebp.org.uk/article/items/iDiscover-new-year-new-themes-new-funding.html</gtr:impact><gtr:outcomeId>56e0ab61bca0b1.51876155</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Host lab</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>001B97BF-9C8D-42DF-8039-A1BD3FC9677C</gtr:id><gtr:impact>in2science is an organization which gives high school students from low income backgrounds the opportunity to work alongside scientists in laboratory settings. My lab has hosted such students for the past three years and will continue to regularly do so.</gtr:impact><gtr:outcomeId>581248fd467f26.64411886</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2014,2015,2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>screening panel for 'the flame challenge'</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>72517D8C-A0D8-435A-B1F1-428596ED2689</gtr:id><gtr:impact>We are on the screening panel for this year's Flame Challenge : &amp;quot;What is sound?&amp;quot;

The Flame Challenge is an international competition where scientists answer the question in a way that is most appropriate for 11-year-olds. Entries are judged by thousands of 5th and 6th grade schoolchildren around the world.</gtr:impact><gtr:outcomeId>56e0ad1719aee4.87375559</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.centerforcommunicatingscience.org/flame-challenge-2015/</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Primary school visit to talk about my work</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>1323842F-7C42-4201-8901-A6D11A4CEB6D</gtr:id><gtr:impact>talked to children (year 1 - year 6) about my 'career in science'. Tried to especially get girls excited about working in research.

activity is to take place in 2 weeks.</gtr:impact><gtr:outcomeId>545d89095d9865.92972217</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>'5Hz Lab' project, Arnolfini, Bristol,</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>5316764E-8D09-4367-98D0-E0EECCA0DB85</gtr:id><gtr:impact>Our work on sensitivity to patterns in sound featured in 5Hz Labs at the Arnolfini in Bristol (16/11/2014)</gtr:impact><gtr:outcomeId>56e0b2a2474dc9.84307259</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.arnolfini.org.uk/whatson/5hz-labs-beyond-the-word</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>79500</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Action on Hearing Loss PhD studentship</gtr:description><gtr:end>2018-11-02</gtr:end><gtr:fundingOrg>Action on Hearing Loss</gtr:fundingOrg><gtr:id>2615D805-C631-461A-9688-A24B9377BB65</gtr:id><gtr:outcomeId>56e0aaab6f39e1.91892078</gtr:outcomeId><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3217500</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>Research and innovation action H2020-ICT-14-1</gtr:description><gtr:end>2019-12-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:id>1FCDA6FE-156F-40B6-A0E4-DA775BDB34B7</gtr:id><gtr:outcomeId>545d86a2348b99.59578119</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Overall our results reveal high sensitivity to auditory scene changes, in stark contrast to the widely studied visual 'change blindness' phenomenon. The findings reveal many previously unknown effects and will lead to increased understanding of change detection. 
Understanding the factors affecting the asymmetry between 'appear' and 'disappear' events:
(1) Sohoglu &amp;amp; Chait (2016) Neural Dynamics of Change detection in crowded acoustic scenes. Neuroimage. Two key questions concerning change detection in crowded acoustic environments are the extent to which cortical processing is specialized for different forms of acoustic change and when in the time-course of cortical processing neural activity becomes predictive of behavioral outcomes. We address these issues by using magnetoencephalography (MEG) to probe the cortical dynamics of change detection in ongoing acoustic scenes containing as many as ten concurrent sources. Each source was formed of a sequence of tone pips with a unique carrier frequency and temporal modulation pattern, designed to mimic the spectrotemporal structure of natural sounds. Our results show that listeners are more accurate and quicker to detect the appearance (than disappearance) of an auditory source in the ongoing scene. Underpinning this behavioral asymmetry are change-evoked responses differing not only in magnitude and latency, but also in their spatial patterns. We find that even the earliest (~50 ms) cortical responses to change are predictive of behavioural outcomes (detection times), consistent with the hypothesized role of local neural transients in supporting change detection. 
(2) Bellahcen, Sohoglu &amp;amp; Chait (final stages of preparation for JEP:HPP). To further our understanding of the computations underlying appearance and disappearance events, in this psychophysics experiment we investigated scene changes comprised of both appearance and disappearance of a source. Overall, our results show that two concurrent changes interfere, and particularly, that item disappearance interferes with the detection of the appearance, suggesting that these two events compete for the listeners' attention. This interaction is abolished in presence of a global loudness change, which also caused a massive distraction
(3) Smith, Le Gal de Kerangal &amp;amp; Chait (in preparation). As described in the grant proposal, one means for investigating the interesting asymmetry between 'appearance' and 'disappearance' events is by introducing gaps of increasing duration at the time of change. We ran a series of experiments investigating this. The results demonstrate, (surprisingly!) that the dominance of appearance (over disappearance) detection persists for very long gaps (up to 6 seconds; the longest we've tested). This is strong evidence in support of an 'appearance' bias. We are currently running a final control experiment after which the results will be submitted for publication. 

Understanding which information from sound listeners use in the course of change detection:

(4a) Aman, Andreou &amp;amp; Chait (submitted) Listeners track the temporal structure of multiple concurrent acoustic sources. This experiment was initially designed to test a hypothesis related to different computation requirements for appear and disappear events. In the course of piloting we discovered a remarkable phenomenon which we ended up investigating more fully (see also, 4b, below). The notion that sensitivity to the statistical structure of the environment is pivotal to perception has recently garnered considerable attention. Here we investigated this issue in the context of hearing. Stimuli were artificial 'sound-scapes' populated by multiple (up to 14) simultaneous 'auditory objects' comprised of tone-pip sequences, each with a distinct frequency and pattern of amplitude modulation. Sequences were either temporally regular or random. We show that listeners' ability to detect abrupt appearance or disappearance of a component is facilitated when scene components were characterized by a temporally regular fluctuation pattern. The patterning of the changing component as well as that of the background (non-changing) components contribute independently to this effect. Remarkably, listeners benefit from regularity even when they are not consciously aware of it. These findings establish that perception of complex acoustic scenes relies on the availability of detailed representations of the regularities extracted from each scene source. 
(4b) Loriette, Christopher &amp;amp; Chait (final stages of preparation for JEP:HPP). This series of behavioural experiments followed up findings from (4a) focusing specifically on disappearing events. We were interested in precisely controlling the temporal patterns of the sequence to understand which of these the system is particularly sensitive to in the process of change detection. We showed that listeners are sensitive to a variety of temporal patterns, but also exhibit interesting performance limitations. We also showed that when monitoring several concurrent sequences, random sequences provide more of an interference (i.e. are more distracting) than regular sequences.
 (4c) Sohoglu &amp;amp; Chait (2016) Detecting and representing predictable structure during auditory scene analysis. eLife. Everyday environments like a busy street bombard our ears with information. And yet most of the time, the human brain quickly and effortlessly makes sense of this information, a process known as auditory scene analysis. According to one popular theory, the brain has a particular sensitivity to regularly repeating features in sensory signals, and uses those regularities to guide scene analysis. Indeed, many biological sounds contain such regularities, like the pitter-patter of footsteps or the fluttering of bird wings. We addressed whether human listeners use regularity for scene analysis and how this is realized neurobiologically. In most previous studies, listeners attended to one sound stream that repeated slowly. Thus, it was unclear how regularity might benefit scene analysis in more realistic settings with many sounds quickly changing over time. We presented listeners with cluttered, artificial auditory scenes comprised of several sources. Listeners were better able to detect new sources appearing partway when the scenes contained temporally regular, rather than irregular, sound sources. This shows that scene analysis benefited from sound regularity. To understand the neurobiological basis of this effect, we (noninvasively) recorded brain activity with magnetoencephalography (MEG). Brain responses increased when the scenes were temporally regular, suggesting that the brain prioritized those sounds and this enabled better detection of appearing sources.
Finally, the impact of regularity on brain responses to appearing sounds occurred later in listeners who were actively focused on the sounds, compared with na&amp;iuml;ve listeners. This was surprising as weakened brain responses are not usually associated with active focusing. However, this effect can be explained if active focusing led to appearing sounds being more expected, as previous research has shown that expectation reduces brain responses.

Understanding how distraction affects change detection
(5a) Petsas, T., Harrison, J., Kashino, M., Furukawa, S., Chait, M. (2016). The effect of distraction on change detection in crowded acoustic scenes. Hearing Research. In this series of behavioural experiments we investigated the effect of distraction on the maintenance of acoustic scene information in short-term memory. Stimuli are artificial acoustic 'scenes' composed of several (up to twelve) concurrent tone-pip streams ('sources'). A gap (1000ms) is inserted partway through the 'scene'; Changes in the form of an appearance of a new source or disappearance of an existing source, occur after the gap in 50% of the trials. Listeners were instructed to monitor the unfolding 'soundscapes' for these events. Distraction was measured by presenting distractor stimuli during the gap. Experiments 1 and 2 used a dual task design where listeners were required to perform a task with varying attentional demands ('High Demand' vs. 'Low Demand') on brief auditory (Experiment 1a) or visual (Experiment 1b) signals presented during the gap. Experiments 2 and 3 required participants to ignore distractor sounds and focus on the change detection task. Our results demonstrate that the maintenance of scene information in short-term memory is influenced by the availability of attentional and/or processing resources during the gap, and that this dependence appears to be modality specific. We also show that these processes are susceptible to bottom up driven distraction even in situations when the distractors are not novel, but occur on each trial. Change detection performance is systematically linked with the, independently determined, perceptual salience of the distractor sound. The findings also demonstrate that the present task is a useful objective means for determining relative perceptual salience. 


(5b) Southwell, R., Baumann, A., Gal, C., Barascud, N., Friston, K., Chait, M. (2017). Is predictability salient? A study of attentional capture by auditory patterns. Philosophical Transactions of the Royal Society B: Biological Sciences. In this series of behavioural experiments, we measured how different sound patterns (regularly vs. randomly repeating sequences of tone pips) affect change detection ability. The results demonstrated that change detection is more affected by random (relative to regular) concurrent (but task irrelevant sounds). In addition to understanding the susceptibility of change detection to distraction the results also reveal fundamental issues about how the brain processes and represented regular patterns. Work in the visual domain has revealed attentional capture by statistically predictable stimuli, consistent with predictive coding accounts which suggest that attention is drawn to sensory regularities. Here, stimuli comprised rapid sequences of tone pips, arranged in regular (REG) or random (RAND) patterns. EEG data demonstrate that the brain rapidly recognises predictable patterns manifested as a rapid increase in responses to REG relative to RAND sequences. This increase is reminiscent of the increase in gain on neural responses to attended stimuli often seen in the neuroimaging literature, and thus consistent with the hypothesis that predictable sequences draw attention. To study potential attentional capture by auditory regularities, we used REG and RAND sequences in two different behavioural tasks designed to reveal effects of attentional capture by regularity. Overall, the pattern of results suggests that regularity does not capture attention.</gtr:description><gtr:exploitationPathways>The ability to detect changes around us is fundamental to successful interaction with the environment. The auditory system plays a major role in this process -we often hear important change-events before we see them. Indeed individuals with hearing impairment tend to rank inability to efficiently respond to such changes among the most debilitating aspects of deafness. However, despite its important implications to quality of life, change detection ability is not currently captured by existing hearing-impairment evaluation techniques which are mainly based on simple audiometry and on measuring speech understanding. Mapping out the characteristics of normal listeners' sensitivity to change can provide a measure against which to evaluate hearing impairment. The testing procedures developed during this project could be used in the clinic or in the context of scientific research, to estimate the degree to which change detection performance differs from 'normal'. This might be useful for evaluating hearing impairment and provide a clearer assessment of benefit to quality of life obtained from a hearing aid. This will benefit individuals, health professionals as well as NHS policy makers. We have recently obtained funds from 'Action on Hearing Loss (AoHL) for a PhD studentship (2015-2018) to investigate these issues.
Furthermore, understanding the properties of listeners' sensitivity to change, what makes certain events pop out and grab attention, while other events go unnoticed is important for designing human-computer interfaces, and other devices intended to help professionals (operating room personnel, air traffic controllers, pilots, etc.) operate effectively in environments where the detection of change is critical. In an age when the environments that such professionals must deal with become increasingly complex, and where many applications actively immerse users in artificial acoustic scenes, understanding the brain processes underlying change detection could inform the design of these systems (for example by programming them so that they produce change events that listeners are particularly sensitive to) or lead to new technological developments that improve auditory change detection through advanced audio signal processing, brain-machine interfaces, etc. Such technological developments could benefit individuals, health professionals, government agencies, and the commercial private sector. Similarly, Art (design of 'sound-scapes') and the computer gaming industry could benefit from the technological advancements described above, as these fields are currently moving to using artificial environments and brain-machine interfaces for the enhancement of user experience.</gtr:exploitationPathways><gtr:id>F6BBA8B5-0C22-4C43-872B-65B5A08FB76E</gtr:id><gtr:outcomeId>56e00bc613fb27.07338655</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8658FD43-1593-4C6C-90BB-32DDA1F0E05A</gtr:id><gtr:title>The effect of distraction by brief transients on auditory change detection</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f758e1d25f5a983bdd8c1ca4cc5c2099"><gtr:id>f758e1d25f5a983bdd8c1ca4cc5c2099</gtr:id><gtr:otherNames>de Kerangal M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58124a8d048237.33867971</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E22F9DE-9376-4734-850F-7EA8F8B1F695</gtr:id><gtr:title>Sensitivity to temporal regularity supports change detection in complex acoustic scenes</gtr:title><gtr:parentPublicationTitle>(submitted )</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ea4e18936e741dfe31bda92dec88a9b7"><gtr:id>ea4e18936e741dfe31bda92dec88a9b7</gtr:id><gtr:otherNames>Aman L</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56e0b6fd352569.47112228</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DE716279-302C-47C3-AD5B-D515466E4A5A</gtr:id><gtr:title>Is predictability salient? A study of attentional capture by auditory patterns.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/074cb4cd90ff8cb87e0b401257716bc6"><gtr:id>074cb4cd90ff8cb87e0b401257716bc6</gtr:id><gtr:otherNames>Southwell R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>58c9e1eedd0b39.47438460</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>04C7794C-00FE-45BB-B410-3A3060282349</gtr:id><gtr:title>The influence of brief auditory and visual distractor events on change detection in complex acoustic scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460d770031052.24092732</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>46EE7F42-6702-4B32-A865-EB775B36E426</gtr:id><gtr:title>The role of temporal regularity in change detection in complex acoustic scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ea4e18936e741dfe31bda92dec88a9b7"><gtr:id>ea4e18936e741dfe31bda92dec88a9b7</gtr:id><gtr:otherNames>Aman L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460dc8d64bc36.89170205</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB377153-5D8C-42C4-A724-B1BB5850203A</gtr:id><gtr:title>Detecting and representing predictable structure during auditory scene analysis.</gtr:title><gtr:parentPublicationTitle>eLife</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2050-084X</gtr:issn><gtr:outcomeId>585d417c021603.65599769</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>03D3AA48-6FD4-4B91-A77A-75F3D6F054F2</gtr:id><gtr:title>The influence of brief auditory and visual distractor events on change detection in complex acoustic scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460d7fe0a4457.05788685</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>284098C6-EF72-4346-8820-F5894284FF5E</gtr:id><gtr:title>Neural dynamics of change detection in crowded acoustic scenes.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>56e0a5d1dfba29.13281622</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>870EAF9B-A5CD-414C-8CCF-881C0E57F9F3</gtr:id><gtr:title>The detection of appearing and disappearing objects in complex acoustic scenes - an MEG study</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5460d8590b0248.47419811</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>672252F3-02D7-4062-B2A8-8A3FCCAAFCA5</gtr:id><gtr:title>SENSITIVITY TO THE EMERGENCE OF PREDICTABLE STRUCTURE IN SOUND SEQUENCES</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cb291a85dd25e4aa6aa14b5d836151bf"><gtr:id>cb291a85dd25e4aa6aa14b5d836151bf</gtr:id><gtr:otherNames>Barascud N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e0a72ba42d95.21541309</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>85099047-2212-4C42-8C16-397023646244</gtr:id><gtr:title>Detecting change in crowded acoustic environments: Insights from MEG.</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e0a4b421e0c9.07616561</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>23B50951-A512-4ECC-B033-D5B60F18325F</gtr:id><gtr:title>Scene Temporal Structure Modulates Early Stages of Perceptual Organization</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a001310ed3aebd75f871745f3715efc"><gtr:id>2a001310ed3aebd75f871745f3715efc</gtr:id><gtr:otherNames>Sohoglu E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56e0a56bbf1ae3.01455513</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E0179E0-5BFF-4FFD-B211-450864A7439C</gtr:id><gtr:title>Eye metrics - a measure of auditory distraction?</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/22f6a291764e3852d3e3c37adb1f1a9d"><gtr:id>22f6a291764e3852d3e3c37adb1f1a9d</gtr:id><gtr:otherNames>Yoneya M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58c9d8c53d36e6.72438196</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>21B0FB0F-A018-49CD-A1F6-23382655AF84</gtr:id><gtr:title>How the brain discovers patterns in sound sequences</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b88f599918c2b8c529113fdc580ec195"><gtr:id>b88f599918c2b8c529113fdc580ec195</gtr:id><gtr:otherNames>Chait M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56e0a97ef071e7.25152434</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7C13D823-6589-4879-9185-7C59C6930D62</gtr:id><gtr:title>The effect of distraction on change detection in crowded acoustic scenes.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3895368b0b3b6fef88c998698bf5337c"><gtr:id>3895368b0b3b6fef88c998698bf5337c</gtr:id><gtr:otherNames>Petsas T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>56e0b69776ae71.96442097</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/K003399/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>F439A20B-A9B0-4A68-B703-7F6AE7570E39</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems neuroscience</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>