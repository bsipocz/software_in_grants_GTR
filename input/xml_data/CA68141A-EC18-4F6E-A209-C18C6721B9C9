<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Computing Science</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C04EA393-5137-4DC1-850C-4C2BD8251E91"><gtr:id>C04EA393-5137-4DC1-850C-4C2BD8251E91</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Brewster</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B313BEA5-9100-4BD6-BFCB-99990CE3DA22"><gtr:id>B313BEA5-9100-4BD6-BFCB-99990CE3DA22</gtr:id><gtr:firstName>Roderick</gtr:firstName><gtr:surname>Murray-Smith</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE042740%2F1"><gtr:id>CA68141A-EC18-4F6E-A209-C18C6721B9C9</gtr:id><gtr:title>Multimodal, Negotiated Interaction in Mobile Scenarios</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E042740/1</gtr:grantReference><gtr:abstractText>Our proposal is to investigate an alternative means of allowing users to interact with content and services in their environment such that the actions they make, movements, gestures, etc., and feedback they receive are continuous, with the user and system negotiating their interactions in a fluid, dynamic way. We believe the appropriate comparison would be dancing, rather than the current command &amp;amp; control metaphor. When someone dances with a partner there is a soft ebb and flow of control; sometimes one person leads, sometimes the other, this changing fluidly as they dance. We are proposing a similar interaction between a user and computer, where sometimes the user leads and at other times the computer according to the context of the interaction. This contrasts with most current approaches where one agent, be it the human or the computer, pre-empts the other and where most interaction is driven by events and proceeds to varying degrees in rigid, over specified waysExample scenario: Exploring a Digitally Enriched EnvironmentAn example of how this approach could be used is that of location-aware information acquisition while walking in a town centre. You might feel a 'tick' on your phone's vibration motor, making you aware that there is information available about something in your environment. Your rich context understanding abilities would tell you how likely this 'tick' was to be of interest, if you ignore the cue and walk on, the negotiation would end there and then. If you are curious, you might gesture with the phone at likely targets in your surroundings, and get a response from several of them. If you are further intrigued, you may continue to interact with these potential targets, possibly moving from the vibro-tactile to an audio display, gaining information by an active exploration of the environment, something we have evolved to do naturally. The user explores the possibilities in the situation by directly engaging (probing or playing) with it, being able to move at will through the space of possibilities, gaining more and more insight during the interaction. The multimodal feedback provided both encodes the system's current interpretation of the user's intention (e.g. moving towards a target) and the probability of the target meeting the user's needs. After working through combinations of vibration and audio, if the joint dynamics of information source, and user continue to intertwine, the display of the mobile device might be used for full details. This example shows a 'schedule' of modalities, and illustrates the negotiation process in a practical and commercially interesting example.</gtr:abstractText><gtr:fund><gtr:end>2010-10-13</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-05-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>397097</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>13000</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia Devices R&amp;amp;D</gtr:description><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:id>A8E1C2DC-A65E-49F5-8FA4-3BD471BE3BDA</gtr:id><gtr:outcomeId>r-361900180.9152776068401de</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>142000</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia Devices</gtr:description><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:id>08F76D0A-9C10-46F4-BDF9-0FF2BC3DB9F5</gtr:id><gtr:outcomeId>5eba0ccc5eba0ce0</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>142000</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia Devices</gtr:description><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:id>082940A2-7FE2-4FAE-9776-00446A08530A</gtr:id><gtr:outcomeId>r-8037249993.5675880683e6d6</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>59000</gtr:amountPounds><gtr:country>France, French Republic</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Orange France Telecom Research labs</gtr:department><gtr:description>Orange/FT Research labs</gtr:description><gtr:fundingOrg>Orange France Telecom</gtr:fundingOrg><gtr:id>03C0AF89-2FEC-433B-A77B-38A8D02948A5</gtr:id><gtr:outcomeId>5eba0c185eba0c2c</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>59000</gtr:amountPounds><gtr:country>France, French Republic</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Orange France Telecom Research labs</gtr:department><gtr:description>Orange/FT Research labs</gtr:description><gtr:fundingOrg>Orange France Telecom</gtr:fundingOrg><gtr:id>56A603C1-A759-4A5B-AD5A-13B436A6F564</gtr:id><gtr:outcomeId>r-525833946.21110890683e1f4</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>13000</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nokia Devices R&amp;amp;D</gtr:description><gtr:fundingOrg>Nokia</gtr:fundingOrg><gtr:id>70DE618A-77D2-4CFC-8BA1-F6EDD4093813</gtr:id><gtr:outcomeId>5eba12265eba123a</gtr:outcomeId><gtr:sector>Private</gtr:sector></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work on 'Stane' audio input technology fed into a new research project by Prof Jones which used the idea as a tap input option on entry level mobile phones in developing countries. It also fed into new audio/touch input approaches in ubiquitous computing settings.

The work on pedestrian navigation had an impact on a major mobile phone manufacturer's way of thinking about pedestrian navigation with mobile phones, and later led to alternative approaches for mobile devices without GPS being used in Developing countries.

The multimodal approach to interaction made it into a mobile phone product of one of the world's largest mobile phone manufacturers, supported by IP licensing from the University of Glasgow</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>B57B8416-8C7B-4D23-AC20-F09C8319F162</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>546b335d9e2bc8.78378970</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We investigated alternative means of allowing users to interact with content and services in their environment such that the actions they make, movements, gestures, etc., and feedback they receive are continuous, with the user and system negotiating their interactions in a fluid, dynamic way. We believe the appropriate comparison would be dancing, rather than the current command &amp;amp; control metaphor. When someone dances with a partner there is a soft ebb and flow of control; sometimes one person leads, sometimes the other, this changing fluidly as they dance. We are proposing a similar interaction between a user and computer, where sometimes the user leads and at other times the computer according to the context of the interaction. This contrasts with most current approaches where one agent, be it the human or the computer, pre-empts the other and where most interaction is driven by events and proceeds to varying degrees in rigid, over specified ways.

We examined a range of scenarios focussed around digital-physical navigation and collaboration, whic hare described in several published papers (e.g. 'I Did It My Way': Moving Away from the Tyranny of Turn-by-Turn Pedestrian Navigation, and Social Gravity: A Virtual Elastic Tether for Casual, Privacy-Preserving Pedestrian Rendezvous). We also developed novel approaches to touch interaction which use mathematical models to allow richer interaction from the whole finger pose, rather than just the tip of the finger (see AnglePose: robust, precise capacitive touch tracking via 3D orientation estimation). This allows you to use the style of finger-touch to control the interface, effectively allowing your finger to be joystick anywhere on the screen. We also developed a library of software that allows richer audio synthesis on mobile phones like the iPhone, in order to allow richer interaction in future systems.

Our work has been disseminated through: top-quality, peer-reviewed journal and conference publications; formal collaboration and secondments with leading industrial research and development organisations; licensing of software; print and broadcast media; and, a large number of invited talks and demonstrations.</gtr:description><gtr:id>1231EC84-599E-44AE-80FB-DD950792AA99</gtr:id><gtr:outcomeId>546b3482aab4e4.97417761</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections,Security and Diplomacy,Transport</gtr:sector></gtr:sectors><gtr:url>http://www.negotiatedinteraction.com</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>729763D4-2C37-4CC8-8677-7FFEF3788F53</gtr:id><gtr:title>GeoPoke: rotational mechanical systems metaphor for embodied geosocial interaction</gtr:title><gtr:parentPublicationTitle>ACM conference proceedings series</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96cb774581f062e7ab733c815f3d6146"><gtr:id>96cb774581f062e7ab733c815f3d6146</gtr:id><gtr:otherNames>Strachan, S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>546b373cda10c4.43998226</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FB22FAFF-41B7-44AD-BA77-9D998EB17C83</gtr:id><gtr:title>Bearing-based selection in mobile spatial interaction</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8ca11ea265f16060495bdec168e86f2d"><gtr:id>8ca11ea265f16060495bdec168e86f2d</gtr:id><gtr:otherNames>Strachan S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>546b35b69a4af7.28359350</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D51B042-1875-4048-A6A9-AFAA8CB2F468</gtr:id><gtr:title>Evaluating haptics for information discovery while walking</gtr:title><gtr:parentPublicationTitle>Proceedings of the British HCI Group Annual Conference on People and Computers</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec595de1b73ae196b9ae5238993037f4"><gtr:id>ec595de1b73ae196b9ae5238993037f4</gtr:id><gtr:otherNames>Robinson,S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>546b3a1db184f1.39895974</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>86E67C16-AEC8-40FB-A829-B9B8AA5391EC</gtr:id><gtr:title>Empowering People Rather Than Connecting Them</gtr:title><gtr:parentPublicationTitle>International Journal of Mobile Human Computer Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e93f66e9ac53f87c52ce7568e7c26230"><gtr:id>e93f66e9ac53f87c52ce7568e7c26230</gtr:id><gtr:otherNames>Murray-Smith R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>546b362475e4d6.06801519</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>298AD29A-4286-4AD8-9CEC-A27EDBEB1920</gtr:id><gtr:title>Exploring casual point-and-tilt interactions for mobile geo-blogging</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/024e0f5c16b40a2c65948d4afbe8619f"><gtr:id>024e0f5c16b40a2c65948d4afbe8619f</gtr:id><gtr:otherNames>Robinson S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>546b3aa9833ad7.78211195</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D9056467-60C4-4656-9AAA-1695BB382BAE</gtr:id><gtr:title>Navigation your way: from spontaneous independent exploration to dynamic social journeys</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/024e0f5c16b40a2c65948d4afbe8619f"><gtr:id>024e0f5c16b40a2c65948d4afbe8619f</gtr:id><gtr:otherNames>Robinson S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>546bb633a07d09.08374579</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E042740/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>