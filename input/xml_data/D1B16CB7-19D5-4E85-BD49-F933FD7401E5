<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:department>School of Computing</gtr:department><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/D3AD31AD-7CBD-4EA4-8B8F-81BFC4E7C55E"><gtr:id>D3AD31AD-7CBD-4EA4-8B8F-81BFC4E7C55E</gtr:id><gtr:firstName>Emanuele</gtr:firstName><gtr:surname>Trucco</gtr:surname><gtr:orcidId>0000-0002-5055-0794</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD080053%2F2"><gtr:id>D1B16CB7-19D5-4E85-BD49-F933FD7401E5</gtr:id><gtr:title>Video-based animation of people</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D080053/2</gtr:grantReference><gtr:abstractText>The goal of this research project is to solve the fundamental problem of re-using multiple-view video capture of people to support interactive animation with the quality of the source video. Research will investigate the resampling of a multiple-view video database for interactive animation of people to enable user control of movement and viewpoint with video-quality rendering. The proposed research will address the underlying problem of representation of articulated and highly dynamic non-rigid structures in video to allow indexing, reuse and manipulation whilst maintaining the visual quality. Ultimately the challenge is to enable video-quality rending of real people by reuse of captured video sequences allowing user control of movement as in conventional animation. Recent research has demonstrated photo-realistic animation of faces and simple objects by resampling video sequences.This proposal aims to take video-based animation to complex articulated objects such as people without full 3D reconstruction which has been shown to result in loss of visual quality in previous work. The new challenge is to synthesise video sequences of novel movements from captured video of different motion. Animating people from video requires several key advances, including: a representation of human posture and movement which supports the generation of sequences of previously unseen movements; a representation of multiple-view video of articulated objects allowing efficient storage and indexing; algorithms to efficiently selecting the most useful examples for the synthesis of a new sequences from a large set of example sequences; and new video synthesis algorithms for articulated objects in previously unseen configurations. Animation of people directly from captured video has the potential to provide enabling technology for next-generation, video-quality content production in film, television and games. Video-based animation of people will allow powerful re-use and manipulation of captured video (e.g., generating novel body movements for an actor), together with seamless compositing within photo realistic scenes.</gtr:abstractText><gtr:fund><gtr:end>2010-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-05-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>249478</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The prototype system we developed was trialled, on a pilot basis, at the Duncan of Jordanstone College of Art, University of Dundee, to explore its suitability for students of the MSc in Animation. The system was not adopted by the MSc as too preliminary, and also because by the end of the project some low-cost commercial systems suitable for artists' use were emerging.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>690E0EEF-728C-4F82-8995-6B338142793D</gtr:id><gtr:impactTypes/><gtr:outcomeId>5463ef1ca95d79.73385392</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We developed novel algorithms for recognizing whole-body actions (e.g., walking, running, jumping) from multi-camera videos. Multi-camera set-ups are common in animation and video production studios, but also in surveillance environments. We also developed novel algorithms to obtain a 3-D articulated stick figures tracking a moving person in a multi-camera video, without markers on the person's body (markerless mocap).</gtr:description><gtr:exploitationPathways>The intended main application was markerless 3-D human motion tracking. At the time the grant was awarded, such systems were cutting-edge research, and the standard was marker-based tracking, which needed time-consuming and invasive preparation of actors being filmed by a mocap system.</gtr:exploitationPathways><gtr:id>7E781376-2BFE-4546-A2B2-35A2A213AEBB</gtr:id><gtr:outcomeId>5463eda47df157.47691871</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>DA559085-FBD9-478C-A237-B83B85D7A1F2</gtr:id><gtr:title>Articulated Human Motion Tracking with HPSO</gtr:title><gtr:parentPublicationTitle>Conference paper see below</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af4eb93351b2c1f35c0a4d8d0ce0800a"><gtr:id>af4eb93351b2c1f35c0a4d8d0ce0800a</gtr:id><gtr:otherNames>V John</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>54640c10500280.71398359</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3587ABE8-16C4-42B5-92A9-0F1B7D99BAEF</gtr:id><gtr:title>Articulated human motion tracking using charting and particle swarm optimization</gtr:title><gtr:parentPublicationTitle>Conference paper see below</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07ee5cbcb0881306ce8919761ca16438"><gtr:id>07ee5cbcb0881306ce8919761ca16438</gtr:id><gtr:otherNames>Vijay John</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_9675095272140959d4</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D13A76E7-42BC-4B21-B89A-3EC85C326DC3</gtr:id><gtr:title>Markerless human articulated tracking using hierarchical particle swarm optimisation</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f77b1eca943d11a60104fcbeeb9494bb"><gtr:id>f77b1eca943d11a60104fcbeeb9494bb</gtr:id><gtr:otherNames>John V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53cff7ff76210f5a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>32AC7608-99CA-429E-85D7-806010F4199B</gtr:id><gtr:title>Human body pose estimation with particle swarm optimisation.</gtr:title><gtr:parentPublicationTitle>Evolutionary computation</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/535c9449ba73dbbc6ecb2c79729b2c90"><gtr:id>535c9449ba73dbbc6ecb2c79729b2c90</gtr:id><gtr:otherNames>Ivekovic S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>1063-6560</gtr:issn><gtr:outcomeId>doi_53d076076d98e632</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D080053/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>D98900AD-B0E3-4751-B6E7-4251D6407797</gtr:id><gtr:grantRef>EP/D080053/1</gtr:grantRef><gtr:amount>272151.96</gtr:amount><gtr:start>2007-01-08</gtr:start><gtr:end>2007-04-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>D1B16CB7-19D5-4E85-BD49-F933FD7401E5</gtr:id><gtr:grantRef>EP/D080053/2</gtr:grantRef><gtr:amount>249478.71</gtr:amount><gtr:start>2007-05-01</gtr:start><gtr:end>2010-07-31</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>