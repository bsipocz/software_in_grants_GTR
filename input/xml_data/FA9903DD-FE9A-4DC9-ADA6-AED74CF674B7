<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/86A3E46F-01F0-4CBF-8692-6345AA2CC0D3"><gtr:id>86A3E46F-01F0-4CBF-8692-6345AA2CC0D3</gtr:id><gtr:name>British Library The</gtr:name><gtr:address><gtr:line1>96 Euston Road</gtr:line1><gtr:postCode>NW1 2DB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/BA8EB263-C726-4DDE-A797-C357800EF65D"><gtr:id>BA8EB263-C726-4DDE-A797-C357800EF65D</gtr:id><gtr:name>I Like Music Ltd</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:department>Computing</gtr:department><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/86A3E46F-01F0-4CBF-8692-6345AA2CC0D3"><gtr:id>86A3E46F-01F0-4CBF-8692-6345AA2CC0D3</gtr:id><gtr:name>British Library The</gtr:name><gtr:address><gtr:line1>96 Euston Road</gtr:line1><gtr:postCode>NW1 2DB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/BA8EB263-C726-4DDE-A797-C357800EF65D"><gtr:id>BA8EB263-C726-4DDE-A797-C357800EF65D</gtr:id><gtr:name>I Like Music Ltd</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C29F5E21-9A4A-4904-B47D-A257EB274AD8"><gtr:id>C29F5E21-9A4A-4904-B47D-A257EB274AD8</gtr:id><gtr:firstName>Samer</gtr:firstName><gtr:surname>Abdallah</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/98842265-278F-4528-A476-8AD3E8D988FF"><gtr:id>98842265-278F-4528-A476-8AD3E8D988FF</gtr:id><gtr:firstName>Tillman</gtr:firstName><gtr:otherNames>Erik</gtr:otherNames><gtr:surname>Weyde</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A815C209-760F-4D7E-BD19-FCA673896A63"><gtr:id>A815C209-760F-4D7E-BD19-FCA673896A63</gtr:id><gtr:firstName>Emmanouil</gtr:firstName><gtr:surname>Benetos</gtr:surname><gtr:orcidId>0000-0002-6820-6764</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/19C895FA-CA99-466F-A797-F5DE4C80403B"><gtr:id>19C895FA-CA99-466F-A797-F5DE4C80403B</gtr:id><gtr:firstName>Nicolas</gtr:firstName><gtr:surname>Gold</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B88C5D25-0175-4636-A1D4-ECB466369769"><gtr:id>B88C5D25-0175-4636-A1D4-ECB466369769</gtr:id><gtr:firstName>Alan</gtr:firstName><gtr:otherNames>Alexander</gtr:otherNames><gtr:surname>Marsden</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/C7C2DC3B-FB56-4089-9220-7047B5694608"><gtr:id>C7C2DC3B-FB56-4089-9220-7047B5694608</gtr:id><gtr:firstName>Daniel</gtr:firstName><gtr:surname>Wolff</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=AH%2FM002454%2F1"><gtr:id>FA9903DD-FE9A-4DC9-ADA6-AED74CF674B7</gtr:id><gtr:title>An Integrated Audio-Symbolic Model of Music Similarity</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>AH/M002454/1</gtr:grantReference><gtr:abstractText>Similarity is a central aspect of music. It can help as a benchmark for testing theories of music perception and cognition, it is useful to answer for practical applications, such as music retrieval and recommendation, and it is an overarching question that relates to many aspects of music. However, theories of musical similarity have mostly focused on symbolic representations, where musical structures such as melodic and harmonic development are addressed but the aspects or expressive performance, such as micro-timing and timbre are ignored. On the other hand, audio based models, typically distances based on audio features, can capture details of the performance such as timbre, dynamics and tempo changes, but little of the musical structure as it unfolds over time.

Recent progress in audio transcription and alignment and the availability of music analysis tools for music collections with audio and symbolic content, which are being developed in the Digital Music Lab (DML) project, are changing the landscape of research in music. Large datasets of acoustic and symbolic music data that are generated or unveiled through the DML and &amp;quot;Optical Music Recognition from Multiple Sources&amp;quot; Big Data projects encourage an approach that can combine symbolic and audio based analyses into a joint similarity mode. This opens a great potential to new tools for research in music information retrieval and musicology, as the interaction between symbolic structure and acoustic information such as timbral texture has rarely been addressed and it could reveal aspects that have been unnoticed or unexplained so far. If successful, it might contribute to breaking the glass ceiling in music recommendation. 

The aim of this project is to develop an initial framework and conduct experiments on an integration of symbolic melodic and structural similarity models with audio based models. The models ability to capture various notions of similarity will be evaluated on cultural information in music collections (e.g. genre, style, composer) as well as user annotations or ratings of similarity. This work will build on the experience of the participants in modelling audio similarity (Wolff &amp;amp; Weyde 2013), audio transcription and the integration of symbolic and audio based models (Benetos, Ewert &amp;amp; Weyde 2014), symbolic melodic similarity (Marsden 2012) and probabilistic music and performance modelling (Spiro, Gold &amp;amp; Rink 2010, Abdallah et al 2012).</gtr:abstractText><gtr:potentialImpactText>This project offers an opportunity to transform musicological research, specifically the research areas of music similarity and structure analysis. The proposed work will enable access to musicologists, technologists, British Library users, and the wider music listening public to large collections of music recordings and scores, and tools for comparing, classifying, and clustering music collections, enabling large-scale systematic musicology research. In specific, the beneficiaries of this project include those directly involved, and those involved through our partner the British Library (BL), and from any likely commercialisation of project outcomes.
Those directly involved are:
(1) Users of the British Library, in particular of the Digital Music Collections
(2) The British Library, in particular the British Library Labs and the British Library Sound Archive
(3) Musicologists, musicians and music enthusiasts accessing the project-created online resources
Those indirectly involved are:
(4) Music technologists, in particular developers working on music recommendation and similarity
(5) Potential licensees and adopters of the music analysis tools showcased by this project
(6) Customers of licensees and adopters, i.e. the wider music listening public

These different constituents benefit in differing ways. Users of the BL will have access to large collections of audio recordings and scores (the latter also automatically aligned to the recordings), as well as high-level annotations of musical structure. From a collection-level, users will be able to visualise collections, as well as compare and cluster large groups of music recordings and scores. The BL will also be able to improve their service and infrastructure, and will be able to exploit the vast amount of data which already exist in the BL Sound Archive, and also link it to its large corpus of transcribed scores (from printed sources) that are being made available through the Optical Music Recognition project.

Musicologists will be able to access this resource online from BL services but also from the online services developed as part of the DML project, and will benefit from automated tools for accessing audio recordings, scores, and high-level information regarding structure of individual music pieces, as well as collection-level analysis tools for music similarity. Musicians will have the opportunity to practice and study using an online dataset providing recordings, scores, and structural information. Music technologists will realise the impact that audio-score integration can lead to improved systems for music recommendation. Potential licensees of music analysis tools will benefit from software that jointly analyse symbolic and audio information for music, and provides reliable similarity measures for music recommendation applications. Customers of music technology tools will benefit from a more insightful organisation of their music collections and will be able to more easily locate music that matches their interests.

We expect beneficiaries (1) and (2) to gain significant benefit during the lifetime of the project, while beneficiaries (3) will receive increased benefit as the main project outcomes are disseminated. Finally, beneficiaries (4)-(6) should see benefit after the end of the project.

The project includes regular interaction between researchers and partners, as well as partners on the DML and Optical Music Recognition projects. We will organise a workshop towards the end of the project, aimed at musicologists and music technologists, where the created datasets and tools for music similarity will be presented. Project documentation and press items for engaging with users and the wider public will also be created, along with a project-specific website, blog, press, and social media presence. Mechanisms to present the project to the public will be sought in conjunction with the Press and Publications Offices of City, UCL and Lancaster.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/1291772D-DFCE-493A-AEE7-24F7EEAFE0E9"><gtr:id>1291772D-DFCE-493A-AEE7-24F7EEAFE0E9</gtr:id><gtr:name>AHRC</gtr:name></gtr:funder><gtr:start>2014-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>61570</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>The musical Beyond the Fence was the first mainly computer generated musical, and the DML infrastructure with ASymMus similarity analysis was used to conduct research that supported the generation.</gtr:description><gtr:id>AF83A68D-43AC-4AD3-8106-C03F332D6687</gtr:id><gtr:impact>The musical Beyond the Fence was performed in the Arts Theatre in London's West End 22 Feb - 5 March 2016.</gtr:impact><gtr:outcomeId>56de41dc938b79.54843975</gtr:outcomeId><gtr:title>Musical Composition and Production: Beyond the Fence</gtr:title><gtr:type>Composition/Score</gtr:type><gtr:url>http://beyondthefencemusical.com/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput><gtr:artisticAndCreativeProductOutput><gtr:description>'Computer Says Show' is a two part television program on the creation of the world's first computer-generated musical 'Beyond the Fence'.</gtr:description><gtr:id>FED2B784-A4CE-4E65-B3B0-19BCEF2B2BE4</gtr:id><gtr:impact>This program was broadcast on Sky Arts on the 25th Feb and 3rd March 2016.</gtr:impact><gtr:outcomeId>56de437cc1a591.67943132</gtr:outcomeId><gtr:title>TV programme: Computer Says Show</gtr:title><gtr:type>Film/Video/Animation</gtr:type><gtr:url>http://www.wingspanproductions.co.uk/news-and-awards/read/48/Beyond-the-Fence-the-world-s-first-computer-generated-musical</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>I Like Music Ltd</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>I Like Music Ltd</gtr:description><gtr:id>BC940B61-C50D-4A82-8F9E-0593132784D0</gtr:id><gtr:impact>The ILM data is an important contribution to enable music analysis on sizeable collections.</gtr:impact><gtr:outcomeId>56e0e19728e709.37869182-1</gtr:outcomeId><gtr:partnerContribution>I Like Music provides access to over 1 million audio tracks of commercial music and several hundred thousand tracks of production music.</gtr:partnerContribution><gtr:piContribution>The DML and ASymMus project provide the hard- and software infrastructure for music analysis.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>The British Library</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>British Library</gtr:description><gtr:id>F480C025-E294-4B2C-BDEA-877AEABCF666</gtr:id><gtr:impact>The collaboration has extended through out the the DML and ASymMus project and is ongoing in the above mentioned projects. We have also applied for new projects with the European Commission (under review) and are planning to apply for a research project with the EPSRC.</gtr:impact><gtr:outcomeId>56dcd4a29e1889.86228444-1</gtr:outcomeId><gtr:partnerContribution>Researchers at City University London and other contributing universities (Queen Mary University of London, University College London, Lancaster University) have created hard and software infrastructure and applications that support the British Library in serving their users with advanced services and remote access.</gtr:partnerContribution><gtr:piContribution>The British Library has been a research partner in the Digital Music Lab and has supported the ASymMus AHRC projects, and the just started Making Sense of Sounds project by Prof Plumbley (EPSRC) and the RAEng fellowship for Dr. Benetos. In these collaborations, the British Library provides access to audio and other media data, the curation and/or digitisation of collections and the creation of metadata.</gtr:piContribution><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>ASyMMus at MTG Seminar, Universitat Pompeu Fabra Barcelona</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>B8F1266E-146F-49B4-8F42-68BC0F410DC1</gtr:id><gtr:impact>The ASyMMus project and its integration into the DML web interface were presented by Daniel Wolff during his departmental talk on music similarity.</gtr:impact><gtr:outcomeId>56dcea89651690.75093003</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://mtg.upf.edu/node/3393</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>ASyMMuS Workshop on Audio-Symbolic Music Similarity Modelling</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>B804FCB8-7810-48B8-8515-AA8383354C58</gtr:id><gtr:impact>The AHRC funded project on An Integrated Audio-Symbolic Model of Music Similarity (ASyMMuS) aims to integrate aspects of audio and symbolic representations, such as scores or MIDI data, in a joint model. By building on the Digital Music Lab structure, the project's aim is to promote a data driven approach to music similarity. This workshop brought together researchers with different approaches to promote discussions on what constitutes and what contributes to music similarity.</gtr:impact><gtr:outcomeId>56dce9fe76c7e1.48956860</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://dml.city.ac.uk/workshops/asymmus-workshop/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML and ASymMus projects at FMA 2015</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>71C41981-EA67-4E8B-A2CF-458DED9F53B3</gtr:id><gtr:impact>Work carried out on analysing world and traditional music as part of the DML and ASymMus projects was presented at the 5th International Workshop on Folk Music Analysis (FMA 2015). FMA took place on 10-12 June in Paris, France. Project-related papers are listed below:

S. Abdallah, A. Alencar-Brayner, E. Benetos, S. Cottrell, J. Dykes, N. Gold, A. Kachkaev, M. Mahey, D. Tidhar, A. Tovell, T. Weyde, and D. Wolff, &amp;quot;Automatic transcription and pitch analysis of the British Library World &amp;amp; Traditional Music Collection&amp;quot;
A. Leroi, M. Mauch, P. Savage, E. Benetos, J. P. Bello, M. Panteli, J. Six, and T. Weyde, &amp;quot;The deep history of music project&amp;quot;</gtr:impact><gtr:outcomeId>56dce95ba10fc4.97205693</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Other audiences</gtr:primaryAudience><gtr:url>http://fma2015.sciencesconf.org</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Transforming Musicology blog post on music similarity</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>1645CFE7-D825-4930-8CAA-E822E44BA15D</gtr:id><gtr:impact>Dr Alan Marsden (Co-I for the ASyMMuS project) wrote a post on the Transforming Musicology blog entitled &amp;quot;Similarity: haven't we heard this before somewhere?&amp;quot;. The post mentions the ASyMMuS project and its connections with other AHRC-funded projects:</gtr:impact><gtr:outcomeId>56dce82c206c35.80318236</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://transforming-musicology.org/blog/2015-04-09_similarity-havent-we-heard-this-before-somewhere/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>ASyMMuS at Lorentz Center Leiden Workshop on Music Similarity</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>523890BE-AA45-4A10-93AE-75FD84EB4D5A</gtr:id><gtr:impact>Several researchers from the ASyMMus and DML projects prominently contributed to the high-profile international workshop &amp;quot;Music Similarity: Concepts, Cognition and Computation&amp;quot;.

The workshop gathered experts on music similarity from Computer Science, Musicology, Music Psychology and related scientific fields. In a highly-motivated series of workgroups and talks, our researchers collaborated with other experts in the field in theoretical concepts and computer models of music similarity.</gtr:impact><gtr:outcomeId>56dce674917b69.60218331</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://www.lorentzcenter.nl/lc/web/2015/669/description.php3?wsid=669&amp;venue=Oort</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML and ASymMus projects at Musical Timbre Workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>2C47902B-3620-4443-8585-5720ACE31D85</gtr:id><gtr:impact>Recent work on instrumentation recognition and on music that was carried out as part of the DML project was presented at the Workshop on Musical Timbre, that took place on 14th November at T&amp;eacute;l&amp;eacute;com ParisTech, in Paris, France.</gtr:impact><gtr:outcomeId>56dce4781072d2.10862731</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>https://musictimbre.wp.mines-telecom.fr</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML and ASyMMuS projects at DMRN+9 workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>5E6C0663-295C-4CC9-879E-3909601F535B</gtr:id><gtr:impact>Current progress on the DML and ASyMMuS projects were presented at the Digital Music Research Network Workshop 2014 (DMRN+9), taking place on Tuesday 16th December at Queen Mary University of London. The list of project-related presentations is as follows:

&amp;quot;The ASyMMuS project: An integrated audio-symbolic model of music similarity&amp;quot;, Emmanouil Benetos, Daniel Wolff, Tillman Weyde (City University London), Nicolas Gold, Samer Abdallah (University College London) and Alan Marsden (Lancaster University)
&amp;quot;Towards analysing big music data - Progress on the DML research project&amp;quot;, Tillman Weyde, Stephen Cottrell, Jason Dykes, Emmanouil Benetos, Daniel Wolff, Dan Tidhar, Alexander Kachkaev (City University London), Mark Plumbley, Simon Dixon, Mathieu Barthet, Steven Hargreaves (Queen Mary University of London), Nicolas Gold, Samer Abdallah (University College London), Aquiles Alancr-Brayner, Mahendra Mahey and Adam Tovell (The British Library)</gtr:impact><gtr:outcomeId>56dce5acad9527.68985606</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://c4dm.eecs.qmul.ac.uk/dmrn/events/dmrnp9/</gtr:url><gtr:year>2014</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>DML project at THATCamp British Library Labs</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>D8EFA18E-1FC5-4A4C-B05E-86CE12389BEF</gtr:id><gtr:impact>DML project members participated at the THATCamp British Library Labs, which took place on 13th February 2015 at the British Library. THATCamp stands for &amp;quot;The Humanities and Technology Camp&amp;quot;, that is an open, inexpensive meeting where humanists and technologists of all skill levels learn and build together in sessions pitched and voted on at the beginning of the day.</gtr:impact><gtr:outcomeId>56dce78e4e0f26.26504796</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://britishlibrary.typepad.co.uk/digital-scholarship/2015/02/thatcamp-british-library-labs.html</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>6000</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Musical theatre project</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>Wingspan Productions Ltd</gtr:fundingOrg><gtr:id>BB3369A9-1DCC-484B-8FFF-4D6625EB2671</gtr:id><gtr:outcomeId>56dc55570c38f7.28698653</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2015-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>404470</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>A Machine Learning Framework for Audio Analysis and Retrieval</gtr:description><gtr:end>2020-03-02</gtr:end><gtr:fundingOrg>Royal Academy of Engineering</gtr:fundingOrg><gtr:fundingRef>RF/128</gtr:fundingRef><gtr:id>04CA5D55-3959-43EC-B67B-1E53B0E9E859</gtr:id><gtr:outcomeId>56db259a5882e3.14212469</gtr:outcomeId><gtr:sector>Learned Society</gtr:sector><gtr:start>2015-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>6000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Automatic segmentation of audio recordings to speech and music</gtr:description><gtr:end>2015-07-02</gtr:end><gtr:fundingOrg>City, University of London</gtr:fundingOrg><gtr:id>F17B1B98-E17A-4BA7-9D68-D8A927AEB5A2</gtr:id><gtr:outcomeId>56db027dba95f7.92292132</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-04-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The infrastructure and methods developed in this project have been used in the Sky Art Musical Theatre project. 

It has also enabled the study of music similarity on large audio collections, with the DML system that is now available to the public.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>C2E28076-0444-4AAC-B054-5009E578F3B6</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56dcf0dda178a8.25204062</gtr:outcomeId><gtr:sector>Creative Economy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Key findings of this project include an information theoretic approach to music similarity estimation based on diverse representations (including audio and symbolic). 

We also produced an updated version of the DML system which includes similarity features and provides visualisations for over 300k audio recordings from the British Library and I Like Music.</gtr:description><gtr:exploitationPathways>The findings of the ASymMus project can help to analyse music collections for their internal and external structure, e.g. when analysing the development of styles as is planned in the Deep History of Music project.</gtr:exploitationPathways><gtr:id>3437ACF6-2EE5-4244-98E4-D73F86DCB302</gtr:id><gtr:outcomeId>56dcf19d299658.42083987</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://dml.city.ac.uk/category/asymmus/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>The features of the audio tracks analysed by the DML and ASymMus projects, including low, mid, and high-level features and similarity data, are available for download through our SPARQL endpoint. Also on request in other formats.</gtr:description><gtr:id>1EC9133C-D56E-49CF-924D-1975355296DC</gtr:id><gtr:impact>This data is the basis for musicological work with the DML interface, which continues to take place.</gtr:impact><gtr:outcomeId>56dd916f1c2f47.86588554</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>DML audio features, high and mid level analysis, and similarity data</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://mirg.city.ac.uk/cp/home</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>https://code.soundsoftware.ac.uk/hg/dml-open-cliopatria.</gtr:description><gtr:id>F6925941-AE61-4B72-B572-68F69D4C0649</gtr:id><gtr:impact>This software implements the back-end information management developed in the DML and ASymMus projects that enables data analysis with the DML. It provides the API for the Web interface and access via SPARQL and Prolog.</gtr:impact><gtr:outcomeId>56dd92bdb190e5.25192690</gtr:outcomeId><gtr:title>DML Research Information and Result Management System</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/hg/dml-open-cliopatria</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>The visualisation layer of the DML, allowing the analysis of music data collections.</gtr:description><gtr:id>54559826-75E5-4F55-BE41-4A57EDF2FCF6</gtr:id><gtr:impact>This software was used in the DML and it's follow-on project ASymMus, as well as in the recent Musical Theatre project.</gtr:impact><gtr:outcomeId>56e0e3e22d4e69.68825292</gtr:outcomeId><gtr:title>DML Visualisation Framework</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://code.soundsoftware.ac.uk/hg/dml-open-vis</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B837E0EA-4371-4BB5-B81C-9A55CF7B3D15</gtr:id><gtr:title>The Digital Music Lab</gtr:title><gtr:parentPublicationTitle>Journal on Computing and Cultural Heritage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>58cab67b8a7009.58620194</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4597DF22-85A7-4567-8081-FD3509DF0D3E</gtr:id><gtr:title>The Beyond The Fence Musical and Computer Says Show Documentary</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dfb3539ddc79baa25031032f45c73d7a"><gtr:id>dfb3539ddc79baa25031032f45c73d7a</gtr:id><gtr:otherNames>Colton S</gtr:otherNames></gtr:author></gtr:authors><gtr:outcomeId>56de449e0e4c63.59398834</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B7FDCDF0-B7B8-48F7-B5E8-4205A9256B9C</gtr:id><gtr:title>Comparative Music Similarity Modelling Using Transfer Learning Across User Groups</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/27aa5fc47786164b3f2e73741f6875cf"><gtr:id>27aa5fc47786164b3f2e73741f6875cf</gtr:id><gtr:otherNames>Wolff D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56de3e03f00e66.12733593</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>63530A06-ADCC-4896-99D4-0ABAF3C1A440</gtr:id><gtr:title>Comparing Models of Symbolic Music using Probabilistic Grammars and Probabilistic Programming</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56e0df44a2fad1.76065335</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2CFAC7F2-85CE-4A96-8263-C68CDE5B55AD</gtr:id><gtr:title>Computational Music Analysis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1fa60ebb5510005177bc3608f9d39d4f"><gtr:id>1fa60ebb5510005177bc3608f9d39d4f</gtr:id><gtr:otherNames>Abdallah S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:isbn>978-3-319-25929-1</gtr:isbn><gtr:outcomeId>56e194750ecf89.93968825</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">AH/M002454/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>0AEFDABE-67A4-48B1-9DB4-99393BDE6065</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0A982A4A-12CF-4734-AFCA-A5DC61F667F3</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Information &amp; Knowledge Mgmt</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>120E89AC-386D-4E25-9417-A3FE4D6FE83A</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Musicology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>D417837F-1924-4F1E-875B-A160B88B4C98</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Traditional Music</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>