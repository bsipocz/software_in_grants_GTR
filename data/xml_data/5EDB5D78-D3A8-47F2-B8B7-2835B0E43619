<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060"><gtr:id>C0E4FAD2-3C8B-410A-B6DF-3B9B9E433060</gtr:id><gtr:name>University of St Andrews</gtr:name><gtr:address><gtr:line1>College Gate</gtr:line1><gtr:line4>St. Andrews</gtr:line4><gtr:line5>Fife</gtr:line5><gtr:postCode>KY16 9AJ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5F812F7E-6C9C-4BB8-8BAD-DD2ED86AF76B"><gtr:id>5F812F7E-6C9C-4BB8-8BAD-DD2ED86AF76B</gtr:id><gtr:firstName>Julie</gtr:firstName><gtr:surname>Harris</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG038708%2F1"><gtr:id>5EDB5D78-D3A8-47F2-B8B7-2835B0E43619</gtr:id><gtr:title>Perception of colour gradients in real and computer-simulated scenes: effects on depth</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G038708/1</gtr:grantReference><gtr:abstractText>Can you tell the difference between real filmed footage of an event, and a computer-rendered counterpart? Despite tremendous progress in animation and graphics, the answer is most likely yes. We still have a long way to go in generating high quality realistic rendered worlds, that have a wide variety of applications, from gaming, through medical and industrial simulators, to architect-designed walk-throughs that give us a feel for how a new building could look. Improving the naturalness and realism of such virtual environments is a key challenge for those involved in computer graphics and rendering, particularly when there is a demand for interactive, real-time applications: we want to walk around in that simulated new building, not just view static photograph-like scenes. One of the reasons that our progress is slow, is that the extraordinary visual capabilities of most humans, though apparently effortless, hide a complex web of visual processing that is not yet fully understood. If we do not yet understand what enhances realism for the human visual system, it is not surprising that progress is slow in developing technology to improve the realism of simulations. The aim of this work will be to elucidate some of the basic perceptual processes that underlie how subtle changes in colour and lightness enhance the realism of our perception of a three-dimensional scene. This human behavioural research underpins the development of graphics and rendering technologies that will deliver enhanced realism for virtual environments.One of the reasons why this problem is so hard is that the real world contains very complex patterns of light and colour that somehow translate into our perceptions of whether something is green, light, dark, near or far away. For example, my coffee cup on the table in front of me contains white, bright specular highlights that contribute to it looking glossy. There are also reflections of the stripy table mat on which it sits, yet I know these to be reflections, not part of the pattern on the mug. There is an attached shadow, cast by light from the window, and I know that is not part of my mug. And there are changes in lightness and colour across the surface, yet I see the mug as being a single colour, made from a single material. In this project we will study how humans distinguish between depth, light source, and material properties. For most real scenes, this is a very difficult computational problem because specific local patches that appear, for example, darker than those around them, can have the value they do for a variety of reasons. For example, the side of your grey filing cabinet may have a very different lightness than the front, because it slopes away from you in depth, and is at a different angle with the light source, compared with the front of the cabinet (and it could actually be a different colour). How does the human visual system achieve a coherent perception of a solid, non-deforming world, despite changes in view-point and lighting? This is a huge question in both human and computer vision research, with direct implications for computer graphics and virtual environment development, but one that is still poorly understood. We will start by exploring very simple visual scenes containing isolated objects, and study how colour and luminance information can influence depth perception. This data will be used to create models that predict when the luminance/colour information will be most useful. In other experiments we will use real objects and realistically computer-rendered scenes that preserve the relationship between objects and light source that would occur in the real world. Our work will give us immediate information about basic visual mechanisms of depth and scene perception that directly informs the fields of computer graphics and image processing, giving guidelines for when realistic luminance and colour gradients are required for a rendered scene to look realistic.</gtr:abstractText><gtr:fund><gtr:end>2013-02-27</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-10-12</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>359463</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Birmingham</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>School of Psychology Birmingham</gtr:department><gtr:description>ViiHM network: Visual image interpretation in humans and machines</gtr:description><gtr:id>D9DC3C59-5716-4598-A6C3-D0B13EDB6017</gtr:id><gtr:impact>None yet</gtr:impact><gtr:outcomeId>545a5843b231a6.96846138-1</gtr:outcomeId><gtr:partnerContribution>A network funded by EPSRC -- Schofield at Birmingham is the PI</gtr:partnerContribution><gtr:piContribution>EPSRC funded network, I am a member.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Harris visit to IMAX 2010</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>FF39B665-ACED-4ABD-92CC-8BB0784A9E27</gtr:id><gtr:impact>Visit to IMAX (Canada) to discuss possible collaboration and consultancy on 3D film content.</gtr:impact><gtr:outcomeId>56c6f7b9d4f308.37929557</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2010</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Harris lab vision exhibit, Fife Science Festival: St. Andrews Open Day</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>D20D96E7-5825-48FE-9688-C49C3997A639</gtr:id><gtr:impact>Demos and hands-on lab activities from the Harris lab including 3D vision and motion perception, vision and driving, visual camouflage. The activities were popular with all age-groups, from small children, to parents, and older adults.

Several people commented that they didn't know vision was linked to neuroscience and psychology, as well as optometry and physics.</gtr:impact><gtr:outcomeId>545cee0f752321.47394181</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2010,2011,2012,2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Harris: Predicting Perceptions Conference 2012</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>F18A6047-7659-424E-BCBA-97AF395B4BA5</gtr:id><gtr:impact>Harris co-organised a conference, Predicting Perceptions: the third international conference on appearance. Edinburgh, April, 2012. the aim was to bring together academics, industrialists, healthcare professionals and others interested in the topic of visual appearance.</gtr:impact><gtr:outcomeId>56cf226ee95331.67853353</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Harris Sutton Trust Lectures</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>8C048941-BC34-4515-923A-2C25899FC4B0</gtr:id><gtr:impact>The talk was on vision, part of a Summer School for disadvantaged teenagers, organised by the Sutton Trust charity. Participants were surprised that vision and perception are part of the psychology and neuroscience curricula but were interested and engaged in the topics.</gtr:impact><gtr:outcomeId>56c6f01ceb4f74.73756357</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2007,2009,2011,2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Harris visit to Technicolor 2011</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9323ED8D-E9C3-450F-8827-6318D061C52D</gtr:id><gtr:impact>Visit to Technicolor, Rennes, France, to discuss possibility for collaboration or consultation on 3D displays.</gtr:impact><gtr:outcomeId>56c6f10ee395e8.35953298</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2011</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>307582</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>BBSRC responsive mode</gtr:description><gtr:end>2018-01-02</gtr:end><gtr:fundingOrg>Biotechnology and Biological Sciences Research Council (BBSRC)</gtr:fundingOrg><gtr:fundingRef>BB/M001660/1</gtr:fundingRef><gtr:id>12513CB1-AF26-401D-B4E0-7B89C5B1521F</gtr:id><gtr:outcomeId>58af048e9d71d5.36194558</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2015-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work is useful for informing those interested in appearance in architecture, museum displays, video game design and other applications requiring an understanding of the brain computations that underlie vision. With this in mind, Harris co-organised a cross-disciplinary conference for vision scientists, technologists and engineers: Predicting Perception: 3rd International Conference on Appearance, 2012.</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>70A14DBD-C3B5-4F84-89C4-8C0D514F5E05</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56cf22e1a42d44.98257095</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Education,Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project set out to explore interactions between different visual cues, for the perception of depth and shape. In this project we have studied how humans distinguish between depth, light source, and material properties, and how varying these various parameters impacts on depth and shape perception. In one strand of the project, we studied how colour and luminance information can influence depth perception. We found that realistic colour/luminance gradients can be used to perceive shape (Lovell, Bloj &amp;amp; Harris, 2012), and that although the visual information is often ambiguous, observers can quickly learn the relationship between location, lighting and shape, so that the cue becomes reliable (Harding, Harris &amp;amp; Bloj, 2012). We explored the effects of colour variation on shape-from-shading information. Contrary to previous findings, we discovered that colour does not always enhance perceived depth, as would be expected if the visual system exploited information about the statistics of the natural environment (Clery, Bloj, Harris, 2013, Journal of Vision). Instead, the effects of colour on shape from shading are likely to be due to process in very early vision, where the separate colour and luminance process channels first interact.In the other main strand of the project, we used optimal integration models to demonstrate that shading information can be used as a reliable cue to depth, when combined with binocular disparity, and that a classic model, the Maximum Likelihood Estimator, provides an excellent description of how the brain combines the visual information (Lovell, Bloj, Harris, 2012, Journal of Vision. 12, 1, p. 1-18. doi: 10.1167/12.1.1). Related work has also shown how depth complexity within a display can alter depth perception (Harris, 2014, Zeiner, Spitschan &amp;amp; Harris, 2014). We have also, for the first time, explored how these two visual cues are combined over time. Our research has demonstrated that shape from shading can be a 'quick and dirty' cue to shape at very short viewing durations (where disparity is not useful). At longer durations, binocular disparity becomes the more reliable source of visual information.</gtr:description><gtr:exploitationPathways>To make new progress in visualisation techniques, it is becoming increasingly important to account for human perception and to progress algorithmic research based on a firm empirical and theoretical basis. Our work involved revealing some of the basic perceptual mechanisms required for visual information to be combined for 3D perception. The results have implications for the development of 3D displays and for the development of animated 3D content. The work is also useful for informing those interested in appearance in architecture, museum displays, video game design and other applications requiring an understanding of the brain computations that underlie vision. With this in mind, Harris co-organised a cross-disciplinary conference for vision scientists, technologists and engineers: Predicting Perception: 3rd International Conference on Appearance, 2012.</gtr:exploitationPathways><gtr:id>A0C4B17C-78B3-4626-AB3A-CD596F4F6F5B</gtr:id><gtr:outcomeId>r-1860439089.92823157796c69a</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Agriculture, Food and Drink,Construction,Creative Economy,Digital/Communication/Information Technologies (including Software),Education,Healthcare,Culture, Heritage, Museums and Collections,Transport</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>461A3877-9EC8-40FA-8C97-AB9322F6D83B</gtr:id><gtr:title>Perception of relative depth interval: systematic biases in perceived depth.</gtr:title><gtr:parentPublicationTitle>Quarterly journal of experimental psychology (2006)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be7e877515155727d7107bd411ee5c9e"><gtr:id>be7e877515155727d7107bd411ee5c9e</gtr:id><gtr:otherNames>Harris JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1747-0218</gtr:issn><gtr:outcomeId>545a16865b3c06.77382127</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>11D5690B-10BA-4291-BFBF-8C39907A0151</gtr:id><gtr:title>Volume perception: Disparity extraction and depth representation in complex three-dimensional environments.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be7e877515155727d7107bd411ee5c9e"><gtr:id>be7e877515155727d7107bd411ee5c9e</gtr:id><gtr:otherNames>Harris JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>5445395cdc9a39.71787224</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B181B709-207D-4072-92BE-80F94C12BB91</gtr:id><gtr:title>Interactions between luminance and color signals: effects on shape.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a85b09b9534ba277bc2b67e120fcd060"><gtr:id>a85b09b9534ba277bc2b67e120fcd060</gtr:id><gtr:otherNames>Clery S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770774ba6ced</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>366417C5-C694-4A8B-B49F-9728031ABD19</gtr:id><gtr:title>How Do Reliability and Timing Influence Cue-Combinations for Shading and Stereo-Disparity?</gtr:title><gtr:parentPublicationTitle>i-Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6851a3dad564bcae60dbb220ef8db9f5"><gtr:id>6851a3dad564bcae60dbb220ef8db9f5</gtr:id><gtr:otherNames>Lovell P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d0390395bc2bc3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>67F4CE86-804C-40A7-9138-2CC8CE7DCB6C</gtr:id><gtr:title>Learning to use illumination gradients as an unambiguous cue to three dimensional shape.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5d5f568901db088d8531160c1d1ca083"><gtr:id>5d5f568901db088d8531160c1d1ca083</gtr:id><gtr:otherNames>Harding G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>doi_53d08108156899eb</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>69A243CA-083D-4265-BF51-02D78F5506D1</gtr:id><gtr:title>Perceptual integration across natural monocular regions.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5155f4f452c006c22844554e02d619fe"><gtr:id>5155f4f452c006c22844554e02d619fe</gtr:id><gtr:otherNames>Zeiner KM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770774cd61df</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7F150C82-FCFB-481B-AFAB-16E80EBB6EDD</gtr:id><gtr:title>Optimal integration of shading and binocular disparity for depth perception.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d8abe4e7022b16d78a97a86546b2989"><gtr:id>2d8abe4e7022b16d78a97a86546b2989</gtr:id><gtr:otherNames>Lovell PG</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d07707744a578b</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G038708/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>F6909716-124F-49F7-A551-465264E4D2F7</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Displays</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>