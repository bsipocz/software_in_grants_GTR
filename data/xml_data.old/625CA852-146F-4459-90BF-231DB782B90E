<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/03D8AFBB-3EA5-4885-B036-BD4F9F4F9849"><gtr:id>03D8AFBB-3EA5-4885-B036-BD4F9F4F9849</gtr:id><gtr:name>University of Sheffield</gtr:name><gtr:address><gtr:line1>Firth Court</gtr:line1><gtr:line2>Western Bank</gtr:line2><gtr:line4>Sheffield</gtr:line4><gtr:line5>South Yorkshire</gtr:line5><gtr:postCode>S10 2TN</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/461244CD-E991-44D9-A2F7-318530121EC1"><gtr:id>461244CD-E991-44D9-A2F7-318530121EC1</gtr:id><gtr:firstName>Jon</gtr:firstName><gtr:surname>Barker</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FI004181%2F1"><gtr:id>625CA852-146F-4459-90BF-231DB782B90E</gtr:id><gtr:title>ACAS: Analysis of Complex Acoustic Scenes</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I004181/1</gtr:grantReference><gtr:abstractText>For speech recognition technology to work reliably in noisy everyday environments, solutions are needed to the more general Machine Listening problems of sound source separation and acoustic scene understanding. This proposal seeks a four week visit to LabRosa at Columbia University, one of the internationally leading research groups working in these areas, with the aim of exploiting the synergy between their research and the statistical speech recognition research conducted by the proposer's host institution. The collaboration will focus on a dataset constructed from recordings made in a noisy domestic living environment that have been recorded as part of an existing EPSRC project, CHiME, for which the proposer is PI. The proposal outlines a number of immediate research lines that will be pursued tackling the problems of voice detection, source separation and binaural source localization. Possibilities for a longer term collaboration in the development of a model-driven acoustic scene analysis framework will also be discussed during the visit.</gtr:abstractText><gtr:potentialImpactText>Machine Listening is an under-developed research area which despite recent growth (e.g. EPSRC projects EP/D055466/1, EP/G007144/1, EP/G039046/1) still lags a long way behind its sister field, Computer Vision. The planned visit and the ensuing collaboration described in this proposal, will exploit the complementary expertise of the visitor and the host's institutions to push the machine listening field a necessary step forward in a number of directions. The immediate research goals focus on the problem of processing speech in noisy environments -- which is motivated by the enormous importance of robust speech recognition as a machine listening technology. However, this initial research is expected to foster a longer term collaboration the scope of which embraces the more general problem of extracting meaning from complex acoustic scenes. The project anticipates machine listening maturing to the point where the list of viable applications is limited only by the imagination. The ultimate beneficiaries of this research will then be the general public who will eventually be using Machine Listening applications as part of their everyday life. More broadly speaking, the UK as a whole will also benefit through the economic advantage arising from the control and/or deployment of Machine Listening technology. Machine Listening technology has the potential to enhance quality of life in a wide variety of ways. Application areas include: * Audio surveillance, i.e. machines able to listen to live audio recordings and act on the basis of their 'understanding' of the acoustic scene. Consider the security benefits, of audio-based burglar-detectors able to detect intruders in poor light; systems able to monitor the well-being of the elderly or infirm in their own homes, responding to the sound of a fall or a cry of distress, for example; public surveillance systems which use audio to supplement CCTV video and which can detect unexpected acoustic events and relay data to manned command centres. * Intelligent hearing aids able to suppress background noise without removing sound sources that might be significant to the listener. * Social robotics, i.e. robots that form a human-like model of the world to enable them to act and interact in an intuitive and natural manner. * Audio information indexing and retrieval -- e.g. systems that process audio or audio-visual archives (such as YouTube) and automatically add consistent semantic search tags. Such automated tagging will be a key technology in the expansion of the semantic web. Although some Machine Listening technology is finding its way into applications available today, the benefits of the mature technology will only be realised by concerted research funding over a long period. However, the field is sufficiently young that well-targeted small efforts have the potential to make a difference. Accordingly, the current proposal is targeting areas where the complementary expertise of the proposer and the host institution can have significant effect - source separation, environmental sound source modelling, model combination and model adaptation. Given the small size of this funding requested, every effort will be taken to maximise the impact of the project. Crucially, the proposer is currently engaged in a larger EPSRC-funded project, CHiME, that is committed to building an automatic speech recognition system built on a computational hearing framework. The current project is aimed to focus on CHiME data sets and the CHiME application scenario but to introduce complementary research -- that could not be pursued without international cooperations -- and that extends the scope of the CHiME proposal. This link to the CHiME project will enable the research outputs to feed directly into the instruments promoting CHiME, e.g. the CHiME demonstration system, the website, the CHiME corpus, the planned ASR competition and CHiME workshop.</gtr:potentialImpactText><gtr:fund><gtr:end>2010-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>9978</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/I004181/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>