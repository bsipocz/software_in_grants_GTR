<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AB307619-D4FA-427E-A042-09DBEBA84669"><gtr:id>AB307619-D4FA-427E-A042-09DBEBA84669</gtr:id><gtr:name>Swansea University</gtr:name><gtr:department>College of Science</gtr:department><gtr:address><gtr:line1>Singleton Park</gtr:line1><gtr:postCode>SA2 8PP</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AB307619-D4FA-427E-A042-09DBEBA84669"><gtr:id>AB307619-D4FA-427E-A042-09DBEBA84669</gtr:id><gtr:name>Swansea University</gtr:name><gtr:address><gtr:line1>Singleton Park</gtr:line1><gtr:postCode>SA2 8PP</gtr:postCode><gtr:region>Wales</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/5BD0BA62-D021-431C-8FCB-FD162D298D1D"><gtr:id>5BD0BA62-D021-431C-8FCB-FD162D298D1D</gtr:id><gtr:firstName>Matt</gtr:firstName><gtr:surname>Jones</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE042171%2F1"><gtr:id>83613637-DD8D-414E-9A6A-05BC1BCF4996</gtr:id><gtr:title>Multimodal, Negotiated Interaction in Mobile Scenarios</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E042171/1</gtr:grantReference><gtr:abstractText>Our proposal is to investigate an alternative means of allowing users to interact with content and services in their environment such that the actions they make, movements, gestures, etc., and feedback they receive are continuous, with the user and system negotiating their interactions in a fluid, dynamic way. We believe the appropriate comparison would be dancing, rather than the current command &amp;amp; control metaphor. When someone dances with a partner there is a soft ebb and flow of control; sometimes one person leads, sometimes the other, this changing fluidly as they dance. We are proposing a similar interaction between a user and computer, where sometimes the user leads and at other times the computer according to the context of the interaction. This contrasts with most current approaches where one agent, be it the human or the computer, pre-empts the other and where most interaction is driven by events and proceeds to varying degrees in rigid, over specified waysExample scenario: Exploring a Digitally Enriched EnvironmentAn example of how this approach could be used is that of location-aware information acquisition while walking in a town centre. You might feel a 'tick' on your phone's vibration motor, making you aware that there is information available about something in your environment. Your rich context understanding abilities would tell you how likely this 'tick' was to be of interest, if you ignore the cue and walk on, the negotiation would end there and then. If you are curious, you might gesture with the phone at likely targets in your surroundings, and get a response from several of them. If you are further intrigued, you may continue to interact with these potential targets, possibly moving from the vibro-tactile to an audio display, gaining information by an active exploration of the environment, something we have evolved to do naturally. The user explores the possibilities in the situation by directly engaging (probing or playing) with it, being able to move at will through the space of possibilities, gaining more and more insight during the interaction. The multimodal feedback provided both encodes the system's current interpretation of the user's intention (e.g. moving towards a target) and the probability of the target meeting the user's needs. After working through combinations of vibration and audio, if the joint dynamics of information source, and user continue to intertwine, the display of the mobile device might be used for full details. This example shows a 'schedule' of modalities, and illustrates the negotiation process in a practical and commercially interesting example.</gtr:abstractText><gtr:fund><gtr:end>2010-06-10</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-06-11</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>273603</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Novel ways to interact with mobile devices and services through: gesture, haptic feedback, sound and movement.</gtr:description><gtr:exploitationPathways>Range of concepts and implementations that can be built into mobile devices and services.</gtr:exploitationPathways><gtr:id>AE987E23-F38A-4069-9F53-1DFE97EC8F59</gtr:id><gtr:outcomeId>56d70ee266e376.48151560</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://cs.swan.ac.uk/negotiatedinteraction/index.php</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>EDBA5B2D-9803-4065-8D74-E353913908EF</gtr:id><gtr:title>Pico-ing into the future of mobile projection and contexts</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c04ecbc68b7bb5d6ee3e5d44ca60e66a"><gtr:id>c04ecbc68b7bb5d6ee3e5d44ca60e66a</gtr:id><gtr:otherNames>Wilson M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55f95b95b2d1140d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>74B64F12-1050-4DAE-889C-BBF294F8406E</gtr:id><gtr:title>Sweep-Shake: Finding Digital Resources in Physical Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/305d71cacfd0481cb9a961ce28edb65b"><gtr:id>305d71cacfd0481cb9a961ce28edb65b</gtr:id><gtr:otherNames>S Robinson</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_9213211073140557bc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>08DBBF50-6CEE-4C6B-AE8D-310FF9B46289</gtr:id><gtr:title>Exploring casual point-and-tilt interactions for mobile geo-blogging</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/024e0f5c16b40a2c65948d4afbe8619f"><gtr:id>024e0f5c16b40a2c65948d4afbe8619f</gtr:id><gtr:otherNames>Robinson S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>56bc7512afe275.97448109</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E744BAF2-949E-42FB-B138-A71E528528E6</gtr:id><gtr:title>Navigation your way: from spontaneous independent exploration to dynamic social journeys</gtr:title><gtr:parentPublicationTitle>Personal and Ubiquitous Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/024e0f5c16b40a2c65948d4afbe8619f"><gtr:id>024e0f5c16b40a2c65948d4afbe8619f</gtr:id><gtr:otherNames>Robinson S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>56bc75110585a8.13045066</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>52C21CAA-F46E-42AF-B018-4DDD687C36B3</gtr:id><gtr:title>Classic and Alternative Mobile Search</gtr:title><gtr:parentPublicationTitle>International Journal of Mobile Human Computer Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f960dc64663392005f3fbcba53da956"><gtr:id>9f960dc64663392005f3fbcba53da956</gtr:id><gtr:otherNames>Jones M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>56d712da1f2d84.17122934</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E042171/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>