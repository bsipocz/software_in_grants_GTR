<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Gatsby Computational Neuroscience Unit</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/BAB88C7A-BAEE-4C37-8607-41E7D988C94E"><gtr:id>BAB88C7A-BAEE-4C37-8607-41E7D988C94E</gtr:id><gtr:firstName>Massimiliano</gtr:firstName><gtr:surname>Pontil</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/91945CBA-4944-48BE-B974-34803116BC43"><gtr:id>91945CBA-4944-48BE-B974-34803116BC43</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:surname>Latham</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FP009069%2F1"><gtr:id>D39D6983-C800-46D5-9DAA-871D1664C7CC</gtr:id><gtr:title>Closed-Loop Multisensory Brain-Computer Interface for Enhanced Decision Accuracy</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/P009069/1</gtr:grantReference><gtr:abstractText>The goals of our interdisciplinary effort are to develop new methodologies for modeling multimodal neural activity underlying multisensory processing and decision making, and to use those methodologies to design closed-loop adaptive algorithms for optimized exploitation of multisensory data for brain-computer communication. We are motivated by the observation that a dismounted soldier or a tank driver routinely makes decisions in time-pressured and stressful conditions based on a multiplicity of multisensory information presented in cluttered and distracting environments. We envision a closed-loop brain-computer interface (BCI) architecture for enhancing decision accuracy. The architecture will collect multimodal neural, physiological, and behavioral data, decode mental states such as attention orientation and situational awareness, and use the decoded states as feedback to adaptively change the multisensory cues provided to the subject, thus closing the loop. To realize such an architecture we will make fundamental advances on four fronts, constituting our research Thrusts: (1) modeling multisensory integration, attention, and decision making, and the associated neural mechanisms; (2) machine-learning algorithms for high-dimensional multimodal data fusion; (3) adaptive tracking of the neural and behavioral models during online operation of the BCI; and (4) adaptive BCI control of multisensory cues for optimized performance. We have assembled a multidisciplinary team with expertise spanning engineering, computer science, and neuroscience. We will take a fully integrated approach to address these challenges by combining rare state-of-the-art experimental capabilities with novel computational modeling. Complementary experiments in rodents, monkeys, and humans will collect multimodal data to study and model multisensory integration, attention, and decision making, and to prototype a BCI for enhanced decision accuracy. Our modeling efforts will span Bayesian inference, stochastic control, adaptive signal processing, and machine learning to develop: novel Bayesian and control-theoretic models of the brain mechanisms; new stochastic models of multimodal data and adaptive inference algorithms for this data; and novel adaptive stochastic controllers of multisensory cues based on the feedback of users' cognitive state.</gtr:abstractText><gtr:fund><gtr:end>2019-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-09-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>529421</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>B70CAAF6-DBF8-41A1-A947-8ED4D154401B</gtr:id><gtr:title>Connecting YARP to the Web with Yarp.js</gtr:title><gtr:parentPublicationTitle>Frontiers in Robotics and AI</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/96ca861f2e2759844b3226dcc007fd56"><gtr:id>96ca861f2e2759844b3226dcc007fd56</gtr:id><gtr:otherNames>Ciliberto C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a37f4b9726597.18311282</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/P009069/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>