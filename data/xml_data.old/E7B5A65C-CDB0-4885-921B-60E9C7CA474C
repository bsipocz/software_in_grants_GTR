<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A"><gtr:id>5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A</gtr:id><gtr:name>Aston University</gtr:name><gtr:department>Sch of Life and Health Sciences</gtr:department><gtr:address><gtr:line1>Aston Triangle</gtr:line1><gtr:line4>Birmingham</gtr:line4><gtr:postCode>B4 7ET</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A"><gtr:id>5BFB9036-9D16-4AB9-A9EF-097BB6FBD69A</gtr:id><gtr:name>Aston University</gtr:name><gtr:address><gtr:line1>Aston Triangle</gtr:line1><gtr:line4>Birmingham</gtr:line4><gtr:postCode>B4 7ET</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/D4FD0B96-2911-44AC-91DA-BDFB5DF79AFC"><gtr:id>D4FD0B96-2911-44AC-91DA-BDFB5DF79AFC</gtr:id><gtr:firstName>Peter</gtr:firstName><gtr:surname>Bailey</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3520E6DF-FD6B-4533-AB49-1F3AC1E1197B"><gtr:id>3520E6DF-FD6B-4533-AB49-1F3AC1E1197B</gtr:id><gtr:firstName>Brian</gtr:firstName><gtr:surname>Roberts</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF016484%2F1"><gtr:id>E7B5A65C-CDB0-4885-921B-60E9C7CA474C</gtr:id><gtr:title>The perceptual organization of speech: Contributions of general and speech-specific factors</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F016484/1</gtr:grantReference><gtr:abstractText>Spoken communication is a fundamental human activity. However, it is fairly uncommon in everyday life for us to hear the speech of a single talker in the absence of other background sounds, and so our auditory system is faced with the challenge of grouping together those sound elements that come from one source and segregating them from those arising from other sources. Without a solution to this auditory scene analysis problem, our perceptions of speech and other sounds would not correspond to the events that produced them. The fact that we can focus our attention on one person speaking in the presence of other talkers indicates that our auditory perceptual system is generally successful at grouping together the sound elements from a source in a complex auditory scene, and segregating them from other sound sources, but our understanding of how this is achieved remains limited. Most research on auditory scene analysis has focused on relatively simple sounds and has identified a number of general principles for the grouping of sound elements. However, at least as currently understood, these principles seem inadequate to explain the perceptual grouping of speech, because speech has acoustic properties that are diverse and rapidly changing. Furthermore, speech is a highly familiar stimulus, and so our auditory system has had the opportunity to learn about speech-specific properties that may assist in the successful perceptual grouping of speech. The aim of this project is to explore how much of our ability to segregate a talker's speech from a sound mixture depends on general-purpose auditory grouping principles that are applicable to all sounds, and how much depends on grouping principles that are specific to speech sounds. The approach is to generate artificial speech-like stimuli with precisely controlled properties, to mix target utterances with carefully designed competitors that offer alternative grouping possibilities, and to measure how manipulating the acoustic properties of these competitors affects the ability of listeners to recognize the target utterance in the mixture. The results of this project will improve our understanding of the perceptual organization of speech and suggest ways to improve the performance of devices such as hearing aids and automatic speech recognizers when they are operating in noisy environments.</gtr:abstractText><gtr:fund><gtr:end>2011-06-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>371204</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>There are no wider social and economic impacts that can be attributed specifically and unequivocally to this project. However, in more general terms, the results obtained during this project suggest approaches by which engineers and computer scientists might improve the performance of devices such as hearing aids and automatic speech recognizers when they are operating in noisy environments.</gtr:description><gtr:id>BCCD7297-4EAB-4EB2-903C-166FDBA89D1A</gtr:id><gtr:impactTypes/><gtr:outcomeId>5464c2626e4801.76606465</gtr:outcomeId></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In everyday life, it is uncommon to hear the speech of a single talker in the absence of other sounds. That we can focus our attention on one person talking in a crowd indicates that our auditory system is usually successful at grouping together the sound elements from a source in a complex auditory scene, and segregating them from other sounds, but we still know relatively little about how this is achieved. Most research on this &amp;quot;scene analysis&amp;quot; problem has focused on simple sounds and has identified a number of general principles for the grouping of sound elements. However, these principles often seem inadequate to explain the perceptual grouping of speech, because speech has acoustic properties that are diverse and rapidly changing. Also, speech is a highly familiar stimulus, and so our auditory system has the opportunity to learn about speech-specific properties that may assist in the successful perceptual grouping of speech.

This project's aim was to explore how much of our ability to separate a talker's speech from a mixture depends on general grouping principles, applicable to all sounds, and how much depends on speech-specific principles. Our approach was to generate artificial speech-like stimuli with precisely controlled properties, particularly the spectral prominences called formants. These are important because they arise as a result of resonances in the air-filled cavities of the talker's vocal tract. Variation in the frequency and amplitude of a formant is an inevitable consequence of change in the size of its associated cavity as the tongue, lips, and jaw move when the talker produces speech. Hence, knowledge of formant frequencies and their change over time is of great benefit to listeners trying to understand a spoken message, and so choosing the right set of formants from a mixture is critical for intelligibility. Simplified versions of target sentences were synthesised and mixed with carefully designed &amp;quot;competitors&amp;quot; offering alternative grouping possibilities for the formants in the target sentence. The impact of these competitors on listeners' recognition of the target sentence in the mixture was measured as the properties of the competitors were manipulated.

The key findings of the project are: (a) Modulation of the formant-frequency contour, but not the amplitude contour, is critical for across-formant grouping; (b) The ability of listeners to reject a competitor formant declines as either the rate or depth of modulation of its frequency contour increases, relative to that of the target sentence; (c) The impact of a competitor does not depend on whether its pattern of variation in formant frequency is plausibly speech-like; 
(d) The ability of listeners to reject a competitor increases as the pitch difference between target and competitor formants increases; (e) Formant-frequency variation conveys information important for speech intelligibility even in contexts often regarded as conveying information about speech-sound identity mainly through other cues. In summary, the results have shown that our ability to segregate a talker's speech from a mixture depends heavily on general-purpose grouping principles and rather less on speech-specific principles than has been suggested by some researchers.</gtr:description><gtr:exploitationPathways>The results obtained during this project suggest approaches by which engineers and computer scientists might improve the performance of devices such as hearing aids and automatic speech recognizers when they are operating in noisy environments.</gtr:exploitationPathways><gtr:id>3A93CA37-7C93-4CC2-BC23-A2E887233641</gtr:id><gtr:outcomeId>5451364d514c34.46712426</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.aston.ac.uk/lhs/staff/az-index/robertsb/perceptual-organization-of-speech/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>C9DB5778-BFDB-4A94-9880-755421BACC69</gtr:id><gtr:title>Effects of differences in fundamental frequency on across-formant grouping in speech perception.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/77b4ebd385367d16c6b1c21141c7ec84"><gtr:id>77b4ebd385367d16c6b1c21141c7ec84</gtr:id><gtr:otherNames>Summers RJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>545221a165ff49.83785541</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>474272E9-AA0F-4AD9-961B-B8F9BF390EBC</gtr:id><gtr:title>Formant-frequency variation and informational masking of speech by extraneous formants: evidence against dynamic and speech-specific acoustical constraints.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. Human perception and performance</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9556e3979759b6654d5c6370d7ebeb3"><gtr:id>c9556e3979759b6654d5c6370d7ebeb3</gtr:id><gtr:otherNames>Roberts B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0096-1523</gtr:issn><gtr:outcomeId>5452225b38e8a2.03431956</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CA5C534A-C731-49C9-83CF-D74DAE91A0C3</gtr:id><gtr:title>Effects of the rate of formant-frequency variation on the grouping of formants in speech perception.</gtr:title><gtr:parentPublicationTitle>Journal of the Association for Research in Otolaryngology : JARO</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/77b4ebd385367d16c6b1c21141c7ec84"><gtr:id>77b4ebd385367d16c6b1c21141c7ec84</gtr:id><gtr:otherNames>Summers RJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1438-7573</gtr:issn><gtr:outcomeId>545222099238a8.49486819</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FEAA3BA5-58C8-4E97-BFCE-96C8DDE28536</gtr:id><gtr:title>The intelligibility of noise-vocoded speech: spectral information available from across-channel comparison of amplitude envelopes.</gtr:title><gtr:parentPublicationTitle>Proceedings. Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9556e3979759b6654d5c6370d7ebeb3"><gtr:id>c9556e3979759b6654d5c6370d7ebeb3</gtr:id><gtr:otherNames>Roberts B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0962-8452</gtr:issn><gtr:outcomeId>545221c562fe25.71815399</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E37D73E-FB21-4561-8FDF-D4D3DD74AD80</gtr:id><gtr:title>Stream segregation of concurrent speech and the verbal transformation effect: Influence of fundamental frequency and lateralization cues.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0e0e639f9ef568cf235ffd2f0e20541"><gtr:id>d0e0e639f9ef568cf235ffd2f0e20541</gtr:id><gtr:otherNames>Stachurski M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>5a2fe4c42a7e94.78324104</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E06FB1D7-44C0-4FB4-8CE3-A66751C7112D</gtr:id><gtr:title>The perceptual organization of sine-wave speech under competitive conditions.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9556e3979759b6654d5c6370d7ebeb3"><gtr:id>c9556e3979759b6654d5c6370d7ebeb3</gtr:id><gtr:otherNames>Roberts B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn><gtr:outcomeId>5452216e0de763.84471338</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8797B102-3522-4889-ADC4-EDF76CB7731F</gtr:id><gtr:title>The verbal transformation effect and the perceptual organization of speech: influence of formant transitions and F0-contour continuity.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d0e0e639f9ef568cf235ffd2f0e20541"><gtr:id>d0e0e639f9ef568cf235ffd2f0e20541</gtr:id><gtr:otherNames>Stachurski M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn><gtr:outcomeId>55087995c00205.26909975</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E7CF5750-3482-4DE4-8699-1802FBB37777</gtr:id><gtr:title>Formant-frequency variation and its effects on across-formant grouping in speech perception.</gtr:title><gtr:parentPublicationTitle>Advances in experimental medicine and biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c9556e3979759b6654d5c6370d7ebeb3"><gtr:id>c9556e3979759b6654d5c6370d7ebeb3</gtr:id><gtr:otherNames>Roberts B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4614-1589-3</gtr:isbn><gtr:issn>0065-2598</gtr:issn><gtr:outcomeId>545222373a2d92.46437405</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F016484/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>