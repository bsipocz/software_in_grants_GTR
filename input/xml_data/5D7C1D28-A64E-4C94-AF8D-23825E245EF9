<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:department>Sch of Computing</gtr:department><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/83D87776-5958-42AE-889D-B8AECF16B468"><gtr:id>83D87776-5958-42AE-889D-B8AECF16B468</gtr:id><gtr:name>University of Leeds</gtr:name><gtr:address><gtr:line1>University of Leeds</gtr:line1><gtr:line4>Leeds</gtr:line4><gtr:line5>West Yorkshire</gtr:line5><gtr:postCode>LS2 9JT</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/FB7CEFF3-DD80-45B4-9CB8-CAA0A46B68C5"><gtr:id>FB7CEFF3-DD80-45B4-9CB8-CAA0A46B68C5</gtr:id><gtr:firstName>Anthony</gtr:firstName><gtr:surname>Cohn</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/DE0CEBB6-9C5F-4FEC-8714-B13526DCB21B"><gtr:id>DE0CEBB6-9C5F-4FEC-8714-B13526DCB21B</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Hogg</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD061334%2F1"><gtr:id>5D7C1D28-A64E-4C94-AF8D-23825E245EF9</gtr:id><gtr:title>Learning about Activities from Video</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D061334/1</gtr:grantReference><gtr:abstractText>Imagine a system that could search the web for video clips containing an activity similar to one already highlighted (e.g. a car parking or two people having a conversation); that could participate in a card game after observing others do the same; and that could detect someone involved in an unfamiliar activity in a car park. Furthermore, suppose that it could do all of these things with no prior knowledge about the specific objects and activities depicted. All of these capabilities can be couched in terms of looking for similar or analogous activities in video clips.A lot of work has been done over the past forty years on devising methods for finding objects and activities in pictures and video clips by hand-crafting computer-models of what they are expected to look like. Ways have now been found to fully automate the creation of such models for objects (e.g. pedestrians) and simple movements (e.g. running) by learning from large sets of pictures and video clips. This should therefore make it possible to search for similar objects and movements with no prior knowledge of those things.Some progress has been made recently on extending this level of automation to handle a limited range of more complex activities in very simple scenes. This has been achieved by firstly learning about the appearance of objects and then learning about the activities in which they are involved using logical induction. Unfortunately there isn't yet an easy way to ensure the object categories produced are appropriate for the activities to be learnt. Our main aim is to resolve this problem by steering the search for object categories towards those that lead to the most coherent set of activities. A consequence of this could be to change the way we think about age-old problems in computer vision and logical reasoning.</gtr:abstractText><gtr:fund><gtr:end>2009-11-14</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-05-15</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>426517</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>4786FA91-D0C5-45A8-BDDC-C9D45FF67F43</gtr:id><gtr:title>Inferring additional knowledge from QTCN relations</gtr:title><gtr:parentPublicationTitle>Information Sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/321cc28661b5ca4d2a0672896f209af1"><gtr:id>321cc28661b5ca4d2a0672896f209af1</gtr:id><gtr:otherNames>Delafontaine M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55f95a95a58acc7b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4E1202E2-FE21-4F4F-95AA-5CE2F4980605</gtr:id><gtr:title>Learning functional object categories from a relational spatio-temporal representation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/89b7d5603610c00dba649ded02a3a408"><gtr:id>89b7d5603610c00dba649ded02a3a408</gtr:id><gtr:otherNames>K Sridhar</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_672126818813ebfcc2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EDA91A69-348F-41B4-BB5E-EF79ACF79B18</gtr:id><gtr:title>Implementing a qualitative calculus to analyse moving point objects</gtr:title><gtr:parentPublicationTitle>Expert Systems with Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/321cc28661b5ca4d2a0672896f209af1"><gtr:id>321cc28661b5ca4d2a0672896f209af1</gtr:id><gtr:otherNames>Delafontaine M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55f95a95a5817d4f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>88A56F02-1F58-4A95-8652-E3548070956C</gtr:id><gtr:title>Explaining Activities as Consistent Groups of Events</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1179c7f17e32b78eb4e2f27aeef9141e"><gtr:id>1179c7f17e32b78eb4e2f27aeef9141e</gtr:id><gtr:otherNames>Damen D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55f95a95a5771ec7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9084354B-5A69-488F-B013-9A4E020BEF33</gtr:id><gtr:title>Building semantic scene models from unconstrained video</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d927b98033a9f06d5dd578758f1043fa"><gtr:id>d927b98033a9f06d5dd578758f1043fa</gtr:id><gtr:otherNames>Dee H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>545fa68c4c3fc4.29052466</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>61D512A0-BD4B-4B0C-AD17-D91B1DEA11BE</gtr:id><gtr:title>Spatial Cognition VI. Learning, Reasoning, and Talking about Space</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d927b98033a9f06d5dd578758f1043fa"><gtr:id>d927b98033a9f06d5dd578758f1043fa</gtr:id><gtr:otherNames>Dee H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:isbn>978-3-540-87600-7</gtr:isbn><gtr:outcomeId>doi_53cfcdfcd1beedec</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>50BFBD16-A8E0-441D-AC98-31B1A6A7F15A</gtr:id><gtr:title>REASONING WITH TOPOLOGICAL AND DIRECTIONAL SPATIAL INFORMATION</gtr:title><gtr:parentPublicationTitle>Computational Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b8a46f3752fd4026773cf1b531707cbf"><gtr:id>b8a46f3752fd4026773cf1b531707cbf</gtr:id><gtr:otherNames>Li S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f9769768c54b86</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>76F5D8F6-ED91-43B6-B149-2ED49CA5A35B</gtr:id><gtr:title>Motion Segmentation by Consensus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cf6dc75f3daff75b39eb93c836c157e8"><gtr:id>cf6dc75f3daff75b39eb93c836c157e8</gtr:id><gtr:otherNames>R Fraile</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_931276240214023366</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ABEED67F-25C3-4B65-AF20-9F3D9FA5A10F</gtr:id><gtr:title>Reasoning about shadows in a mobile robot environment</gtr:title><gtr:parentPublicationTitle>Applied Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d70ca5ccfda8c4c94eb3f1d1dd31c95e"><gtr:id>d70ca5ccfda8c4c94eb3f1d1dd31c95e</gtr:id><gtr:otherNames>Fenelon V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f9769768bbf7c5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>48C2DBE8-C345-4100-A56E-087FDCFDB32E</gtr:id><gtr:title>Navigational strategies in behaviour modelling</gtr:title><gtr:parentPublicationTitle>Artificial Intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d927b98033a9f06d5dd578758f1043fa"><gtr:id>d927b98033a9f06d5dd578758f1043fa</gtr:id><gtr:otherNames>Dee H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_55f94f94f563cdff</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D061334/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>