<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:department>Experimental Psychology</gtr:department><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/4A348A76-B2D0-4DDD-804A-CE735A6D3798"><gtr:id>4A348A76-B2D0-4DDD-804A-CE735A6D3798</gtr:id><gtr:name>University of Bristol</gtr:name><gtr:address><gtr:line1>Senate House</gtr:line1><gtr:line4>Bristol</gtr:line4><gtr:line5>Avon</gtr:line5><gtr:postCode>BS8 1TH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/3F2F0DDC-1DE5-4F0C-A129-F6513FEE44FA"><gtr:id>3F2F0DDC-1DE5-4F0C-A129-F6513FEE44FA</gtr:id><gtr:firstName>Christopher</gtr:firstName><gtr:surname>Benton</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A2E622D2-DF66-4AF8-9494-A1E5629245A4"><gtr:id>A2E622D2-DF66-4AF8-9494-A1E5629245A4</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>George</gtr:otherNames><gtr:surname>Lovell</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/0846378D-427A-4051-95F1-9A87CA64F63E"><gtr:id>0846378D-427A-4051-95F1-9A87CA64F63E</gtr:id><gtr:firstName>Tom</gtr:firstName><gtr:surname>Troscianko</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3F5FF400-3C1E-434C-A2E6-73F6536F2ECA"><gtr:id>3F5FF400-3C1E-434C-A2E6-73F6536F2ECA</gtr:id><gtr:firstName>Iain</gtr:firstName><gtr:otherNames>Donald</gtr:otherNames><gtr:surname>Gilchrist</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE037372%2F1"><gtr:id>B2889382-4782-4793-89E5-7DBAA9437BA3</gtr:id><gtr:title>Natural dynamic scenes and human vision</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E037372/1</gtr:grantReference><gtr:abstractText>There is a widespread, and reasonable, assumption that our visual system has developed to see and interpret our natural surroundings. This has been investigated in the past by studying the relationship between the structure of natural scenes and the properties of biological systems looking at those scenes. However, much past work has suffered from two assumptions known to be false: first, that nothing moves in the scenes, and second, that the observers do not move their eyes. The reason for this over-simplification has been the technical difficulty of adding these important variables. We have assembled a team of researchers in two Universities (Bristol and Cambridge) who, together, have the necessary expertise to take on this task. We will collect a large number of images and video clips of outdoor scenes in which there is natural movement, such as leaves rustling in the wind, or of objects in motion. We will study how the information in these scenes is encoded by the visual brain, both with theoretical models and with experiments involving human observers looking at the video clips and having to decide whether or not successive video clips are the same, or different from each other.What makes the modelling challenging is the second issue to be explored here - namely, that we move our gaze to a particular place because only one part of our retina, the fovea, has high spatial resolution. The eye movements that we make provide us with sequential information about a scene. We want our model to know (a) how this information is taken up when the eye is looking at one place (it is said to be fixating ), and (b) how it is combined with information from previous and future fixation locations. In other words, how does vision integrate information across eye movements?The novelty of this work is manifold. First, we will calibrate video and still cameras to obtain accurate images of natural scenes from which we can work out how human cones at each location would respond when looking at the scene. There has not been a study of the time-varying properties of natural scenes, and we will provide a resource both for this study and other interested researchers. Furthermore, we will study the interplay between fixation and information uptake and storage in human vision, for natural scenes. Finally, we will develop a computational model capable of predicting to what extent human observers will notice differences between two scenes when they move their eyes, and when scenes contain movement. Such a model is useful for many applications, such as measuring whether people will notice errors in the quality of graphics images, and for estimating the degree to which people will notice the presence of camouflaged objects in the scene.</gtr:abstractText><gtr:fund><gtr:end>2010-06-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>313390</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D9ED8704-D700-42A2-AD95-6B29AB68DB80</gtr:id><gtr:title>Search for gross illumination discrepancies in images of natural objects.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2d8abe4e7022b16d78a97a86546b2989"><gtr:id>2d8abe4e7022b16d78a97a86546b2989</gtr:id><gtr:otherNames>Lovell PG</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770775fc1462</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6DE2D7EF-DA1B-4D15-A8AB-F47DD17EE9A8</gtr:id><gtr:title>Perception of suprathreshold naturalistic changes in colored natural images.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770773735c49</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C191444E-726E-4F3D-9744-961B103A6D66</gtr:id><gtr:title>Perception of differences in natural-image stimuli</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Applied Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3726c6d84f259139df5ad90f1e82f928"><gtr:id>3726c6d84f259139df5ad90f1e82f928</gtr:id><gtr:otherNames>To M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d07407484c6166</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D65DC0CE-3A4B-44A3-A223-CC48457011E8</gtr:id><gtr:title>Perception of differences in naturalistic dynamic scenes, and a V1-based model.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a85e83acf53a04346d42fb4f53556f21"><gtr:id>a85e83acf53a04346d42fb4f53556f21</gtr:id><gtr:otherNames>To MP</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>5675dea4b409d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD1666C3-6F36-4748-9734-A0E31E1EB195</gtr:id><gtr:title>Camouflage and visual perception.</gtr:title><gtr:parentPublicationTitle>Philosophical transactions of the Royal Society of London. Series B, Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0ec31868382c1a7f5334244d3294bcd4"><gtr:id>0ec31868382c1a7f5334244d3294bcd4</gtr:id><gtr:otherNames>Troscianko T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0962-8436</gtr:issn><gtr:outcomeId>doi_53d04b04b9a915f2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B25F2F4E-AFF4-4BCB-9965-DC6DB30689EE</gtr:id><gtr:title>Summation of perceptual cues in natural visual scenes.</gtr:title><gtr:parentPublicationTitle>Proceedings. Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3726c6d84f259139df5ad90f1e82f928"><gtr:id>3726c6d84f259139df5ad90f1e82f928</gtr:id><gtr:otherNames>To M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0962-8452</gtr:issn><gtr:outcomeId>doi_53d049049e8ce621</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E037372/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>