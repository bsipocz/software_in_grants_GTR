<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:department>Faculty of Engineering &amp; the Environment</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/B13BF903-1E76-4185-A4F0-A020EFC4D95D"><gtr:id>B13BF903-1E76-4185-A4F0-A020EFC4D95D</gtr:id><gtr:firstName>David</gtr:firstName><gtr:surname>Smith</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/D86D3775-AE22-497E-A196-5C6EEE8122FC"><gtr:id>D86D3775-AE22-497E-A196-5C6EEE8122FC</gtr:id><gtr:firstName>Steven</gtr:firstName><gtr:surname>Bell</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD505593%2F1"><gtr:id>D99D3F35-7CAC-431C-8CE7-12658BD21B69</gtr:id><gtr:title>Developing a clinical indicator of depth of anaesthesia based on auditory evoked potentials</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D505593/1</gtr:grantReference><gtr:abstractText>When patients go for a serious operation in a hospital they often need a general anaesthetic which puts them to sleep during the operation and stops them feeling any pain. However there are occasionally problems with the general anaesthetic and patients wake up in the middle of the operation. This is particularly a problem in operations when patients are given muscle relaxants to stop them from moving around, as the patients are unable to move and tell the staff they are awake, even though they may be undergoing painful surgery. Anaesthetists are therefore keen to develop ways to monitor when people wake up in operations. One way to do this is to this is to measure the response of the brain to sound, This often uses click sounds which are played through headphones. Research dating back to the 1980s has shown that the response of the brain to sound changes as you fall asleep. However, the response of the brain is very small (about a millionth of a volt). It is therefore hard to measure such a small signal accurately in an operating theatre where there are often many sources of electrical interference. This means that the result of the measurement can be unreliable and so using the existing technique, it might be difficult to tell if a patient is awake or asleep.At the University of Southampton, we have been looking at new mathematical techniques to better measure this small signal from the brain, called an auditory evoked potential. We have specifically been looking at ways to change the stimulating sound to evoke a response from the brain in the most efficient manner. We have investigated ways of presenting the stimulus in an irregular way that allows us to present the stimulus very fast. This is important as we can record lots of similar signals quickly and by averaging them together, obtain a better quality measurement. The special sequences we use to present the sounds quickly are known as maximum length sequences. We have also changed the sounds from a click to a fast frequency sweep called a chirp. These chirps have been shown to produce a larger response from the brain than a click. They are thought to stimulate lots of nerve cells in the auditory system to fire at the same time. Putting the two techniques together, we have found that we can improve the measurement of the small signal from the brain many times in normal hearing subjects.We have started to investigate this new technique for measuring the response of the brain to sound in a small number of patients undergoing operations and we have seen changes that occur with anaesthesia. However, the pattern of changes we found with anaesthetic differs from that found in earlier studies. This might be because we have measured the response more accurately. However we have only tested a small number of subjects so far and we do not know if the new technique for measuring the brain response is more reliable than the old one. This research project will test the technique on a larger number of subjects to see if we really can identify awareness in operations correctly and also to verify the pattern of changes in the response of the brain with anaesthetics that we found in our earlier study. It will also compare the new technique for measuring the brain response to sound to an older existing method that has been used in a number of research projects. We also want to further improve the mathematical techniques to extract the small signal reliably and to categorise the brain signals so that the patient state (awake or asleep) can be clearly indicated to doctors. If the new technique is found to be very reliable, it might become a standard way to monitor if people wake up in operations.</gtr:abstractText><gtr:fund><gtr:end>2009-01-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-02-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>159369</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>0407FD69-F29E-4185-9300-EF2394846A74</gtr:id><gtr:title>Filtering to match hearing aid insertion gain to individual ear acoustics.</gtr:title><gtr:parentPublicationTitle>Trends in amplification</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/abe12d4780be3a60b07653693e094ca9"><gtr:id>abe12d4780be3a60b07653693e094ca9</gtr:id><gtr:otherNames>Bell SL</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1084-7138</gtr:issn><gtr:outcomeId>doi_53d07a07a39b0e4f</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6E5B858A-B11F-44E2-9D1D-1F5368A485FA</gtr:id><gtr:title>Auditory evoked potentials for monitoring during anaesthesia: a study of data quality.</gtr:title><gtr:parentPublicationTitle>Medical engineering &amp; physics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1ee67412a92e00fca6c69aa262e130f5"><gtr:id>1ee67412a92e00fca6c69aa262e130f5</gtr:id><gtr:otherNames>Notley SV</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1350-4533</gtr:issn><gtr:outcomeId>doi_53d000000ecea6fd</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D505593/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>67</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EFFEC6B1-6BC8-4C9D-9D77-02CEF5E4E301</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Biomedical neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>34</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E48C802F-8897-4353-910A-09D09331BB82</gtr:id><gtr:percentage>23</gtr:percentage><gtr:text>Medical science &amp; disease</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>