<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/961756BF-E31F-4A13-836F-0A09BA02385C"><gtr:id>961756BF-E31F-4A13-836F-0A09BA02385C</gtr:id><gtr:name>University of Exeter</gtr:name><gtr:department>Engineering Computer Science and Maths</gtr:department><gtr:address><gtr:line1>University of Exeter</gtr:line1><gtr:line2>Clydesdale House</gtr:line2><gtr:line3>Clydesdale Road</gtr:line3><gtr:line4>Exeter</gtr:line4><gtr:postCode>EX4 4QX</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/961756BF-E31F-4A13-836F-0A09BA02385C"><gtr:id>961756BF-E31F-4A13-836F-0A09BA02385C</gtr:id><gtr:name>University of Exeter</gtr:name><gtr:address><gtr:line1>University of Exeter</gtr:line1><gtr:line2>Clydesdale House</gtr:line2><gtr:line3>Clydesdale Road</gtr:line3><gtr:line4>Exeter</gtr:line4><gtr:postCode>EX4 4QX</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2C7C1A46-3E61-4551-AF6C-30F8BC9BAF00"><gtr:id>2C7C1A46-3E61-4551-AF6C-30F8BC9BAF00</gtr:id><gtr:name>Jaguar Cars Ltd</gtr:name><gtr:address><gtr:line1>Engineering Centre</gtr:line1><gtr:line2>Abbey Road</gtr:line2><gtr:line3>Whitley</gtr:line3><gtr:line4>Coventry</gtr:line4><gtr:line5>Warwickshire</gtr:line5><gtr:postCode>CV3 4LF</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E7F9C354-BDE2-4709-83C9-20A172361054"><gtr:id>E7F9C354-BDE2-4709-83C9-20A172361054</gtr:id><gtr:name>Transport Research Laboratory Ltd</gtr:name><gtr:address><gtr:line1>Crowthorne House</gtr:line1><gtr:line2>Nine Miles Ride</gtr:line2><gtr:postCode>RG40 3GA</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/585341F1-872F-4BDE-9470-D33E7410CCCF"><gtr:id>585341F1-872F-4BDE-9470-D33E7410CCCF</gtr:id><gtr:firstName>Nicolas</gtr:firstName><gtr:surname>Pugeault</gtr:surname><gtr:orcidId>0000-0002-3455-6280</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FN035399%2F1"><gtr:id>634DB1ED-C125-477B-82FC-4FCD063244E0</gtr:id><gtr:title>DEVA - Autonomous Driving with Emergent Visual Attention</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N035399/1</gtr:grantReference><gtr:abstractText>How does a racer drive around a track? Approaching a bend in the road, a driver needs to monitor the road, steer around curves, manage speed and plan a trajectory avoiding collisions with other cars - and all of this, fast and accurately. For robots this remains a challenge: despite progress in computer vision over the last decades, artificial vision systems remain far from human vision in performance, robustness and speed. As a consequence, current prototypes of self-driving cars rely on a wide variety of sensors to palliate the limitations of their visual perception. One crucial aspect that distinguishes human from artificial vision is our capacity to focus and shift our attention. This project will propose a new model of visual attention for a robot driver, and investigate how attention focusing can be learnt automatically by trying to improve the robot's driving.

How and where we focus our attention when solving a task such as driving is studied by psychologists, and the numerous models of attention can be sorted in two categories: first, top-down models capture how world knowledge and expectations guide our attention when performing a specific task; second, bottom-up models characterise how properties of the visual signal make specific regions capture our attention, a property often referred to as saliency. Yet, from a robotics perspective, there remains a lack of a unified framework describing the interplay of bottom-up and top-down attention, especially for a dynamic, time-critical task such as driving. In the racing scenario described above, the driver must take quick and decisive action to steer around bends and avoid obstacles - efficient use of attention is therefore critical.

This project will investigate the hypothesis that our attention mechanisms are learnt on a task specific basis, in a such a way as to provide our visual system optimal information for performing the task. We will investigate how state-of-the-art computer vision and machine learning approaches can be used to learn attention, perception and action jointly to allow a robot driver to compete with humans on a racing simulator, using visual perception only.

A generic learning framework for task-specific attention will be developed that is applicable across a broad range of visual tasks, and bears
the potential for reducing the gap with human performance by a critical reduction in current processing times.</gtr:abstractText><gtr:potentialImpactText>This project will have impact in three communities:
(1) Computer vision and robotics community
(2) Car safety and autonomous cars industry
(3) Psychologists in attention research

The computer vision and robotics community will benefit directly from the new knowledge and techniques developed during this project. By proposing a new approach to reduce the amount of visual data to be processed while solving robotic tasks, the proposed framework could lead to significant improvements in efficiency for vision-based robotics. Additionally, the proposed scenario will offer new insights on the applicability of the embodied cognition paradigm to a wider class of computer vision problems. To ensure maximal impact, in addition to the academic papers, the code will be released within two popular code bases: ROS and OpenCV. Additionally, the software required to interface with the racing simulator will also be released to foster comparison.

Moreover, this project will devise new tools and approaches to the driver assistance and driver-less cars industry. Monitoring the driver's attention is becoming an essential concern as more sophisticated cars also provide more distractions for the driver. This project will provide a better understanding on the ideal gaze patterns when driving. In addition, the attention process developed in this work will provide efficient alternatives to current vision-based driving systems, potentially reducing the reliance on additional sensors.

Finally, this project also has the potential to impact the psychological community by providing a new analysis tool for eye gaze in dynamic tasks based on the proposed model. Eye tracking is a popular paradigm for the analysis of human subjects' attention shifts, applied to a broad range of cognitive tasks. The proposed approach will provide a new tool for analysing attentional patterns, by comparing human gaze locations with locations where an optimal information processing system would focus its attention when solving the given task.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98938</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/N035399/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>