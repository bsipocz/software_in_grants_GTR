<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AEC15D5A-BFD5-4D34-978F-500C126C4FDC"><gtr:id>AEC15D5A-BFD5-4D34-978F-500C126C4FDC</gtr:id><gtr:name>University of Central Lancashire</gtr:name><gtr:department>Sch of Comput Engin and Physical Sci</gtr:department><gtr:address><gtr:line4>Preston</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>PR1 2HE</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AEC15D5A-BFD5-4D34-978F-500C126C4FDC"><gtr:id>AEC15D5A-BFD5-4D34-978F-500C126C4FDC</gtr:id><gtr:name>University of Central Lancashire</gtr:name><gtr:address><gtr:line4>Preston</gtr:line4><gtr:line5>Lancashire</gtr:line5><gtr:postCode>PR1 2HE</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/51BF3421-2251-4475-B9AA-7FC784CE1ACB"><gtr:id>51BF3421-2251-4475-B9AA-7FC784CE1ACB</gtr:id><gtr:firstName>Bogdan</gtr:firstName><gtr:otherNames>Jozef</gtr:otherNames><gtr:surname>Matuszewski</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A3DEB3E6-FD7B-4081-AF2F-63374BDDAB0D"><gtr:id>A3DEB3E6-FD7B-4081-AF2F-63374BDDAB0D</gtr:id><gtr:firstName>Lik-Kwan</gtr:firstName><gtr:surname>Shark</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD077540%2F1"><gtr:id>9AA25411-9B15-491A-B5A5-24AADE5A9514</gtr:id><gtr:title>Metrology Guided Radiotherapy</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D077540/1</gtr:grantReference><gtr:abstractText>Radiotherapy cures local cancer by repeatedly targeting a tumour with small doses of radiation in 'fractions'. Though healthy tissues are irradiated, image assisted pre-treatment planning keeps this to a minimum. CT scans allow the body surface, tumour and critical organs to be seen to scale, so that the optimum shapes and directions of a set of radiation beams can be calculated. These are used daily in a treatment regime that may last weeks. The corresponding dose distribution is estimated and radiobiology can be used to predict the probabilities of cure and complications. How a patient will move or change during treatment itself, is unknown. Hence, an expert specifies a tolerance margin around the tumour and assumes everything else will stay as seen in the pre-treatment CT scan. On this simplified basis the patient is positioned on each day of the treatment.When treatment is in progress, and radiation is being directed at the tumour, there is no monitoring of the patient's position or internal anatomy. Hence, a precisely planned treatment is delivered in a manner that is effectively blind. This situation persists, despite complex new treatments and image guided radiotherapy (IGRT) that now includes 'cone beam' imaging (CBI), which the investigators helped to develop. IGRT radiation dose and CBI practical limitations are new causes for concern. MEGURATH introduces metrology guided radiotherapy (MGRT), where the patient is measured, imaged and modelled during treatment delivery. It researches non-invasive, radiation-free, real-time 3D patient positional monitoring based on optoelectronic sensors using structured light to map the body surface. A prototype system, with unrivalled performance, has been successfully piloted by the investigators in the treatment room. This will be developed to include radical concepts of multi-colour, adaptive sensing, where the structured light projected onto the body surface is first pre-adapted to the shape information available in patient's CT planning scan and then refined during use. The MEGURATH sensors will be synchronised with novel low radiation dose CBI based on acquiring images of the patient between treatment beams. This approach has been piloted by the investigators along with an innovative CBI collimator design that has the potential to halve patient dose, yet improve contrast in the reconstructed volume image. In a feedback loop, the CBI will then be optimally corrected for measured motion that is not necessarily periodic. Reconstructive imaging will then be combined with dynamic deformation modelling, to quantify changes in the shapes and positions of the tumour and nearby organs. Pilot work using sensor measurements to deform treatment plans has been reported by the investigators. Extending this approach across the irradiated part of the body will make it possible to describe the shape changes that occurred in the patient during irradiation. This will be the first time that a point by point model of the patient during treatment has been constructed from live measurements. In turn, this will finally make it possible to use radiobiology to calculate the probabilities of tumour cure and complications for the treatment actually delivered, and to compare this with the treatment that was planned.MEGURATH has strong, diverse theoretical components. It also has an ambitious programme for the translation of science and technology into the first purpose built IGRT research facility in the UK. It is materially supported by the manufacturers of IGRT and treatment planning equipment. Hence, it offers a unique opportunity to advance clinical practice beyond IGRT to MGRT and to use the skills of scientists, mathematicians and clinicians to address cancer treatment at some of the most significant and mobile disease sites, not least breast, lung and pelvis.</gtr:abstractText><gtr:fund><gtr:end>2010-04-15</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-01-16</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>325752</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>201041</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Cross-Disciplinary Feasibility Account</gtr:description><gtr:end>2011-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/H024913/1</gtr:fundingRef><gtr:id>12A40B55-B7EB-45BF-94FF-E1AFC48F00CE</gtr:id><gtr:outcomeId>5ed978d25ed978f0</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2009-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>154386</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Collaborating for Success Through People</gtr:description><gtr:end>2009-03-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/F013698/1</gtr:fundingRef><gtr:id>933C01FE-43CD-4167-BB5D-F676241C0FFF</gtr:id><gtr:outcomeId>5ed890e85ed890fc</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2007-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>65000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Research Equipment Development (RED) Fund</gtr:description><gtr:fundingOrg>University of Central Lancashire</gtr:fundingOrg><gtr:fundingRef>Dynamic Real-Time Three Dimensional Surface Sensor for Biomedical and Biometrics Applications</gtr:fundingRef><gtr:id>B09B50F7-DBBF-4DD8-8B8B-E3470C99F0E2</gtr:id><gtr:outcomeId>54628d8da24904.66362945</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The particularly novel contributions this research has made are in developing new algorithms for segmentation, volume registration, surface modelling, and object tracking. The original algorithms developed for organ segmentation include: a new hybrid algorithm implemented in the level set framework; segmentation with imposed topological constraints; interactive organ delineation utilising combinatorial optimisation; segmentation with learned prior shape constraints. The original algorithms developed for registration include: a flexible particle registration technique and various novel diffeomorphic registration approaches. Another important contribution includes marker tracking and CBCT motion corrected reconstruction. A number of the corresponding software tools, implementing these algorithms, have been made publically available. One measure of the impact the research has made is the high number of the software downloads. Another example of the impact the research has made is that many of the algorithms developed during the research have been successfully used for other medical and biomedical applications including segmentation and deformation estimation of myocardium in tagged MR images, segmentation of extraocular muscles and facial articulation recognition, thus proving their generality and flexibility.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>02CD07D6-BD03-41A1-8190-59532F9284B5</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>m-3298674554.8422213de2447d2</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Radiation therapy exploits the extra susceptibility of many cancers to repeated assault by radiation when compared to healthy tissues. The radiation beams are applied sequentially to a target volume from several different directions. The treatments are commonly divided into daily fractions, delivered over several weeks. Though healthy tissues are also irradiated, image assisted pre-treatment planning keeps this to a minimum. CT scans allow the body surface, tumour and critical organs to be seen to scale, so that the optimum shapes and directions of a set of radiation beams can be calculated. These are used daily in a treatment regime that may last weeks. The corresponding dose distribution is estimated and radiobiology is used to predict the probabilities of cure and complications. How a patient will move or change during treatment itself is unknown. Hence, an expert specifies a tolerance margin around the tumour and assumes all other tissues will stay as seen in the pre-treatment CT scan. On this simplified basis the patient is positioned on each day of the treatment.

Prior to Megurath, when treatment was in progress and radiation directed at the tumour, no monitoring was conducted of the patient's position or internal anatomy. Hence, a precisely planned treatment was delivered in a manner that was effectively blind.

Megurath pioneered metrology guided radiotherapy (MGRT), where the patient is measured, imaged and modelled during treatment delivery. It successfully introduced data from a non-invasive, radiation-free, real-time surface sensor employed for 3D patient positional monitoring and deformation measurement into the treatment room. Together the X-ray and optical modalities have produced spatially and temporally corresponding dynamic measurements of internal anatomy, body surface and skin features during irradiation, enabling estimation of the target tissues deformation and determination of the impact this deformation has on planned versus delivered radiation dose.

Megurath was a three stream project with three partners each delivering specific aspects of the work. Each partner held a separate grant under a common case of support and work programme. The primary contribution of this grant, was the development of novel medical image analysis methods providing support for clinical objectives of the project. Megurath image analysis software consists of a number of advanced tools for noise removal, feature detection, correspondence search, tracking, segmentation, registration and surface manipulation. 

Particularly important novel results have been obtained for tumour/organ segmentation with imposed shape and topological constraints; fast interactive organ delineation; flexible registration techniques enabling prediction of tissues deformation from measured organ surface deformations; segmentation and registration utilising statistical models; robust markers tracking in sequence of CT images; model-base surface registration enabling also efficient surface categorisation. These algorithms have been described in various publications and disseminated nationally and internationally.

Many of the algorithms developed in this work have been successfully used for other medical and biomedical applications including segmentation and deformation estimation of myocardium in tagged MR images, segmentation of extraocular muscles and facial articulation recognition, thus proving their generality and flexibility.

Considerable cross-disciplinary follow-on work was funded from the EPSRC Cross-Disciplinary Feasibility Account programme</gtr:description><gtr:exploitationPathways>Many of the image processing algorithms developed during this research project have been successfully used for other medical and biomedical applications, including: segmentation and deformation estimation of myocardium in tagged MR images, segmentation of extraocular muscles, segmentation of cellular structures or face and facial articulation recognition, thus demonstrating their generality and flexibility. It is also expected that the proposed methodologies will be used in a wide spectrum of other applications, beyond the medical image processing domain. This is evident from a broad spectrum of applications for which the hybrid level set segmentation algorithm (developed on the project) is used by the research community. Additionally the findings of this project were instrumental for outlining the work for the EPSRC funded ECSON and TeRaFS projects. One of the aspects of the project has focussed on efficient surface measurements and analysis. These results can have possible implications for the novel ways of surface sensing with numerous possible applications, e.g. in interactive video games. Indeed such ideas have been at the origin of the recently awarded FP7 funded SEMEOTICONS project.</gtr:exploitationPathways><gtr:id>C7FAE993-C4D9-42F2-B02D-F10CA169C04E</gtr:id><gtr:outcomeId>r-1056858264.89409277d657f4a</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>BA0E3B0B-2AA5-411E-AD28-AD53A853C0A4</gtr:id><gtr:title>Direct inverse deformation field approach to pelvic-area symmetric image registration</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1394074880959f8eabae4cf1447a3ea0"><gtr:id>1394074880959f8eabae4cf1447a3ea0</gtr:id><gtr:otherNames>Papiez B.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>546341a045b2f2.24556066</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>42D08D14-8E32-4D27-82CA-D78A76FE42CB</gtr:id><gtr:title>Multiphase active contour segmentation constrained by evolving medial axes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/54ef4e976b8ee523985958d729d2e145"><gtr:id>54ef4e976b8ee523985958d729d2e145</gtr:id><gtr:otherNames>Yan Zhang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-5653-6</gtr:isbn><gtr:outcomeId>doi_53d058058b323b9c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>99A73958-A630-4C4A-B6B1-76A2A5A2B711</gtr:id><gtr:title>Automatic tracking of implanted fiducial markers in cone beam CT projection images.</gtr:title><gtr:parentPublicationTitle>Medical physics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24ce877edbeabfc1b057b56225a95cf2"><gtr:id>24ce877edbeabfc1b057b56225a95cf2</gtr:id><gtr:otherNames>Marchant TE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0094-2405</gtr:issn><gtr:outcomeId>doi_53d06e06e46a87fc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FA6E391A-7CC2-45D3-8510-10D062F9E398</gtr:id><gtr:title>Level set segmentation of extraocular muscles in MRI images for thyroid-associated-ophtalmopathy diagnosis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a37b732a378a1dc28a9ed8f7ecba13e"><gtr:id>2a37b732a378a1dc28a9ed8f7ecba13e</gtr:id><gtr:otherNames>A. Histace (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_899790223713d6732a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3CE96CF8-8960-4D1D-9B84-EB8FFD0D02EE</gtr:id><gtr:title>Segmentation of myocardial boundaries in tagged cardiac MRI using active contours: a gradient-based approach integrating texture analysis.</gtr:title><gtr:parentPublicationTitle>International journal of biomedical imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b83f8bc8daa2d12d1b5d127dbbe0089f"><gtr:id>b83f8bc8daa2d12d1b5d127dbbe0089f</gtr:id><gtr:otherNames>Histace A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1687-4188</gtr:issn><gtr:outcomeId>doi_53d0760767e6ed19</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>523AEE45-68CA-4848-96AF-1D7A5DE7D909</gtr:id><gtr:title>A computationally efficient method for automatic registration of orthogonal x-ray images with volumetric CT data.</gtr:title><gtr:parentPublicationTitle>Physics in medicine and biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7771ec39b9bd63d704f50fabca5f24ae"><gtr:id>7771ec39b9bd63d704f50fabca5f24ae</gtr:id><gtr:otherNames>Chen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>0031-9155</gtr:issn><gtr:outcomeId>doi_53d0400405d7af8d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4C735092-018D-4294-889D-98CE98F6B7A1</gtr:id><gtr:title>A fully automatic segmentation method for myocardial boundaries of left ventricle in tagged MR images</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/f0b92e2ae80223327c6d7943506c37ab"><gtr:id>f0b92e2ae80223327c6d7943506c37ab</gtr:id><gtr:otherNames>Y. Zhang (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_7025755976140b379a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8294ABDF-E446-418D-9147-0568F469D21E</gtr:id><gtr:title>Segmentation des muscles oculomoteurs en IRM cerebro-orbitale pour l'aide au diagnostic de l'exophtalmie</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/060c952e80e99c760eb50c945e735955"><gtr:id>060c952e80e99c760eb50c945e735955</gtr:id><gtr:otherNames>Breuilly M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>5462952539eb55.46642973</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>587E3698-A63B-4F5F-AF89-5BFE2006A257</gtr:id><gtr:title>Diffusion Filters for Structured Noise Removal</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/88978f5069f2ec2c10b20b53a3d864df"><gtr:id>88978f5069f2ec2c10b20b53a3d864df</gtr:id><gtr:otherNames>B. J. Matuszewski (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_396093352913d95c5c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E83EE48-578B-4539-8A67-238BDE39F0B1</gtr:id><gtr:title>Symmetric Image Registration with Directly Calculated Inverse Deformation Field</gtr:title><gtr:parentPublicationTitle>Annals of the British Machine Vision Association</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ec125911b9031326a83e56001d2ffbb2"><gtr:id>ec125911b9031326a83e56001d2ffbb2</gtr:id><gtr:otherNames>Bartlomiej Papiez (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_6997812923137932a0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0072CE1D-42E1-460D-BE2A-958D5DC89C3B</gtr:id><gtr:title>Actes des Journees Francaises de Radiologie (JFR)</gtr:title><gtr:parentPublicationTitle>Actes des Journees Francaises de Radiologie (JFR)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d04d6c015ff611c08a6cdf2b1c3548ac"><gtr:id>d04d6c015ff611c08a6cdf2b1c3548ac</gtr:id><gtr:otherNames>Portefaix P.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>5464af69e91246.94322605</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B2E6CA6-A2D6-40E8-BE3C-814C945F857C</gtr:id><gtr:title>Statistical Model of Shape Moments with Active Contour Evolution for Shape Detection and Segmentation</gtr:title><gtr:parentPublicationTitle>Journal of Mathematical Imaging and Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d895db59e431e55ec84105d0da4fab0d"><gtr:id>d895db59e431e55ec84105d0da4fab0d</gtr:id><gtr:otherNames>Zhang Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_53cfdcfdc57f2627</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C6D8690D-3600-46A7-9801-331ECF09C23D</gtr:id><gtr:title>Improved 3-D facial representation through statistical shape model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b06b237138b2331a46bf450e8715019e"><gtr:id>b06b237138b2331a46bf450e8715019e</gtr:id><gtr:otherNames>Quan W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-7992-4</gtr:isbn><gtr:outcomeId>doi_53d058058b484647</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BE862DF3-6EAF-49A3-929C-9BF16BE0328A</gtr:id><gtr:title>Facial Expression Biometrics Using Statistical Shape Models</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Advances in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b06b237138b2331a46bf450e8715019e"><gtr:id>b06b237138b2331a46bf450e8715019e</gtr:id><gtr:otherNames>Quan W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53d0760767ab4d56</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C73F2DD9-0C56-486D-9C9A-24D2FA94FCFE</gtr:id><gtr:title>Marker tracks post-processing for accurate fiducial marker position estimation in cone beam CT projection images</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3b8bde4b7db75dee1ecac5f21ddc395c"><gtr:id>3b8bde4b7db75dee1ecac5f21ddc395c</gtr:id><gtr:otherNames>Bogdan J Matuszewski (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_885430257113d9a860</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>90BCF912-FA18-41A6-936D-8500D434EC4B</gtr:id><gtr:title>Reduction of motion artefacts in on-board cone beam CT by warping of projection images.</gtr:title><gtr:parentPublicationTitle>The British journal of radiology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24ce877edbeabfc1b057b56225a95cf2"><gtr:id>24ce877edbeabfc1b057b56225a95cf2</gtr:id><gtr:otherNames>Marchant TE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0007-1285</gtr:issn><gtr:outcomeId>doi_53d07d07dbdba5b2</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D077540/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>16595C3C-600D-4AD2-B394-16E06F96495F</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Med.Instrument.Device&amp; Equip.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>