<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/06706DF4-EA2A-4E30-9F6B-67019B7A478D"><gtr:id>06706DF4-EA2A-4E30-9F6B-67019B7A478D</gtr:id><gtr:firstName>Joachim</gtr:firstName><gtr:surname>Gross</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F7075D7F-1E12-4B81-A7F3-2A9DF28D7097"><gtr:id>F7075D7F-1E12-4B81-A7F3-2A9DF28D7097</gtr:id><gtr:firstName>Pascal</gtr:firstName><gtr:surname>Belin</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FJ003654%2F1"><gtr:id>316636D2-AB96-40C5-8458-C36B42005CAE</gtr:id><gtr:title>Cerebral processing of affective nonverbal vocalizations: a combined fMRI and MEG study.</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/J003654/1</gtr:grantReference><gtr:abstractText>Recognizing and interpreting emotions in other persons is crucial for social interactions. In particular, people of all cultures are able to recognize emotions in vocalizations without speech such as laughs, cries or screams of fear. But how our brain analyses emotion in voices remains poorly understood, compared to how we perceive emotion in faces, for example. In this project we combine a range of advanced techniques to precisely map the brain network involved in recognizing emotions from the voice and determine its exact time-course. We will first use recent morphing technology to manipulate a database of affective voices in order to generate new vocalizations with more or less intense, possibly ambiguous expressions (e.g., pleasure mixed with fear). We will then measure a number of parameters in the vocalizations thus generated. A large group of listeners will be asked to rate each vocalization on what emotion they think is expressed, on how intense and on how positive/negative they think it is. We will also precisely measure important physical properties of the sounds such as their intensity and pitch. In parallel we will use state-of-the art, complementary brain imaging techniques (functional magnetic resonance imaging and magneto-encephalography) to measure brain activity in a smaller number of participants while they listen to the affective vocalizations and perform simple tasks-a Male/Female gender categorisation task, and a Fear/Anger/Pleasure emotion categorization task. The combination of these two brain imaging techniques will allow measurements of cerebral activity with high time (millisecond) and space (millimetre) accuracy. Analysis of this high-resolution, high density dataset will use the most recent algorithms to address three important, unresolved questions. First we want to differentiate the part of the brain that reacts to the acoustics in the sounds - a vocalization of pleasure sounds different from an angry shout-from that part of the brain that reflects genuine affective value-these two vocalizations express different affective states in the speaker. This important distinction has generally not been adequately addressed in past studies. Second, we want to better understand, in those parts of the brain genuinely related to emotional processing, exactly to which parameter they react: To the emotional category, e.g., a response to fear but not to pleasure? To the negative/positive dimension, e.g., a response to all threatening sounds but not happy or joyful sounds? Or to the task being performed by the subject, e.g., a response during an emotional task but not a gender task? Third, we want to better understand the time-course of processing of affective information at different nodes of the network of brain areas involved in processing these vocalizations. If what has been observed with facial expressions of emotion also applies to voices, then we should observe a very fast, probably unconscious reaction to affective vocalizations, in a 'fast route' that bypasses detailed analysis for the sake of a fast reaction. Advanced algorithms will allow us to determine the precise time course of neuronal activity in different parts of the brain network and understand how different emotional parameters affect the speed of brain reaction. Overall, the results of this project will allow us to understand how the brain processes a socially central dimension of voices-the emotion they carry-and what are the key parameters. They will contribute to the advancement of knowledge, but also in the longer term to a better understanding of impairments of emotion processing in pathologies such as autism or schizophrenia. They also have high potential importance for the growing industry of automated voice processing, as engineers need to know not only how to best automatically recognize emotions in people but also how to best generate realistic emotions in artificial voices.</gtr:abstractText><gtr:technicalSummary>Recognizing and interpreting emotions in other persons is crucial for social interactions. People of all cultures are able to recognize emotions in affective nonverbal vocalizations such as laughs, cries or screams of fear, but the underlying cerebral mechanisms remain poorly understood compared to those of facial expression processing, for example. In this project we combine functional magnetic resonance imaging (fMRI) and magneto-encephalography (MEG) to precisely map the brain network involved in detecting and recognizing emotions from the voice and determine its exact time-course. We will first use voice morphing to generate, from a validated database, a set of vocalizations with large variation in perceived affective properties, including ambiguous expressions. We will characterize this set of stimuli both acoustically by detailed analyses, and perceptually through an extensive rating study. In parallel we will measure cerebral activity with MEG and fMRI (in separate sessions) in a group of normal adult volunteers while they listen to the affective vocalizations and perform either an implicit (gender) or explicit (affect) categorisation task. Analysis of this high-resolution, high density dataset will address three important, unresolved questions. First we want to dissociate the contributions of acoustical vs. perceptual information on activity at different nodes of the cerebral network involved in processing affective nonverbal vocalizations. Second we want to better understand the influence of several parameters on the activity in different parts of this network: perceived valence and arousal; emotional category and categorical ambiguity; conscious attention to affective content. Third, we want to characterize the time-course of activity at different nodes of this network. In particular we will test the hypothesis of fast processing of affective information suggested by studies of facial expression processing.</gtr:technicalSummary><gtr:potentialImpactText>Outside of academia, the research is likely to have positive impact on several user groups in the longer term. Person with auditory perception deficits, such as cochlear implant patients and persons with hearing aids experience difficulties recognizing affective states in voice, with negative impact on their social interactions. As good social interactions are a primary determinant of healthy ageing, contributing to a better perception of affective states is likely to have positive outcomes on interactions and quality of life in older populations. Unfortunately, manufacturers of hearing aids and cochlear implants have largely concentrated so far on enhancing speech intelligibility - for obvious reasons. But other, socially-central aspects of vocal communication, such as the detection and recognition of affective states, have been comparatively neglected. The research we conduct in normal participants has the potential to be translated into clinical practice by providing mechanisms to enhance algorithms for auditory decoding in cochlear implants, or optimizing training and rehabilitation strategies. We have experience enhancing the impact of our work through links with industry. We have had funded collaborations with France -Telecom and with Cochlear, one of the leading cochlear implant manufacturers, and this research could lead to further similar collaborations in the longer term. Another potential pathway to impact in this research is to develop links with the growing industry of 'social computing'. Our results have the potential to be of interest for designers of software for automated extraction of affective states. Automated recognition of speech is now at a commercial stage, but automated recognition of affective states for more socially rewarding human-computer interactions is still poor. Information on the solutions to this problem found by our brain over millions of years of evolution could potentially give important clues to the design of more parsimonious, and especially more robust to degradation, recognition systems. Links will the industry in this domain will be specifically sought for with assistance from the Research and Enterprise office e at university of Glasgow. Another important user group potentially affected by the research is the wider public. There is an enormous public and media interest in the voice and social interactions. Therefore we have had in general an excellent response to our activities to engage the public in our work. The laboratory has engaged with the public at several occasions such as at the Glasgow Science Centre during Brain Awareness Week. In addition to publishing in academic journals and presenting at Scientific Conferences, we engage with the press to improve the impact of our findings. The PI has received specific training with the media thanks to the BBSRC-organized Media Training Course, which has been instrumental in enhancing the profile of the laboratory's research (several journal and radio appearances in the past year, including the Sunday Telegraph and BBC4). The research will have an important impact on the career of Dr Patricia Bestelmeyer, the co-investigator research assistant. Dr Bestelmeyer has a growing international reputation in visual and auditory perception and cognition, and she has gained valuable new skills in design and analysis of fMRI data in the PI's laboratory. The proposed project will extend her expertise into a new domain (auditory affective processing) and the use of a new technique (MEG). The project should allow Dr Bestelmeyer to acquire additional experience, research skills and publications, and she should be in a position to apply to faculty positions and start her own research programme as an independent researcher.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-09-20</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2012-09-21</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>263101</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>03542C63-3D66-45CF-B49D-D1160834C48C</gtr:id><gtr:title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/824e1403e61b9b9bd56cf5f36e17589f"><gtr:id>824e1403e61b9b9bd56cf5f36e17589f</gtr:id><gtr:otherNames>Pernet CR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>5675e7757808e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F0B13D33-0713-4505-AD07-DCBB69133B1D</gtr:id><gtr:title>Biasing the perception of ambiguous vocal affect: a TMS study on frontal asymmetry.</gtr:title><gtr:parentPublicationTitle>Social cognitive and affective neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cc8b38e36354cf1e1f91fcdbacd35357"><gtr:id>cc8b38e36354cf1e1f91fcdbacd35357</gtr:id><gtr:otherNames>Donhauser PW</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1749-5016</gtr:issn><gtr:outcomeId>doi_55f93c93c20075f5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D1601C22-F455-4D8B-802C-33FF14D885A5</gtr:id><gtr:title>Voice selectivity in the temporal voice area despite matched low-level acoustic cues.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9c1bf060d52cdd15bcb8b7a10b939853"><gtr:id>9c1bf060d52cdd15bcb8b7a10b939853</gtr:id><gtr:otherNames>Agus TR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn><gtr:outcomeId>5a6617aff2f345.02599322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96E520FC-DA52-43F2-8D51-A9E8A66C0DC9</gtr:id><gtr:title>A language-familiarity effect for speaker discrimination without comprehension.</gtr:title><gtr:parentPublicationTitle>Proceedings of the National Academy of Sciences of the United States of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/7c839cda98a9ce16930f901e86423657"><gtr:id>7c839cda98a9ce16930f901e86423657</gtr:id><gtr:otherNames>Fleming D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0027-8424</gtr:issn><gtr:outcomeId>doi_55f93c93c1f5c705</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2845562D-F651-448A-9870-5607C7A32956</gtr:id><gtr:title>Single-subject analyses of magnetoencephalographic evoked responses to the acoustic properties of affective non-verbal vocalizations.</gtr:title><gtr:parentPublicationTitle>Frontiers in neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0a0819ba8e2534e8ddffa133675564d8"><gtr:id>0a0819ba8e2534e8ddffa133675564d8</gtr:id><gtr:otherNames>Salvia E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1662-453X</gtr:issn><gtr:outcomeId>pm_55f93c293c20963da</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/J003654/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>