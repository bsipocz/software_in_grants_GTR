<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/ED6A6B32-663C-4A62-A33B-2C6A68E2E102"><gtr:id>ED6A6B32-663C-4A62-A33B-2C6A68E2E102</gtr:id><gtr:name>University of Essex</gtr:name><gtr:department>Computer Sci and Electronic Engineering</gtr:department><gtr:address><gtr:line1>Wivenhoe Park</gtr:line1><gtr:line4>Colchester</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CO4 3SQ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/ED6A6B32-663C-4A62-A33B-2C6A68E2E102"><gtr:id>ED6A6B32-663C-4A62-A33B-2C6A68E2E102</gtr:id><gtr:name>University of Essex</gtr:name><gtr:address><gtr:line1>Wivenhoe Park</gtr:line1><gtr:line4>Colchester</gtr:line4><gtr:line5>Essex</gtr:line5><gtr:postCode>CO4 3SQ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/83F9583C-4526-4F2F-9C5F-81988EEA2E1D"><gtr:id>83F9583C-4526-4F2F-9C5F-81988EEA2E1D</gtr:id><gtr:firstName>Riccardo</gtr:firstName><gtr:surname>Poli</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/6EE3CDF6-DFD1-4849-BF0B-41ED776D70E5"><gtr:id>6EE3CDF6-DFD1-4849-BF0B-41ED776D70E5</gtr:id><gtr:firstName>Luca</gtr:firstName><gtr:surname>Citi</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/CDE42193-A340-4F46-8144-B439BD3E8C39"><gtr:id>CDE42193-A340-4F46-8144-B439BD3E8C39</gtr:id><gtr:firstName>Caterina</gtr:firstName><gtr:surname>Cinel</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FP009204%2F1"><gtr:id>E55B4879-8FE7-40AD-8E09-E5A6D326EA59</gtr:id><gtr:title>ONR-15-FOA-0011 MURI Topic #3 - Closed-Loop Multisensory Brain-Computer Interface for Enhanced Decision Accuracy</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/P009204/1</gtr:grantReference><gtr:abstractText>The goals of our interdisciplinary effort are to develop new methodologies for modeling multimodal neural activity underlying multisensory processing and decision making, and to use those methodologies to design closed-loop adaptive algorithms for optimized exploitation of multisensory data for brain-computer communication. We are motivated by the observation that a dismounted soldier or a tank driver routinely makes decisions in time-pressured and stressful conditions based on a multiplicity of multisensory information presented in cluttered and distracting environments. We envision a closed-loop brain-computer interface (BCI) architecture for enhancing decision accuracy. The architecture will collect multimodal neural, physiological, and behavioral data, decode mental states such as attention orientation and situational awareness, and use the decoded states as feedback to adaptively change the multisensory cues provided to the subject, thus closing the loop. To realize such an architecture we will make fundamental advances on four fronts, constituting our research Thrusts: (1) modeling multisensory integration, attention, and decision making, and the associated neural mechanisms; (2) machine-learning algorithms for high-dimensional multimodal data fusion; (3) adaptive tracking of the neural and behavioral models during online operation of the BCI; and (4) adaptive BCI control of multisensory cues for optimized performance. We have assembled a multidisciplinary team with expertise spanning engineering, computer science, and neuroscience. We will take a fully integrated approach to address these challenges by combining rare state-of-the-art experimental capabilities with novel computational modeling. Complementary experiments in rodents, monkeys, and humans will collect multimodal data to study and model multisensory integration, attention, and decision making, and to prototype a BCI for enhanced decision accuracy. Our modeling efforts will span Bayesian inference, stochastic control, adaptive signal processing, and machine learning to develop: novel Bayesian and control-theoretic models of the brain mechanisms; new stochastic models of multimodal data and adaptive inference algorithms for this data; and novel adaptive stochastic controllers of multisensory cues based on the feedback of users' cognitive state.</gtr:abstractText><gtr:potentialImpactText>DoD/MoD

A soldier makes decisions in time-pressured and stressful conditions, based on a cluttered visual and auditory scene containing moving objects and flashes, exposed to different lighting conditions, and various auditory cues. By discovering the fundamental mechanisms underlying multisensory integration, attention, and decision making, and by developing new machine learning and control-theoretic methods to model, decode, and control the neural mechanisms underlying key mental states, we will develop a closed-loop BCI to enhance decision accuracy under such adverse scenarios. This will significantly advance DoD/MoD efforts to increase situational awareness and national security.

Civilian Impacts

It will also serve civilian and commercial needs, such as pilot, vehicular, and control command interfaces. Given the truly interdisciplinary nature of this work, which spans neuroscience, engineering, and computer science, this proposal will generate new programs of study to train graduate students in these emerging interdisciplinary areas. We will train 14 graduate students and postdocs per year. These highly qualified trainees will have a positive impact on the US/UK economy, science and engineering.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>519258</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>CEC8A1F9-0371-4345-8C1D-8BE9E51159B7</gtr:id><gtr:title>Adaptive user modelling in car racing games using behavioural and physiological data</gtr:title><gtr:parentPublicationTitle>User Modeling and User-Adapted Interaction</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2a1b74dac4caa61c000445c99306bcb7"><gtr:id>2a1b74dac4caa61c000445c99306bcb7</gtr:id><gtr:otherNames>Georgiou T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a5789e94c7177.30177692</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B96E54F8-1A20-4DB4-8B8A-62155FBA4609</gtr:id><gtr:title>Group Augmentation in Realistic Visual-Search Decisions via a Hybrid Brain-Computer Interface.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6c3ad5cad4f66fd45caf82dba6c67fda"><gtr:id>6c3ad5cad4f66fd45caf82dba6c67fda</gtr:id><gtr:otherNames>Valeriani D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn><gtr:outcomeId>5a733580e45e04.85947572</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/P009204/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>