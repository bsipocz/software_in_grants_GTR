<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name><gtr:address><gtr:line1>BBSRC</gtr:line1><gtr:line2>Polaris House</gtr:line2><gtr:line3>North Star Avenue</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:postCode>SN2 1UH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/A7B8CC0E-9CB6-4CAA-A626-B243B4AD860D"><gtr:id>A7B8CC0E-9CB6-4CAA-A626-B243B4AD860D</gtr:id><gtr:firstName>Cecilia</gtr:firstName><gtr:otherNames>Mary</gtr:otherNames><gtr:surname>Heyes</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/44B3B477-75B9-44EB-8916-318136B765B2"><gtr:id>44B3B477-75B9-44EB-8916-318136B765B2</gtr:id><gtr:firstName>Alan</gtr:firstName><gtr:surname>Johnston</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FF037503%2F1"><gtr:id>6731269B-3BB0-4DF1-A999-BBB5B9F0D96E</gtr:id><gtr:title>Analysing Dynamic Change in Faces</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/F037503/1</gtr:grantReference><gtr:abstractText>Humans are very good at understanding and interpreting the motion of others peoples faces. We can effortlessly recognise emotions and interpret subtle facial behaviours such as sardonic smiles, thoughtful frowns or questioning looks, but the question remains, how do we do this? We need new computer based tools to be able to explore this fascinating area of psychology. In this project we will develop a new form of three-dimensional camera system that will allow us to record the movements of people's faces and then process this video information to discover the components of movements that go to make them up. Once we are able to discover the parts of movements that add together to make familiar facial expression we can use this to be able to create new faces; in much the same way as a music mixing desk allows you to blend together different sounds, we will have software that allows us to mix new faces with whatever expressions we select. Using this new tool we can then carry out experiments to look at how we process faces and imitate other people's facial movement. We will examine how observing the movement in one persons face can be translated into movements of our own face to imitate the action. Because the faces we use are created in the computer we can manipulate them in any way we like. This new technology will allow us to address a large set of basic questions. Can we imitate a person if the face seen only from the side or if it is shown upside down? Do we do better when we imitate our self, a friend or a stranger? We can even create caricatures of faces, where we exaggerate particular movements, to evaluate how these facial gestures are represented in the human face processing system. A better understanding of how imitation works will help us understand social behaviours and their development, and also help in developing computer systems that can both recognise and react to our facial expressions. The new face mixing software will also have commercial applications, for example it can be of use in the computer games and entertainment industry. Movements from one persons face can be used as the instructions to be transferred to create another persons face making the same movement. This will allow for example a voice actor to control the movements of a characters face in addition to simply providing the expressive dialogue, the generation of high quality realistic synthetic actors or faster more efficient ways to video conference over your mobile phone.</gtr:abstractText><gtr:fund><gtr:end>2011-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-06-17</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>330794</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Royal Society Summer Exhibition</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>2880ED40-8D26-4A1B-94D1-ECB55A91A497</gtr:id><gtr:impact>The exhibit generated surprise that it was possible to produce and drive a photorealistic avatar. This prompted discussion about what such a tool might be used for.

The work was selected for press attention.</gtr:impact><gtr:outcomeId>5462365ec76905.64918660</gtr:outcomeId><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2010</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>70000</gtr:amountPounds><gtr:country>Australia, Commonwealth of</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Australian Research Council Research Grant</gtr:description><gtr:end>2012-09-02</gtr:end><gtr:fundingOrg>Australian Research Council (ARC)</gtr:fundingOrg><gtr:fundingRef>DP0986898</gtr:fundingRef><gtr:id>C6E2AE7B-E4F6-47E9-97BD-B71485771F6A</gtr:id><gtr:outcomeId>5eca747c5eca7490</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2009-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>244000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Research Project Grant Full</gtr:description><gtr:end>2017-01-02</gtr:end><gtr:fundingOrg>The Leverhulme Trust</gtr:fundingOrg><gtr:fundingRef>RPG-2013-218</gtr:fundingRef><gtr:id>19E121CB-9FF5-4D42-8A0A-73FBB79A2BB7</gtr:id><gtr:outcomeId>54622e038e6212.63679270</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work has a number of potential applications including mapping faces between different views of the face, and mapping between different lighting conditions. This work is now supported by NTT Japan through a studentship jointly funded by NTT and UCL. It can also be used for performance-driven animation and the analysis of faction action. The aspect will be progressed though further collaboration with computer scientists and through knowledge transfer activity. A project has been presented through the Security and Resilience Industry Suppliers Community (RISC)'s Academic Marketplace.

We also undertook extensive public engagement in the project, with media coverage for our results and presentation at the Royal Society Summer exhibition in 2011.</gtr:description><gtr:firstYearOfImpact>2011</gtr:firstYearOfImpact><gtr:id>2A105AD9-057A-4FD0-8AE9-16F3479407A3</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545902746ad500.30720145</gtr:outcomeId><gtr:sector>Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Our focus was on the development and use of new tools for facial motion analysis and expression mapping between faces. Faces vary in colour as well as image brightness but colour is not used effectively in image motion or stereo algorithms so we developed a new approach to image motion analysis that characterised the bright-dark, yellow-blue and red-green opponent channels of the human colour system as chromatic derivatives. These derivatives were incorporated into our existing spatio-temporal brightness derivative method for motion and binocular disparity calculation resulting in improved performance. 

The prime motivation of the computer vision work was to build tools supporting new methods for studying the perception of facial motion. A major aim was to generate a photorealistic average avatar allowing the separation of facial motion from form. This was achieved using 2D image-based performance-driven animation. We constructed a photorealistic avatar using Principle Components Analysis (PCA) over vectors encoding the differences between single frames of movie sequence and a reference frame. This can deliver an expression space for a given person. We examined the psychological validity of a PCA-based expression space. 

By adapting to facial images at the ends of a particular dimension of facial variation (e.g. the first principal component) we could shift the appearance of expressions away from the adapting expression without shifting the perception of faces arrayed along a second orthogonal direction. This showed adaptation within expression space and that images which were statistically orthogonal were also perceptually orthogonal. 

The idea that faces are represented as relative to a mean face, a standard model in face perception, raises questions about over which set of faces is the mean constructed. We built PCA spaces across individuals rather than across expressions to investigate &amp;quot;family resemblance&amp;quot; between different classes. 
We used a novel technique of mapping a vector representing a deviation of a male face from the male mean into a female face space. This resulted in a female &amp;quot;sibling&amp;quot;. We showed that the &amp;quot;sibling pairs&amp;quot; looked more alike than a random pairing indicating &amp;quot;family resemblance&amp;quot; may be encoded by similar vectors referenced to the average of classes of faces. 

The same technology can be used to visualise our prejudices. We found that average Conservative and Labour MP's faces were indistinguishable. However average faces rated as strongly Labour or strongly Conservative did look distinctively different and were correctly matched to their stereotypical category by participants in a follow-up experiment. Our ability to imitate facial expressions is puzzling as we rarely see our own faces. We tested the ability of participants to identify a facial action projected onto a computer graphic avatar as being generated by themselves, a friend or another person. If based on experience they should find it easiest to recognise a friend. However we found participants could recognise themselves and their friends from the motion alone when upright but only themselves for upside down faces. Disrupting the timing of the motion showed self-recognition was based on rhythmic cues we have about our own facial motion. 

We also undertook extensive public engagement in the project, with media coverage for our results and presentation at the Royal Society Summer exhibition in 2011.</gtr:description><gtr:exploitationPathways>The technology allows mapping of facial movement between photorealistic faces allowing the generation of a photorealistic performance-driven avatar. It can also be used to facial motion analysis. The work is currently being extended by one EPSRC student (CoMPLEX DTC) one BBSRC DTC student and one student funded by UCL and NTT (Japan). We have been invited to advertise the products of this research on the UK's Security and Resilience Industry Suppliers Community (RISC) website. http://www.riscuk.org/academia/academic-marketplace/facial-motion-analysis/</gtr:exploitationPathways><gtr:id>1915103C-AA00-4DB1-9B91-EFFA0E3E7F6F</gtr:id><gtr:outcomeId>5458f72b587d81.12504598</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D1CA28B8-61E5-4DDA-820F-762D48CDBF35</gtr:id><gtr:title>Exploring expression space: adaptation to orthogonal and anti-expressions.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a657a8acd50b386edc6cb5523249e9d"><gtr:id>5a657a8acd50b386edc6cb5523249e9d</gtr:id><gtr:otherNames>Cook R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d0770774376e45</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A716A52C-0DC7-4FB7-9DBB-6DBCB6685AEF</gtr:id><gtr:title>Facial self-imitation: objective measurement reveals no improvement without visual feedback.</gtr:title><gtr:parentPublicationTitle>Psychological science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a657a8acd50b386edc6cb5523249e9d"><gtr:id>5a657a8acd50b386edc6cb5523249e9d</gtr:id><gtr:otherNames>Cook R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0956-7976</gtr:issn><gtr:outcomeId>doi_53d079079e0c696d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FD878AD2-284E-46E5-B5CA-C4582F4DF9BF</gtr:id><gtr:title>Relative faces: encoding of family resemblance relative to gender means in face space.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9d9d2237f7ed12bf56017af8f6a106a5"><gtr:id>9d9d2237f7ed12bf56017af8f6a106a5</gtr:id><gtr:otherNames>Griffin HJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d077077415ed93</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A870D389-9048-4083-9A98-03FC72D59CD0</gtr:id><gtr:title>Identifying regions that carry the best information about global facial configurations.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3e833ce2cf831276390d731be024788"><gtr:id>d3e833ce2cf831276390d731be024788</gtr:id><gtr:otherNames>Berisha F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d077077343d0c5</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>01A8D8E1-D743-4D66-AF04-AA16C14E2540</gtr:id><gtr:title>Recognition from facial motion in dynamic average avatars</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/64c19b1cd39c84cab1fefbf1e1803821"><gtr:id>64c19b1cd39c84cab1fefbf1e1803821</gtr:id><gtr:otherNames>Nagle F.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>56e0a0d365c189.81123325</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>56628B7D-A76A-4BF6-BCD5-889BF84DF7CE</gtr:id><gtr:title>Perceiving dynamic faces</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8aafb8590e656447f6058c577f606dca"><gtr:id>8aafb8590e656447f6058c577f606dca</gtr:id><gtr:otherNames>Johnston A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>56e0a0d385bb79.65109168</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D959984C-2746-42AC-8E25-4ADC07A4A7DF</gtr:id><gtr:title>How Different is Different? Criterion and Sensitivity in Face-Space.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c652db988f2bf49322d8a01d8ec351a8"><gtr:id>c652db988f2bf49322d8a01d8ec351a8</gtr:id><gtr:otherNames>Hill H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>56e0a0d4486ff5.06001756</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A668DBB6-FDD3-4926-AD86-FDC43E1F4C53</gtr:id><gtr:title>Self-recognition of avatar motion: how do I know it's me?</gtr:title><gtr:parentPublicationTitle>Proceedings. Biological sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a657a8acd50b386edc6cb5523249e9d"><gtr:id>5a657a8acd50b386edc6cb5523249e9d</gtr:id><gtr:otherNames>Cook R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0962-8452</gtr:issn><gtr:outcomeId>doi_53d04a04a4facf96</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>095083BF-6857-4A73-9420-DBEB58722C20</gtr:id><gtr:title>Recognising faces: effects of lighting direction, inversion, and brightness reversal.</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8aafb8590e656447f6058c577f606dca"><gtr:id>8aafb8590e656447f6058c577f606dca</gtr:id><gtr:otherNames>Johnston A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>56e0a0d26a0cd2.49629398</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E02B552-672F-49CF-8850-5C359DE7081C</gtr:id><gtr:title>Judging political affiliation from faces of UK MPs.</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1675327c5a6e74d4dfdeec2da99572a9"><gtr:id>1675327c5a6e74d4dfdeec2da99572a9</gtr:id><gtr:otherNames>Roberts T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn><gtr:outcomeId>doi_53d0390395fe9fda</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>83079E16-6790-4A2A-A265-E6BF9B32CF21</gtr:id><gtr:title>Illusory feature slowing: evidence for perceptual models of global facial change.</gtr:title><gtr:parentPublicationTitle>Psychological science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5a657a8acd50b386edc6cb5523249e9d"><gtr:id>5a657a8acd50b386edc6cb5523249e9d</gtr:id><gtr:otherNames>Cook R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0956-7976</gtr:issn><gtr:outcomeId>56e0a0d140e9a9.06196130</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/F037503/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>