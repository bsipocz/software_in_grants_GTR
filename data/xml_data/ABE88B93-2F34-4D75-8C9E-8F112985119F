<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:department>Sch of Engineering and Informatics</gtr:department><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name><gtr:address><gtr:line1>Polaris House</gtr:line1><gtr:line2>North Star Avenue</gtr:line2><gtr:line4>Swindon</gtr:line4><gtr:line5>Wiltshire</gtr:line5><gtr:postCode>SN2 1ET</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/ED3F72AB-DBB3-4540-9EA9-DEF85D5A7AA4"><gtr:id>ED3F72AB-DBB3-4540-9EA9-DEF85D5A7AA4</gtr:id><gtr:firstName>Andrew</gtr:firstName><gtr:surname>Philippides</gtr:surname><gtr:orcidId>0000-0001-5503-0467</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/E14B0B9C-C871-466F-AA68-6A8E92E1F2FE"><gtr:id>E14B0B9C-C871-466F-AA68-6A8E92E1F2FE</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:surname>Graham</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8B26AD4B-F84A-4BCA-8780-BFFDAF9358BF"><gtr:id>8B26AD4B-F84A-4BCA-8780-BFFDAF9358BF</gtr:id><gtr:firstName>Phil</gtr:firstName><gtr:surname>Husbands</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/559A7B49-356D-419E-934B-3305197CAE4D"><gtr:id>559A7B49-356D-419E-934B-3305197CAE4D</gtr:id><gtr:firstName>Thomas</gtr:firstName><gtr:surname>Collett</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/755B9F46-84CA-4554-8837-E92D0880F93F"><gtr:id>755B9F46-84CA-4554-8837-E92D0880F93F</gtr:id><gtr:firstName>Bartholomew</gtr:firstName><gtr:surname>Baddeley</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FF010052%2F1"><gtr:id>ABE88B93-2F34-4D75-8C9E-8F112985119F</gtr:id><gtr:title>Developing probabilistic frameworks for the detailed analysis of insect visual navigation behaviours</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/F010052/1</gtr:grantReference><gtr:abstractText>Ants, bees and wasps are able to successfully find their way back to their nest sites following long and often convoluted foraging trips. The fact that these impressive navigational feats are achieved despite their relatively poor visual acuity and simple nervous systems makes them particularly interesting systems for study. Most current models of insect navigation have tended to ignore problems of noise. In this project we intend to construct a general probabilistic framework for the modelling and analysis of real and synthetic behaviours subject to noise. The idea stems from the probabilistic Simultaneous Localisation and Mapping (SLAM) approach to robot navigation, which assumes that: (1) all information and output is (at least to some degree) uncertain, and (2) motor commands and their sensory consequences are strongly coupled. Robust mapping performance is achieved by combining multiple sources of uncertain information. In SLAM, the navigating agent relies on having models of both its sensors and motors. The models incorporate what a sensor reading should be if the agent is a given distance from a sensed object, and how its position will change in response to a given motor command. With these two models, the agent can predict the sensory consequences of its actions, and, by combining these predictions with the actual sensor readings, obtain a better estimate of its own position and of the mapped features. By replacing the robot measurement and movement models with models of insect sensory and motor capabilities we can turn the SLAM approach into a tool for analysing real and simulated behaviours. The sole constraint on the models is that they specify a probability distribution over the outputs of the model rather than just a single point prediction. The sensory and motor models that we employ can be applied to problems at different levels. Once constructed, the sensor models and framework provide a standardised test environment for comparing different navigation algorithms. In the proposed framework, navigation strategies can be expressed as closed-loop motor models, that is, motor models that take into account sensory feedback. By allowing different navigation algorithms to be implemented, subject to the same constraints on visual acuity and accuracy of movement, more meaningful comparisons can be made. We can also investigate how the fine structure of movements affect localisation performance by comparing how uncertainty in the estimated positions of landmarks changes in response to different motor patterns. Some patterns of movement will provide more useful information than others and we can investigate how closely observed behaviours match theoretically optimal movements that maximally reduce uncertainty within the SLAM framework. Having built and tested the motor and sensory models in simulation, they will be transferred to a large gantry robot fitted with a panoramic camera to allow a more comprehensive evaluation of different control algorithms. The gantry robot set up will also enable us to investigate how insects solve the data-association problem, which is an outstanding issue in robot navigation. Data-association refers to the problem of recognising whether a visual feature has been seen before or not. Errors in data-association are the main reason for failure in current SLAM approaches. To approach this question in real insects, we will record the fine structure of their movements during acquisition and then use the gantry to emulate their visual input during the course of learning. Finally, these techniques provide a potential solution to the problem of automated video tracking for data acquisition. The movement models described above can be incorporated into a video tracking system. By having some idea of where an insect is likely to move to, it is easier to track the agent's position from frame to frame, thereby leading to more robust tracking performance.</gtr:abstractText><gtr:technicalSummary>Our ultimate goal is to provide a general mathematical framework for analysing insect visual learning and navigation in a novel way that recognises both the importance of dealing with uncertainty and the relationship between actions and their sensory consequences. Such a framework has recently been applied with great success to the related Simultaneous Localisation And Mapping (SLAM) problem in robotics. By employing a probabilistic approach to handling uncertainty, recent approaches are able to map extended environments. We intend to employ the same probabilistic tools to the problem of modelling uncertainty within the insect navigation paradigm. This will provide a genuinely novel approach to the analysis of insect behaviour. In order to implement the proposed approach we need to construct probabilistic sensory and motor models. We can utilise existing models of both the optics and motor systems of insects as well as more abstract behavioural descriptions by adapting them into probabilistic forms. While simple in concept, these models are non-trivial and their construction will constitute a significant part of the research effort. We will test the motor and sensory models in simulation and implement the approach on a large gantry robot fitted with a panoramic camera. A major benefit of the proposed framework is that hypothetical insect control schemes, visual algorithms and navigation models can be directly compared using the final certainty, reliability and accuracy of position estimates as a metric for comparison. Moreover, determining accurate motor models will allow us to build better video tracking software for automated data acquisition in behavioural experiments with ants and bees. Finally, we will investigate how insects solve the data-association problem (recognising whether a visual feature has been seen before or not), an outstanding issue in robot navigation and the main reason for failure in SLAM approaches.</gtr:technicalSummary><gtr:fund><gtr:end>2010-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2007-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>271688</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>300000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>EU ICT FET</gtr:description><gtr:end>2012-05-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>225167</gtr:fundingRef><gtr:id>70D3E7D0-DA21-40BC-B7A8-BC16F7294F31</gtr:id><gtr:outcomeId>56409cdfcc1e67.30993049</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2009-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The framework developed in this project is being used to predict which parts of the world are being learned about by insects based on a probabilistic interpretation of motion parallax cues. The second strand to this project is concerned with an investigation of route learning in ants. In contrast to previous studies which model a route as a series of waypoints, route following was re-cast as a classification problem. By classifying views as either on or off a route, one can obtain a measure of view familiarity that can be used to learn routes through visual clutter, employing remarkably crude information. The approach has proved to be of great interest to both biologists and roboticists as it provides a novel way to analyse the type of information that is available in low resolution panoramic scenes and provides a model and behaviour which tally with the experience and insights of insect ethologists. 

The work also led directly to a number of very successful outreach events. These efforts culminated in the team (primarily Graham, Philippides and Baddeley) being selected to present an exhibit at the 2010 Royal Society Summer Science Festival showcasing their interdisciplinary approach to insect navigation research. The exhibition was held at the Southbank Centre and attracted almost 50,000 visitors, with the exhibit proving extremely popular and successful. Indeed, we were asked back to the Royal Society to present our experiences and tips to the 2011 exhibitors. It also led to a number of requests for articles aimed at school students.</gtr:description><gtr:id>3422D038-8BE2-4EA3-8BB6-ED68F570618F</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5447baf67c17c4.61706706</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>A new methodology was developed in the project: a probabilistic framework for the analysis of insect visual navigation behaviours. This framework was adapted from the field of probabilistic robotics making use of approaches develped for the simultaneous localisation and mapping (SLAM) problem. This requires a mobile agent,when placed at an unknown spot in an unknown environment, to incrementally construct a consistent map of the environment at the same time as determining its location in the map. We do not suggest that insects form the sort of maps that are used in SLAM. However, the approach provides a tool to quantify the information about the environment that an insect might extract on a given journey. Byreplacing the measurement model that describes a robot's camera's optics with one that describes the optics of an insect, and similarly, replacing the robot process model with a motor model that describes the insects's movements we demonstrated how to construct a closedloop system for investigating natural active vision. For the sensor model we assume that the insect is able to observe a given feature if it falls within the field of view and there is a direct line of sight to it. We further assume that the measurement noise and therefore the visual acuity is fixed. The motor model that we employ in the methodology assumes that we do not have access to the control input. Instead it is assumed that all forces and torques that act on the insect are small and normally distributed around zero so at each time-step the velocity is assumed to remain constant while the covariance of the state estimate increases. This very simple and general motor model is sufficient to implement SLAM and allow analysis of the insect journeys. Using this probabilistic framework it is possible, for instance, to follow bee orientation flights, tracking how the structure of a flight reduces the uncertainty in the bee's estimate of its own position and of visual features within the environment. Such information is a powerful tool in probing theories about what the insect does and does not attend to during navigation and how their journeys are structured to best employ such information. Full details can be found in: Baddeley et al. (2009) Biological Cybernetics 101(3):169-182 (see publications section for full details). A suite of software was developed to allow analysis of insect navigation data with tools adapted from probabilistic robotics encapsulating the methodology outlined above. The software was developed in MATLAB and we plan to make it publicly available. The software provides a probabilistic framework making use of visual sensor and motor models of a particular insect. It provides a tool to quantify the visual information about the environment that an insect might extract on a given journey (e.g. bee on a flight). The simulation software can be run in parallel with transcribed video of insect behaviours to accurately map the analysis to the experimental data. A MATLAB skyline simulation package was developed during the project and has been made available to other reserachers.This set of MATLAB functions allows for the construction of 3D model visual worlds to study accurate simulated animal behaviour. Accurate 360 degree skyline panoramas can be efficiently rendered from the models by implicitly utilising the highly optimised in-built figure rendering capabilities of MATLAB. Tools are provided for creating and appropriately scaling outlines of objects that have been photographed. The software is currently being used by biologists to reconstruct experimental environments at a research centre in Alice Springs, Australia. The project has led to a proposed general method for building 3D models of the world. The technology will incorporate visual SLAM together with novel hardware technologies (KINECT) to automatically build accurate 3D models using a single handheld camera.</gtr:description><gtr:exploitationPathways>This work has led to new ways of thinking about how insects navigate which can (and has) inspired new algoithms for robot navigation.</gtr:exploitationPathways><gtr:id>7CFFADE9-953D-46E4-B456-39D7FE52B596</gtr:id><gtr:outcomeId>5447b86c6ff011.85431628</gtr:outcomeId><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Pharmaceuticals and Medical Biotechnology</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>F44DAA3C-40EC-4369-AA8E-553096E43613</gtr:id><gtr:title>A model of ant route navigation driven by scene familiarity.</gtr:title><gtr:parentPublicationTitle>PLoS computational biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e115e0e9ff6306ba8105d235ff782fe2"><gtr:id>e115e0e9ff6306ba8105d235ff782fe2</gtr:id><gtr:otherNames>Baddeley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1553-734X</gtr:issn><gtr:outcomeId>54380f9684aa84.65419408</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>200C5987-301A-4103-ADCE-08A120271EFF</gtr:id><gtr:title>Bumblebee calligraphy: the design and control of flight motifs in the learning and return flights of Bombus terrestris.</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4cb15900d6debffb42c0e188dd3d22fe"><gtr:id>4cb15900d6debffb42c0e188dd3d22fe</gtr:id><gtr:otherNames>Philippides A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>5675d81949c18</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F1F4D7F6-28B6-4016-B8CA-478EC414F4B3</gtr:id><gtr:title>Holistic visual encoding of ant-like routes: Navigation without waypoints</gtr:title><gtr:parentPublicationTitle>Adaptive Behavior</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2f4a310c3fa0a2226567a1a9bde8d0c3"><gtr:id>2f4a310c3fa0a2226567a1a9bde8d0c3</gtr:id><gtr:otherNames>Bart Baddeley (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>1059-7123</gtr:isbn><gtr:outcomeId>m_4285087505139899a6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5B19FA78-CD4A-4C7D-ABEA-04234C2812E8</gtr:id><gtr:title>Holistic visual encoding of ant-like routes: Navigation without waypoints</gtr:title><gtr:parentPublicationTitle>Adaptive Behavior</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e115e0e9ff6306ba8105d235ff782fe2"><gtr:id>e115e0e9ff6306ba8105d235ff782fe2</gtr:id><gtr:otherNames>Baddeley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d07a07a2f56381</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A1B4F55-3DE9-4FCD-938B-83AE5726BB1A</gtr:id><gtr:title>Coordinating compass-based and nest-based flight directions during bumblebee learning and return flights.</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/26ea0825b0a7c4a03fa9af4bececcfe5"><gtr:id>26ea0825b0a7c4a03fa9af4bececcfe5</gtr:id><gtr:otherNames>Collett TS</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>5675d81a21f9d</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DB1669B9-B95E-482D-9A87-FEF828BAD66F</gtr:id><gtr:title>How might ants use panoramic views for route navigation?</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4cb15900d6debffb42c0e188dd3d22fe"><gtr:id>4cb15900d6debffb42c0e188dd3d22fe</gtr:id><gtr:otherNames>Philippides A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>doi_53d07d07d1f4ee6a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F576B699-5108-49EC-8C12-7164320610B3</gtr:id><gtr:title>Preferred viewing directions of bumblebees (Bombus terrestris L.) when learning and approaching their nest site.</gtr:title><gtr:parentPublicationTitle>The Journal of experimental biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ab1e01a9e8932ced3813d4999f936175"><gtr:id>ab1e01a9e8932ced3813d4999f936175</gtr:id><gtr:otherNames>de Ibarra NH</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0022-0949</gtr:issn><gtr:outcomeId>doi_53d07d07d102c3ca</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>01FD0C48-C0E3-4E09-93E0-60456A18CE7F</gtr:id><gtr:title>Spike-timing dependent plasticity and the cognitive map.</gtr:title><gtr:parentPublicationTitle>Frontiers in computational neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/767588090a2d653c4e09007891dd6560"><gtr:id>767588090a2d653c4e09007891dd6560</gtr:id><gtr:otherNames>Bush D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>1662-5188</gtr:issn><gtr:outcomeId>pm_5435ad35ad3ce0a15</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5D902156-8DA4-481A-A384-8BEF68078C03</gtr:id><gtr:title>What can be learnt from analysing insect orientation flights using probabilistic SLAM?</gtr:title><gtr:parentPublicationTitle>Biological cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e115e0e9ff6306ba8105d235ff782fe2"><gtr:id>e115e0e9ff6306ba8105d235ff782fe2</gtr:id><gtr:otherNames>Baddeley B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0340-1200</gtr:issn><gtr:outcomeId>doi_53cfd6fd6b16933b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>76D1089A-3D7F-4841-8205-30E78B40E45E</gtr:id><gtr:title>Ants use the panoramic skyline as a visual cue during navigation.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d693d4fddeeaa7d92b2143eeecd6dfd7"><gtr:id>d693d4fddeeaa7d92b2143eeecd6dfd7</gtr:id><gtr:otherNames>Graham P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>doi_53cfecfec0b7d111</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>93B63EC5-A6E2-4B4F-A5F7-52397325D5F7</gtr:id><gtr:title>Using neural networks to understand the information that guides behavior: a case study in visual navigation.</gtr:title><gtr:parentPublicationTitle>Methods in molecular biology (Clifton, N.J.)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4cb15900d6debffb42c0e188dd3d22fe"><gtr:id>4cb15900d6debffb42c0e188dd3d22fe</gtr:id><gtr:otherNames>Philippides A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1064-3745</gtr:issn><gtr:outcomeId>56b8c49cc31786.75539752</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/F010052/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>