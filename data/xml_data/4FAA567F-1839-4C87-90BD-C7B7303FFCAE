<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/079B183E-0464-441E-A20A-9A403D5D910F"><gtr:id>079B183E-0464-441E-A20A-9A403D5D910F</gtr:id><gtr:firstName>Ian</gtr:firstName><gtr:surname>Reid</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/7EEAB738-BCBA-493E-94E9-60D394577D5E"><gtr:id>7EEAB738-BCBA-493E-94E9-60D394577D5E</gtr:id><gtr:firstName>David</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>Murray</gtr:surname><gtr:orcidId>0000-0001-5309-5080</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH050795%2F1"><gtr:id>4FAA567F-1839-4C87-90BD-C7B7303FFCAE</gtr:id><gtr:title>Long-term, High Order Visual Mapping</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H050795/1</gtr:grantReference><gtr:abstractText>By observing a static scene with a moving video camera, it is possibleto estimate the trajectory of the camera through the environment,simultaneously build a map of the environment simply by repeatedobservation of the motion of visual features in the videoimages. The robotics community generally refers to this problem asvision-based Simultaneous Localisation and Mapping (SLAM) and thecomputer vision community calls it Structure From Motion (SFM).It is currently possible perform real-time camera localisation andmapping (i.e. visual SLAM) in a small environment on any modernlaptop, suitable for applications such as augmented reality orproviding visual odometry to a service robot. However, currently themaps that are built to enable localisation are usually short-lived,difficult to interpret by a human, and are rarely re-used by devicesother than the ones used to build the map. These maps are also easilycorrupted by moving objects or other dynamics such as cyclic changes.A significant challenge now faced is to build maps that can adequatelyrepresent a non-static environment by taking account of movingobjects, evolve over long periods of time, and provide more usefulrepresentations of the world with high level information. Applicationswhich would benefit from the longer lasting, semantically meaningfulmaps include: personal robotics and assistive devices, beneficial inan aging society to confer greater independence to the infirm; changedetection for military and civilian surveillance such as in IED(imporvised explosive device) detection; driver assistance tools, forautomatic reasoning for vehicles in complex and clutteredenvironments. Our research therefore aims to address fundamentalissues in visual mapping in order to provide sound underpinnings forthe above commercial applications. More specifically, the focus ofthe current proposal is:(i) to build representations that can be updated to take into accountchanges in appearance and structure and be used by different visualsensors;(ii) to investigate representations and algorithms for extraction ofhigh-level semantic information to improve the mapping process,develop scene understanding, to provide a more natural interface tocognitive processing.The use of a mobile observer taking continuous measurements of theenvironment provides opportunities for learning and leveraging contextin novel ways. We aim to achieve our goals through combined use ofgeometric data (map), photometric data (the image stream) andhigh-level contextual information (in which observations areunderstood in terms of their relationship to the bigger picture ).In doing so we expect to build an environment model not as a simpleunstructured point cloud, nor as a flat collection of visual featuredescriptors, but as a semantically meaningful hierarchy. Such arepresentation would clearly be of great benefit for high-levelreasoning and human interaction. Moreover we argue that it is onlythrough such higher-level representations that the robustness requiredfor truly long-term mapping will emerge.We aim to support these theoretical and practical advances inalgorithms and representations for visual mapping, with real, robustimplementations, which run in real-time.</gtr:abstractText><gtr:potentialImpactText>Through our previous projects in Simultaneous Localisation and Mapping (and others by our colleagues, e.g. Newman in Oxford and Davison in Imperial), the UK currently enjoys a pre-eminent global position in the field of visual SLAM research. Our past research in SLAM has achieved significant impact both in academic and in industrial environments. Our SLAM research, through our know-how, algorithms and software, has had -- and has further potential for -- real economic impact within the UK, having been used as the basis for development in UK industry. In some cases the details of this are subject to non-disclosure agreements, but notably our work on SLAM with moving objects in radar has been transferred to UK industry (Servowatch Ltd) who are actively developing products for marine situational awareness. Our aim in the current project is to extend the range of applications of visual SLAM by concentrating on the mapping aspects of the problem. Some aspects of this new work are close to exploitation. However some of the more ambitious aspects of the project represent more basic research, that is nevertheless necessary in order to necessary achieve the next generation of smarter SLAM systems. Applications which will benefit from the research proposed include: personal robotics and assistive devices, beneficial in an aging society to confer greater independence to the infirm; change detection for military and civilian surveillance such as in IED (improvised explosive device) detection; driver assistance tools, for automatic reasoning for vehicles in complex and cluttered environments. Drawing on our previous experience and expectations of likely beneficiaries, we propose to maximise the impact of our research in the following ways: (i) by publishing in both the computer vision literature and the robotics literature; (ii) through release of appropriate software on the basis of free-for-academic use; (iii) through existing and future relationships with industry; (iv) establishing and cultivating new industrial relationships through attendance at relevant industrial trade fairs (eg the MoD's Defence Research conferences) (v) if apporpriate seeking formal protection through the patent system, and pursuing licensing opportujities through the University's IP company Isis Innovation. (vi) career development of key individuals (in particular, in providing the springboard to an academic career for the named researcher)</gtr:potentialImpactText><gtr:fund><gtr:end>2014-06-09</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-06-10</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>767539</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The work has contributed to current commercial funding of research work in the laboratory.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>F449A22F-89D9-4292-88D1-E114A5408EE6</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56d6e0862a9b49.48721346</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Interacting with the world requires both an understanding of the 3D geometry of a scene and of its semantic meaning. Remarkable achievements have been made in both of these directions, with systems like PTAM, DTAM and KinectFusion able to provide 3D information in real time, while learning and inference models have been used to infer semantic labels in a range of problems. This grant has contributed to the combined treatment of reconstruction and interpretation of scenes. We developed a framework to employ Decision Tree Fields and Regression Tree Fields for the 3D scene labeling task, showing that both classifiers are very competitive, outperforming the then state-of-the-art, and delivering significantly faster inference steps. The framework was used to estimate the coarse scene layout and ground plane, which are a prerequisite for high level features commonly used for 3D scene labeling.

The second key area of work was in dense monocular SLAM, SLAM where we demonstrated the combination of live dense reconstruction dense 3D tracking and reconstruction based on shape priors, allowing for full scaled 3D reconstruction with the known objects segmented from the scene. The segmentation enhances the clarity, accuracy and completeness of the maps built by the dense SLAM system, while the dense 3D data aids the segmentation process, yielding faster and more reliable convergence than when using 2D image data alone.</gtr:description><gtr:exploitationPathways>The outputs are of broad interest to the computer vision research community, and of particular interest to researchers and developers working on automatic reconstruction for navigation and augmented reality.</gtr:exploitationPathways><gtr:id>076DA819-310A-49FE-8BA6-8119579DC9C1</gtr:id><gtr:outcomeId>56d6e00b7a5082.38749970</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>http://www.robots.ox.ac.uk/ActiveVision</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>The Oxford Active Vision Library (OxVisionLib) is a loose collection of computer vision library and applications provided by the Active Vision Lab in the Department of Engineering Science, University of Oxford. These include
InfiniTAM - A multi-platform framework for real-time, large-scale depth fusion and tracking.
gSLICr - A library for much faster-than-real-time superpixel segmentation.
LibISR - A library for tracking multiple 3D objects from depth images.
PTAM - Parallel Tracking and Mapping
fastHOG \ Tech Report \ single GPU version \ multi GPU version \ git (by Ashwin Nanjappa) \ ubuntu build guide
PWP3D \ Paper \ single object and view, VS2008 32bit \ multiple objects and views, VS2010 64bit \ git (by Lu Ma at CU-Boulder)</gtr:description><gtr:id>303ACCDD-A630-48D8-B457-1062ACAC045C</gtr:id><gtr:impact>The software has been successful in bringing industrial funding and consultancies into the laboratory. Some elements are licensed when used commercially.</gtr:impact><gtr:outcomeId>56d6cc985ebc55.74567670</gtr:outcomeId><gtr:title>Oxford Active Vision Library</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://www.robots.ox.ac.uk/OxVisionLib/</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>67E32722-42D4-4C20-B16D-5A25A3838147</gtr:id><gtr:title>Latent Data Association: Bayesian Model Selection for Multi-target Tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be343f1cfd88e0cfde84f70a9955e82b"><gtr:id>be343f1cfd88e0cfde84f70a9955e82b</gtr:id><gtr:otherNames>Segal A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546395c5579246.36488149</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0CC544D6-2AD8-4050-9451-58977297D0A0</gtr:id><gtr:title>Efficient 3D Scene Labeling Using Fields of Trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c080f96287ede86b62b705f02d46680e"><gtr:id>c080f96287ede86b62b705f02d46680e</gtr:id><gtr:otherNames>Kahler O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54639629bfaf03.62482282</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6F2A309A-650E-416B-B03C-118E3B7D3E98</gtr:id><gtr:title>Efficient 3D Scene Labelling using Fields of Trees</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c90bac0ae9ceca03214af79c31b5117a"><gtr:id>c90bac0ae9ceca03214af79c31b5117a</gtr:id><gtr:otherNames>Kahler, O.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54639700c01181.10807079</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7E3FF565-F48C-46D8-BF46-7B870CC6490F</gtr:id><gtr:title>Hybrid Inference Optimization for robust pose graph estimation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/be343f1cfd88e0cfde84f70a9955e82b"><gtr:id>be343f1cfd88e0cfde84f70a9955e82b</gtr:id><gtr:otherNames>Segal A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5463974ebc82e7.31490149</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>833810BC-1760-4900-9A3E-1DE36723FE16</gtr:id><gtr:title>Simultaneous 3D tracking and reconstruction on a mobile phone</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/66d436891ba6ed5b5b6299667c48a319"><gtr:id>66d436891ba6ed5b5b6299667c48a319</gtr:id><gtr:otherNames>Prisacariu V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5463990681bdb8.43848899</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C95E5744-FEC3-4200-AC11-8F7249C0F88E</gtr:id><gtr:title>PWP3D: Real-Time Segmentation and Tracking of 3D Objects</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/66d436891ba6ed5b5b6299667c48a319"><gtr:id>66d436891ba6ed5b5b6299667c48a319</gtr:id><gtr:otherNames>Prisacariu V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5463940e08ccc5.78282290</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E96DAB25-F13F-4168-9CCF-FAA0E35D1A03</gtr:id><gtr:title>Manhattan scene understanding using monocular, stereo, and 3D features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c372ba28f064372e637d0605898178f6"><gtr:id>c372ba28f064372e637d0605898178f6</gtr:id><gtr:otherNames>Flint A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-1101-5</gtr:isbn><gtr:outcomeId>546393314d7759.21190902</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6BCFFA61-8966-4125-AAF3-825C84CD53C5</gtr:id><gtr:title>Dense Reconstruction Using 3D Object Shape Priors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cc795005f9157eadf662406778eacaed"><gtr:id>cc795005f9157eadf662406778eacaed</gtr:id><gtr:otherNames>Dame A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546395408c5b55.30050107</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>328AF444-2E27-4E04-8CA3-ECEDA7CC1AD6</gtr:id><gtr:title>Structured learning of human interactions in TV shows.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4298dccc6543ff29c6de79aba38d1206"><gtr:id>4298dccc6543ff29c6de79aba38d1206</gtr:id><gtr:otherNames>Patron-Perez A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>546394d8adc0f5.07308493</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>586F324B-498A-41E6-B562-0CD58D12E8FC</gtr:id><gtr:title>Real-Time 3D Tracking and Reconstruction on Mobile Phones.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0af101c6767c1ff1bbb6fd3667d44e75"><gtr:id>0af101c6767c1ff1bbb6fd3667d44e75</gtr:id><gtr:otherNames>Prisacariu VA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn><gtr:outcomeId>546396b6b944e1.41207680</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H050795/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>