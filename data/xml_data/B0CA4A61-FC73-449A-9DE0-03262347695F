<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/EE325D76-40A5-49DD-9DEC-B36D4B8F266F"><gtr:id>EE325D76-40A5-49DD-9DEC-B36D4B8F266F</gtr:id><gtr:name>Chuo University</gtr:name><gtr:address><gtr:line1>6-5-2-401</gtr:line1><gtr:line2>Josui-Honcho</gtr:line2><gtr:line3>Kodaira-Shi</gtr:line3><gtr:line4>Tokyo</gtr:line4><gtr:line5>187-0022</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E594FDB4-DD6F-441F-90D6-C423A2916446"><gtr:id>E594FDB4-DD6F-441F-90D6-C423A2916446</gtr:id><gtr:name>Manchester Metropolitan University</gtr:name><gtr:department>Sch of Computing, Maths and Digital Tech</gtr:department><gtr:address><gtr:line1>Cavendish North Building</gtr:line1><gtr:line4>Manchester</gtr:line4><gtr:postCode>M15 6BG</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E594FDB4-DD6F-441F-90D6-C423A2916446"><gtr:id>E594FDB4-DD6F-441F-90D6-C423A2916446</gtr:id><gtr:name>Manchester Metropolitan University</gtr:name><gtr:address><gtr:line1>Cavendish North Building</gtr:line1><gtr:line4>Manchester</gtr:line4><gtr:postCode>M15 6BG</gtr:postCode><gtr:region>North West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name><gtr:address><gtr:line1>BBSRC</gtr:line1><gtr:line2>Polaris House</gtr:line2><gtr:line3>North Star Avenue</gtr:line3><gtr:line4>Swindon</gtr:line4><gtr:postCode>SN2 1UH</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/EE325D76-40A5-49DD-9DEC-B36D4B8F266F"><gtr:id>EE325D76-40A5-49DD-9DEC-B36D4B8F266F</gtr:id><gtr:name>Chuo University</gtr:name><gtr:address><gtr:line1>6-5-2-401</gtr:line1><gtr:line2>Josui-Honcho</gtr:line2><gtr:line3>Kodaira-Shi</gtr:line3><gtr:line4>Tokyo</gtr:line4><gtr:line5>187-0022</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Japan</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/1B1F5402-434B-4D94-9E76-5034BCBEA7EE"><gtr:id>1B1F5402-434B-4D94-9E76-5034BCBEA7EE</gtr:id><gtr:firstName>Nicholas</gtr:firstName><gtr:otherNames>Paul</gtr:otherNames><gtr:surname>Costen</gtr:surname><gtr:orcidId>0000-0001-9454-8840</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD054818%2F1"><gtr:id>B0CA4A61-FC73-449A-9DE0-03262347695F</gtr:id><gtr:title>Cognitive Systems Foresight: Human and computer face recognition from video sequences</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D054818/1</gtr:grantReference><gtr:abstractText>This 3-year project is a collaboration between experts in computer vision techniques for recognising faces and experts in the psychology of how humans interpret faces. The aim is to produce a generative computational model of human facial processing, capable of both synthesizing and interpreting sequences of face images.This project will form the central part of an effort to build a computational model which will account for both familiar and unfamiliar moving face-recognition phenomena. This will draw from and contribute to the psychological knowledge in this area. It will have particular relevance to the effects of movement on recognition and its interaction with the process of facial learning. It is known that there are two separate psychological effects of movement, firstly representational enhancement'', which relates to the construction of 3D static representations of the face and is associated with relatively unfamiliar faces. The other, a dynamic signature'', is the characteristic motion pattern of an individual. This is associated with more familiar faces; previous relevant computational work has concentrated upon representational enhancement issues.The particular aim of this research will be to construct a generative computational model of human facial processing, able to cope with variations in familiarity. To achieve this, we need to (a) build a system capable of learning the static characteristics of faces from multiple frames and (b) build a description of behaviour and its variation. In addition, in humans, we need to (c) investigate the process of facial movement learning and (d) investigate the representation of dynamic signatures. To support this research, a cross-cultural database of moving faces suitable for human and computer experiments will be collected. The computational model will be used to generate data for psychological experiments investigating the effects of movement on recognition. The results of these experiments will inform development of automatic face interpretation algorithms.</gtr:abstractText><gtr:fund><gtr:end>2010-05-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>185913</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Chuo University</gtr:collaboratingOrganisation><gtr:country>Japan</gtr:country><gtr:description>Chuo University</gtr:description><gtr:id>5682E9AE-DAD4-4932-AC88-793C9112ADC3</gtr:id><gtr:outcomeId>b9b28ff4b9b29008-1</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2006-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project investigated techniques for pooling information about the identity of individuals from a range of images of faces. Working within an appearance-model framework, which provides inherently facial parameters, we were able to very significantly improve recognition rates by using dimension-reduction techniques to capture the types of variation present across sequences of images, but not the amounts of these variations (since the amount will vary between trials).</gtr:description><gtr:exploitationPathways>These results could be extended to characterization and recognition of more general body- movements, and also to capture the sequence of variations, using graphical models such as HMMs.</gtr:exploitationPathways><gtr:id>71597251-B386-4441-A36F-6490AADB2A21</gtr:id><gtr:outcomeId>5457bbe2a692f8.45189794</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>13908EA2-1DD2-46DB-95A0-C4E85CFFE950</gtr:id><gtr:title>An Evaluation of Video-to-Video Face Verification</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Information Forensics and Security</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/69f40e7b3b679711a5a7b68fef3f7eae"><gtr:id>69f40e7b3b679711a5a7b68fef3f7eae</gtr:id><gtr:otherNames>Poh N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d05e05e2319fb9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0E3D03C9-D34F-486B-A695-6AF0D5B22F74</gtr:id><gtr:title>Tracking face localization with a hierarchical progressive face model</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52aeb92d4d4b1e91e13d472a62728c99"><gtr:id>52aeb92d4d4b1e91e13d472a62728c99</gtr:id><gtr:otherNames>Hui Fang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>m_708932125813e261bc</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8F7D575C-94CE-433B-8303-EEF99234C5B7</gtr:id><gtr:title>Behavioural consistency extraction for facial verification</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52aeb92d4d4b1e91e13d472a62728c99"><gtr:id>52aeb92d4d4b1e91e13d472a62728c99</gtr:id><gtr:otherNames>Hui Fang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_989490197513e25ff0</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4108103D-EA13-451B-95AB-72AAC1D8E854</gtr:id><gtr:title>Tracking human pose with multiple activity models</gtr:title><gtr:parentPublicationTitle>Pattern Recognition</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/66bb2b6f03262d79e73cf25d42b8fe60"><gtr:id>66bb2b6f03262d79e73cf25d42b8fe60</gtr:id><gtr:otherNames>Darby J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d0040044c6dd2e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8C3D51AF-7E72-44F4-BC79-2A4368091443</gtr:id><gtr:title>From rank-N to rank-1 face recognition based on motion similarity</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/52aeb92d4d4b1e91e13d472a62728c99"><gtr:id>52aeb92d4d4b1e91e13d472a62728c99</gtr:id><gtr:otherNames>Hui Fang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>m_735480535213e26108</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>06DB457B-D6C8-4947-B84C-2FB422C8D558</gtr:id><gtr:title>Activity Classification for Interactive Game Interfaces</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Games Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/66bb2b6f03262d79e73cf25d42b8fe60"><gtr:id>66bb2b6f03262d79e73cf25d42b8fe60</gtr:id><gtr:otherNames>Darby J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>5a2fdc7223d534.77759021</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D054818/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>