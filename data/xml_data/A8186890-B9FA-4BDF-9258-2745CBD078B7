<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:department>Dept of Computing</gtr:department><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/562FAE6C-A85A-45A8-B90C-4F2F79902BAC"><gtr:id>562FAE6C-A85A-45A8-B90C-4F2F79902BAC</gtr:id><gtr:firstName>Maja</gtr:firstName><gtr:surname>Pantic</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH016988%2F1"><gtr:id>A8186890-B9FA-4BDF-9258-2745CBD078B7</gtr:id><gtr:title>Pain rehabilitation: E/Motion-based automated coaching</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H016988/1</gtr:grantReference><gtr:abstractText>Almost 1 in 7 UK citizens suffer from chronic pain, much of which is mechanical lower back pain with no treatable pathology. Pain management rehabilitative programs suffer from two shortcomings: (a) there are too few resources in the health care system to treat all patients face-to-face; (b) current approaches fail to integrate treatment of interrelated physiological and psychological factors. Combining expertise from engineering, clinical health sciences, and industry through a multidisciplinary team of investigators and advisors, this proposal seeks to address both shortcomings by (a) developing a set of methods for automatically recognising audiovisual cues associated with pain, behavioural patterns typical of pain, and affective states influencing pain and activity, and (b) integrating these methods into an interactive computer system that will provide appropriate feedback and prompts to the patient based on his/her behaviour measured during self-directed exercise and fitness-building sessions. This intelligent system will enable and motivate patients to continue their progress in extending activity inside (and in the longer term, outside) the clinical environment and thus will facilitate their reintegration into social and working life.In doing so, the project aims to make major contributions in a number of research areas. First, the project will significantly expand the state of the art in the field of emotion recognition by extending current methods in affective body gesture recognition, facial expression recognition and affective vocalisation recognition to deal with naturalistic rather than acted emotional expressions. This entails theoretical and practical contributions to important challenges such as detection and tracking of human behavioural cues in real world unconstrained environments, spatiotemporal analysis of complex dynamic stimuli such as non-linguistic vocalisations, facial expressions, and body movements, and spatiotemporal fusion of multimodal data streams in which constraints of synchronisation are relaxed. Second, the project will advance our understanding of how affect and pain-related moods such as fear, depression, and frustration in particular, interact with motor behaviour to not only modify pain expressions, but also to produce guarded movements that can exacerbate pain or worsen disability. This, in turn, will contribute to the body of work on cognitive behavioural models of pain to improve the diagnosis and management of pain behaviours. Finally, the project will contribute a novel user-centred approach to developing patients' understanding of their body movement during self-directed therapy; it will also identify what type of feedback best promotes engagement and encourages persistence, and offsets the negative effects of pain-related anxiety, frustration, and low-mood. Patients, clinicians and members of the advisory team will be periodically involved in the design of the interface and the testing of the system through design workshops. The system will eventually be made available for long term testing to the Pain Management Centre of the National Hospital of Neurology and Neurosurgery, London.</gtr:abstractText><gtr:fund><gtr:end>2014-11-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>374438</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The consortium collected a large database of multimodal recordings of human behaviour in rehabilitation scenario in which they experienced pain while performing rehabilitation exercises. The database has been properly documented, annotated in terms of pain level as judged by human experts, and released according to ethical clearance guidelines. This database has a very large potential impact as it allows academics and scientists all over the world to study the
problem of pain estimation by humans and machines based on various signals including facial expressions captured at a very high frequency and resolution.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>888ECDC8-3169-4C26-81E7-08033E9B527A</gtr:id><gtr:impactTypes><gtr:impactType>Societal</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56dd9c087d4c89.87015220</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>(a) Pain intensity, as shown in rehabilitation-related scenarios, can be automatically estimated from facial expressions with high Pearson correlation coefficient (CORR &amp;gt;= 0.5). This can be done either by firstly recognising facial actions (i.e. facial action units) underlying the expression of pain, or by estimating the intensity of facial expression of pain directly from the extent of changes in facial features such as the displacement of facial characteristic points.

(b) The best results are achieved if accurate facial point trackers are used and facial point locations and displacements are used to represent changes in the observed facial expressions.

(c) Discriminative machine learning approaches perform robustly for the target problem (i.e. pain intensity estimation) but cannot handle missing data, which is typical in real-world scenarios as occlusions and self-occlussions often occur. For this problem, it has been shown that a generative approach (i.e. newly-proposed Latent Trees) has a superior performance.</gtr:description><gtr:exploitationPathways>Some of the developed methodologies are publicly available in http://ibug.doc.ic.ac.uk/resources</gtr:exploitationPathways><gtr:id>7F8042F4-CC33-4428-A5A8-F004A003382E</gtr:id><gtr:outcomeId>56dda088654f18.04861806</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://www.uclic.ucl.ac.uk/people/n.berthouze/EPain.html</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>10E124B2-1212-4396-8435-852867C18A69</gtr:id><gtr:title>The MAHNOB Laughter database</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d696a3ffad1fbc789b4c8bde33d233ac"><gtr:id>d696a3ffad1fbc789b4c8bde33d233ac</gtr:id><gtr:otherNames>Petridis S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f95e95e625d14e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>694319CB-0A4B-4634-A893-50A8CEF0543C</gtr:id><gtr:title>Meta-Analysis of the First Facial Expression Recognition Challenge.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07ace7bdb133df3c8600c99b30b8cb7f"><gtr:id>07ace7bdb133df3c8600c99b30b8cb7f</gtr:id><gtr:otherNames>Valstar MF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1083-4419</gtr:issn><gtr:outcomeId>doi_55f95e95e6417eac</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3089CC82-4627-4109-8153-F9D880B4E19F</gtr:id><gtr:title>The first facial expression recognition and analysis challenge</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1b9a853b60513d251b361d0f3d79897"><gtr:id>d1b9a853b60513d251b361d0f3d79897</gtr:id><gtr:otherNames>Valstar M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54649a64e98823.68883824</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>395F8370-675A-49BF-9C67-4AD7C412B126</gtr:id><gtr:title>A dynamic appearance descriptor approach to facial actions temporal modeling.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8eed2e88ba85cfe5e39476ae0da4c7d3"><gtr:id>8eed2e88ba85cfe5e39476ae0da4c7d3</gtr:id><gtr:otherNames>Jiang B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>546389afc4c792.72177551</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>15FC0CF9-F991-40E0-AA0F-A44B9907BF19</gtr:id><gtr:title>Visual-only Discrimination between Native and Non-Native Speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a14cdf5d4ed685cf32877d75bc5f895"><gtr:id>6a14cdf5d4ed685cf32877d75bc5f895</gtr:id><gtr:otherNames>Georgakis C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>546497f9c4e2b4.70172654</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B1838610-1F57-42BD-9A48-172480041B95</gtr:id><gtr:title>Fully automatic recognition of the temporal phases of facial actions.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/07ace7bdb133df3c8600c99b30b8cb7f"><gtr:id>07ace7bdb133df3c8600c99b30b8cb7f</gtr:id><gtr:otherNames>Valstar MF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:issn>1083-4419</gtr:issn><gtr:outcomeId>doi_53d05f05f7c998d8</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25C7BB12-8CC6-482B-8579-74BABB95A66B</gtr:id><gtr:title>Continuous Pain Intensity Estimation from Facial Expressions</gtr:title><gtr:parentPublicationTitle>Advances in Visual Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a3f9064522f4b0daef138d5d8dd6a6e5"><gtr:id>a3f9064522f4b0daef138d5d8dd6a6e5</gtr:id><gtr:otherNames>Kaltwang S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>54638e7b809ae2.28331671</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0D8C663C-0627-4ECA-8D39-DBEB3D06A41B</gtr:id><gtr:title>Latent trees for estimating intensity of Facial Action Units</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a3f9064522f4b0daef138d5d8dd6a6e5"><gtr:id>a3f9064522f4b0daef138d5d8dd6a6e5</gtr:id><gtr:otherNames>Kaltwang S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56dd88787f1824.49478022</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C0A21361-BFC6-4570-BA8F-4CC6EB51C267</gtr:id><gtr:title>A dynamic appearance descriptor approach to facial actions temporal modeling.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on cybernetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8eed2e88ba85cfe5e39476ae0da4c7d3"><gtr:id>8eed2e88ba85cfe5e39476ae0da4c7d3</gtr:id><gtr:otherNames>Jiang B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>2168-2267</gtr:issn><gtr:outcomeId>doi_55f95e95e62e99ff</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2C5E4B25-F59B-448E-9768-6F85DB130A17</gtr:id><gtr:title>Facial Action Detection Using Block-Based Pyramid Appearance Descriptors</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8eed2e88ba85cfe5e39476ae0da4c7d3"><gtr:id>8eed2e88ba85cfe5e39476ae0da4c7d3</gtr:id><gtr:otherNames>Jiang B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464887e422165.40327289</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>87A0D849-BBE9-4E93-9455-7119AD8A0C28</gtr:id><gtr:title>The Automatic Detection of Chronic Pain-Related Expression: Requirements, Challenges and the Multimodal EmoPain Dataset</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Affective Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/aff0b9f9080aa0f1db5c2bf5ca4b438f"><gtr:id>aff0b9f9080aa0f1db5c2bf5ca4b438f</gtr:id><gtr:otherNames>Aung M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>5675edf479b5b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F486BE2B-A038-4539-9336-A587D3B497B6</gtr:id><gtr:title>Meta-Analyis of the First Facial Expression Recognition Challenge</gtr:title><gtr:parentPublicationTitle>IEEE Transactions of Systems, Man and Cybernetics -- Part B</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d1b9a853b60513d251b361d0f3d79897"><gtr:id>d1b9a853b60513d251b361d0f3d79897</gtr:id><gtr:otherNames>Valstar M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464997f00f197.53044484</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9372DB0C-CB25-40F7-B94E-6B229B9C2AD6</gtr:id><gtr:title>The MAHNOB Laughter database</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d696a3ffad1fbc789b4c8bde33d233ac"><gtr:id>d696a3ffad1fbc789b4c8bde33d233ac</gtr:id><gtr:otherNames>Petridis S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546496d9a18244.77308230</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6B54C47F-890E-4465-BD41-8FAFE55051D7</gtr:id><gtr:title>Human Behavior Sensing for Tag Relevance Assessment</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/40240b2f78a1bbaae8b9d06f34c07bd0"><gtr:id>40240b2f78a1bbaae8b9d06f34c07bd0</gtr:id><gtr:otherNames>Soleymani M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546496621486a7.80146948</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>13538177-6CB1-4319-BFE2-CC5CE8E264E3</gtr:id><gtr:title>Bimodal Log-linear Regression for Fusion of Audio and Visual Features</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ca97c3da5025fae211a933687e55edf"><gtr:id>2ca97c3da5025fae211a933687e55edf</gtr:id><gtr:otherNames>Rudovic O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>54648df4ef7273.34787409</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1F8EFAA0-7020-4D8C-85BA-C873C197AC1F</gtr:id><gtr:title>Facial landmarking for in-the-wild images with local inference based on global appearance</gtr:title><gtr:parentPublicationTitle>Image and Vision Computing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e104c917c1340b94c797cc0de9587047"><gtr:id>e104c917c1340b94c797cc0de9587047</gtr:id><gtr:otherNames>Martinez B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>doi_55f9539537ec2a31</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BA5F863B-6BA5-4050-A5C9-D0081719B833</gtr:id><gtr:title>Visual-only discrimination between native and non-native speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6a14cdf5d4ed685cf32877d75bc5f895"><gtr:id>6a14cdf5d4ed685cf32877d75bc5f895</gtr:id><gtr:otherNames>Georgakis C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>58c15458998210.25312150</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DC5357EB-4715-48CE-94E3-F495486B4B05</gtr:id><gtr:title>Decision Level Fusion of Domain Specific Regions for Facial Action Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8eed2e88ba85cfe5e39476ae0da4c7d3"><gtr:id>8eed2e88ba85cfe5e39476ae0da4c7d3</gtr:id><gtr:otherNames>Jiang B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56dda193c7ea70.50365427</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EB61DA0B-132C-43B7-8C66-5FCC5A1E82DB</gtr:id><gtr:title>Automatic Pain Intensity Estimation with Heteroscedastic Conditional Ordinal Random Fields</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2ca97c3da5025fae211a933687e55edf"><gtr:id>2ca97c3da5025fae211a933687e55edf</gtr:id><gtr:otherNames>Rudovic O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>546494e6142eb2.08078785</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C1A2453E-041C-4838-AFFD-AB256E27C4BC</gtr:id><gtr:title>Decision Level Fusion of Domain Specific Regions for Facial Action Recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8eed2e88ba85cfe5e39476ae0da4c7d3"><gtr:id>8eed2e88ba85cfe5e39476ae0da4c7d3</gtr:id><gtr:otherNames>Jiang B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>54648d02d60450.24387583</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B853BF6A-FC9F-45A1-B821-083ADB9A0D0A</gtr:id><gtr:title>Local evidence aggregation for regression-based facial point detection.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e104c917c1340b94c797cc0de9587047"><gtr:id>e104c917c1340b94c797cc0de9587047</gtr:id><gtr:otherNames>Martinez B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>54638ad5e9e2e2.00194146</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AC1EC488-887A-4A30-BDA2-CFD0508F744F</gtr:id><gtr:title>Local evidence aggregation for regression-based facial point detection.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e104c917c1340b94c797cc0de9587047"><gtr:id>e104c917c1340b94c797cc0de9587047</gtr:id><gtr:otherNames>Martinez B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn><gtr:outcomeId>doi_55f95e95e63810a1</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H016988/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>67</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>FD25826C-8B50-43A3-8871-3FF08D051906</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Biomechanics &amp; Rehabilitation</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>34</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>