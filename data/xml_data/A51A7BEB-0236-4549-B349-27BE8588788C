<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Electronic and Electrical Engineering</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/262F9B31-8190-42B7-ACBA-F6C90D52BE23"><gtr:id>262F9B31-8190-42B7-ACBA-F6C90D52BE23</gtr:id><gtr:name>iniVation</gtr:name><gtr:address><gtr:line1>Raeffelstrasse 25</gtr:line1><gtr:postCode>CH-8045</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/0EA12231-4E56-43AF-9C00-D44AB5C600FB"><gtr:id>0EA12231-4E56-43AF-9C00-D44AB5C600FB</gtr:id><gtr:name>FOCAL International Limited</gtr:name><gtr:address><gtr:line1>79 College Road</gtr:line1><gtr:city>Harrow</gtr:city><gtr:postCode>HA1 1BD</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/414A80D3-0CE1-4C0A-A742-6A27B517EB39"><gtr:id>414A80D3-0CE1-4C0A-A742-6A27B517EB39</gtr:id><gtr:name>SOUNDMOUSE Ltd</gtr:name><gtr:address><gtr:line1>26 Litchfield Street</gtr:line1><gtr:line2>Covent Garden</gtr:line2><gtr:postCode>WC2H 9TZ</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/43FCF783-154B-4EA3-807A-4448792248BC"><gtr:id>43FCF783-154B-4EA3-807A-4448792248BC</gtr:id><gtr:name>Yamaha Motor Co. Ltd.</gtr:name><gtr:address><gtr:line1>2500 Shingai</gtr:line1><gtr:line2>Iwata</gtr:line2><gtr:postCode>438-8501</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/9D0F153B-D290-4D94-9831-66D7DDCD1AB7"><gtr:id>9D0F153B-D290-4D94-9831-66D7DDCD1AB7</gtr:id><gtr:firstName>Yiannis</gtr:firstName><gtr:surname>Andreopoulos</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FR025290%2F1"><gtr:id>A51A7BEB-0236-4549-B349-27BE8588788C</gtr:id><gtr:title>Deep Learning from Crawled Spatio-Temporal Representations of Video (DECSTER)</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/R025290/1</gtr:grantReference><gtr:abstractText>Video has been one of the most pervasive forms of online media for some time. Several statistics show that video traffic will dominate IP networks within the next five years. Yet, video remains one of the least-manageable elements of the big data ecosystem. This project argues that this difficulty stems primarily from the fact that all advanced computer vision and machine learning algorithms view video as a stream of frames of picture elements. This is despite the fact that pixel-domain representations are known to be notoriously difficult to manage in machine learning systems, mainly due to: their high volume, high redundancy between successive frames, and artifacts stemming from camera calibration under varying illumination. 

We propose to abandon pixel representations and consider spatio-temporal activity information that is directly extractable from compressed video bitstreams or neuromorphic vision sensing (NVS) hardware. The first key outcome of the project will be to design deep neural networks (DNNs) that ingest such activity information in order to derive state-of-the-art classification, action recognition and retrieval results within large video datasets. This will be achieved at record-breaking speed and comparable accuracy to the best DNN designs that utilize pixel-domain video representations and/or optical flow calculations. The second key outcome will be to design and prototype a crawler-based bitstream parsing and analysis service, where some of the parsing and processing will be carried out by a bitstream crawler running on a remote repository, while the back-end processing will be carried out by high-performance servers in the cloud. This will enable for the first time the continuous parsing of large compressed video content libraries and NVS repositories with new &amp;amp; improved versions of crawlers in order to derive continuously-improved semantics or track changes and new content elements, in a manner similar to how search engine bots continuously crawl web content. These outcomes will pave the way for exabyte-scale video datasets to be newly-discovered and analysed over commodity hardware.</gtr:abstractText><gtr:potentialImpactText>Industrial stakeholders and the general public will benefit from the results of this research via the development of advanced video classification and retrieval services at scale and resource levels that are impossible to achieve with conventional pixel-based video analysis systems. Therefore, the project outcomes may enable a wide range of new and emerging consumer video and Internet-of-Things (IoT) related applications, thus helping to meet public expectations for the future of advanced visual computing systems. The role of industry and in particular our industrial partners, will be of paramount importance here, especially in view of the significance of the widespread adoption of new media processing technologies in numerous vertical sectors such as advertising, surveillance, recommendation services, etc. The dissemination of our research outputs to standardisation bodies, such as the on-going work of ISO/IEC MPEG on CDVA and ISO ISAN extensions, will facilitate this impact.

Our industrial partners are in a leading position to exploit the research outcomes within their products and services (e.g., Soundmouse for the creative industries sector and Yamaha Motor for smart vehicles) and the planned interactions with them will substantially facilitate this. Overall, as detailed in the Impact document, this encompasses three large areas: creative content production and management systems, cloud computing services for media processing, and IoT-oriented vehicle and surveillance systems.</gtr:potentialImpactText><gtr:fund><gtr:end>2021-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2018-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>496561</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">EP/R025290/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>