<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/7801F008-7C77-45E7-90E9-4345B47D138E"><gtr:id>7801F008-7C77-45E7-90E9-4345B47D138E</gtr:id><gtr:name>University of Plymouth</gtr:name><gtr:department>Sch of Computing &amp; Mathematics</gtr:department><gtr:address><gtr:line1>Drake Circus</gtr:line1><gtr:line4>Plymouth</gtr:line4><gtr:line5>Devon</gtr:line5><gtr:postCode>PL4 8AA</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/7801F008-7C77-45E7-90E9-4345B47D138E"><gtr:id>7801F008-7C77-45E7-90E9-4345B47D138E</gtr:id><gtr:name>University of Plymouth</gtr:name><gtr:address><gtr:line1>Drake Circus</gtr:line1><gtr:line4>Plymouth</gtr:line4><gtr:line5>Devon</gtr:line5><gtr:postCode>PL4 8AA</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/3E039279-1CF2-4D28-AE2A-02AAA2182A97"><gtr:id>3E039279-1CF2-4D28-AE2A-02AAA2182A97</gtr:id><gtr:firstName>Tony</gtr:firstName><gtr:surname>Belpaeme</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG008353%2F1"><gtr:id>D70F2E8B-1D46-45DC-B5A1-3382C97F1642</gtr:id><gtr:title>Linguistic and direct transmission of concepts in robot-human networks</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G008353/1</gtr:grantReference><gtr:abstractText>Human intelligence relies on concepts. Anything we talk about is associated with concepts: each word is connected to a concept in our brain, and saying that word evokes a similar concept in anyone within earshot. Concepts however do not only serve language, they also help us structure our thoughts and make plans. They are fundamental to human intelligence, so much that when recreating human-like intelligence on a robot, the robot will need concepts that are similar to those of humans or that are coordinated with human concepts. It is impossible to program concepts for a robot, not only because there are too many concepts, but also because concepts are notoriously hard to describe in a programming language. Another problem is that concepts need to be grounded physical reality: a robot needs to experience a concept through its sensors for the concept to become meaningful. Perhaps a better approach is to let a robot learn concepts just like people do. A number of concepts are learned by exploring our environment, but most of our concepts have been taught to us by our caretakers. Recently it has become clear that language plays a crucial role in concept learning, both for young children and for adults: language provides additional information which aids concept learning, for example, it delineates concepts and helps make distinctions between concepts that are otherwise hard to differentiate. In this project we will build two robots that will learn the meaning of words through interacting with people, much in the same way that young children learn conceptual knowledge from hearing adults speak to them about objects, relations and actions. It takes children almost three years to master a few hundred word and related concepts, as long as the duration of this project. However, we could speed up the process of word-concept learning by using training more than one robot, thus reducing the training time needed, and then downloading the missing knowledge from one robot to the other. Such telepathic access to concepts is impossible for humans: we need to resort to pointing out examples of concepts and speaking about them, but direct transfer should be easy to arrange for robots. However, bluntly copying information from one robot to another will most certainly upset the conceptual knowledge already present in the receiving robot. To avoid this, direct transfer of conceptual knowledge needs to proceed with care in order to not disturb already present knowledge.The project has two major aims. One is to study how a robot needs to behave in order to elicit conceptual knowledge from people. Therefore we will build a robot face, containing cameras and microphones, on a long articulated neck. The neck allows to robot to look around the room, but also allows it to scrutinise objects laid out on a table in front of the robot. The robot will be able to seek eye contact, engage in joint attention and interpret gestures related to concept learning. It will engage in activities, such as asking its human teacher to confirm a word or play a round of spot the X , to check its knowledge and, if necessary, adapt it. The second major aim of the project is designing computer algorithms that efficiently learn concepts from interaction involving real-world scene and words. Children are particularly good at this, and the reason for it is that they use a number of constraints to help their learning. We want to program these constraints into our robot learning mechanisms. Finally, we want to study the fast direct exchange of knowledge between robots, and we believe that we can reuse the aforementioned algorithms to allow robots to teach each other new concepts and words. The robots will use the internet as a medium to interact and are no longer limited by the slow real world to do show and tell teaching. Learning thousands of concepts might, instead of the years it takes children, now take only a few minutes.</gtr:abstractText><gtr:fund><gtr:end>2011-09-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2008-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>192290</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>3990</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>South West Regional Development Agency</gtr:description><gtr:end>2011-06-02</gtr:end><gtr:fundingOrg>South West of England Regional Development Agency (SWRDA)</gtr:fundingOrg><gtr:fundingRef>HH102000-122</gtr:fundingRef><gtr:id>208E8F25-1B32-4D9B-86E8-737190205C4F</gtr:id><gtr:outcomeId>5ebbf0785ebbf08c</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3990</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>South West Regional Development Agency</gtr:description><gtr:end>2011-06-02</gtr:end><gtr:fundingOrg>South West of England Regional Development Agency (SWRDA)</gtr:fundingOrg><gtr:fundingRef>HH102000-122</gtr:fundingRef><gtr:id>E4F62295-71EA-463B-BD54-FE66E48221AC</gtr:id><gtr:outcomeId>r-9472714783.1327420685cffa</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-04-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>31300</gtr:amountPounds><gtr:country>Netherlands, Kingdom of the</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nederlands Fonds vr Wetensch. Onderzoek</gtr:description><gtr:end>2014-07-02</gtr:end><gtr:fundingOrg>Netherlands Organisation for Scientific Research (NWO)</gtr:fundingOrg><gtr:fundingRef>IG-11-20</gtr:fundingRef><gtr:id>4B76B105-FDB7-4048-9DA6-616ABE8421F1</gtr:id><gtr:outcomeId>5ebbf7da5ebbf7ee</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-08-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>31300</gtr:amountPounds><gtr:country>Netherlands, Kingdom of the</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Nederlands Fonds vr Wetensch. Onderzoek</gtr:description><gtr:end>2014-07-02</gtr:end><gtr:fundingOrg>Netherlands Organisation for Scientific Research (NWO)</gtr:fundingOrg><gtr:fundingRef>IG-11-20</gtr:fundingRef><gtr:id>CE33E754-0EEF-45A4-A38B-2A05DB99F489</gtr:id><gtr:outcomeId>r-5826596287.8489830685dc48</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-08-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1209000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>European Commission (EC)</gtr:description><gtr:end>2014-08-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>FP7-ICT-248116</gtr:fundingRef><gtr:id>1162DB64-F7E3-4533-B300-9671183AF28E</gtr:id><gtr:outcomeId>r-8242173947.61943606a0cc1a</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2010-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>1209000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>European Commission (EC)</gtr:description><gtr:end>2014-08-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>FP7-ICT-248116</gtr:fundingRef><gtr:id>755E05E7-9586-41D5-91FF-8CC95C6C03C8</gtr:id><gtr:outcomeId>5ec637c25ec637d6</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2010-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The research on the one hand provided insights in social machine learning: where a robot or computer engages on a social level with a human in order to elicit better learning examples for its machine learning algorithms.

On the other hand, the retro-projected face technology we developed has been copied by several academic institutions and is now being sold by EngineeredArts, UK and also forms the basis of a start-up based in France.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>363E6DA1-FBE9-4142-8CD6-79406686832F</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>545d00eda5fab6.98705495</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The project set out to develop two strands of research in cognitive robotics. The first focused on using simulation and robotic experiments for the social acquisition of language. The second strand focused on the design, implementation and validation of novel robotic interfaces to facilitate social learning. Both strands furthered the understanding of autonomous acquisition of knowledge by artificial agents and robots, and progressed our understanding of how to develop general purpose learning methods able to exhibit social learning and human-like category learning. We also made considerable progress in the design of human-robot interfaces by designing a novel robot face. The face has potential to be used to support social learning, but is versatile enough to be deployed in a range of human-robot interaction scenarios.</gtr:description><gtr:exploitationPathways>Social robots, robot assisted therapy and commercial/marketing purposes. A commercial projection face robot based on the LightHead robot. The robot was commissioned by the University of North Carolina, Chapel Hill from EngineeredArts, Cornwall, UK. A start-up company (Syntheligence, Paris, France) has been founded to commercialise the technology. The retro-projected face technology is also commercialised by EngineeredArts, UK.</gtr:exploitationPathways><gtr:id>4CCD6360-13DD-4870-A788-235AE7DC57E5</gtr:id><gtr:outcomeId>r-9076056004.57271277608314</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:sectors><gtr:url>http://www.tech.plym.ac.uk/SoCCE/CONCEPT/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>04D0634B-8C4B-4F5D-ABAC-5811A543F406</gtr:id><gtr:title>WORD AND CATEGORY LEARNING IN A CONTINUOUS SEMANTIC DOMAIN: COMPARING CROSS-SITUATIONAL AND INTERACTIVE LEARNING</gtr:title><gtr:parentPublicationTitle>Advances in Complex Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/25c830b3a5585b47a10802e5ff248dc2"><gtr:id>25c830b3a5585b47a10802e5ff248dc2</gtr:id><gtr:otherNames>BELPAEME T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d072072f4a435c</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EA71F3F4-BCBC-4480-B2DD-3CF7AF126907</gtr:id><gtr:title>Epigenetic Robotics Architecture (ERA)</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Autonomous Mental Development</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/04b4c15db4a7a05420f8193feca6792b"><gtr:id>04b4c15db4a7a05420f8193feca6792b</gtr:id><gtr:otherNames>Morse A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>doi_53d05c05ca357d76</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8760CA45-9A0D-45A5-93C6-3477741C719B</gtr:id><gtr:title>Posture affects how robots and infants map words to objects.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a16100ccaa96aaa17bcc03f596bd100d"><gtr:id>a16100ccaa96aaa17bcc03f596bd100d</gtr:id><gtr:otherNames>Morse AF</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>56a8d603547bb6.95797995</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>AF7D1269-51CC-40B4-9754-0370A8F9DD2D</gtr:id><gtr:title>Cognitive architecture for human-robot interaction: Towards behavioural alignment</gtr:title><gtr:parentPublicationTitle>Biologically Inspired Cognitive Architectures</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d7f7703d486283d9f728fb8b1d884c5c"><gtr:id>d7f7703d486283d9f728fb8b1d884c5c</gtr:id><gtr:otherNames>Baxter P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>2212-683X</gtr:issn><gtr:outcomeId>56a8d603b75659.17186909</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>19EF751D-1016-43E1-845B-48870F867EBB</gtr:id><gtr:title>Coordination of meaning within different embodiments through linguistic interactions</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/75a229c453b371228dd4d5717d6fd51d"><gtr:id>75a229c453b371228dd4d5717d6fd51d</gtr:id><gtr:otherNames>Tony Belpaeme (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_23606818441408c744</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7C655924-C851-4A73-A7FD-5794F5D02643</gtr:id><gtr:title>Beyond the individual: new insights on language, cognition and robots</gtr:title><gtr:parentPublicationTitle>Connection Science</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6d7ce540024a50b8bd8ff7d2427850e4"><gtr:id>6d7ce540024a50b8bd8ff7d2427850e4</gtr:id><gtr:otherNames>Lopes L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d03d03d0d7e80a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D418A5D-BD42-4F6D-B87C-240EB4983BD9</gtr:id><gtr:title>Research Commentaries on Cangelosi's &amp;quot;Solutions and Open Challenges for the Symbol Grounding Problem&amp;quot;</gtr:title><gtr:parentPublicationTitle>International Journal of Signs and Semiotic Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8675e7bdf8b0fe6b8e241eb41e0c906b"><gtr:id>8675e7bdf8b0fe6b8e241eb41e0c906b</gtr:id><gtr:otherNames>Harnad S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d088088a5cd4c6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E4EEBEBF-0BB4-48B7-8DA2-1E5EE898F01C</gtr:id><gtr:title>Why Robots Should Be Social: Enhancing Machine Learning through Social Human-Robot Interaction.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/2c6c66ca152fd7964be455f703013ddd"><gtr:id>2c6c66ca152fd7964be455f703013ddd</gtr:id><gtr:otherNames>de Greeff J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>56a8d6634cf642.17195699</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G008353/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>