<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/E5A82D2C-5AD4-488A-ACFF-566345A5D6DA"><gtr:id>E5A82D2C-5AD4-488A-ACFF-566345A5D6DA</gtr:id><gtr:name>Heriot-Watt University</gtr:name><gtr:department>S of Mathematical and Computer Sciences</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Riccarton</gtr:line2><gtr:line3>Ricarton</gtr:line3><gtr:line4>Currie</gtr:line4><gtr:postCode>EH14 4AS</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/E5A82D2C-5AD4-488A-ACFF-566345A5D6DA"><gtr:id>E5A82D2C-5AD4-488A-ACFF-566345A5D6DA</gtr:id><gtr:name>Heriot-Watt University</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Riccarton</gtr:line2><gtr:line3>Ricarton</gtr:line3><gtr:line4>Currie</gtr:line4><gtr:postCode>EH14 4AS</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/268676F3-8E18-4E32-95B0-9952D98BEC0C"><gtr:id>268676F3-8E18-4E32-95B0-9952D98BEC0C</gtr:id><gtr:firstName>Oliver</gtr:firstName><gtr:surname>Lemon</gtr:surname><gtr:orcidId>0000-0001-9497-4743</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8267F66E-28E2-4583-B8AD-1485E88091DB"><gtr:id>8267F66E-28E2-4583-B8AD-1485E88091DB</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>Anthony</gtr:otherNames><gtr:surname>Crook</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FG069840%2F1"><gtr:id>B6B1A776-9FD2-4D5D-B8AB-434745F73F54</gtr:id><gtr:title>Scaling up Statistical Spoken Dialogue Systems for real user goals using automatic belief state compression</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/G069840/1</gtr:grantReference><gtr:abstractText>Spoken dialogue systems (SDS) are increasingly being deployed in avariety of commercial applications ranging from traditional CallCentre automation (e.g. travel information) to new ``troubleshooting''or customer self-service lines (e.g. help fixing broken internetconnections).SDS are notoriously fragile (especially to speech recognition errors),do not offer natural ease of use, and do not adapt to differentusers. One of the main problems for SDS is to maintain an accurateview of the user's goals in the conversation (e.g. find a good indianrestaurant nearby, or repair a broadband connection) underuncertainty, and thereby to compute the optimal next system dialogueaction (e.g. offer a restaurant, ask for clarification). Recentresearch in statistical spoken dialogue systems (SSDS) hassuccessfully addressed aspects of these problems but, we shall show,it is currently hamstrung by an impoverished representation of usergoals, which has been adopted to enable tractable learning withstandard techniques.In the field as a whole, currently only small and unrealistic dialogueproblems (usually less than 100 searchable entities) are tackled withstatistical learning methods, for reasons of computationaltractability.In addition, current user goal state approximations in SSDS make itimpossible to represent some plausible user goals, e.g. someone whowants to know about nearby cheap restaurants and high-quality onesfurther away. This renders dialogue management sub-optimal and makesit impossible to deal adequately with the following types of userutterance: ``I'm looking for french or italian food'' and ``NotItalian, unless it's expensive''. User utterances with negations anddisjunctions of various sorts are very natural, and exploit the fullpower of natural language input, but current SSDS are unable toprocess them adequately. Moreover, much work in dialogue systemevaluation shows that real user goals are generally sets of items withdifferent features, rather than a single item. People like to explorepossible trade offs between features of items.Our main proposal is therefore to: a) develop realistic large-scale SSDS with an accurate, extended representation of user goals, and b) to use new Automatic Belief Compression (ABC) techniques to plan over the large state spaces thus generated.Techniques such as Value-Directed Compression demonstrate thatcompressible structure can be found automatically in the SSDS domain(for example compressing a test problem of 433 states to 31 basisfunctions).These techniques have their roots in methods for handling the largestate spaces required for robust robot navigation in realenvironments, and may lead to breakthroughs in the development ofrobust, efficient, and natural human-computer dialogue systems, withthe potential to radically improve the state-of-the-art in dialoguemanagement.</gtr:abstractText><gtr:fund><gtr:end>2012-12-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>297342</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>1930000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>ERC Advanced Research Grant (STAC)</gtr:description><gtr:end>2017-05-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>269427</gtr:fundingRef><gtr:id>6C9945A3-03E5-49BB-BFBE-EBD0011DD503</gtr:id><gtr:outcomeId>56ded8041a1d92.20637099</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-06-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>645000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EC FP7 ICT grant: SpaceBook</gtr:description><gtr:end>2014-02-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>270019</gtr:fundingRef><gtr:id>60F639EE-11EC-47A8-A9FE-C2553F4EFA44</gtr:id><gtr:outcomeId>5ec595ec5ec59600</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>100000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Amazon Alexa Challenge</gtr:description><gtr:fundingOrg>Amazon.com</gtr:fundingOrg><gtr:id>0759B656-5D4A-4445-B843-A0DD36E726FA</gtr:id><gtr:outcomeId>58bd5329222f14.03335729</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2016-11-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>900000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>Horizon 2020 ICT : MuMMER project - Multimodal Mall Entertainment Robot</gtr:description><gtr:end>2020-02-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>688147</gtr:fundingRef><gtr:id>3F1D5979-9640-4C8A-81CB-BDD18ECD2959</gtr:id><gtr:outcomeId>56d71a7cea3b21.93825540</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2016-03-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>3209918</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>EUR</gtr:currCode><gtr:currCountryCode>Austria</gtr:currCountryCode><gtr:currLang>de_AT</gtr:currLang><gtr:description>EC FP7 ICT project: JAMES: Joint Action for Multimodal Embodied Social Systems</gtr:description><gtr:end>2014-09-02</gtr:end><gtr:fundingOrg>European Commission</gtr:fundingOrg><gtr:fundingRef>270435</gtr:fundingRef><gtr:id>0B90BEF8-3B7C-4339-AC57-05A2473F0181</gtr:id><gtr:outcomeId>5ec59a745ec59a88</gtr:outcomeId><gtr:sector>Public</gtr:sector><gtr:start>2011-02-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>This outputs of this research are useful in industrial and commercial development of novel speech and natural language interfaces such as future variants and extensions of Apple's iPhone speech interface Siri, Microsoft's Cortana, and Google's Now and Voice Search applications. Similar future application domains would be in interaction with virtual characters in areas such as education, healthcare, games, automated customer service, and human-robot interaction. Other applications are in hands-busy and eyes-busy operating situations, such as while driving and in medical contexts, where speech interfaces to information services are useful. In addition, advanced and natural speech interfaces are useful for blind, disabled, and ageing users who cannot easily use traditional interaction devices such as keyboards and screens. Finally, such advanced speech interfaces can be used to open up information services for illiterate users, for example in some developing countries.</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>A1CC547B-CA0B-4B96-A94D-2610BC287A6D</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5446688fde52b8.00879559</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project has helped to develop more robust, efficient, and natural human-computer speech interfaces. Such interfaces are becoming used more frequently in everyday life -- for example in the Apple iPhone speech interface &amp;quot;Siri&amp;quot; and Google's &amp;quot;Now&amp;quot; and Voice Search applications. In this project we experimented with new computational models and statistical machine learning methods for tackling two main problems for such interfaces: 1) allowing users of speech systems to express more complex and natural goals, and 2) scaling these systems up to handle larger spoken dialogue problems. To do this, we invented new representations of complex user goals (for example &amp;quot;I want french food, or else italian if there's one close to me&amp;quot;), and we investigated techniques for&amp;quot;Automatic Belief Compression&amp;quot; that allow such large-scale, high-dimensional computational problems to be reduced in size to a lower, more tractable dimension.



In practical terms we developed and deployed real telephone-based speech interfaces that implemented these ideas, and we tested them both in simulation and with members of the public, using crowdsourcing methods. We collected and analysed data from 2193 calls from 85 users.


Our key findings have been that methods for automatically compressing such problems can produce speech systems which are almost as effective as those where expert human designers have hand-crafted a suitable lower-dimensional problem space. We also developed new knowledge about the effectiveness of a variety of different automatic compression methods. In addition, we developed a new method for automatic belief compression, which overcomes several problems with previous approaches. 



We published a number of conference papers reporting this work, and contributed to 2 books on new statistical learning methods for the development of speech interfaces. We have recently written 3 journal papers reporting our findings.</gtr:description><gtr:exploitationPathways>This outputs of this research can be used in industrial and commercial development of novel speech and natural language interfaces such as future variants and extensions of Apple's iPhone speech interface Siri and Google's Now and Voice Search applications. Similar future application domains would be in interaction with virtual characters in areas such as education, healthcare, games, automated customer service, and human-robot interaction. Other applications are in hands-busy and eyes-busy operating situations, such as while driving and in medical contexts, where speech interfaces to information services are useful. In addition, advanced and natural speech interfaces are useful for blind, disabled, and ageing users who cannot easily use traditional interaction devices such as keyboards and screens. Finally, speech interfaces can be used to open up information services for illiterate users, for example in some developing countries. This research can be used in new interfaces and technologies for human-computer interaction - in particular for future speech interfaces and multimodal systems (these are interfaces which combine human communication channels such as speech, gesture, facial expression, body pose, gaze, graphics, natural language, and touch). The research allows more natural expression of user search goals using natural language, and develops computational methods for decision-making in such systems. The exploitation routes are therefore primarily in new interfaces and technologies for human-computer interaction, for example in human-robot interaction and in speech and multimodal interfaces, such as speech interfaces used with mobile phones, in cars, or for disabled users.</gtr:exploitationPathways><gtr:id>E62254B5-40C6-42BD-A7FC-ED46A8EA94B1</gtr:id><gtr:outcomeId>r-4327380214.19139677741c3a</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>https://sites.google.com/site/abcpomdp/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>A collection of real user spoken dialogues with our automated dialogue systems, as described in Crook et al. 2014</gtr:description><gtr:id>25936538-BD54-446F-A2D5-E3D4B847DE5B</gtr:id><gtr:impact>Use of data in EC FP7 projects such as SpaceBook and PARLANCE</gtr:impact><gtr:outcomeId>545a084d082a28.78499268</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Spoken dialogue data - ABC</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>https://sites.google.com/site/abcpomdp/home</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>A set of new statistical algorithms for spoken dialogue management -- see Crook et al. 2014</gtr:description><gtr:id>2EA1F22D-D551-45F7-94C5-8ECAA9C62CF2</gtr:id><gtr:impact>Some of the algorithms developed are used for dialogue management in current/recent projects such as EC FP7 PARLANCE and SpaceBook and JAMES</gtr:impact><gtr:outcomeId>545a0a1ff32886.18631020</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>ABC dialogue management algorithms</gtr:title><gtr:type>Computer model/algorithm</gtr:type><gtr:url>https://sites.google.com/site/abcpomdp/home</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Automated spoken dialogue system using a fully statistical end-to-end architecture (see publications).</gtr:description><gtr:id>048D541A-7CE1-4D33-A525-287BC5C54440</gtr:id><gtr:impact>Used in future projects, e.g. EPSRC-follow on and EC FP7 project such as SpaceBook and PARLANCE</gtr:impact><gtr:outcomeId>545a0c423fa434.62519546</gtr:outcomeId><gtr:title>End-to-end statistical spoken dialogue systems software and architecture</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:yearFirstProvided>2009</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>AC04F298-1568-405F-ACB5-B4B84285E3D4</gtr:id><gtr:title>Parallel Computing and Practical Constraints when applying the Standard POMDP Belief Update Formalism to Spoken Dialogue Management</gtr:title><gtr:parentPublicationTitle> International Workshop Series on Spoken Dialogue Systems Technology (IWSDS)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05db8eee1bb06ec902f4584efcd2e1b0"><gtr:id>05db8eee1bb06ec902f4584efcd2e1b0</gtr:id><gtr:otherNames> Paul Crook (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_30851724971400af50</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>029A15F6-8DB3-429D-9221-19589289DFFB</gtr:id><gtr:title>On the Linear Belief Compression of POMDPs: A re-examination of current methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4c372a1ffdc8102c3ffdb02d28b69bec"><gtr:id>4c372a1ffdc8102c3ffdb02d28b69bec</gtr:id><gtr:otherNames>Zhuoran Wang</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d714517da180.65413678</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F219795D-287C-4784-9206-368CA2332B9F</gtr:id><gtr:title>Learning and Evaluation of Dialogue Strategies for New Applications: Empirical Methods for Optimization from Small Data Sets</gtr:title><gtr:parentPublicationTitle>Computational Linguistics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6ed8915c4f7fbd63030ebe07f5e97060"><gtr:id>6ed8915c4f7fbd63030ebe07f5e97060</gtr:id><gtr:otherNames>Rieser V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d076076d0c9334</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>17D5A4B0-6646-4283-A2AD-5D659FDF899F</gtr:id><gtr:title>Natural Language Generation as Incremental Planning Under Uncertainty: Adaptive Information Presentation for Statistical Dialogue Systems</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6ed8915c4f7fbd63030ebe07f5e97060"><gtr:id>6ed8915c4f7fbd63030ebe07f5e97060</gtr:id><gtr:otherNames>Rieser V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464d05a25fe98.02020286</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>7862FB2E-15B4-4952-B5B1-AC1A40245254</gtr:id><gtr:title>Real user evaluation of a POMDP spoken dialogue system using automatic belief compression</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/15b986cff6e389551bec4b50fa375da7"><gtr:id>15b986cff6e389551bec4b50fa375da7</gtr:id><gtr:otherNames>Crook P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f952952aba6fac</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4CC53A33-C64F-4CF6-882B-A15DDE4B2E01</gtr:id><gtr:title>Representing Uncertainty about Complex User Goals in Statistical Dialogue Systems</gtr:title><gtr:parentPublicationTitle>SIGDIAL</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05db8eee1bb06ec902f4584efcd2e1b0"><gtr:id>05db8eee1bb06ec902f4584efcd2e1b0</gtr:id><gtr:otherNames> Paul Crook (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_16301073281400b09a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4B720804-FA08-40F2-9B9D-53DA434E38CE</gtr:id><gtr:title>Machine learning for adaptivity in spoken dialogue systems</gtr:title><gtr:parentPublicationTitle>ACM Transactions in Speech and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e7d97a15016c9bed29e7058769098809"><gtr:id>e7d97a15016c9bed29e7058769098809</gtr:id><gtr:otherNames>O Lemon</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_53d074074e090a43</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A937EF7-86BD-48D7-B4B5-1F6F22C66F16</gtr:id><gtr:title>Adaptive Generation in Dialogue Systems Using Dynamic User Modeling</gtr:title><gtr:parentPublicationTitle>Computational Linguistics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5386977c99e4220ee0e0ef8132a7effc"><gtr:id>5386977c99e4220ee0e0ef8132a7effc</gtr:id><gtr:otherNames>Janarthanam S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f95d95df3abbd6</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B5039D24-87B1-4917-825B-CC49539D8C51</gtr:id><gtr:title>Data-Driven Methods for Adaptive Spoken Dialogue Systems: Computational Learning for Conversational Interfaces</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e7d97a15016c9bed29e7058769098809"><gtr:id>e7d97a15016c9bed29e7058769098809</gtr:id><gtr:otherNames>O Lemon</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>146144802</gtr:isbn><gtr:outcomeId>i_99366901973c067cfa</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4B3F97A5-372B-40A8-A910-C6025E86C5B5</gtr:id><gtr:title>Lossless Value Directed Compression of Complex User Goal States for Statistical Spoken Dialogue Systems</gtr:title><gtr:parentPublicationTitle>Interspeech</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05db8eee1bb06ec902f4584efcd2e1b0"><gtr:id>05db8eee1bb06ec902f4584efcd2e1b0</gtr:id><gtr:otherNames> Paul Crook (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_12652517031400ae24</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0529AA7B-FBD0-414A-A436-5914B23A75C4</gtr:id><gtr:title>Learning what to say and how to say it: Joint optimisation of spoken dialogue management and natural language generation</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0d9ae1a40c18c69d3c9eac73b6e925f6"><gtr:id>0d9ae1a40c18c69d3c9eac73b6e925f6</gtr:id><gtr:otherNames>Lemon O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5464cdb4ea75f6.31304076</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>964BA032-FE4D-46CD-BE3C-F3977F61A792</gtr:id><gtr:title>Reinforcement Learning for Adaptive Dialogue Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6ed8915c4f7fbd63030ebe07f5e97060"><gtr:id>6ed8915c4f7fbd63030ebe07f5e97060</gtr:id><gtr:otherNames>Rieser V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>54579f9253cd76.29024192</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CEBA7547-DDB7-4A59-BC3D-E339693AB540</gtr:id><gtr:title>Reinforcement Learning for Adaptive Dialogue Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/239fb76a36412c94319cb28d27ff9e0d"><gtr:id>239fb76a36412c94319cb28d27ff9e0d</gtr:id><gtr:otherNames>V Rieser</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-3-642-24941-9</gtr:isbn><gtr:outcomeId>i_37518401133c0a1144</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>D2A5A494-51CC-438C-8A01-27812AD9337A</gtr:id><gtr:title>A Statistical Spoken Dialogue System using Complex User Goals and Lossless Value Directed Compression</gtr:title><gtr:parentPublicationTitle>EACL</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/05db8eee1bb06ec902f4584efcd2e1b0"><gtr:id>05db8eee1bb06ec902f4584efcd2e1b0</gtr:id><gtr:otherNames> Paul Crook (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>m_77470288781400ad5c</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/G069840/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>15BC6F17-6453-42B4-836A-01286E6D8068</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Comput./Corpus Linguistics</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>