<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/06706DF4-EA2A-4E30-9F6B-67019B7A478D"><gtr:id>06706DF4-EA2A-4E30-9F6B-67019B7A478D</gtr:id><gtr:firstName>Joachim</gtr:firstName><gtr:surname>Gross</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/F7075D7F-1E12-4B81-A7F3-2A9DF28D7097"><gtr:id>F7075D7F-1E12-4B81-A7F3-2A9DF28D7097</gtr:id><gtr:firstName>Pascal</gtr:firstName><gtr:surname>Belin</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=BB%2FI022287%2F1"><gtr:id>E89B1AA4-0B16-47E1-B040-20C44B48C61D</gtr:id><gtr:title>Audiovisual integration of identity information from the face and voice: behavioural fMRI and MEG studies.</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/I022287/1</gtr:grantReference><gtr:abstractText>Often when we hear a voice we create an image of the speaker which turns out to be quite wrong when we see the face. This is because our brain combines information from the face and voice of persons to make better estimates of a person's characteristics and allow better, more efficient social interactions. But how the brain combines information on a person's identity from her face and voice is poorly understood. In this project we combine a range of state-of-the-art techniques (face and voice morphing, magnetic resonance imaging, magnetoencephalography) to investigate the brain mechanisms of audiovisual integration of identity information. Participants will be played video clips showing a person saying a simple syllable in which the face and voice contain similar or different identity information (e.g., Bob's voice with Sam's face), and be asked to categorise the person's identity (e.g., Bob vs. Sam). While they perform this task, participants' brain activity will be measured with very high, millisecond and millimetre precision. The results will allow us to understand how two sensory modalities - audition and vision - are integrated in our brain during a task important in our everyday social interactions. They will advance our understanding of how the brain recognizes persons from their face and voice, with important potential outcome for hearing impaired persons and impact on the growing technology for automated person recognition.</gtr:abstractText><gtr:technicalSummary>The cerebral mechanisms of person recognition through their face and voice are poorly understood. Much research has focused on audiovisual speech perception, but little scientific effort has been devoted to the multimodal integration of other types of paralinguistic, socially relevant information present in both voices and faces, such as identity. In this project we combine voice morphing, functional magnetic resonance imaging and magnetoencephalography to investigate the brain mechanisms of audiovisual integration of identity information. Participants will be played dynamic, synchronous video clips (a person saying a simple syllable) in which identity information contained in face and voice is independently and parametrically manipulated via 'video morphing'. While they perform different tasks on these stimuli (an implicit 1-back task, and an explicit identity categorisation task), participants' brain activity will be measured with very high, millisecond and millimetre precision, first in MEG, then in fMRI. Analyses will characterize the cortical architecture involved in integrating face and voice identity information. According to the latest developments in the field of multimodal integration, we will obtain converging evidence from several complementary analysis strategies: comparison of bimodal vs. unimodal stimuli; manipulation of stimulus informativeness (via morphing); manipulation of stimulus audiovisual congruence; comparison of implicit vs. explicit task; correlation with behavioural audiovisual gain. These complementary analyses are expected to bring important new knowledge on the cerebral bases of audiovisual integration in the non-linguistic domain, with important potential outcomes for hearing impaired persons and impact on the growing technology for automated person recognition.</gtr:technicalSummary><gtr:potentialImpactText>The research is likely to have significant impact for a wide range of academic communities in cognitive neuroscience and other domains as it touches several domains: auditory and visual perception and cognition, multimodal integration, social neuroscience, comparative cognition (cf. Academic beneficiaries). Outside of academia, the research will have potential benefit to several user groups in the longer term. Person with auditory perception deficits, such as cochlear implant patients and persons with hearing aids, increase their reliance on the visual modality in a way that could be maladaptive. The research we conduct in normal participants has the potential to be translated into clinical practice by providing mechanisms to enhance algorithms for auditory decoding in cochlear implants, or optimizing training and rehabilitation strategies. We have experience enhancing the impact of our work through links with industry. We have had funded collaborations with France -Telecom and with Cochlear, one of the leading cochlear implant manufacturers, and this research could lead to further similar collaborations in the longer term. Another potential pathway to impact in this research is to develop links with the growing industry of 'social computing'. Our results have the potential to be of interest for designers of software for automated person recognition from multiple sources (audio, video). Automated, artificial recognition of face alone, or voice alone, have been achieved to various degrees but by very different communities with quite different technologies, when computation problems posed by recognition in vision and audition are very similar in nature. Information on the solutions to this problem found by our brain over millions of years of evolution could potentially give important clues to the design of more parsimonious, and especially more robust to degradation, recognition systems. Links will the industry in this domain will be specifically sought for with assistance form Research and Enterprise. If during the course of this work exploitable intellectual property is created, colleagues in the University's Research and Enterprise Department will assess it, and where applicable will protect it and develop an exploitation plan such that any potentially valuable results obtained in the course of the research are exploited in order to provide a suitable return to the University and the researchers, and in a manner whereby such exploitation provides maximum benefit for the UK economy. Another important user group potentially affected by the research is the wider public. There is an enormous public and media interest in face and voice, social interactions. Therefore we have had in general an excellent response to our activities to engage the public in our work. The laboratory has engaged with the public at several occasions such as at the Glasgow Science Center during Brain Awareness Week. In addition to publishing in academic journals and presenting at Scientific Conferences, we engage with the press to improve the impact of our findings. The PI has received specific training with the media thanks to the BBSRC-organized Media Training Course, which has been instrumental in enhancing the profile of the laboratory's research (several journal and radio appearances in the past year, including the Sunday Telegraph and BBC4). The research will have an important impact on the career of the research assistant who will be performing the work, as well as on that of Ms Rebecca Watson, a BBSRC-funded PhD student. Ms Watson is performing her thesis work on the topic of audiovisual integration, and she has provided some of the pilot data included in the case for support. Both researchers will benefit from the research that will contribute to further extending their skills in two domains - vision and audition- using two methodologies - fMRI and MEG. Both will be involved in user engagement activities during the course of the research.</gtr:potentialImpactText><gtr:fund><gtr:end>2014-08-09</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2012-04-27</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>247404</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>A80E3AE4-F75C-4E0E-8CA7-7F80EFE9A410</gtr:id><gtr:title>Gender differences in the temporal voice areas.</gtr:title><gtr:parentPublicationTitle>Frontiers in neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/372483e62f705d246d4aafee3c69806f"><gtr:id>372483e62f705d246d4aafee3c69806f</gtr:id><gtr:otherNames>Ahrens MM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1662-453X</gtr:issn><gtr:outcomeId>doi_55f93c93c11f0689</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1A8E73E5-19BF-43A4-8FAE-EDF391012664</gtr:id><gtr:title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices.</gtr:title><gtr:parentPublicationTitle>NeuroImage</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/824e1403e61b9b9bd56cf5f36e17589f"><gtr:id>824e1403e61b9b9bd56cf5f36e17589f</gtr:id><gtr:otherNames>Pernet CR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1053-8119</gtr:issn><gtr:outcomeId>5675e7757808e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5E7AEDDA-D68D-46BF-9BAD-5B58E2C2ED84</gtr:id><gtr:title>Norm-based coding of voice identity in human auditory cortex.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/61c63c6387b5f46a9dbc77b6a8956757"><gtr:id>61c63c6387b5f46a9dbc77b6a8956757</gtr:id><gtr:otherNames>Latinus M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn><gtr:outcomeId>doi_55f93c93c114f847</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>089D86A8-1738-40A3-B3E7-5E103E7D3C9C</gtr:id><gtr:title>Crossmodal interactions during non-linguistic auditory processing in cochlear-implanted deaf patients.</gtr:title><gtr:parentPublicationTitle>Cortex; a journal devoted to the study of the nervous system and behavior</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/d3cf089690afa25440697499e3a567a2"><gtr:id>d3cf089690afa25440697499e3a567a2</gtr:id><gtr:otherNames>Barone P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0010-9452</gtr:issn><gtr:outcomeId>585d34f09b9655.95937561</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">BB/I022287/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects/><gtr:researchTopics><gtr:researchTopic><gtr:id>D05BC2E0-0345-4A3F-8C3F-775BC42A0819</gtr:id><gtr:text>Unclassified</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>