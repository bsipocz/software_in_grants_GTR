<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:department>Centre for Speech Technology Research</gtr:department><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/2DB7ED73-8E89-457A-A395-FAC12F929C1A"><gtr:id>2DB7ED73-8E89-457A-A395-FAC12F929C1A</gtr:id><gtr:name>University of Edinburgh</gtr:name><gtr:address><gtr:line1>Old College</gtr:line1><gtr:line2>South Bridge</gtr:line2><gtr:line3>Mayfield Road</gtr:line3><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH8 9YL</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/67DCA04C-40B7-45BC-B5BD-E5ED376AA1AE"><gtr:id>67DCA04C-40B7-45BC-B5BD-E5ED376AA1AE</gtr:id><gtr:firstName>Junichi</gtr:firstName><gtr:surname>Yamagishi</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FJ002526%2F1"><gtr:id>FC8CF6C0-25FF-4D7C-B9FF-158FAADE1DC7</gtr:id><gtr:title>Deep architectures for statistical speech synthesis</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/J002526/1</gtr:grantReference><gtr:abstractText>Speech synthesis is the conversion of written text into speech output. Applications range from telephone dialogue systems to computer games and clinical applications. Current speech synthesis systems have a very limited range of difference voices available. This is because it is complex and expensive to create them.

Unfortunately, that is a big problem for many interesting applications, including one we are focusing on in this proposal: assistive communication aids for people with vocal problems due to Motor Neurone Disease and other conditions. At the moment, these people are forced to use devices with inappropriate voices, very often in the wrong accent and sometimes even of the wrong sex! This is a disincentive for them to communicate, even with their own family, since they do not &amp;quot;own&amp;quot; the voice and it does not reflect their identity. The voice is an integral part of identity, and we are creating the technology to allow people to communicate in their own voice, when their natural speech has become hard to understand or they can no longer speak at all.

The technology we will develop has a lot of other applications too: it will enable a speech synthesiser to adjust not only the speaker identity but many other properties too. For example, adjusting speaking effort will simulate what human talkers do in noisy conditions to make their speech more intelligible. Our starting point is a technique we have pioneered, called speaker adaptation.

Speaker adaptation has proven to be highly successful in enabling the flexible transformation of the characteristics of a text-to-speech synthesis system, based on a small amount of recorded speech. It can be used for changing the characteristics of the speech to a different speaker or speaking style. However, current methods do not use any deep knowledge about speech and does not generalise across similar situations. This is considerably less natural and flexible than human speech production, in which speech is controlled by human talkers based simply on prior experience. For instance, we effortlessly adapt our speech in noisy environments, compared with quiet environments, in order to increase intelligibility. The current adaptation techniques that we have pioneered are completely automatic, but they do not enable this prior knowledge to be incorporated in a straightforward way.

In some preliminary work, we have developed a model which includes information about the movement of the speech articulators: the tongue, lips and so on. Then, using our knowledge of how humans alter their speech production in the presence of noise (hyper- &amp;amp; hypo-articulation), we have demonstrated that it is possible to improve the intelligibility of synthetic speech in noise.

The current proposal is to extend and generalise this preliminary work, in order to integrate many other types of knowledge about human speech into this model. We will develop a new model which allows us to include more information about how speech is produced, as well as information about how it is perceived and how external factors, such as background noise, affect speech.

One important application of this technology is to create personalised speech synthesis for people with disordered speech (caused by Motor Neurone Disease, for example). Current technology for creating voices does not work for these people, because their speech is usually already disordered. Our technique can actually correct this, and produce speech which sounds like the person, but is more intelligible than their current natural speech. We have already produced a proof-of-concept system demonstrating that this works. The current proposal will make the technology available and affordable to a wide range of people.</gtr:abstractText><gtr:potentialImpactText>Societal Impact

Our research into personalised speech technology for assistive applications will significantly improve the quality of life of people with communicative disorders, enabling them to play a full part in society. The voice is such an integral part of identity that when damage occurs sufferers may withdraw from social interaction and interaction with patent's family. Ironically, current voice communication aids compound this effect due to their small and inappropriate range of voices, sometimes of the wrong gender, almost always of the wrong accent (US-accented voices are the default, even in the UK market). In contrast, our new technology provides a voice that sounds like the user. This is something long requested by the users of AAC devices. For them, speech synthesis is not just an optional extra function to read out text, but a function for social communication with identity.

We will conduct voice reconstruction trials at both the Euan McDonald Centre for Motor Neurone Disease research and the Anne Rowling Regenerative Neurology Clinic under the approval of NHS Lothian, aiming for 50 new patients annually. In addition, our creation of 'voice banking' service will raise awareness amongst the public of more general issues surrounding vocal health, which is important in itself but can also be an early indicator of other problems. The MND association of Scotland has promised to help us raise awareness of the service and its benefits nationwide.


Economic impact 

I am a member of the teams developing the two main free open-source research software packages for speech synthesis: HTS and Festival. Festival is pre-installed in most major Linux distributions by default. HTS has been used in many academic institutes and there are products based on HTS on the market around the world. These two toolkits are very influential and provide an immediate pathway to impact.

The outcomes of the proposed fellowship will be released under open source licenses in the form of software and data. The proposed deep model will add new capabilities to the HTS toolkit: controllability of synthetic speech. Control is one of the fundamental challenges facing speech synthesis and if this problem can be solved, many new applications become possible, leading to new commercial opportunities and economic impact

The proposed deep multi-layer models will introduce a new method of direct and detailed control, based on specification of articulatory, formant, loudness, or glottal features. This brings control over the overall speaking style and of local properties. Commercial applications that will benefit from this include speech output in noisy environments (e.g., in-car navigation), computer games and other applications requiring more variety and expressivity in their speech output


Academic impact

Our new model combines the advantages of controllability available in conventional articulatory synthesisers such as ``VocalTract Lab'' and formant synthesisers such as the Klatt model (the basis of DECTalk, as used by Prof. Stephen Hawking), with the advantages automation and quality available in modern speech synthesis. This will be very useful in other fields such as speech perception and phonetics research, where the the Klatt model (which produces very poor quality speech) is currently the main tool. Another important layer is the auditory layer and we expect that the links we will make between speech audition and production will provide novel capabilities for speech synthesisers.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-10-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2011-11-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>741163</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Invited talk at the XHUMED event</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>19E9F406-AFB4-457F-A0BC-7BDB1D8D5BED</gtr:id><gtr:impact>Christophe Veaux gave a talk on computer-based voice reconstruction techniques for MND patient at the scientific event called xHumed | Dead Good Thinking in Birmingham.</gtr:impact><gtr:outcomeId>56a1d423350ec6.79253674</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://xhumed.co.uk</gtr:url><gtr:year>2013</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Speech synthesis is the conversion of written text into speech output. Applications range from telephone dialogue systems to computer games and clinical applications. Current speech synthesis systems have a very limited range of difference voices available. This is because it is complex and expensive to create them.

The current proposal is to extend and generalise the state-of-the-art speech synthesis in order to integrate many other types of knowledge about human speech into this model. We have developed a series of new statistical models which allow us to include more information about how speech is produced, as well as information about how it is perceived and how external factors, such as background noise, affect speech.

One important application of this technology is to create personalised speech synthesis for people with disordered speech (caused by Motor Neurone Disease, for example). Our technique has been proved to actually correct this, and to produce speech which sounds like the person, but is more intelligible than their current natural speech through clinical trials that we have carried out. 

In the following we give you a few examples of the achievements that we have proposed to integrate many other types of knowledge about human speech into the models. 

First we have proposed and published a technique to consider articulation information in speech synthesis better than before in the prestigious IEEE journal:

Zhenhua Ling, Korin Richmond, Junichi Yamagishi, &amp;quot;Articulatory control of HMM-based parametric speech synthesis using feature-space-switched multiple regression&amp;quot; IEEE Audio, Speech, &amp;amp; Language Processing, volume 21, issue 1, pp. 207-219, January 2013

We have also published an IEEE journal paper that mentioned a novel framework that uses glottis information of speech production for speech synthesis better than before:

Jo&amp;atilde;o P. Cabral, Korin Richmond, Junichi Yamagishi, and Steve Renals, &amp;quot;Glottal Spectral Separation for Speech Synthesis,&amp;quot; IEEE Journal of Selected Topics in Signal Processing, vol.8, no.2, pp.195,208, April 2014

We have also proposed a novel way to adaptively use noise environmental information for changing speech synthesis outputs so that we can increase speech intelligibility automatically: 

Cassia Valentini-Botinhao, Junichi Yamagishi, Simon King, Ranniery Maia &amp;quot;Intelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the Glimpse Proportion&amp;quot; Computer &amp;amp; Speech Language, Volume 28, Issue 2, March 2014, Pages 665-6862013 

For clinical application of speech synthesis, we have built infrastructures for clinicians to automatically construct personalized voices of MND patients from speech recordings and also a new communication app to use the voices on iPad/iPhone devices. We have delivered the personalised voices to about 100 MND patients as personalized communication devices and most of them gave us very positive feedback about their QOL. This clearly shows a strong evidence of the social impact of the achievements of the fundamental research that we have carried out. In fact, this research outcome has been broadcasted by TV, radio and newspaper many times. The result will be published as a chapter of the following book. 

Christophe Veaux, Junichi Yamagishi, Simon King, Shuna Colville, Philippa Rewaj, Siddharthan Chandran, Gergely Bakos &amp;quot;Speech Synthesis Technologies for Individuals with Vocal Disabilities: Voice banking and voice reconstruction&amp;quot; in Evaluating the Role of Speech Technology in Medical Case Management, Hemant Patil and Manisha Kulshreshtha (Eds) De Gruyter Studium</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>61AEB599-35A1-4248-A692-39A34A03DEE6</gtr:id><gtr:impactTypes><gtr:impactType>Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5464aa2958a9d6.51832433</gtr:outcomeId><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>High-quality speech synthesis software based on speech technologies developed during my fellowship.</gtr:description><gtr:grantRef>EP/J002526/1</gtr:grantRef><gtr:id>0AB17E74-619B-429A-946E-491D1F36D938</gtr:id><gtr:impact>I have formally licensed the high-quality speech synthesizer to two companies for a commercial basis.</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:outcomeId>56a1b7e529fb14.92564874</gtr:outcomeId><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>High-quality speech synthesizer, HTS voice</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>As planned in the original proposal, we have been exploring the usefulness of a range of possible vocal tract layers, glottal layers and external information layers such as noises in the new statistical speech-production-oriented speech synthesis framework. 

These achievements have changed meanings and roles of speech synthesis significantly. Speech synthesis was just a function to read out given texts. However, thanks to the outcomes of the current project, speech synthesis starts to means more useful and advanced functionalities clearly beyond that. 

It has 'ears' to listen to environments. It has a 'brain' to mimick/clone somebody's voices. It has 'tongue' as well as 'glottis' like human. This clearly makes TTS applications such as spoken dialogue systems, robots, assistive technology attractive and meaningful to the society and researchers in future. For instance, we have also reconstructed a hundred of MND patient's voices and have confirmed that the new speech synthesis may change their quality of life.</gtr:description><gtr:exploitationPathways>As described earlier, the findings of our research project clearly make speech applications that use speech synthesis such as spoken dialogue systems, robots, assistive technology more attractive and meaningful. 

Meantime, the attractive speech synthesis techniques start to bring us massive amounts of voice data - more than has ever been used for speech synthesis before - and will enable us tackle various challenging new research topics in speech synthesis. 

All subcomponents of conventional systems assume a fixed amount of voice and text data, processed in &amp;quot;batch mode&amp;quot; currently. Ideally, the statistical speech models at the core of the system should continuously improve, given this incoming data: this is not possible using current batch-based approaches. New algorithms need to be found, to take advantage of such a massive and never-ending data stream. 

This will be a new research area. Advances in this area would benefit the next generation of personalised user interfaces, and also approaches to analyse streams of speech and audio data which are based on similar statistical modelling approaches. Areas of impact will thus extend to multimodal user interfaces, and analysis and indexing of online and broadcast media. 

This is an excellent fit to EPSRC's &amp;quot;Towards an Intelligent Information Infrastructure&amp;quot; cross-ICT priority theme, and is strongly linked to proposed activities in &amp;quot;Data to Knowledge&amp;quot;.</gtr:exploitationPathways><gtr:id>DC2869BD-3991-47D9-A304-F13A9F3150C2</gtr:id><gtr:outcomeId>5464b493e0cba0.79426684</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>http://researchmap.jp/read0205283/?lang=english</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs><gtr:productOutput><gtr:description>Adaptive speech synthesis may be be used to develop personalised synthetic voices for people who have a vocal pathology. In 2009 Dr. Sarah Creer from University of Sheffield and I have successfully applied it to clinical voice banking for laryngectomees (individuals who have had their vocal cords removed due to a developing cancer) to reconstruct their voices. In 2010, I have &amp;quot;implanted&amp;quot; the personalised synthetic voice of a patient who has motor neurone disease into their assistive communication device. Such a personalised voice can lead to far more natural communication for patients, particularly with family. A &amp;quot;voice reconstruction&amp;quot; trial has been tested with about 100 patients in total at the Euan MacDonald Centre for MND Research and the Anne Rowling Regenerative Neurology Clinic in Edinburgh.</gtr:description><gtr:id>606FD779-0354-4D31-899D-25E41E1718EF</gtr:id><gtr:impact>We have recorded about 100 MND patients at the Euan MacDonald Centre for MND Research and the Anne Rowling Regenerative Neurology Clinic in Edinburgh and have constructed personalized speech synthesizers based on their disordered voices. We have received and analyzed feedback from the patients and we have confirmed that this new speech synthesis technology can improve their quality-of-life.</gtr:impact><gtr:outcomeId>5464c03ec5ad59.48662773</gtr:outcomeId><gtr:stage>Initial development</gtr:stage><gtr:status>Actively seeking support</gtr:status><gtr:title>Clinical trial of personalized speech synthesis voices for MND patients</gtr:title><gtr:type>Health and Social Care Services</gtr:type><gtr:yearDevCompleted>2015</gtr:yearDevCompleted></gtr:productOutput></gtr:productOutputs><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>This CSTR VCTK Corpus includes speech data uttered by 109 native speakers of English with various accents. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald &amp;amp; Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximise the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf

All speech data was recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, were downsampled to 48 kHz based on STPK, and were manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies.</gtr:description><gtr:id>06044875-048E-4F33-868E-90DDC8935E68</gtr:id><gtr:impact>This is the first free corpus that is designed and appropriate for speaker-adaptive speech synthesis. This starts to become a standard database to build and compare speaker-adaptive speech synthesis systems and voice conversion systems. This was also used even for speaker verification systems.</gtr:impact><gtr:outcomeId>5464c1d0125495.31351301</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>CSTR VCTK Corpus -- Multi-speaker English Corpus for CSTR Voice Cloning Toolkit</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The database has been used in the first Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2015). Genuine speech is collected from 106 speakers (45 male, 61 female) and with no signi?cant channel or background noise effects. Spoofed speech is generated from the genuine data using a number of different spoo?ng algorithms. The full dataset is partitioned into three subsets, the ?rst for training, the second for development and the third for evaluation. More details can be found in the evaluation plan in the summary paper.</gtr:description><gtr:id>712E3846-598B-4989-BCF2-0FC130AB549A</gtr:id><gtr:impact>Automatic speaker verification (ASV) offers a low-cost and flexible biometric solution to person authentication. While the reliability of ASV systems is now considered sufficient to support mass-market adoption, there are concerns that the technology is vulnerable to spoofing, also referred to as presentation attacks. Spoofing refers to an attack whereby a fraudster attempts to manipulate a biometric system by masquerading as another, enrolled person. Acknowledged vulnerabilities include attacks through impersonation, replay, speech synthesis and voice conversion.

This database has been used for the 2015 ASVspoof challenge, which aims to encourage further progress through (i) the collection and distribution of a standard dataset with varying spoofing attacks implemented with multiple, diverse algorithms and (ii) a series of competitive evaluations. The first ASVspoof challenge was held during the 2015 edition of INTERSPEECH in Dresden, Germany. The challenge has been designed to support, for the first time, independent assessments of vulnerabilities to spoofing and of countermeasure performance and to facilitate the comparison of different spoofing countermeasures on a common dataset, with standard protocols and metrics.</gtr:impact><gtr:outcomeId>56a1a4cb263644.04033581</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Automatic Speaker Verification Spoofing and Countermeasures Challenge (ASVspoof 2015) Database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>This dataset is associated with the paper &amp;quot;'SAS: A speaker verification spoofing database containing diverse attacks': presents the first version of a speaker verification spoofing and anti-spoofing database, named SAS corpus. The corpus includes nine spoofing techniques, two of which are speech synthesis, and seven are voice conversion. We design two protocols, one for standard speaker verification evaluation, and the other for producing spoofing materials. Hence, they allow the speech synthesis community to produce spoofing materials incrementally without knowledge of speaker verification spoofing and anti-spoofing. To provide a set of preliminary results, we conducted speaker verification experiments using two state-of-the-art systems. Without any anti-spoofing techniques, the two systems are extremely vulnerable to the spoofing attacks implemented in our SAS corpus&amp;quot;.</gtr:description><gtr:id>EE7F540C-27C7-43ED-9916-378335ED7E67</gtr:id><gtr:impact>This SAS database is the first version of a standard dataset for spoofing and anti-spoofing research. Currently, the SAS corpus includes speech generated using nine spoofing methods, each of which comprises around 300000 spoofed trials. To the best of our knowledge, this is the first attempt to include such a diverse range of spoofing attacks in a single database. The SAS corpus is publicly available at no cost.</gtr:impact><gtr:outcomeId>56a1a35e1164e6.36580517</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Spoofing and Anti-Spoofing (SAS) corpus v1.0</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>The Voice Conversion Challenge (VCC) 2016, one of the special sessions at Interspeech 2016, deals with speaker identity conversion, referred as Voice Conversion (VC). The task of the challenge was speaker conversion, i.e., to transform the voice identity of a source speaker into that of a target speaker while preserving the linguistic content. Using a common dataset consisting of 162 utterances for training and 54 utterances for evaluation from each of 5 source and 5 target speakers, 17 groups working in VC around the world developed their own VC systems for every combination of the source and target speakers, i.e., 25 systems in total, and generated voice samples converted by the developed systems. The objective of the VCC was to compare various VC techniques on identical training and evaluation speech data. The samples were evaluated in terms of target speaker similarity and naturalness by 200 listeners in a controlled environment. This dataset consists of the participants' VC submissions and the listening test results for naturalness and similarity.</gtr:description><gtr:id>02FCEF6E-073E-4133-8A92-85D93C84CC2E</gtr:id><gtr:impact>17 groups working in VC around the world have used this database and have developed their own VC systems.</gtr:impact><gtr:outcomeId>58c9580a48b9c3.44032406</gtr:outcomeId><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>The Voice Conversion Challenge 2016 database</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://datashare.is.ed.ac.uk/handle/10283/2211</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>HTS is an open source toolkit for statistical speech synthesis. I am a member of a team developing the the free open-source research software packages for speech synthesis.</gtr:description><gtr:id>B80F141F-91AC-4C1E-8F49-352CD5A81108</gtr:id><gtr:impact>The HTS toolkit is used worldwide by both academic and commercial organisations, such as Microsoft, Nuance, Toshiba, Pentax, and Google. The number of downloads of HTS exceeds 10,000 and various commercial products using HTS are on the market. Therefore, this toolkit is a very influential platform for me to disseminate outcomes and form an immediate pathway to impact.</gtr:impact><gtr:outcomeId>56a1babc0dd3a5.64447489</gtr:outcomeId><gtr:title>HTS ver 2.3</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://hts.sp.nitech.ac.jp</gtr:url><gtr:yearFirstProvided>2015</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>994CD3B9-5A87-48EE-BB8B-A9AFE7E25EF6</gtr:id><gtr:title>SAS: A speaker verification spoofing database containing diverse attacks</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>567a7d6a1a0be3.89309793</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2BA881C9-C6BB-4267-BED7-703FC734BE7A</gtr:id><gtr:title>ASVspoof: The Automatic Speaker Verification Spoofing and Countermeasures Challenge</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1932-4553</gtr:issn><gtr:outcomeId>5a3623e78eafe9.84066922</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5F93A166-42A0-46DF-8CF9-B4575DBEBD6F</gtr:id><gtr:title>Towards Personalized Synthesized Voices for Individuals with Vocal Disabilities: Voice Banking and Reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e95ade432385b6636659af37ed310795"><gtr:id>e95ade432385b6636659af37ed310795</gtr:id><gtr:otherNames>Veaux, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>56990707584dc5.91887429</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>95926611-F007-42C1-96FC-6343525D5F19</gtr:id><gtr:title>Mel Cepstral Coefficient Modification Based on the Glimpse Proportion Measure for Improving the Intelligibility of HMM-Generated Synthetic Speech in Noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>568baff64d8666.53309707</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>251F846D-2DBE-4E11-9EF5-63C534D53D72</gtr:id><gtr:title>Noise-robust whispered speech recognition using a non-audible-murmur microphone with VTS compensation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/851a472230d2cd69a9cab22daa89e275"><gtr:id>851a472230d2cd69a9cab22daa89e275</gtr:id><gtr:otherNames>Yang C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-2506-6</gtr:isbn><gtr:outcomeId>5464b7c8c62386.95538343</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>08104DB4-C1B0-4CB7-81F2-C6B9B2141FDF</gtr:id><gtr:title>Improving intelligibility in noise of HMM-generated speech via noise-dependent and -independent methods</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c9ddd7f6.96142116</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>739682C3-64E6-4A43-ACF9-D08C213778E9</gtr:id><gtr:title>Cepstral analysis based on the glimpse proportion measure for improving the intelligibility of HMM-based synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5464b7cc108bd1.22136271</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>53A3D5F8-2EF5-4BE4-B478-AD794DB41B7B</gtr:id><gtr:title>Glottal Spectral Separation for Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0354e6b9c5de838fe3e29ef2e8d29910"><gtr:id>0354e6b9c5de838fe3e29ef2e8d29910</gtr:id><gtr:otherNames>Cabral J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464b7ca94e2d4.37071543</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9A1AF3E4-2065-4F83-A97A-D44FE81A3230</gtr:id><gtr:title>Reactive Control of Expressive Speech Synthesis Using Kinect Skeleton Tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0eabedd8bcfa55302aaf70ed894bfaca"><gtr:id>0eabedd8bcfa55302aaf70ed894bfaca</gtr:id><gtr:otherNames>Clark, Robert A.J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>568bd2529e7672.29430003</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F3780A16-A6C2-43C2-8921-0EC763E7CF12</gtr:id><gtr:title>Using HMM-based Speech Synthesis to Reconstruct the Voice of Individuals with Degenerative Speech Disorders</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e95ade432385b6636659af37ed310795"><gtr:id>e95ade432385b6636659af37ed310795</gtr:id><gtr:otherNames>Veaux, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5699075fd6ac92.29965181</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F7673CF7-6122-450A-889F-8B9D57C0BE63</gtr:id><gtr:title>Adapting and Controlling DNN-Based Speech Synthesis Using Input Codes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a53ae8531c09d9fade191518775ef78b"><gtr:id>a53ae8531c09d9fade191518775ef78b</gtr:id><gtr:otherNames>Luong H-T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5893691860c287.99892052</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6F4BF72C-8976-4955-8E1F-5FB29E43A402</gtr:id><gtr:title>Anti-Spoofing for Text-Independent Speaker Verification: An Initial Database, Comparison of Countermeasures, and Human Performance</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d50448a8a82.54589190</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>96C48BAB-17EA-4C30-9793-C9846359DFB6</gtr:id><gtr:title>Advanced speech synthesis technologies for vocal disabilities</gtr:title><gtr:parentPublicationTitle>Journal of Information Processing and Management</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/cd45d575d507c22ea3facdcde57b3066"><gtr:id>cd45d575d507c22ea3facdcde57b3066</gtr:id><gtr:otherNames>YAMAGISHI J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f13c85254</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>65123C5F-7D53-47E7-AD5D-36DC4920B522</gtr:id><gtr:title>Adapting and controlling DNN-based speech synthesis using input codes</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c5fb6c52f3ec241192baab3db89c69aa"><gtr:id>c5fb6c52f3ec241192baab3db89c69aa</gtr:id><gtr:otherNames>Luong H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a7325de800c15.50396218</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A9E99654-F415-422F-8B69-86B8FCBDF4C3</gtr:id><gtr:title>Analysis of unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis using KLD-based transform mapping</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6737e652eab0bdf1e92854855ba8f137"><gtr:id>6737e652eab0bdf1e92854855ba8f137</gtr:id><gtr:otherNames>Oura K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7cca9d6b4.41311791</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C60A1F8A-0AF8-4C39-B644-72E30F44912B</gtr:id><gtr:title>Analysis of speaker clustering strategies for HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/3e49f2bafd907f5c670d3407eae6733e"><gtr:id>3e49f2bafd907f5c670d3407eae6733e</gtr:id><gtr:otherNames>Dall R.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7ccd01431.36116826</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E97DC1B4-7AFB-4437-9B8E-B1CD80724E47</gtr:id><gtr:title>Lightly supervised GMM VAD to use audiobook for speech synthesiser</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9724130ba82aa3a7bfba4d8bb066ff6b"><gtr:id>9724130ba82aa3a7bfba4d8bb066ff6b</gtr:id><gtr:otherNames>Mamiya Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c96d2ee2.82489300</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>98CB6655-4F75-43A8-8848-44F5695C2C48</gtr:id><gtr:title>The use of articulatory movement data in speech synthesis applications: An overview - Application of articulatory movements using machine learning algorithms -</gtr:title><gtr:parentPublicationTitle>Acoustical Science and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1863a1af108e484cfbba1abf3daa6033"><gtr:id>1863a1af108e484cfbba1abf3daa6033</gtr:id><gtr:otherNames>Richmond K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>567930f8e5efc5.99748892</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2E560598-88A2-4C4A-92B8-71CC35A0794B</gtr:id><gtr:title>Evaluation of Speaker Verification Security and Detection of HMM-Based Synthetic Speech</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fd07305f88d78c03e0f1f557a9d76139"><gtr:id>fd07305f88d78c03e0f1f557a9d76139</gtr:id><gtr:otherNames>De Leon P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7cb13ad02.53542351</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>501DD609-B3D5-4E37-973F-30EB85B752A4</gtr:id><gtr:title>Using neighbourhood density and selective SNR boosting to increase the intelligibility of synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>568b9f107a6b17.69332253</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>102972C6-48BB-4A1A-B87B-D0D1A49159EB</gtr:id><gtr:title>Combining perceptually-motivated spectral shaping with loudness and duration modification for intelligibility enhancement of HMM-based synthetic speech in noise</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4ee279d6b0887d0abf52ac6218b3af7a"><gtr:id>4ee279d6b0887d0abf52ac6218b3af7a</gtr:id><gtr:otherNames>Valentini-Botinhao C.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>19909772 2308457X</gtr:issn><gtr:outcomeId>5464b7cbdcff06.58014252</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>62D1D469-D601-4EAF-8C15-66A06ECA64E4</gtr:id><gtr:title>Combining Vocal Tract Length Normalization With Hierarchical Linear Transformations</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3e397272cbec8689ecfec75bccb6ef6"><gtr:id>e3e397272cbec8689ecfec75bccb6ef6</gtr:id><gtr:otherNames>Saheer L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464b6b0e41a14.26155321</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>51AC484A-1D7B-4D3E-9576-D36603A7C494</gtr:id><gtr:title>Combining Perceptually-Motivated Spectral Shaping with Loudness and Duration Modification for Intelligibility Enhancement of HMM-Based Synthetic Speech in Noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>568bd3ecbc11b9.76051394</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>73748833-7A0D-40B6-9A5B-FB69E81ED8E0</gtr:id><gtr:title>Spoofing and countermeasures for speaker verification: A survey</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/131f1e500d0d5a44eb53b0062c6d254e"><gtr:id>131f1e500d0d5a44eb53b0062c6d254e</gtr:id><gtr:otherNames>Wu Z</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>568b909c7c9009.29285108</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>0302749F-6417-4E08-9C74-03D8B4DDB53D</gtr:id><gtr:title>Intelligibility enhancement of HMM-generated speech in additive noise by modifying Mel cepstral coefficients to increase the glimpse proportion</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f97497464ebc4b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3068020B-0B9E-4747-B91D-04A26637AA2B</gtr:id><gtr:title>Evaluation of objective measures for intelligibility prediction of HMM-based synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/9f9429acc9c9c877f8366e1f63939feb"><gtr:id>9f9429acc9c9c877f8366e1f63939feb</gtr:id><gtr:otherNames>Valentini-Botinhao C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0538-0</gtr:isbn><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7cc35ba29.12840075</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>EAAAA60C-AB51-4DA8-9703-F89FA16FED73</gtr:id><gtr:title>Feature-space transform tying in unified acoustic-articulatory modelling for articulatory control of HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bc720e26998eaf8e1e7606b8e2b6bd1c"><gtr:id>bc720e26998eaf8e1e7606b8e2b6bd1c</gtr:id><gtr:otherNames>Ling Z.-H.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7cae2a097.86697731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FF0407E2-005B-413F-9CC6-D576107760D6</gtr:id><gtr:title>Impacts of machine translation and speech synthesis on speech-to-speech translation</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b002fa9bfa95f9bd6456417932f71bae"><gtr:id>b002fa9bfa95f9bd6456417932f71bae</gtr:id><gtr:otherNames>Hashimoto K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>5464b7ca21a498.93840360</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F0A39DAB-CE35-4121-A15D-881FAA4264D3</gtr:id><gtr:title>Building personalised synthetic voices for individuals with severe speech impairment</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/dd3b1d1091b2b0dd95c42a5e9b15a9c7"><gtr:id>dd3b1d1091b2b0dd95c42a5e9b15a9c7</gtr:id><gtr:otherNames>Creer S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7cc5b1766.09655731</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>00B70365-0BD6-4F62-9B07-84C94DF405AC</gtr:id><gtr:title>An Autoregressive Recurrent Mixture density Network For Parametric Speech Synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d33cf2ec743344310cadecc6294d895"><gtr:id>8d33cf2ec743344310cadecc6294d895</gtr:id><gtr:otherNames>Wang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>589368c88e8987.09337070</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DD4A94EB-7100-4949-B52E-F8823A5BC9E6</gtr:id><gtr:title>Using Adaptation to Improve Speech Transcription Alignment in Noisy and Reverberant Environments</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bbbb69cd6372e90627cc5992199f7fb4"><gtr:id>bbbb69cd6372e90627cc5992199f7fb4</gtr:id><gtr:otherNames>Mamiya, Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>568ba07fab5da5.91167032</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>476EB210-B0F7-43C5-846E-62146F0DB875</gtr:id><gtr:title>Speech intelligibility enhancement for HMM-based synthetic speech in noise</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/686f887af459585a07547e4353e10e7b"><gtr:id>686f887af459585a07547e4353e10e7b</gtr:id><gtr:otherNames>Valentini-Botinhao, C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>568bb095bc25c2.84712252</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>55EB1439-A937-46C9-BBAD-219C2A0ED19F</gtr:id><gtr:title>A Deep Generative Architecture for Postfiltering in Statistical Parametric Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/fbfd9fb6e692d07596464ca94848bf67"><gtr:id>fbfd9fb6e692d07596464ca94848bf67</gtr:id><gtr:otherNames>Chen L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>5675f11e6b4a7</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>65784E1B-3E59-42E1-A165-1764659E8CB1</gtr:id><gtr:title>Investigation of Using Continuous Representation of Various Linguistic Units in Neural Network Based Text-to-Speech Synthesis</gtr:title><gtr:parentPublicationTitle>IEICE Transactions on Information and Systems</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e98fe1348ed65d25b2e4aa2b02cb3723"><gtr:id>e98fe1348ed65d25b2e4aa2b02cb3723</gtr:id><gtr:otherNames>WANG X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>585d4556e56ff2.08107496</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A327C7A8-FD85-48D6-ABCC-6502ABE865F7</gtr:id><gtr:title>A fixed dimension and perceptually based dynamic sinusoidal model of speech</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4270d19e312228c6755fbae911c04249"><gtr:id>4270d19e312228c6755fbae911c04249</gtr:id><gtr:otherNames>Hu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>5464b7cd393fd4.31903875</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>4AF7BB14-DF55-4EED-9AC8-73FF5CD214AF</gtr:id><gtr:title>ALISA: An automatic lightly supervised speech segmentation and alignment tool</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/318072fc41ab4f4f937acb7487c265e6"><gtr:id>318072fc41ab4f4f937acb7487c265e6</gtr:id><gtr:otherNames>Stan A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:outcomeId>56792bf761d2f1.35430342</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>FDEDE7BF-98F1-4623-983A-B844179BC876</gtr:id><gtr:title>An autoregressive recurrent mixture density network for parametric speech synthesis</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8d33cf2ec743344310cadecc6294d895"><gtr:id>8d33cf2ec743344310cadecc6294d895</gtr:id><gtr:otherNames>Wang X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:outcomeId>5a7325c5252958.72239954</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>887C95AC-9A9B-432E-9CF3-F1BDEC4FE9FC</gtr:id><gtr:title>Combining vocal tract length normalization with hierarchial linear transformations</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e3e397272cbec8689ecfec75bccb6ef6"><gtr:id>e3e397272cbec8689ecfec75bccb6ef6</gtr:id><gtr:otherNames>Saheer L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn><gtr:outcomeId>5464b7cbb46805.74861415</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>033141F9-EDC6-41B1-BA5A-629B98666573</gtr:id><gtr:title>Speech synthesis technologies for individuals with vocal disabilities: Voice banking and reconstruction</gtr:title><gtr:parentPublicationTitle>Acoustical Science and Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/183dc4a8fc3549b73816452269943b6c"><gtr:id>183dc4a8fc3549b73816452269943b6c</gtr:id><gtr:otherNames>Yamagishi J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f93d93de23e603</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>3DAE160D-0D57-4DE1-861C-21A9752660BE</gtr:id><gtr:title>Speech Synthesis Based on Hidden Markov Models</gtr:title><gtr:parentPublicationTitle>Proceedings of the IEEE</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/837f196e3b692bddce90b7dea72e8d0b"><gtr:id>837f196e3b692bddce90b7dea72e8d0b</gtr:id><gtr:otherNames>Tokuda K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7c7eef173.74403544</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9D48D282-1904-40A8-A890-6616F354CF86</gtr:id><gtr:title>Articulatory Control of HMM-Based Parametric Speech Synthesis Using Feature-Space-Switched Multiple Regression</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4d35805892fea2bd41853d9893397711"><gtr:id>4d35805892fea2bd41853d9893397711</gtr:id><gtr:otherNames>Zhen-Hua Ling</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5464b7cc841c95.90092937</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DE013147-47DE-438E-86EA-BDFB145044CE</gtr:id><gtr:title>Formant-controlled HMM-based speech synthesis</gtr:title><gtr:parentPublicationTitle>Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a4fed7bcb649bcbb30e92f767023ce10"><gtr:id>a4fed7bcb649bcbb30e92f767023ce10</gtr:id><gtr:otherNames>Lei M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:issn>19909772</gtr:issn><gtr:outcomeId>5464b7cabb9282.50099431</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/J002526/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>