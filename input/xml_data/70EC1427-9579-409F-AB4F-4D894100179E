<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:department>Computer Science</gtr:department><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3A5E126D-C175-4730-9B7B-E6D8CF447F83"><gtr:id>3A5E126D-C175-4730-9B7B-E6D8CF447F83</gtr:id><gtr:name>University College London</gtr:name><gtr:address><gtr:line1>Gower Street</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>WC1E 6BT</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/1595F1E6-9AAE-47D5-9A26-B0F4E93B9F7E"><gtr:id>1595F1E6-9AAE-47D5-9A26-B0F4E93B9F7E</gtr:id><gtr:name>Intuitive Surgical Inc</gtr:name><gtr:address><gtr:line1>1266 Kifer Road</gtr:line1><gtr:postCode>94086-5304</gtr:postCode><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/EB4118EE-7395-4BE1-9EFE-80B2962FB869"><gtr:id>EB4118EE-7395-4BE1-9EFE-80B2962FB869</gtr:id><gtr:firstName>Danail</gtr:firstName><gtr:surname>Stoyanov</gtr:surname><gtr:orcidId>0000-0002-0980-3227</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role><gtr:role><gtr:name>FELLOW</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FP012841%2F1"><gtr:id>70EC1427-9579-409F-AB4F-4D894100179E</gtr:id><gtr:title>Robotic Assisted Imaging</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Fellowship</gtr:grantCategory><gtr:grantReference>EP/P012841/1</gtr:grantReference><gtr:abstractText>The paradigm of modern surgical treatment is to reduce the invasive trauma of procedures by using small keyhole ports to enter the body. Robotic assistant systems provide tele-manipulated instruments that facilitate minimally invasive surgery by improving the ergonomics, dexterity and precision of controlling manual keyhole surgery instruments. Robotic surgery is now common for minimally invasive prostate and renal cancer procedures. But imaging inside the body is currently restricted by the access port and only provides information at visible organ surfaces which is often insufficient for easy localisation within the anatomy and avoiding inadvertent damage to healthy tissues. 

This project will develop robotic assisted imaging which will exploit the autonomy and actuation capabilities provided by robotic platforms, to optimise the images that can be acquired by current surgical imaging modalities. In the context of robotic assisted surgery, now an established surgical discipline, advanced imaging can help the surgeon to operate more safely and efficiently by allowing the identification of structures that need to be preserved while guiding the surgeon to anatomical targets that need to be removed. Providing better imaging and integration with the robotic system will result in multiple patient benefits by ensuring safe, accurate surgical actions that lead to improved outcomes.

To expose this functionality, new theory, computing, control algorithms and real-time implementations are needed to underpin the integration of imaging and robotic systems within dynamic environments. Information observed by the imaging sensor needs to feed back into the robotic control loop to guide automatic sensor positioning and movement that maintains the alignment of the sensor to moving organs and structures. This level of automation is largely unexplored in robotic assisted surgery at present because it involves multiple challenges in visual inference, reconstruction and tracking; calibration and re-calibration of sensors and various robot kinematic strategies; integration with surgical workflow and user studies. Combined with the use of pre-procedural planning, robotic assisted imaging can lead to pre-planned imaging choices that are motivated by different clinical needs. 

As well as having direct applications in surgery, the robotic assisted imaging paradigm will be applicable to many other sectors transformed by robotics, for example manufacturing or inspection, especially when working within non-rigid environments. For this cross sector impact to be achieved the project will build the deep theoretical and robust software platforms that are ideally suited for foundational fellowship support.</gtr:abstractText><gtr:potentialImpactText>The impact of the proposed fellowship research will be widespread and has cross disciplinary potential. With the predicted uptake of robotic systems in the near future, this has applications in multiple sectors:

Healthcare: The focus for end-point outcomes of the fellowship project, is to improve real-time imaging during surgery by using the current systems for robotic prostate and kidney surgeries, as well as, the currently available intra-operative imaging modalities by developing computational support and capabilities. This has the potential to reduce surgical complications, patient readmission, disease specific indications such as kidney ischemia damage during vessel clamping, and generally all operative risks while increasing the success of surgery. These objectives, if achieved, will have a significant measureable impact on reducing mortality in patients and increasing the quality of life post-surgery, specifically for this fellowship in kidney and prostate cancers, but with potential applications in a wide range of clinical indications. Such impact would have obvious societal and economic measures.

Impact on Medical Technology: Facilitating new imaging during surgery has important synergies with surgical instrumentation, especially with emerging robotic systems. Multinational corporations in this space have recently made significant investments and acquisitions (Google, Johnson &amp;amp; Johnson - Ethicon, Medtronic, Stryker, and others) which are likely to result in a surge in surgical robot devices in the next five years. This will be a fertile ground for commercial exploitation of project outputs. The industrial partner, Intuitive Surgical, also provides a clear translational path within their platform, which is already used in over 500k procedures per year in different surgical specialisations. Advisory group steering will also help with links to medical device exploitation pathways. 

Academic Impact: Computer vision techniques for real-time surgical procedures is a highly active and challenging area that is strongly represented at international conferences such as MICCAI, IPCAI and more recently ICRA and IROS by linking to robotics. Additional dissemination will happen at the leading vision meetings like CVPR, ECCV and ICCV for theoretical work on fundamental vision problems both in geometry (calibration and reconstruction) and inference (detection, tracking). The intersection of these areas incorporating sensor modelling, error propagation handling, highly novel nonlinear motion modelling, visual servo and control system integration will lead to publications in such high profile meetings as well as in high-impact journals (see Pathways to Impact and Cost of Support).

Impact on the Wider Research Community: Combination of imaging methodologies and robotics through i) data fusion, ii) multimodality systems, and iii) modelling and inverse problems, are recognised active topics, not only in surgical robotics and medical imaging, but more widely e.g. in seismology, non-destructive testing, nuclear maintenance and decommissioning, manufacturing and aerospace. Cross-fertilization between such different application areas is naturally stimulated through UCL Robotics, CMIC and the various other application focused vehicles throughout UCL. The Alan Turing Institute, in close proximity to UCL, will allow participation in wider programmes of research in large scale data processing exploiting theoretical developments in mathematics, signal processing and machine learning techniques. This and the EPSRC Networks will allow a rapid and effective mechanism to present project results.

Impact on teaching: By developing software throughout the project within structured frameworks, the project outputs will contribute to toolbox software that will be used within new and established programmes such as the MSc in Robotics and Computation and the MRes in Robotics.</gtr:potentialImpactText><gtr:fund><gtr:end>2022-06-30</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2017-07-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1239250</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>0CE44491-6605-408A-95CB-712387886D64</gtr:id><gtr:title>Intelligent viewpoint selection for efficient CT to video registration in laparoscopic liver surgery.</gtr:title><gtr:parentPublicationTitle>International journal of computer assisted radiology and surgery</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/1bf62e8acb886961558ef9ca8eb64624"><gtr:id>1bf62e8acb886961558ef9ca8eb64624</gtr:id><gtr:otherNames>Robu MR</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1861-6410</gtr:issn><gtr:outcomeId>5a915d7d19da71.16411250</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8A7B0860-793F-4740-B177-E45244D7CCC8</gtr:id><gtr:title>Bayesian Estimation of Intrinsic Tissue Oxygenation and Perfusion From RGB Images.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on medical imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/bd82d2282685ad5be4a70b6d49034f67"><gtr:id>bd82d2282685ad5be4a70b6d49034f67</gtr:id><gtr:otherNames>Jones G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0278-0062</gtr:issn><gtr:outcomeId>5a2fea65043e02.06460584</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>CF347B06-77E2-4581-A197-67A8004D3F69</gtr:id><gtr:title>Comparative Validation of Polyp Detection Methods in Video Colonoscopy: Results From the MICCAI 2015 Endoscopic Vision Challenge.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on medical imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/03f89715eadb12ffe2134bda0a0d84ee"><gtr:id>03f89715eadb12ffe2134bda0a0d84ee</gtr:id><gtr:otherNames>Bernal J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0278-0062</gtr:issn><gtr:outcomeId>5a915d7b822d88.20689171</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B2FED48C-BED3-4653-9617-4255DB64DECC</gtr:id><gtr:title>Medical-grade Sterilizable Target for Fluid-immersed Fetoscope Optical Distortion Calibration.</gtr:title><gtr:parentPublicationTitle>Journal of visualized experiments : JoVE</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/8cf0ef52097e9d1cb8c89754ff2e5dd1"><gtr:id>8cf0ef52097e9d1cb8c89754ff2e5dd1</gtr:id><gtr:otherNames>Nikitichev DI</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1940-087X</gtr:issn><gtr:outcomeId>5a35e3bb454709.37764781</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/P012841/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5CBA14F4-F235-45B6-A9DD-5937D5C166CC</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Electrical Engineering</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>FB535BD0-E265-4C0A-8532-32DCB83A3951</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Tools, technologies &amp; methods</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A759BB04-AFFE-4780-BD31-9A2707BC44BA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Medical Imaging</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>6624C9D3-BA2C-4506-9A85-9816946CA97A</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Robotics &amp; Autonomy</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>