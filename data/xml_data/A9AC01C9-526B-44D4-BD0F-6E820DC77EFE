<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/94171FA3-03C4-4FBF-8649-293ADD4E5F40"><gtr:id>94171FA3-03C4-4FBF-8649-293ADD4E5F40</gtr:id><gtr:name>VisioWave S.A.</gtr:name><gtr:address><gtr:line1>Route de la Pierre 22</gtr:line1><gtr:line4>1024 Ecublens</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Switzerland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/92BED6E6-D64C-46AC-9396-87F84F3A232C"><gtr:id>92BED6E6-D64C-46AC-9396-87F84F3A232C</gtr:id><gtr:name>Intel Corporation</gtr:name><gtr:address><gtr:line1>2200 Mission College Blvd</gtr:line1><gtr:line2>PO Box 58119</gtr:line2><gtr:line4>Santa Clara</gtr:line4><gtr:line5>CA 95052-8119</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/DB8CF661-5A9D-4C40-8401-51C641BA965C"><gtr:id>DB8CF661-5A9D-4C40-8401-51C641BA965C</gtr:id><gtr:firstName>Andrea</gtr:firstName><gtr:surname>Cavallaro</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FD033772%2F1"><gtr:id>A9AC01C9-526B-44D4-BD0F-6E820DC77EFE</gtr:id><gtr:title>Multi-modal object tracking in a network of audiovisual sensors</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/D033772/1</gtr:grantReference><gtr:abstractText>The aim of this project is to develop a unified scheme cooperative multi-modal and multi-sensor tracking. The multi-sensor network will be composed of stereo microphones coupled with omni-directional and with pan-tilt-zoom cameras. Sound information will be used to discriminate ambiguous visual observations as well as to extend the coverage area of the sensors beyond the field of view of the cameras. Although single modality as well as multi-modality trackers have achieved some success, a number of important tracking issues remain open for enabling the adoption of these algorithms in real-world scenarios. Among these issues, three important inter-related problems will be addressed in this project, namely the definition of a generic and flexible feature representation for a target, a reliable mechanism to update the target model based on incoming observations, and a robust multi-sensor handover strategy. First, we will develop a robust and adaptive representation of objects based on their acoustical and visual attributes while moving across the network of heterogeneous sensors. Next, object models will be defined based on the observation that temporal representation of a target is expected to lie in a low-dimensional manifold in the high-dimensional multi-modal feature space. Finally, the object model will be used to control and guide the evolution of the target state in order to help intra-sensor occlusion handling and inter-sensor handover. To evaluate the tracking scheme, we will create a test corpus and its associated ground-truth data for use in the project as well as for distribution to the research community to facilitate comparisons.</gtr:abstractText><gtr:fund><gtr:end>2008-07-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2006-08-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>125526</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>8A5D7EEB-67AA-4219-89FF-974CBC237097</gtr:id><gtr:title>Accurate appearance-based Bayesian tracking for maneuvering targets</gtr:title><gtr:parentPublicationTitle>Computer Vision and Image Understanding</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5098d061daa4df4ffa7ca7fd71aa433c"><gtr:id>5098d061daa4df4ffa7ca7fd71aa433c</gtr:id><gtr:otherNames>Maggio E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:outcomeId>doi_53cfecfec46054c9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A279C2C5-16FD-498B-A639-98EA46A97F8F</gtr:id><gtr:title>Adaptive Multifeature Tracking in a Particle Filtering Framework</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5098d061daa4df4ffa7ca7fd71aa433c"><gtr:id>5098d061daa4df4ffa7ca7fd71aa433c</gtr:id><gtr:otherNames>Maggio E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2007-01-01</gtr:date><gtr:outcomeId>doi_53d05d05d6c03d51</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>635F86DE-C3B3-4496-9BC7-12FA0F00D276</gtr:id><gtr:title>Efficient Multitarget Visual Tracking Using Random Finite Sets</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5098d061daa4df4ffa7ca7fd71aa433c"><gtr:id>5098d061daa4df4ffa7ca7fd71aa433c</gtr:id><gtr:otherNames>Maggio E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d05d05d6d20649</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>2A82AD49-C08D-464D-98EA-FDA06F2360A6</gtr:id><gtr:title>Target Detection and Tracking With Heterogeneous Sensors</gtr:title><gtr:parentPublicationTitle>IEEE Journal of Selected Topics in Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24d1b3e6d53a1595ea6c891f51bc0e7b"><gtr:id>24d1b3e6d53a1595ea6c891f51bc0e7b</gtr:id><gtr:otherNames>Huiyu Zhou</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d05a05acf0922b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>B69F2B7A-A29D-4FAC-A440-10556485AC52</gtr:id><gtr:title>Multiview Trajectory Mapping Using Homography with Lens Distortion Correction</gtr:title><gtr:parentPublicationTitle>EURASIP Journal on Image and Video Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b4bf514c6642dfc8971c2a141c24ee67"><gtr:id>b4bf514c6642dfc8971c2a141c24ee67</gtr:id><gtr:otherNames>Kayumbi G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:outcomeId>doi_53d07607671c4318</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/D033772/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>13B6D44B-6EAF-464B-A4CC-1C08F8DDE5A0</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Mobile Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>