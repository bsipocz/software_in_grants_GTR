<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/C4145C68-58B8-441E-86DD-A4079B65B478"><gtr:id>C4145C68-58B8-441E-86DD-A4079B65B478</gtr:id><gtr:name>MRC Institute of Hearing Research</gtr:name><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/C4145C68-58B8-441E-86DD-A4079B65B478"><gtr:id>C4145C68-58B8-441E-86DD-A4079B65B478</gtr:id><gtr:name>MRC Institute of Hearing Research</gtr:name><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/C58FB8AA-2008-4B00-8D24-CF61820224F5"><gtr:id>C58FB8AA-2008-4B00-8D24-CF61820224F5</gtr:id><gtr:firstName>Sharon</gtr:firstName><gtr:surname>Thomas</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=MC_U135079301"><gtr:id>156E4777-816A-4672-BFCC-BEC5C5AC9917</gtr:id><gtr:title>Visual and audiovisual perceptual learning and training</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Intramural</gtr:grantCategory><gtr:grantReference>MC_U135079301</gtr:grantReference><gtr:abstractText>The identification of visual speech (lipreading, speechreading) is a notoriously difficult task. Speechreading is neither easy nor accurate but is indispensable to multitudes of hearing-impaired people who use it daily to effectively communicate with others. Good skills in speechreading are associated with better quality of life in people with profound hearing impairments. Despite this, there have been few experimental investigations into effective speechreading training. Our research has shown that repeated exposure to variations of words can lead to improved visual speech discrimination. We have also demonstrated that computer-based audiovisual exposure can substantially improve the ability to recognize poor-quality auditory speech over using auditory stimuli alone. In addition, we have shown that, deaf and hearing-impaired individuals are aware of accent and its potential adverse effects upon speechreading. We have also shown that a talkers accent can be discriminated from visual speech alone. Third, these visual differences not only allow observers to identify accent, they also impact on speechreading performance, especially for good speechreaders. Overall, our research demonstrates that computer-based training tasks are effective in improving the ability of observers to understand visual and audiovisual speech. These findings have important implications for the development of speechreading training and testing systems.</gtr:abstractText><gtr:technicalSummary>Seeing the face of a talker can dramatically improve the intelligibility of speech sounds in quiet and noisy environments. The influence of visual speech on auditory perception is pervasive and highly resistant to variation in the visual stimulus. However, despite this influence, the identification of visual speech (lipreading, speechreading) is a notoriously difficult task. Speechreading is neither easy nor accurate but is indispensable to multitudes of hearing-impaired people who use it daily to effectively communicate with others. Good skills in speechreading are associated with better quality of life in people with profound hearing impairments. Despite this, there have been few experimental investigations into effective speechreading training. We wished to establish training tasks that show large learning effects, and the techniques that lead to transfer of learning to different speech tokens, angles of view and other talkers. Our experiments have shown that repeated exposure to variations of traditionally categorized low-visibility words can lead to improved discrimination. We have also demonstrated generalization to novel stimuli, talkers and angle of view. Additionally, we sought to optimise audiovisual training techniques with subjects with normal hearing, using high-fidelity visual signals accompanied by acoustical signals processed to simulate the characteristics of cochlear-implant speech. Research has demonstrated that normal hearing participants can show rapid adaptations towards distorted speech, with substantial gains in intelligibility found after only short periods of exposure. We have demonstrated that computer-based audiovisual exposure can substantially improve the ability to recognize distorted speech over using auditory stimuli alone. A speechreader who has learned particular patterns of lip movements in a particular cultural environment may have great difficulty generalizing their skill to unfamiliar face movements that accompany unfamiliar accents. This aspect of speechreading has never been previously explored. We aimed to remedy the lack of previous research by examining whether accent does have an effect upon the comprehension of visual speech, the nature of that effect, and whether it can be mitigated through training and exposure. Our experiments have shown that, first, deaf and hearing-impaired individuals are aware of accent and its potential adverse effects upon speechreading. Second, accent can be discriminated using visual speech. Third, these visual differences not only allow observers to identify accent, they also impact on speechreading performance, especially for good speechreaders. Overall, our results to date show that unsupervised computer-based training tasks are effective in improving the ability of naove observers to understand visual and audiovisual distorted speech. These findings have important implications for the development of speechreading training and testing systems.</gtr:technicalSummary><gtr:fund><gtr:end>2008-03-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/C008C651-F5B0-4859-A334-5F574AB6B57C"><gtr:id>C008C651-F5B0-4859-A334-5F574AB6B57C</gtr:id><gtr:name>MRC</gtr:name></gtr:funder><gtr:start>2004-04-01</gtr:start><gtr:type>EXPENDITURE_ACTUAL</gtr:type><gtr:valuePounds>359950</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">MC_U135079301</gtr:identifier></gtr:identifiers><gtr:healthCategories><gtr:healthCategory><gtr:id>5585DDA6-FCF6-4B64-8332-1BDFE62CE94E</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Ear</gtr:text></gtr:healthCategory><gtr:healthCategory><gtr:id>3C193D18-12BD-4B15-8347-037BA623E0FF</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Mental Health</gtr:text></gtr:healthCategory></gtr:healthCategories><gtr:researchActivities><gtr:researchActivity><gtr:id>878A242A-6B84-462E-BEA5-346583F6CA54</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>1.2  Psychological and socioeconomic processes</gtr:text></gtr:researchActivity><gtr:researchActivity><gtr:id>CA7C1014-EC90-43EB-B9F2-C12CD9DD1A1C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>5.6  Psychological and behavioural</gtr:text></gtr:researchActivity></gtr:researchActivities><gtr:researchSubjects/><gtr:researchTopics/><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>