<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:department>School of Health Sciences</gtr:department><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/CD35D908-C2AF-4C14-9BC4-519C775CDB6E"><gtr:id>CD35D908-C2AF-4C14-9BC4-519C775CDB6E</gtr:id><gtr:name>City University London</gtr:name><gtr:address><gtr:line1>Northampton Square</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>EC1V 0HB</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/99F5082C-DE7A-48BF-872D-03E150CEDD53"><gtr:id>99F5082C-DE7A-48BF-872D-03E150CEDD53</gtr:id><gtr:firstName>Michael</gtr:firstName><gtr:surname>Morgan</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/09CF3B75-7707-4B8D-8EFC-47EA288E5D65"><gtr:id>09CF3B75-7707-4B8D-8EFC-47EA288E5D65</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Adam</gtr:otherNames><gtr:surname>Solomon</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE064604%2F1"><gtr:id>B285A93E-4696-4536-8DE8-F7D7E978169D</gtr:id><gtr:title>Image statistics in the visual system</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E064604/1</gtr:grantReference><gtr:abstractText>The overall aim of our project is discover and describe statistical calculations performed by the human visual system.Our perceptions are thought to result from the combination of prior knowledge with data we gather from the environment. Of course, the visual system cannot afford to be too meticulous a scientist and measure everything. Real environments are just too complex and dynamic. Instead, the visual system must settle for being a clever statistician and quickly estimate various visual features.Exactly which visual features can be quickly estimated is a matter of some debate, but there is no question that spatial orientation (i.e. tilt) is one of them. The primary evidence for orientation's special status in the visual system comes from search experiments, in which observers must find a target region, having a unique orientation, amongst distractors, having different orientations. When the average orientation of the target region is sufficiently different from that of the distractors, then the number of distractors will be irrelevant; the target just pops out. Thus, we know that the visual system can compute mean orientation without too much conscious effort. What about orientation variance? That is, would a region with lots of different orientations pop out from regions with largely similar orientations? If the visual system proves capable of this type of statistical analysis, can it do something similar with other visual features, like colour and size?</gtr:abstractText><gtr:fund><gtr:end>2010-10-03</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-10-04</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>188325</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>541000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC</gtr:description><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>Efficiencies</gtr:fundingRef><gtr:id>14F44D37-16E3-49F9-B378-692FDA37C7B7</gtr:id><gtr:outcomeId>5ed4aaaa5ed4aabe</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>541000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>EPSRC</gtr:description><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>Efficiencies</gtr:fundingRef><gtr:id>A57922FE-89A8-4ACF-A35F-971055953125</gtr:id><gtr:outcomeId>r-9486172347.78234106a8686c</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The overall aim of our project was to discover and describe statistical calculations performed by the human visual system.

Our perceptions are thought to result from the combination of prior knowledge with data we gather from the environment. Of course, the visual system cannot afford to be too meticulous a scientist and measure everything. Real environments are just too complex and dynamic. Instead, the visual system must settle for being a clever statistician and quickly estimate various visual features.

Exactly which visual features can be quickly estimated remains a matter of some debate, but there is no question that spatial orientation (i.e. tilt) is one of them. For example, the observer may infer a forest scene if there are more than 8 or 9 trees in the picture. If most of those trees aren't within a few degrees of vertical, the observer may infer that the camera was tilted.

We have now determined that people use relatively few trees (or lines) when quickly computing average tilt. They are much more efficient when asked to make a judgement regarding the range of tilts. Futhermore, we have discovered that estimates of average and range are not independent. When the range of tilts is large, estimates of the average tend to be closer to vertical than when the range is small. This latter result is consistent with a stable prior, or prejudice, for those orientations most prevalent in natural scenes.

The primary evidence for orientation's special status in the visual system comes from search experiments, in which observers must find a &amp;quot;target region,&amp;quot; having a unique orientation, amongst &amp;quot;distractors,&amp;quot; having different orientations. When the average orientation of the target region is sufficiently different from that of the distractors, then the number of distractors will be irrelevant; the target just &amp;quot;pops out.&amp;quot;

Consider the plus sign (+), which has two perpendicular orientations at the same position. Our observers were much faster when asked to find a plus sign amongst minuses (-) and slashes (/,\) than they were when asked to find one of the latter (e.g. a minus sign) amongst pluses. This asymmetry further suggests that the range or multiplicity of orientations in different parts of a scene can be can be accurately and simultaneously estimated at a glance.</gtr:description><gtr:id>9333F429-BEE7-40C6-A5A2-ACB6DF7FA574</gtr:id><gtr:outcomeId>r-4090672828.3415294779795d4</gtr:outcomeId><gtr:sectors/></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>9229988F-B02B-410E-B721-06D707C92DDC</gtr:id><gtr:title>A 'dipper' function for texture discrimination based on orientation variance.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c89fbda010ed9ccdf13dab11a9ff97d9"><gtr:id>c89fbda010ed9ccdf13dab11a9ff97d9</gtr:id><gtr:otherNames>Morgan M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>doi_53d07707755d58c3</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ACD78B5D-C0FE-441A-9BEF-368C2441F409</gtr:id><gtr:title>Orientation uncertainty reduces perceived obliquity.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b055152fc2a7f21c92d74a7991c5ffba"><gtr:id>b055152fc2a7f21c92d74a7991c5ffba</gtr:id><gtr:otherNames>Tomassini A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_53d00f00f03118ac</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>BF20450F-02FB-48C5-BBD6-A89E60647DFB</gtr:id><gtr:title>The history of dipper functions.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/decc6d3fa6a7a8bfc2e7504d77de3596"><gtr:id>decc6d3fa6a7a8bfc2e7504d77de3596</gtr:id><gtr:otherNames>Solomon JA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>doi_53d08808823d4f62</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>C7CBA271-F975-4F30-9D30-3263A57F9AAB</gtr:id><gtr:title>Which way is down? Positional distortion in the tilt illusion.</gtr:title><gtr:parentPublicationTitle>PloS one</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b055152fc2a7f21c92d74a7991c5ffba"><gtr:id>b055152fc2a7f21c92d74a7991c5ffba</gtr:id><gtr:otherNames>Tomassini A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1932-6203</gtr:issn><gtr:outcomeId>545cb91c150172.25607204</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>668FC086-B321-45F9-8864-ACF178BF3687</gtr:id><gtr:title>Awareness is the key to attraction: dissociating the tilt illusions via conscious perception.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b055152fc2a7f21c92d74a7991c5ffba"><gtr:id>b055152fc2a7f21c92d74a7991c5ffba</gtr:id><gtr:otherNames>Tomassini A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>545cb91c455143.52775755</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>DEF55A81-D92E-4E6F-9110-0EB04A74ADF8</gtr:id><gtr:title>Coherent plaids are preattentively more than the sum of their parts.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/76f3952f92e732de39deb7987027450e"><gtr:id>76f3952f92e732de39deb7987027450e</gtr:id><gtr:otherNames>Nam JH</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>doi_53d0880882646adb</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E064604/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>6723A70B-A523-40AB-9740-B6AD2A0677B7</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Medical &amp; health interface</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>EFFEC6B1-6BC8-4C9D-9D77-02CEF5E4E301</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Biomedical neuroscience</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>