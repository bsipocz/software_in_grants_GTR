<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.ukri.org:80/organisation/79D9DA15-08E2-4987-8C27-8B97654A692D"><gtr:id>79D9DA15-08E2-4987-8C27-8B97654A692D</gtr:id><gtr:name>Samsung</gtr:name><gtr:address><gtr:line1>Business Consulting Center</gtr:line1><gtr:line2>Samsung SDS</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:department>Vision Speech and Signal Proc CVSSP</gtr:department><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/536116AC-C155-4A24-A743-309BD68E50CE"><gtr:id>536116AC-C155-4A24-A743-309BD68E50CE</gtr:id><gtr:name>Defence Science &amp; Tech Lab DSTL</gtr:name><gtr:address><gtr:line1>Defence Science &amp; Tech Lab - MOD</gtr:line1><gtr:line2>Porton Down</gtr:line2><gtr:line4>Salisbury</gtr:line4><gtr:postCode>SP4 0JQ</gtr:postCode><gtr:region>South West</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>CO_FUNDER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/46387D84-F71E-4B7D-8C7D-9C288F113510"><gtr:id>46387D84-F71E-4B7D-8C7D-9C288F113510</gtr:id><gtr:name>Imperial College London</gtr:name><gtr:address><gtr:line1>South Kensington Campus</gtr:line1><gtr:line2>Exhibition Road</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SW7 2AZ</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/79D9DA15-08E2-4987-8C27-8B97654A692D"><gtr:id>79D9DA15-08E2-4987-8C27-8B97654A692D</gtr:id><gtr:name>Samsung</gtr:name><gtr:address><gtr:line1>Business Consulting Center</gtr:line1><gtr:line2>Samsung SDS</gtr:line2><gtr:region>Outside UK</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/DAD387A7-797C-4DC5-B27F-A52A9FCDC601"><gtr:id>DAD387A7-797C-4DC5-B27F-A52A9FCDC601</gtr:id><gtr:firstName>Philip JB</gtr:firstName><gtr:surname>Jackson</gtr:surname><gtr:orcidId>0000-0001-7933-5935</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/B1CDD839-FF89-4EBA-8F72-453446B6FE49"><gtr:id>B1CDD839-FF89-4EBA-8F72-453446B6FE49</gtr:id><gtr:firstName>Wenwu</gtr:firstName><gtr:surname>Wang</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0"><gtr:id>09B23A79-8DB6-4FB1-8DDE-A6F5CC941CB0</gtr:id><gtr:firstName>Josef</gtr:firstName><gtr:surname>Kittler</gtr:surname><gtr:orcidId>0000-0002-8110-9205</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FH012842%2F1"><gtr:id>AB9C930A-DA7A-40A2-8C0A-2D71F6980AB2</gtr:id><gtr:title>Multi-Modal Blind Source Separation for Robot Audition</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H012842/1</gtr:grantReference><gtr:abstractText>This proposal draws on expertise in blind source separation and multimodal (audio-visual) speech processing within the Centre for Vision Speech and Signal Processing at University of Surrey. The objective is to perform source separation of the target speech in the presence of multiple competing sound sources in room environments and thereby ultimately provide progress towards automatic machine perception of auditory scenes within an un-controlled natural environment. The fundamental novelty in this work is to exploit visual cues for enhancing the operation of frequency domain blind source separation algorithms. Exploitation of such audio-visual processing is targeted at mitigating the permutation problem, the underdetermined problem (i.e. when the number of sources is greater than the number of microphones), and the reverberation problem, which currently limits the practical applicability of blind source separation algorithms. The focus of the work is therefore on the signal processing algorithms and software tools that can be used to perform automatic separation of sound signals, e.g., for a robot. The body of work in this proposal is underpinned by the substantial experience of the investigators, two from the areas of blind source separation and digital speech processing, and one from the area of computer vision and pattern recognition. The outcomes of the proposed research will be of considerable value to the UK defence industry working especially in the areas of target separation, detection and multi-path mitigation (or dereverberation), with applications in, for example, human-robot interaction, security surveillance and human-computer interaction.</gtr:abstractText><gtr:fund><gtr:end>2012-10-07</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-10-08</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>115288</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Imperial College London</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Collaboration with Imperial College London</gtr:description><gtr:id>C44996DF-7DD0-444D-A19D-BAB145638424</gtr:id><gtr:outcomeId>b9d3d538b9d3d556-1</gtr:outcomeId><gtr:piContribution>We have established collaboration with Dr Wei Dai at Imperial College London for investigating sparsity based techniques for blind source separation, thanks to the regular meetings and interactive events organised by the MoD University Defence Research Centre in Signal processing.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Samsung</gtr:collaboratingOrganisation><gtr:country>Global</gtr:country><gtr:description>joint development of audio-visual speech enhancement demonstration software for smart phones</gtr:description><gtr:id>23D18BFC-2112-4E3E-9A93-0266874C0BBD</gtr:id><gtr:impact>A software toolkit for lip tracking written in C language
Data collected through Samsung a smart phone S4</gtr:impact><gtr:outcomeId>5446e01fc876a5.47195688-1</gtr:outcomeId><gtr:partnerContribution>Converted the Matlab code of the lip tracking algorithms into C code and tested on the mobile phones.</gtr:partnerContribution><gtr:piContribution>We contributed to extensive tests of the audio-visual speech enhancement for real-life audio visual recordings made by smart phones.</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Seminar presented in Beihang University</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>9D763319-5312-404F-9C46-D5B362C31BFD</gtr:id><gtr:impact>Part of the results of this project has been presented in a seminar in Beihang University, Beijing, China.</gtr:impact><gtr:outcomeId>r-7103098311.5575270bcf98c6</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2011</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Poster Presentation on BBC Audio Research Partnership Launch Meeting</gtr:description><gtr:form>A formal working group, expert panel or dialogue</gtr:form><gtr:id>2CB09C17-B454-491D-8325-1D8CEF43D10D</gtr:id><gtr:impact>We presented the following poster &amp;quot;Audio and Audio-Visual Source Separation for Machine Listening&amp;quot; in the BBC Audio Research Partnership Launch Meeting, in MediaCityUK, Manchester. The poster contains some results from this project.</gtr:impact><gtr:outcomeId>r-9492939197.9174120bc59222</gtr:outcomeId><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:year>2011</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>20000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Impact Acceleration Account</gtr:description><gtr:end>2015-06-02</gtr:end><gtr:fundingOrg>Engineering and Physical Sciences Research Council (EPSRC)</gtr:fundingOrg><gtr:fundingRef>EP/H012842/1</gtr:fundingRef><gtr:id>86B6B707-9BB8-4F4C-8817-6DAB9A67B431</gtr:id><gtr:outcomeId>5446ddca435c19.94629389</gtr:outcomeId><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-12-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>58000</gtr:amountPounds><gtr:country>Global</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Enhancing speech quality using lip tracking</gtr:description><gtr:end>2014-03-02</gtr:end><gtr:fundingOrg>Samsung</gtr:fundingOrg><gtr:id>8C66C959-D003-427E-9F40-2DDDA168F688</gtr:id><gtr:outcomeId>5446dc1109e6a7.80419038</gtr:outcomeId><gtr:sector>Private</gtr:sector><gtr:start>2013-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The project has attracted follow-up funding from Samsung Electronics and EPSRC impact acceleration account to further develop the proposed algorithm (implemented in Matlab) into a demonstration software (in real-time C) that could be potentially deployed for smart phones.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>E262919F-FECB-4FCA-9E7E-3F9B6B106F0D</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>543d54c65699f0.76565416</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>In this research, we have attempted to use both the audio and visual modalities for source separation of target speech from acoustic mixtures acquired in room environments, which contain multiple competing speech interferences and sound sources. The outcomes of this research have offered new insights towards machine perception of auditory scenes within an un-controlled natural environment, in particular, for the cross-modal fusion and interactions within the blind source separation (BSS) framework. The key findings of this research include:



1. Visual information is helpful for mitigating the scale and permutation ambiguities associated with traditional audio BSS algorithms 



Source separation of convolutive speech mixtures is often performed in the time-frequency domain using, e.g. short-time Fourier transform (STFT), where the convolutive BSS problem is converted into multiple instantaneous BSS problems over different frequency channels, and then solved by using e.g. independent component analysis (ICA) algorithms at each frequency bin. However, due to the inherent indeterminacies associated with the classical ICA model, the orders and amplitudes of the source components estimated at these frequency channels may not be consistent, leading to the well-known problems in frequency domain BSS, namely the permutation and scale ambiguities.



We found that the visual information from concurrent video signals can be used as an additional cue for correcting the permutation and scale ambiguities of audio source separation algorithms. To use the visual information, we have developed a two-stage method including off-line training and online separation. We characterise statistically the audio-visual (AV) coherence in the off-line training stage, by mapping the AV data into the feature space, where we have taken the Mel-frequency cepstrum coefficients (MFCCs) as audio features, and the lip width and height as visual features, and then combined them to form an audio-visual feature space. We then model the features based on e.g. Gaussian mixture models (GMM) and evaluate their parameters using an adapted expectation maximisation (AEM) algorithm. In the online separation stage, we have developed a novel iterative sorting scheme based on coherence maximisation and majority voting, in order to correct the permutation ambiguities of the frequency components. To address the scaling ambiguity, we have used a group of scaling parameters, calculated in each Mel-frequency band using the bi-modal coherence and interpolated across the adjacent frequency bands expanded, which are then directly applied to the ICA-separated spectral components in each frequency bin. We have also adopted a robust feature selection scheme to improve the performance of the proposed AV-BSS system for the data corrupted by outliers, such as background noise and room reverberations.



2. Visual information is helpful for detecting voice activity and for separating sources from noisy mixtures



Voice activity, indicating whether the speaker is uttering or remains silent, provides useful information about the concurrent number of speakers present in the auditory scene and therefore informs whether the BSS problem is determined, over-determined or underdetermined (i.e. the number of sources is greater than that of the sensors). Detecting the voice activity of the speakers is an important and also a very challenging problem in robot audition research. The majority of research in voice activity detection (VAD) is conducted in the audio domain, whose performance, however, deteriorates severely in a multi-source and noisy environment.



We found that visual information from the video signals associated with the contemporary audio can be used to improve considerably the audio-domain VAD performance. We have proposed a new visual VAD approach which combines lip-reading with binary classification for determining the activity of speech utterances. More specifically, we have developed a new lip-reading method, which is robust to head rotations and changes of illuminations. In our proposed lip extraction algorithm, greedy active contour models (ACM) are used to drive the landmark points towards the lip contours, where a template matching is used to cope with head rotations, and shape energy constraints are applied to avoid points bending abruptly (which often occurs if the image resolution is very low). Using the lip features obtained in lip-reading, we then form a binary VAD classifier based on the Adaboosting technique, which combines or boosts a set of 'weak' classifiers to obtain a 'strong' classifier with a lower error rate.



We also found that the visual VAD can be used to further improve the performance of the aforementioned AV-BSS algorithm. This is achieved as follows. First, in the off-line training stage, we apply the Adaboost training algorithm to the labelled visual features, which are extracted from the video signal associated with a target speaker. The trained Adaboost model is then used for visual VAD for detecting the silent periods in the target speech, using the accompanied contemporary video. Finally, these periods of the signal are suppressed by the multi-band spectral subtraction algorithm, as a post-processing stage for the proposed AV-BSS algorithm. 





3. Dictionary learning based sparse coding provides an alternative way for audio-visual coherence modelling, offering improved BSS performance (over the feature-based technique) for separating reverberant and noisy speech mixtures acquired in real room environments



We found that both speech signals and lip movements are 'sparse' by nature or can be made sparse in a transform domain, where the term 'sparse' is used to refer to that only few values in the signals (or their transformed coefficients) are non-zeros. Using sparse representations, we could potentially design more effective BSS algorithms for noisy, reverberant, and/or underdetermined speech mixtures, as under such a representation, (i) the noise components or coefficients become less prominent as compared with the signal components, and (ii) the possibility that speech sources overlap with each other is reduced. 



Under the sparse coding framework, we have proposed a novel audio-visual dictionary learning (AVDL) technique for modelling the AV coherence. This new method attempts to code the local temporal-spatial (TS) structures of an AV sequence, resembling the technique of locality-constrained linear coding. We address several challenges associated with AVDL, including, for example, cross-modality differences in size, dimension and sampling rate, as well as the issues on scalability and computational complexity. Our proposed AVDL algorithm follows a commonly employed two-stage coding-learning process, but features with new contributions in both coding and learning stages including, for example, bi-modality balanced and scalable matching criterion, size and dimension adaptive dictionary, a fast search index for efficient coding, and a varying sparsity for different modalities. Each AV atom in our dictionary contains both an audio atom and a visual atom spanning the same temporal length. The audio atom is the magnitude spectrum of an audio segment, which is found to be more robust to convolutive noise as compared with the time-domain representations. The visual atom is composed of several consecutive frames of image patches, focusing on the movement of the whole mouth region. The AVDL algorithm has been applied in the offline training stage of the AV-BSS algorithm, as an alternative to the aforementioned feature-based AEM algorithm.



We have also developed a new time-frequency masking technique using the AVDL, where two parallel mask generation processes are combined to derive an AV mask, which is then used to separate the source from the mixtures. The audio mask can be obtained by using conventional BSS techniques based on ICA, or time-frequency techniques based on various cues, such as spatial, statistical, temporal, or spectral cues, evaluated using the EM algorithm. The visual mask is generated by comparing the reconstructed audio sequence using the AVDL algorithm with the observed (recorded) AV sequence, and it accommodates the information about the reliability and confidence of the likelihood that each time-frequency unit of the mask being occupied by a specific source that is suggested by the audio mask. The visual mask is used to re-weight the audio mask, resulting in the AV mask that is effective in suppressing the adverse effect of noise and room reverberations on the separation results. We have evaluated extensively our AVDL based AV-BSS algorithm on real speech and video data, using the performance metrics such as signal to distortion ratio (SDR), signal to interference ratio (SIR), signal to noise ratio (SNR), perceptual evaluations of speech quality (PESQ), and perceptual evaluation of audio source separation (PEASS). We have observed considerably improved separation performance as compared with the state-of-the-art baselines including both audio-only and audio-visual BSS methods.</gtr:description><gtr:exploitationPathways>The research results of this project could be used by several UK (and/or international) industry sectors, such as defence (target detection and tracking), security (automated crime detection and security surveillance), health-care (assisted living), and creative (human-computer interactions) industries, where the techniques of multi-modal data fusion, multi-channel signal separation and deconvolution, and corrupted sensor signal enhancement are commonly required. This research has the potential to be commercialised by industry sectors, if further developmental activities can be grounded and facilitated by e.g. Knowledge Transfer Partnership (KTP), Knowledge Transfer Accounts (KTA), and/or the Centre for Defence Enterprise.</gtr:exploitationPathways><gtr:id>E926C1F8-9442-409E-8815-E847B97381BB</gtr:id><gtr:outcomeId>r-4428033963.24444774861d0</gtr:outcomeId><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Healthcare,Security and Diplomacy</gtr:sector></gtr:sectors><gtr:url>http://www.see.ed.ac.uk/drupal/sites/default/files/CDE%20UDRC%20Poster%20(O11).pdf</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>We have developed software packages for implementing the proposed multimodal blind source separation systems described in our publications.</gtr:description><gtr:id>7EC31CF4-263D-489A-80D1-F722A7B3F796</gtr:id><gtr:outcomeId>r-5246027545.3012771e6ee2c</gtr:outcomeId><gtr:title>Software packages</gtr:title><gtr:type>Software</gtr:type></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>97DD3A44-526A-4A18-91C3-929F93610C8F</gtr:id><gtr:title>Robust Feature Selection for Scaling Ambiguity Reduction in Audio-Visual Convolutive BSS</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0f9f970334ca48b69c1b73099da1b9d8"><gtr:id>0f9f970334ca48b69c1b73099da1b9d8</gtr:id><gtr:otherNames>Qingju Liu (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_25850177661401ba8a</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5FB8DFFE-9EFA-4193-973F-DCB34993332E</gtr:id><gtr:title>Bimodal coherence based scale ambiguity cancellation for target speech extraction and enhancement</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0f9f970334ca48b69c1b73099da1b9d8"><gtr:id>0f9f970334ca48b69c1b73099da1b9d8</gtr:id><gtr:otherNames>Qingju Liu (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_76136714061401bbde</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>5CC07226-DC53-41F7-AD21-10827E770450</gtr:id><gtr:title>Blind source separation and visual voice activity detection for target speech extraction</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-4577-0887-9</gtr:isbn><gtr:outcomeId>doi_53d058058393c870</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>44517A48-DE21-4D5B-B96C-13F58D9DC67E</gtr:id><gtr:title>Audio-visual convolutive blind source separation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0f9f970334ca48b69c1b73099da1b9d8"><gtr:id>0f9f970334ca48b69c1b73099da1b9d8</gtr:id><gtr:otherNames>Qingju Liu (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:outcomeId>m_90737448711401bb34</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>25A6BB6B-4FB2-4361-8A39-0BF6D7E10B97</gtr:id><gtr:title>A visual voice activity detection method with adaboosting</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/40ffd4c30836a8e1449bb24fe886baa4"><gtr:id>40ffd4c30836a8e1449bb24fe886baa4</gtr:id><gtr:otherNames>Qingju Liu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-1-84919-661-1</gtr:isbn><gtr:outcomeId>doi_53d03103161ebd9e</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>ABD0F95E-B0EC-4F83-B173-E0824CCAC9A0</gtr:id><gtr:title>A multistage approach to blind separation of convolutive speech mixtures</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/6fed7532ed564a631ea2ba91bdf981de"><gtr:id>6fed7532ed564a631ea2ba91bdf981de</gtr:id><gtr:otherNames>Jan T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>doi_55f95e95e5b16b13</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8E64DA8A-D661-46C0-92A0-7870F07B00D0</gtr:id><gtr:title>Reverberant speech separation based on audio-visual dictionary learning and binaural cues</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0182-4</gtr:isbn><gtr:outcomeId>doi_53d05c05c7a64bde</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>A856A37D-5C87-473D-9EC4-172AB384A394</gtr:id><gtr:title>Use of bimodal coherence to resolve the permutation problem in convolutive BSS</gtr:title><gtr:parentPublicationTitle>Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_53d00a00a7bc3e19</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1EC99195-AE18-464D-9E3B-D867D535F237</gtr:id><gtr:title>Sparse coding with adaptive dictionary learning for underdetermined blind speech separation</gtr:title><gtr:parentPublicationTitle>Speech Communication</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/afd9c82d54f841388e52c6a8216d950f"><gtr:id>afd9c82d54f841388e52c6a8216d950f</gtr:id><gtr:otherNames>Xu T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>doi_55f952952da49339</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>92F98558-46C2-455C-8120-9E11A2CAAA10</gtr:id><gtr:title>Source Separation of Convolutive and Noisy Mixtures Using Audio-Visual Dictionary Learning and Probabilistic Time-Frequency Masking</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>543d5694f1f972.04929121</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>E9C3EE16-9CEC-4E42-AC9D-03CC1B3AEB7F</gtr:id><gtr:title>Multimodal Blind Source Separation with a Circular Microphone Array and Robust Beamforming</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/a94eaf08ddb28204f409c7c6b29f88ae"><gtr:id>a94eaf08ddb28204f409c7c6b29f88ae</gtr:id><gtr:otherNames>Syed Mohsen  Naqvi (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>m_90884184551407ca24</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>6C57FFB3-47F2-42FD-AB0F-C68F4B6F2E12</gtr:id><gtr:title>Interference Reduction in Reverberant Speech Separation With Visual Voice Activity Detection</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/ed08539c69d588fe33466bed7e154c1d"><gtr:id>ed08539c69d588fe33466bed7e154c1d</gtr:id><gtr:otherNames>Liu Q</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f952952dadd3ca</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1C2ECAC2-5AF7-4092-A98E-2A964D1691FA</gtr:id><gtr:title>Joint Mixing Vector and Binaural Model Based Stereo Source Separation</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/af463c0336f79ef0dbc212494ac75443"><gtr:id>af463c0336f79ef0dbc212494ac75443</gtr:id><gtr:otherNames>Alinaghi A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>doi_55f9779779f7304c</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H012842/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>