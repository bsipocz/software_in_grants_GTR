<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/8A0FC07A-04CD-4F7A-9095-1D2E6C1D918F"><gtr:id>8A0FC07A-04CD-4F7A-9095-1D2E6C1D918F</gtr:id><gtr:name>University of Hull</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>Cottingham Road</gtr:line1><gtr:line4>Hull</gtr:line4><gtr:line5>North Humberside</gtr:line5><gtr:postCode>HU6 7RX</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/8A0FC07A-04CD-4F7A-9095-1D2E6C1D918F"><gtr:id>8A0FC07A-04CD-4F7A-9095-1D2E6C1D918F</gtr:id><gtr:name>University of Hull</gtr:name><gtr:address><gtr:line1>Cottingham Road</gtr:line1><gtr:line4>Hull</gtr:line4><gtr:line5>North Humberside</gtr:line5><gtr:postCode>HU6 7RX</gtr:postCode><gtr:region>Yorkshire and The Humber</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/D3DB5801-1EDB-4075-AD77-388AE741BCC2"><gtr:id>D3DB5801-1EDB-4075-AD77-388AE741BCC2</gtr:id><gtr:firstName>James</gtr:firstName><gtr:surname>Ward</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/3305199D-1664-4AA6-A750-43AB93D3CFC9"><gtr:id>3305199D-1664-4AA6-A750-43AB93D3CFC9</gtr:id><gtr:firstName>Nick</gtr:firstName><gtr:otherNames>Edward</gtr:otherNames><gtr:surname>Barraclough</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/4F9A90A7-1BC9-4761-A82B-BC98C363B7F1"><gtr:id>4F9A90A7-1BC9-4761-A82B-BC98C363B7F1</gtr:id><gtr:firstName>Tjeerd</gtr:firstName><gtr:surname>Jellema</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=ES%2FI004521%2F1"><gtr:id>97026455-4766-47FC-B1B8-498DB5A36E1B</gtr:id><gtr:title>Investigating biases in the perception of human behaviour using immersive virtual reality</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/I004521/1</gtr:grantReference><gtr:abstractText>Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.</gtr:abstractText><gtr:fund><gtr:end>2011-07-01</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2011-02-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>238082</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Guided police training; improved Reckitt Benckiser product development and marketing</gtr:description><gtr:id>51654C6B-1C0F-4FE4-A35F-25C4566E76A4</gtr:id><gtr:impactTypes><gtr:impactType>Policy &amp; public services</gtr:impactType></gtr:impactTypes><gtr:outcomeId>5447d2581f32a3.95205286</gtr:outcomeId><gtr:sector>Chemicals,Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Objective 1: Does visual adaptation bias our judgment of actions and human behaviour in 'real life' situations? We compared visual adaptation to naturalistically presented 3D action scenes viewed within an immersive virtual reality environment against action adaptation measured using conventional small screen presentations. We found that perception of peoples's actions, and inferences we make about their internal mental states, are biased strongly by action adaptation when viewing naturalistic scenes as for 2D. Perception and interpretation of the behaviour of individuals is dependent, not only on what the individual is doing, but also on the adaptive effect that other individuals within the social environment have on our action coding mechanisms (Keefe et al. in prep a). We have made freely available a database of 2783 2D and 3D action videos generated during the period of this project (Keefe et al. 2014). 
Objective 2: How long and how strong is the effect of adaptation on action and social perception? We found that both action recognition, and our ability to derive someone's expectation of their environment from their behaviour, are already biased by exposure to just one action, builds up rapidly over time and lasts for over 10s (Keefe et al. in prep a). Biases in the perception of emotional body language builds up fast with action exposure, does not transfer from one action to another, and decays at a speed dependent upon the identity of the actors. Identity independent emotional action adaptation decays over 10s, identity dependent emotional action adaptation, however, appears much longer lasting and did not decay over any of the periods we tested (Wincenciak et al. submitted Cognition).
Objective 3: Does visual adaptation bias judgments of the internal belief states of other individuals? Visual adaptation biases our judgments of both other peoples' expectations of their physical environment and judgments of other peoples' internal emotional states. These effects are mediated via visual adaptation to the kinematics of the actions being executed rather than adaptation of higher-order cognitive mechanisms. See also above points.
Objective 4: Does visual adaptation bias our judgment of the trustworthiness of individuals? Visual adaptation of facial trustworthiness has 2 effects. Adaptation biases perception of facial trustworthiness so that faces look less like the adaptor in female observers only (Wincenciak et al. 2013). However, in both female and male observers, adaptation improves discrimination of facial trustworthiness around the adaptor (Keefe et al. 2013).
Objective 5: Does the perception of trustworthiness rely on mechanisms that are sensitive or insensitive to body parts? We examined how perception of emotion (a proxy for trustworthiness; Oosterhof &amp;amp; Todorov, 2008) was reliant on mechanisms sensitive to body parts as pilot testing indicated perception of trustworthiness from actions showed a complex interaction with action type and dynamics. Both emotion and gender relies on body-part dependent coding mechanisms (Keefe et al. in prep b).
 Objective 6: Does visual adaptation bias judgments of human behaviour in security professionals? Two month duration initial Police training has little influence on judgments. Long term experience can improve perception of certain behaviours. Short term adaptation improves discrimination of human emotional behaviour (Barraclough et al. in prep).
Further research: We are submitting a grant to test (using psychophysics and fMRI) the influence of social context (crowds) on the representation of human actions and human behaviour. We are further investigating the benefitial effects of visual adaptation on social perception.</gtr:description><gtr:exploitationPathways>This project was to investigate biases in the performance of action perception mechansims resulting from visual adaptation. This was principally an academic goal and therefore the majority of exploitation is via academic routes. Our aim is to publish the primary research in high impact international peer reviewed journals and publicise our research at national and international conferences. We have published 3 primary research articles to date and a further 6 are planned (3 to be submitted within 3 months). We have delayed publication and are combining multiple studies within single papers to ensure the highest overall academic impact. The research from this project has been presented at conferences to date eleven times, including national (Experimental Psychology Society, Applied Vision Association) and international (Vision Sciences Society, European Society for Cognitive Psychology, European Conference on Visual Percpetion) events. We will continue to publicise research from this grant when published at further conferences, and via press releases and interaction with national and international media.
Throughout the duration of the project we have liased every 6 months with Humberside Police (and recently South Yorkshire Police) to ensure that their expertise can guide the research, and to disseminate research findings to help improve Police recruit training and inform Policing practices.
We have made freely available an online database of 2783 2D and 3D action videos that can be used by others (Keefe et al. 2014). This has attracted considerable interest and we hope it will provide a lasting legacy for the project, and be useful for others interested in studying perception of human action and human behaviour.</gtr:exploitationPathways><gtr:id>BD81F557-0CEF-4525-ACF1-6CD91C02CB3A</gtr:id><gtr:outcomeId>5447d2a5c0b236.56096796</gtr:outcomeId><gtr:sectors><gtr:sector>Security and Diplomacy,Other</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>EABFF891-A906-45BC-8765-6112A9C54F31</gtr:id><gtr:title>Emotional Actions Are Coded via Two Mechanisms: With and without Identity Representation.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4886ec98e83a580ce8da7399c13337c5"><gtr:id>4886ec98e83a580ce8da7399c13337c5</gtr:id><gtr:otherNames>Wincenciak J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>58b93aa3ac6189.71536659</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>8B2D2233-651D-430B-8D24-05E6015EB73F</gtr:id><gtr:title>Adaptation improves face trustworthiness discrimination.</gtr:title><gtr:parentPublicationTitle>Frontiers in psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5c8d7a85ae832c2bfe627549f601404d"><gtr:id>5c8d7a85ae832c2bfe627549f601404d</gtr:id><gtr:otherNames>Keefe BD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>1664-1078</gtr:issn><gtr:outcomeId>doi_55f9449446f140de</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>84487DA8-DCBB-455C-99A0-8CD5D31F52B7</gtr:id><gtr:title>Other peoples' actions interact within our visual system</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b6c7ba4d6c3c8abf941270322e1d862c"><gtr:id>b6c7ba4d6c3c8abf941270322e1d862c</gtr:id><gtr:otherNames>Barraclough N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:outcomeId>56d9c2bbd02be1.30525818</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9240D71C-8AC2-4B91-9B1A-24A26B8FF385</gtr:id><gtr:title>A database of whole-body action videos for the study of action, emotion, and untrustworthiness.</gtr:title><gtr:parentPublicationTitle>Behavior research methods</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5c8d7a85ae832c2bfe627549f601404d"><gtr:id>5c8d7a85ae832c2bfe627549f601404d</gtr:id><gtr:otherNames>Keefe BD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1554-351X</gtr:issn><gtr:outcomeId>doi_55f9449446fa23e9</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>9CD9E14C-E01F-4A79-8285-4258404A267E</gtr:id><gtr:title>Visual adaptation enhances action sound discrimination.</gtr:title><gtr:parentPublicationTitle>Attention, perception &amp; psychophysics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c07a4301680808f3a16eca5d6df5a426"><gtr:id>c07a4301680808f3a16eca5d6df5a426</gtr:id><gtr:otherNames>Barraclough NE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>1943-3921</gtr:issn><gtr:outcomeId>58b93aa3821872.76943528</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>11BBECD0-56C5-4F2F-A376-3662182F7759</gtr:id><gtr:title>Adaptation to facial trustworthiness is different in female and male observers.</gtr:title><gtr:parentPublicationTitle>Vision research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/4886ec98e83a580ce8da7399c13337c5"><gtr:id>4886ec98e83a580ce8da7399c13337c5</gtr:id><gtr:otherNames>Wincenciak J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:issn>0042-6989</gtr:issn><gtr:outcomeId>doi_55f9449446e7322b</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>707CDF0D-DDB2-4D77-9B01-77A8FBF455B7</gtr:id><gtr:title>Action adaptation during natural unfolding social scenes influences action recognition and inferences made about actor beliefs.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/5c8d7a85ae832c2bfe627549f601404d"><gtr:id>5c8d7a85ae832c2bfe627549f601404d</gtr:id><gtr:otherNames>Keefe BD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn><gtr:outcomeId>58b93aa3524770.13152938</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>F613CC48-3CD4-4F02-ABDF-61C82C89004E</gtr:id><gtr:title>A bias-free measure of crossmodal audiovisual action adaptation</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/b6c7ba4d6c3c8abf941270322e1d862c"><gtr:id>b6c7ba4d6c3c8abf941270322e1d862c</gtr:id><gtr:otherNames>Barraclough N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:outcomeId>56d9c2bc14fa95.36907977</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/I004521/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>