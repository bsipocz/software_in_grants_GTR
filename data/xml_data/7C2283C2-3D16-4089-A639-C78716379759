<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.ukri.org/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.ukri.org:80/organisation/3AF70AC4-19B0-4B79-8A06-D63F374FC0E7"><gtr:id>3AF70AC4-19B0-4B79-8A06-D63F374FC0E7</gtr:id><gtr:name>Queen Margaret University Edinburgh</gtr:name><gtr:department>Clinical Audiology Speech &amp;Lang Res Cen</gtr:department><gtr:address><gtr:line1>Queen Margaret University Drive</gtr:line1><gtr:line2>Musselburgh</gtr:line2><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH21 6UU</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.ukri.org:80/organisation/3AF70AC4-19B0-4B79-8A06-D63F374FC0E7"><gtr:id>3AF70AC4-19B0-4B79-8A06-D63F374FC0E7</gtr:id><gtr:name>Queen Margaret University Edinburgh</gtr:name><gtr:address><gtr:line1>Queen Margaret University Drive</gtr:line1><gtr:line2>Musselburgh</gtr:line2><gtr:line4>Edinburgh</gtr:line4><gtr:postCode>EH21 6UU</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.ukri.org:80/person/82872A8F-54F4-4D75-A1F8-6AF48CAF5126"><gtr:id>82872A8F-54F4-4D75-A1F8-6AF48CAF5126</gtr:id><gtr:firstName>James</gtr:firstName><gtr:otherNames>M</gtr:otherNames><gtr:surname>Scobbie</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/69722FE7-16BC-48EB-844A-714D62B4B41C"><gtr:id>69722FE7-16BC-48EB-844A-714D62B4B41C</gtr:id><gtr:firstName>Robin</gtr:firstName><gtr:surname>Lickley</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/A9A1AC7D-2287-424D-8999-350272BB1B67"><gtr:id>A9A1AC7D-2287-424D-8999-350272BB1B67</gtr:id><gtr:firstName>William John</gtr:firstName><gtr:surname>Hardcastle</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.ukri.org:80/person/8B861C29-013E-4E01-AD7F-643E9E413D85"><gtr:id>8B861C29-013E-4E01-AD7F-643E9E413D85</gtr:id><gtr:firstName>Sonja</gtr:firstName><gtr:surname>Schaeffler</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.ukri.org:80/projects?ref=EP%2FE016359%2F1"><gtr:id>7C2283C2-3D16-4089-A639-C78716379759</gtr:id><gtr:title>An Edinburgh Speech Production Facility</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E016359/1</gtr:grantReference><gtr:abstractText>The proposal is for a facility designed to record and analyse the movements of the lips, tongue, and jaw during spoken dialogue. This facility will be the first of its kind in the UK, and will be useful for applications in speech recognition and speech synthesis, as well as for developing theories of the cognitive representations and processes involved in normal and impaired speech production. The first output of the facility will be a database of recorded dialogue that will be useful for researchers interested in the relationships between speech movement and acoustics (important for speech technology applications), as well as in the particular types of pronunciations that speakers use during spontaneous dialogue.</gtr:abstractText><gtr:fund><gtr:end>2010-08-31</gtr:end><gtr:funder url="http://gtr.ukri.org:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-06-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>82388</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>http://www.speech-graphics.com/ Speech Graphics have used EMA data as part of the underpinnings for their lip-synch animation, used in gaming and other creative industries.</gtr:description><gtr:firstYearOfImpact>2013</gtr:firstYearOfImpact><gtr:id>89F1B333-3FE1-40F2-BB5B-7044F846359F</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:outcomeId>56ba03cebe3a26.37019876</gtr:outcomeId><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We created a publicly available corpus of speech recordings that includes synchronized articulatory and acoustic records of speech in dialogue, for free use. Our facility is available for further funded use; we offer calibration, gluing, recording, and data post-processing services. We commissioned the development of data analysis software, available through Articulate Instruments Ltd.

The DoubleTalk articulatory speech corpus includes synchronised audio and articulatory trajectories for 12 speakers of English. The corpus was collected at the Edinburgh Speech Production Facility (ESPF) using two synchronized Carstens AG500 electromagnetic articulometers. The first release of the corpus comprises orthographic transcriptions aligned at phrasal level to EMA and audio data for each of 6 mixed-dialect speaker pairs. It is available from the ESPF online archive (http://espf.ppls.ed.ac.uk/frontend.php/project/espf-doubletalk). A variety of tasks were used to elicit a wide range of speech styles, including monologue (a modified Comma Gets a Cure and spontaneous story-telling), structured spontaneous dialogue (Map Task and Diapix), a wordlist task, a memory-recall task, and a shadowing task. 
To enable wider use of EMA data from the ESPF facility, Articulate Instruments Ltd produced an extra component of the AAA software programme specifically to handle EMA data. This commerically-available software, designed for articulatory speech analysis, therefore provides the opportunity for users familiar with other articulatory data to access and analyse EMA data without having to learn new software. In addition, the company's contribution to the design of the facility enables synchronised collection of EPG (electropalatography) data.</gtr:description><gtr:exploitationPathways>http://www.speech-graphics.com/
Speech Graphics have used EMA data for underpinning their acoustically lip-synched facial animation.</gtr:exploitationPathways><gtr:id>C43946A8-2D9E-4180-8F0A-2189EF231179</gtr:id><gtr:outcomeId>5464d24090d1f6.03377468</gtr:outcomeId><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Other</gtr:sector></gtr:sectors><gtr:url>http://www.lel.ed.ac.uk/projects/ema/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication><gtr:id>D1E4FD7A-CB7D-4C3D-A365-99272CF4023C</gtr:id><gtr:title>Between the regular and the particular in speech and language</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/79db588aed0b2ef0132e702b30b8f47d"><gtr:id>79db588aed0b2ef0132e702b30b8f47d</gtr:id><gtr:otherNames>C Geng</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9783631586860</gtr:isbn><gtr:outcomeId>m_485946838513aa8c88</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>1BFB00E3-B739-4B07-A1B3-479C96E5A0D2</gtr:id><gtr:title>Recording speech articulation in dialogue: Evaluating a synchronized double electromagnetic articulography setup</gtr:title><gtr:parentPublicationTitle>Journal of Phonetics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/24179b49774713b48feec4cbdb6f059c"><gtr:id>24179b49774713b48feec4cbdb6f059c</gtr:id><gtr:otherNames>Geng C</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5433b721e06bb1.17436322</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>42DD0DC6-7183-4F96-AFC0-FB815B87B514</gtr:id><gtr:title>Audible aspects of speech preparation</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6ebb7378417e1f6f06954e49d67b937"><gtr:id>e6ebb7378417e1f6f06954e49d67b937</gtr:id><gtr:otherNames>Scobbie JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>r_9502729589cb1506c2</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>32248A33-E4B2-4751-87AE-F1C5C3A1DC68</gtr:id><gtr:title>The Edinburgh Speech Production Facility DoubleTalk Corpus</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/c1629ffb6608cc3c8323188f6e4f0f5e"><gtr:id>c1629ffb6608cc3c8323188f6e4f0f5e</gtr:id><gtr:otherNames>Scobbie J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:outcomeId>5460966664a022.30441899</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>35387F78-1A64-4DF4-BC46-76B6A38403D2</gtr:id><gtr:title>A common co-ordinate system for mid-sagittal articulatory measurement</gtr:title><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/e6ebb7378417e1f6f06954e49d67b937"><gtr:id>e6ebb7378417e1f6f06954e49d67b937</gtr:id><gtr:otherNames>Scobbie JM</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:outcomeId>5460ee774e9576.65536508</gtr:outcomeId></gtr:publication><gtr:publication><gtr:id>549283F2-C912-4534-A064-9CEB89052E64</gtr:id><gtr:title>Foreign Accent Conversion Through Concatenative Synthesis in the Articulatory Domain</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.ukri.org:80/person/0fbb3ce1131ab01fcfe9342a197b9ff4"><gtr:id>0fbb3ce1131ab01fcfe9342a197b9ff4</gtr:id><gtr:otherNames>Felps D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:outcomeId>doi_55f950950f34cc44</gtr:outcomeId></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E016359/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>CC2B62EB-22CD-45F9-A6D2-0CE29B6D90FD</gtr:id><gtr:percentage>10</gtr:percentage><gtr:text>Cognitive Science Appl. in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>